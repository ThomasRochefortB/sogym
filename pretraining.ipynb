{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SB3 version: 2.2.1\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'       #Disactivate multiprocessing for numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import gymnasium as gym\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "import stable_baselines3\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize, SubprocVecEnv\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList, CheckpointCallback, StopTrainingOnNoModelImprovement\n",
    "\n",
    "from sogym.mmc_optim import run_mmc\n",
    "from sogym.env import sogym\n",
    "from sogym.expert_generation import generate_expert_dataset, generate_mmc_solutions, generate_dataset\n",
    "from sogym.utils import profile_and_analyze,ImageDictExtractor, CustomBoxDense\n",
    "from sogym.callbacks import FigureRecorderCallback, MaxRewardCallback, GradientNormCallback, GradientClippingCallback\n",
    "from sogym.pretraining import pretrain_agent, ExpertDataSet\n",
    "\n",
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import random_split, Dataset\n",
    "from IPython.display import display\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print('SB3 version:', stable_baselines3.__version__)\n",
    "# Let's make the code device agnostic:\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters\n",
    "params = {\n",
    "    'observation_type': 'topopt_game',\n",
    "    'vol_constraint_type': 'hard',\n",
    "    'resolution': 50\n",
    "}\n",
    "\n",
    "# Train environment\n",
    "train_env = sogym(mode='train', check_connectivity=True, **params)\n",
    "# Evaluation environment\n",
    "eval_env = sogym(mode='test', check_connectivity=False, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To generate expert trajectories using the conventional optimization process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = train_env.reset()\n",
    "cfg = {\n",
    "            'optimizer':'hybrid', #optimiser choice\n",
    "            'xInt':0.25, #initial interval of components in x\n",
    "            'yInt':0.25, #initial interval of components in y\n",
    "            'E':1.0, #Young's modulus\n",
    "            'nu':0.3, #Poisson ratio\n",
    "            'h':1, #thickness\n",
    "            'dgt0':5, #significant digit of sens.\n",
    "            'scl':1, #scale factor for obj\n",
    "            'p':6,  #power of super ellipsoid\n",
    "            'lmd':100, #power of KS aggregation   \n",
    "            'maxiter':1000, # maximum number of outer iterations\n",
    "            'alpha':1e-9, # This is the threshold level in the Heaviside function\n",
    "            'epsilon':0.2, #This is the regularization term in the Heaviside function\n",
    "            'maxinnerinit':1, # This is the maximum number of inner iterations for GCMMA\n",
    "            'switch':-0.000002, # This is the switch criteria for the hybrid optimizer\n",
    "            'convergence_threshold':2e-4, #This is the threshold for the relative change in the objective function\n",
    "            'xmin':(0.0, 0.0, 0.0, 0.00, 0.00, -np.pi),\n",
    "            'xmax':(train_env.dx, train_env.dy, 0.7*min(train_env.dx,train_env.dy), 0.05*min(train_env.dx,train_env.dy),0.05*min(train_env.dx,train_env.dy), np.pi)\n",
    "        }\n",
    "\n",
    "#run_mmc(train_env.conditions,train_env.nelx,train_env.nely,train_env.dx,train_env.dy,plotting='contour',verbose=0,cfg=cfg)\n",
    "dataset_folder = \"./dataset/topologies/mmc\"\n",
    "#generate_mmc_solutions(key=0,dataset_folder=\"/home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/mmc\")\n",
    "generate_dataset(dataset_folder= dataset_folder, num_threads=32, num_samples=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To visualize a random expert trajectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sogym.utils import visualize_expert_trajectory\n",
    "import os\n",
    "import random\n",
    "#visualize a random expert trajectory:\n",
    "\n",
    "file_path = os.path.join(dataset_folder, random.choice(os.listdir(dataset_folder)))\n",
    "visualize_expert_trajectory(train_env, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sogym.expert_generation import check_duplicates, copy_unique_files\n",
    "\n",
    "#Check if there are any duplicate Boundary conditions in the expert trajectories:\n",
    "\n",
    "# Adjust the percentage as needed, e.g., 50 for 50%\n",
    "check_duplicates(dataset_folder, percentage=100)\n",
    "\n",
    "# Specify the path to the unique_files.txt file\n",
    "unique_files_file = 'unique_files.txt'\n",
    "# Specify the destination folder for the unique files\n",
    "destination_folder = '/home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/unique_combined'\n",
    "# Copy the unique files to the destination folder\n",
    "copy_unique_files(unique_files_file, destination_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sogym.expert_generation import generate_expert_dataset\n",
    "import pickle\n",
    "\n",
    "# Specify the number of permutations to generate\n",
    "num_permutations = None\n",
    "observation_type = \"topopt_game\"\n",
    "\n",
    "# Specify the environment configuration (optional)\n",
    "env_kwargs = {\n",
    "    'mode': 'train',\n",
    "    'observation_type': observation_type,\n",
    "    'vol_constraint_type': 'hard',\n",
    "    'seed': 42,\n",
    "    'resolution' : 50,\n",
    "    'check_connectivity':True\n",
    "}\n",
    "\n",
    "directory_path = \"/home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/unique_combined\"\n",
    "generate_expert_dataset(directory_path,\n",
    "                        env_kwargs,\n",
    "                        observation_type=observation_type, \n",
    "                        plot_terminated=False,\n",
    "                        num_permutations = num_permutations, \n",
    "                        file_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sogym.pretraining import load_expert_dataset\n",
    "chunk_dir = '/home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/expert/unique_combined_topopt_game_20240515_071024'\n",
    "expert_dataset = load_expert_dataset(chunk_dir, train_env)\n",
    "#print length of expertdataset:\n",
    "print(len(expert_dataset)/8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_policy = \"MlpPolicy\" if observation_type == 'box_dense' else \"MultiInputPolicy\"\n",
    "\n",
    "feature_extractor = ImageDictExtractor if observation_type == 'image' or observation_type == 'topopt_game' else CustomBoxDense\n",
    "\n",
    "# Load the YAML file\n",
    "env=train_env\n",
    "\n",
    "with open(\"algorithms.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Extract the parameters for the desired algorithm\n",
    "algorithm_name = \"PPO\"  # or \"TD3\"\n",
    "algorithm_params = config[algorithm_name]\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=feature_extractor,\n",
    "    net_arch = config['common']['net_arch'],\n",
    "    share_features_extractor = False\n",
    ")\n",
    "\n",
    "# Create the model based on the algorithm name and parameters\n",
    "if algorithm_name == \"SAC\":\n",
    "    model = SAC(env=env,\n",
    "                policy = chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                device=device, \n",
    "                **algorithm_params)\n",
    "\n",
    "elif algorithm_name == \"PPO\":\n",
    "    model = PPO(env=env, \n",
    "                policy = chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                device = device, \n",
    "                **algorithm_params)\n",
    "    \n",
    "    \n",
    "\n",
    "elif algorithm_name == \"TD3\":\n",
    "    # Create the action noise object\n",
    "    n_actions = env.action_space.shape[-1]\n",
    "    action_noise_params = algorithm_params.pop(\"action_noise\")\n",
    "    action_noise = NormalActionNoise(mean=action_noise_params[\"mean\"] * np.ones(n_actions),\n",
    "                                     sigma=action_noise_params[\"sigma\"] * np.ones(n_actions))\n",
    "    model = TD3(env=env,\n",
    "                policy =chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                action_noise=action_noise,\n",
    "                device=device, \n",
    "                **algorithm_params)\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Create the tb_log_name string\n",
    "tb_log_name = f\"{algorithm_name}_{current_datetime}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "total_params = sum(p.numel() for p in model.policy.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "data = {k: v for k, v in observation.items()}\n",
    "# Assuming you have a PyTorch model named 'model' and the input size is (3, 224, 224)\n",
    "summary(model.policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todays_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "pretrain_agent(\n",
    "    model,\n",
    "    expert_dataset,\n",
    "    env,\n",
    "    test_env = eval_env,\n",
    "    batch_size=4096,\n",
    "    epochs=2000,\n",
    "    scheduler_gamma=1.0,\n",
    "    learning_rate= 3e-2,\n",
    "    log_interval=5,\n",
    "    no_cuda=False,\n",
    "    seed=1,\n",
    "    verbose=True,\n",
    "    test_batch_size=1024,\n",
    "    early_stopping_patience=100,\n",
    "    plot_curves=True,\n",
    "    tensorboard_log_dir=\"tb_logs/imitation/PPO_{}\".format(todays_date)\",\n",
    "    checkpoint_dir=\"checkpoints/imitation/PPO_{}\".format(todays_date),\n",
    "    load_checkpoint=None,\n",
    "    comet_ml_api_key=\"No20MKxPKu7vWLOUQCFBRO8mo\",\n",
    "    comet_ml_project_name=\"pretraining_rl\",\n",
    "    comet_ml_experiment_name=\"PPO_{}\".format(todays_date),\n",
    "    eval_freq = 5,\n",
    "    l2_reg_strength=0.001,\n",
    "    max_grad_norm = 10.0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SB3_update",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
