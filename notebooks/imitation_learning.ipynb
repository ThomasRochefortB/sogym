{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sogym.env import sogym\n",
    "train_env = sogym(mode='train',observation_type='box_dense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sogym.expert_generation import generate_dataset\n",
    "generate_dataset(num_threads =2, num_samples=10,dataset_folder = \"/Users/thomasrochefort/Documents/GitHub/so_gym/dataset/topologies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms import bc\n",
    "from imitation.algorithms.adversarial.airl import AIRL\n",
    "from imitation.rewards.reward_nets import BasicShapedRewardNet\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize, SubprocVecEnv\n",
    "from imitation.util import logger as imit_logger\n",
    "\n",
    "# Create the vectorized environment\n",
    "num_cpu = 4 # Number of processes to use\n",
    "venv= make_vec_env(lambda:train_env, n_envs=num_cpu,vec_env_cls=SubprocVecEnv)\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    net_arch=dict(pi=[32,32,32], vf=[32,32,32]),\n",
    ")\n",
    "learner = PPO(\"MlpPolicy\",\n",
    "    env=venv,\n",
    "    n_steps=2048//num_cpu,  \n",
    "    batch_size=2048, \n",
    "    tensorboard_log=\"tb_logs\",\n",
    "\n",
    ")\n",
    "\n",
    "custom_logger = imit_logger.configure(\n",
    "        folder=\"test_logs\",\n",
    "        format_strs=[\"tensorboard\", \"stdout\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=train_env.observation_space,\n",
    "    action_space=train_env.action_space,\n",
    "    demonstrations=trajectories,\n",
    "    rng=rng,\n",
    "    custom_logger =custom_logger,\n",
    "    ent_weight=3e-3,\n",
    "    l2_weight=1e-3,\n",
    "    batch_size=1024,\n",
    "    policy = learner.policy\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_net = BasicShapedRewardNet(\n",
    "    venv.observation_space, venv.action_space,\n",
    "    #reward_hid_sizes=(32,32),\n",
    "    #potential_hid_sizes=(32, 32),\n",
    "    #discount_factor=1.0,\n",
    "    #dropout_prob=0.5\n",
    ")\n",
    "\n",
    "#learner=PPO.load('imit',env=venv,device='cuda')\n",
    "\n",
    "airl_trainer = AIRL(\n",
    "    demonstrations=trajectories,\n",
    "    demo_batch_size=2048,\n",
    "    debug_use_ground_truth=True,\n",
    "    init_tensorboard=True,\n",
    "    log_dir=\"test_logs\",\n",
    "    n_disc_updates_per_round=1,\n",
    "    venv=venv,\n",
    "    gen_algo=learner,\n",
    "    reward_net=reward_net,\n",
    "    custom_logger= custom_logger\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airl_trainer.train(4000000)  # Note: set to 300000 for better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('imit',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=sogym(mode='train',observation_type='box_dense')\n",
    "env = make_vec_env(lambda:env, n_envs=1,vec_env_cls=SubprocVecEnv)\n",
    "model=learner.load('imit')\n",
    "#model.set_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs=env.reset()\n",
    "dones=False\n",
    "\n",
    "while dones== False:\n",
    "    action, _states = model.predict(obs,deterministic=False)\n",
    "    print(action)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "  \n",
    "print(\"Reward:\",rewards[0])\n",
    "fig=env.env_method('plot')[0]\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sogym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
