{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94e6157f-d831-41f7-b27c-f949e2253f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SB3 version: 2.2.1\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'       #Disactivate multiprocessing for numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import gymnasium as gym\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "import stable_baselines3\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize, SubprocVecEnv\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList, CheckpointCallback, StopTrainingOnNoModelImprovement\n",
    "\n",
    "from sogym.env import sogym\n",
    "from sogym.mmc_optim import run_mmc\n",
    "from sogym.env import sogym\n",
    "from sogym.expert_generation import generate_expert_dataset, generate_mmc_solutions, generate_dataset\n",
    "from sogym.utils import profile_and_analyze,ImageDictExtractor, CustomBoxDense\n",
    "from sogym.callbacks import FigureRecorderCallback, MaxRewardCallback, GradientNormCallback, GradientClippingCallback\n",
    "from sogym.pretraining import pretrain_agent, ExpertDataSet\n",
    "\n",
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import random_split, Dataset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print('SB3 version:', stable_baselines3.__version__)\n",
    "# Let's make the code device agnostic:\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02b8cad-4629-49cf-a15e-ebb7c9a5b6c1",
   "metadata": {},
   "source": [
    "---\n",
    "### Environment test and visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a4374a-b735-4474-8427-30b1d55dfe22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's test the environment using the check_env util from SB3:\n",
    "observation_type = 'topopt_game'\n",
    "train_env = sogym(mode='train',observation_type=observation_type,vol_constraint_type='hard',resolution=50,check_connectivity = True)\n",
    "eval_env = sogym(mode='test',observation_type=observation_type,vol_constraint_type='hard',resolution=50,check_connectivity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27032137",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs,info=train_env.reset()\n",
    "dones = False\n",
    "#use deepcopy to save \n",
    "while dones== False:\n",
    "    action = train_env.action_space.sample()\n",
    "    print(action)\n",
    "    obs, rewards, dones,truncated, info = train_env.step(action)\n",
    "    print(rewards)\n",
    "train_env.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edcde2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will check your custom environment and output additional warnings if needed\n",
    "check_env(sogym(mode='train',observation_type='topopt_game'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10de06f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Specify the number of episodes to run\n",
    "num_episodes = 20\n",
    "# Call the profile_and_analyze function\n",
    "result_df = profile_and_analyze(num_episodes, train_env)\n",
    "# Print the resulting DataFrame\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f888890",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = train_env.reset()\n",
    "cfg = {\n",
    "            'optimizer':'mma', #optimiser choice\n",
    "            'xInt':0.25, #initial interval of components in x\n",
    "            'yInt':0.25, #initial interval of components in y\n",
    "            'E':1.0, #Young's modulus\n",
    "            'nu':0.3, #Poisson ratio\n",
    "            'h':1, #thickness\n",
    "            'dgt0':5, #significant digit of sens.\n",
    "            'scl':1, #scale factor for obj\n",
    "            'p':6,  #power of super ellipsoid\n",
    "            'lmd':100, #power of KS aggregation   \n",
    "            'maxiter':500, # maximum number of outer iterations\n",
    "            'alpha':1e-9, # This is the threshold level in the Heaviside function\n",
    "            'epsilon':0.2, #This is the regularization term in the Heaviside function\n",
    "            'maxinnerinit':1, # This is the maximum number of inner iterations for GCMMA\n",
    "            'switch':-0.000002, # This is the switch criteria for the hybrid optimizer\n",
    "            'convergence_threshold':2e-4, #This is the threshold for the relative change in the objective function\n",
    "            'xmin':(0.0, 0.0, 0.0, 0.00, 0.00, -np.pi),\n",
    "            'xmax':(train_env.dx, train_env.dy, 0.7*min(train_env.dx,train_env.dy), 0.05*min(train_env.dx,train_env.dy),0.05*min(train_env.dx,train_env.dy), np.pi)\n",
    "        }\n",
    "\n",
    "#run_mmc(train_env.conditions,train_env.nelx,train_env.nely,train_env.dx,train_env.dy,plotting='contour',verbose=0,cfg=cfg)\n",
    "dataset_folder = \"/home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/mmc\"\n",
    "#generate_mmc_solutions(key=0,dataset_folder=\"/home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/mmc\")\n",
    "generate_dataset(dataset_folder= dataset_folder, num_threads=32, num_samples=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a5035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the training environment on a random problem statement and visualize a 'successful' solution:\n",
    "reward = 0.0\n",
    "while reward==0.0:\n",
    "    obs = train_env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = train_env.action_space.sample()\n",
    "        obs, reward, done,truncated, info = train_env.step(action)\n",
    "        \n",
    "# print(\"Volume: \", train_env.volume)\n",
    "print(\"Reward \",reward)\n",
    "\n",
    "train_env.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a21145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axes for the subplots\n",
    "fig, axes = plt.subplots(nrows=5, ncols=5, figsize=(10, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Initialize the index for the current subplot\n",
    "subplot_index = 0\n",
    "\n",
    "# Let's visualize the training environment on a random problem statement and visualize a 'successful' solution:\n",
    "reward = 0.0\n",
    "while reward == 0.0:\n",
    "    obs = train_env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = train_env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = train_env.step(action)\n",
    "        \n",
    "        # Plot the current observation image\n",
    "        axes[subplot_index].imshow(obs['strain_energy'].T, cmap='gray')\n",
    "        axes[subplot_index].axis('off')\n",
    "        axes[subplot_index].set_title(f\"Timestep {subplot_index+1}\")\n",
    "        \n",
    "        # Increment the subplot index\n",
    "        subplot_index += 1\n",
    "        \n",
    "        # If all subplots are filled, display the plot and reset the index\n",
    "        if subplot_index == len(axes):\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            subplot_index = 0\n",
    "\n",
    "# Print the reward\n",
    "print(\"Reward:\", reward)\n",
    "\n",
    "# Plot the final state of the training environment\n",
    "train_env.plot()\n",
    "\n",
    "# Display any remaining subplots\n",
    "if subplot_index > 0:\n",
    "    for i in range(subplot_index, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1b3f4a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  37%|███▋      | 14902/40602 [09:56<12:11, 35.13file/s]\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Failed to log system metrics: [sys.ram,sys.cpu,sys.load]\n",
      "Processing files: 100%|█████████▉| 40469/40602 [27:22<00:08, 16.31file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify the number of permutations to generate\n",
    "num_permutations = None\n",
    "observation_type = \"topopt_game\"\n",
    "\n",
    "# Specify the environment configuration (optional)\n",
    "env_kwargs = {\n",
    "    'mode': 'train',\n",
    "    'observation_type': observation_type,\n",
    "    'vol_constraint_type': 'hard',\n",
    "    'seed': 42,\n",
    "    'resolution' : 50,\n",
    "    'check_connectivity':True\n",
    "}\n",
    "\n",
    "directory_path = \"/home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/mmc\"\n",
    "expert_observations, expert_actions = generate_expert_dataset(directory_path,env_kwargs, plot_terminated=False,num_permutations = num_permutations, file_fraction=1.0)\n",
    "# Save the dataset\n",
    "import pickle\n",
    "\n",
    "# Save the data using pickle\n",
    "with open('expert_dataset_topopt_noperm.pkl', 'wb') as f:\n",
    "    pickle.dump({'expert_observations': expert_observations, 'expert_actions': expert_actions}, f, protocol=4)\n",
    "print(len(expert_observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ae43524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the data using pickle\n",
    "with open('expert_dataset_topopt_noperm.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "expert_observations = data['expert_observations']\n",
    "expert_actions = data['expert_actions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "baea7b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3917,  0.2451,  0.8290, -1.0000,  0.9998,  1.0000],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming you have the expert_dataset defined\n",
    "expert_dataset = ExpertDataSet(expert_observations, expert_actions, train_env)\n",
    "# Get a random sample from the dataset\n",
    "sample_idx = np.random.randint(len(expert_dataset))\n",
    "sample = expert_dataset[sample_idx]\n",
    "\n",
    "# Extract the observation and reward from the sample\n",
    "observation, action = sample\n",
    "\n",
    "# Subplot with image, strain_energy, and structure_strain_energy observations:\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot image observation\n",
    "axes[0].imshow(observation['image'].T, cmap='gray', origin='lower')\n",
    "axes[0].axis('on')\n",
    "axes[0].set_title(\"Image Observation\")\n",
    "\n",
    "# Plot strain_energy observation\n",
    "axes[1].imshow(observation['structure_strain_energy'].T, origin='lower')\n",
    "axes[1].axis('on')\n",
    "axes[1].set_title(\"Structure Strain Energy Observation\")\n",
    "\n",
    "print(action)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "62c7c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = train_env.reset()\n",
    "\n",
    "#use action and plot the result\n",
    "obs, rewards, dones,truncated, info = train_env.step(np.array(action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4bedb95c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc044157f40>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(obs['image'].T,origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73ad778e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:155: UserWarning: You have specified a mini-batch size of 1024, but because the `RolloutBuffer` is of size `n_steps * n_envs = 64`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 64\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=64 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "chosen_policy = \"MlpPolicy\" if observation_type == 'box_dense' else \"MultiInputPolicy\"\n",
    "\n",
    "feature_extractor = ImageDictExtractor if observation_type == 'image' or observation_type == 'topopt_game' else CustomBoxDense\n",
    "\n",
    "# Load the YAML file\n",
    "env=train_env\n",
    "\n",
    "with open(\"algorithms.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Extract the parameters for the desired algorithm\n",
    "algorithm_name = \"PPO\"  # or \"TD3\"\n",
    "algorithm_params = config[algorithm_name]\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=feature_extractor,\n",
    "    net_arch = config['common']['net_arch'],\n",
    "    share_features_extractor = False\n",
    ")\n",
    "\n",
    "# Create the model based on the algorithm name and parameters\n",
    "if algorithm_name == \"SAC\":\n",
    "    model = SAC(env=env,\n",
    "                policy = chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                device=device, \n",
    "                **algorithm_params)\n",
    "\n",
    "elif algorithm_name == \"PPO\":\n",
    "    model = PPO(env=env, \n",
    "                policy = chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                device = device, \n",
    "                **algorithm_params)\n",
    "\n",
    "elif algorithm_name == \"TD3\":\n",
    "    # Create the action noise object\n",
    "    n_actions = env.action_space.shape[-1]\n",
    "    action_noise_params = algorithm_params.pop(\"action_noise\")\n",
    "    action_noise = NormalActionNoise(mean=action_noise_params[\"mean\"] * np.ones(n_actions),\n",
    "                                     sigma=action_noise_params[\"sigma\"] * np.ones(n_actions))\n",
    "    model = TD3(env=env,\n",
    "                policy =chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                action_noise=action_noise,\n",
    "                device=device, \n",
    "                **algorithm_params)\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Create the tb_log_name string\n",
    "tb_log_name = f\"{algorithm_name}_{current_datetime}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "090eedd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 18,875,917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MultiInputActorCriticPolicy              6\n",
       "├─ImageDictExtractor: 1-1                --\n",
       "│    └─ReLU: 2-1                         --\n",
       "│    └─ModuleDict: 2-2                   --\n",
       "│    │    └─Sequential: 3-1              22,784\n",
       "│    │    └─Sequential: 3-2              93,248\n",
       "│    │    └─Sequential: 3-3              16,768\n",
       "│    │    └─Sequential: 3-4              16,768\n",
       "│    │    └─Sequential: 3-5              93,248\n",
       "│    │    └─Sequential: 3-6              16,768\n",
       "├─ImageDictExtractor: 1-2                (recursive)\n",
       "│    └─ReLU: 2-3                         --\n",
       "│    └─ModuleDict: 2-4                   (recursive)\n",
       "│    │    └─Sequential: 3-7              (recursive)\n",
       "│    │    └─Sequential: 3-8              (recursive)\n",
       "│    │    └─Sequential: 3-9              (recursive)\n",
       "│    │    └─Sequential: 3-10             (recursive)\n",
       "│    │    └─Sequential: 3-11             (recursive)\n",
       "│    │    └─Sequential: 3-12             (recursive)\n",
       "├─ImageDictExtractor: 1-3                --\n",
       "│    └─ReLU: 2-5                         --\n",
       "│    └─ModuleDict: 2-6                   --\n",
       "│    │    └─Sequential: 3-13             22,784\n",
       "│    │    └─Sequential: 3-14             93,248\n",
       "│    │    └─Sequential: 3-15             16,768\n",
       "│    │    └─Sequential: 3-16             16,768\n",
       "│    │    └─Sequential: 3-17             93,248\n",
       "│    │    └─Sequential: 3-18             16,768\n",
       "├─MlpExtractor: 1-4                      --\n",
       "│    └─Sequential: 2-7                   --\n",
       "│    │    └─Linear: 3-19                 8,651,264\n",
       "│    │    └─Tanh: 3-20                   --\n",
       "│    │    └─Linear: 3-21                 262,656\n",
       "│    │    └─Tanh: 3-22                   --\n",
       "│    │    └─Linear: 3-23                 262,656\n",
       "│    │    └─Tanh: 3-24                   --\n",
       "│    └─Sequential: 2-8                   --\n",
       "│    │    └─Linear: 3-25                 8,651,264\n",
       "│    │    └─Tanh: 3-26                   --\n",
       "│    │    └─Linear: 3-27                 262,656\n",
       "│    │    └─Tanh: 3-28                   --\n",
       "│    │    └─Linear: 3-29                 262,656\n",
       "│    │    └─Tanh: 3-30                   --\n",
       "├─Linear: 1-5                            3,078\n",
       "├─Linear: 1-6                            513\n",
       "=================================================================\n",
       "Total params: 18,875,917\n",
       "Trainable params: 18,875,917\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "total_params = sum(p.numel() for p in model.policy.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "data = {k: v for k, v in observation.items()}\n",
    "# Assuming you have a PyTorch model named 'model' and the input size is (3, 224, 224)\n",
    "summary(model.policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0166b0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 14:04:19.907945: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-12 14:04:20.439075: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvrtc.so.11.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/cv2/../../lib64:/local/cuda-11.3/lib64:/local/TensorRT-7.2.2.3/lib\n",
      "2024-04-12 14:04:20.439211: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvrtc.so.11.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/cv2/../../lib64:/local/cuda-11.3/lib64:/local/TensorRT-7.2.2.3/lib\n",
      "2024-04-12 14:04:20.439218: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: tensorboard, torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/thomasrb/pretraining-rl/850d285f52df491798d15d5d19c3df94\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [4096/259852 (2%)]\tLoss: 1.542861\tGrad Norm: 4.901312\tLR: 1.000000\n",
      "Train Epoch: 1 [24576/259852 (9%)]\tLoss: 1.594207\tGrad Norm: 7.205245\tLR: 1.000000\n",
      "Train Epoch: 1 [45056/259852 (17%)]\tLoss: 2.338184\tGrad Norm: 9.999999\tLR: 1.000000\n",
      "Train Epoch: 1 [65536/259852 (25%)]\tLoss: 1.828519\tGrad Norm: 10.000000\tLR: 1.000000\n",
      "Train Epoch: 1 [86016/259852 (33%)]\tLoss: 2.163317\tGrad Norm: 9.999999\tLR: 1.000000\n",
      "Train Epoch: 1 [106496/259852 (41%)]\tLoss: 1.673116\tGrad Norm: 8.843509\tLR: 1.000000\n",
      "Train Epoch: 1 [126976/259852 (48%)]\tLoss: 2.012176\tGrad Norm: 10.000000\tLR: 1.000000\n",
      "Train Epoch: 1 [147456/259852 (56%)]\tLoss: 1.757353\tGrad Norm: 9.999999\tLR: 1.000000\n",
      "Train Epoch: 1 [167936/259852 (64%)]\tLoss: 1.932213\tGrad Norm: 9.999999\tLR: 1.000000\n",
      "Train Epoch: 1 [188416/259852 (72%)]\tLoss: 1.534308\tGrad Norm: 8.438760\tLR: 1.000000\n",
      "Train Epoch: 1 [208896/259852 (80%)]\tLoss: 1.725425\tGrad Norm: 9.999999\tLR: 1.000000\n",
      "Train Epoch: 1 [229376/259852 (88%)]\tLoss: 1.487565\tGrad Norm: 8.014029\tLR: 1.000000\n",
      "Train Epoch: 1 [249856/259852 (95%)]\tLoss: 1.020797\tGrad Norm: 0.643041\tLR: 1.000000\n",
      "Train set: Average loss: 1.7457\n",
      "Test set: Average loss: 1.0952, Average MAE: 0.8327\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 2 [4096/259852 (2%)]\tLoss: 1.091375\tGrad Norm: 2.822068\tLR: 0.980000\n",
      "Train Epoch: 2 [24576/259852 (9%)]\tLoss: 0.953275\tGrad Norm: 0.535190\tLR: 0.980000\n",
      "Train Epoch: 2 [45056/259852 (17%)]\tLoss: 0.954993\tGrad Norm: 0.539534\tLR: 0.980000\n",
      "Train Epoch: 2 [65536/259852 (25%)]\tLoss: 0.928393\tGrad Norm: 0.530788\tLR: 0.980000\n",
      "Train Epoch: 2 [86016/259852 (33%)]\tLoss: 0.913964\tGrad Norm: 0.513940\tLR: 0.980000\n",
      "Train Epoch: 2 [106496/259852 (41%)]\tLoss: 0.886345\tGrad Norm: 0.537436\tLR: 0.980000\n",
      "Train Epoch: 2 [126976/259852 (48%)]\tLoss: 0.866501\tGrad Norm: 0.882208\tLR: 0.980000\n",
      "Train Epoch: 2 [147456/259852 (56%)]\tLoss: 0.865496\tGrad Norm: 0.497652\tLR: 0.980000\n",
      "Train Epoch: 2 [167936/259852 (64%)]\tLoss: 0.830875\tGrad Norm: 0.522434\tLR: 0.980000\n",
      "Train Epoch: 2 [188416/259852 (72%)]\tLoss: 0.813911\tGrad Norm: 0.644962\tLR: 0.980000\n",
      "Train Epoch: 2 [208896/259852 (80%)]\tLoss: 0.788835\tGrad Norm: 0.510702\tLR: 0.980000\n",
      "Train Epoch: 2 [229376/259852 (88%)]\tLoss: 0.790402\tGrad Norm: 0.580749\tLR: 0.980000\n",
      "Train Epoch: 2 [249856/259852 (95%)]\tLoss: 0.771889\tGrad Norm: 0.494615\tLR: 0.980000\n",
      "Train set: Average loss: 0.8716\n",
      "Test set: Average loss: 0.7582, Average MAE: 0.6975\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 3 [4096/259852 (2%)]\tLoss: 0.757951\tGrad Norm: 0.503344\tLR: 0.960400\n",
      "Train Epoch: 3 [24576/259852 (9%)]\tLoss: 0.747244\tGrad Norm: 0.508642\tLR: 0.960400\n",
      "Train Epoch: 3 [45056/259852 (17%)]\tLoss: 0.719735\tGrad Norm: 0.448630\tLR: 0.960400\n",
      "Train Epoch: 3 [65536/259852 (25%)]\tLoss: 0.720994\tGrad Norm: 0.435210\tLR: 0.960400\n",
      "Train Epoch: 3 [86016/259852 (33%)]\tLoss: 0.712766\tGrad Norm: 0.742091\tLR: 0.960400\n",
      "Train Epoch: 3 [106496/259852 (41%)]\tLoss: 0.805757\tGrad Norm: 2.421806\tLR: 0.960400\n",
      "Train Epoch: 3 [126976/259852 (48%)]\tLoss: 3.433575\tGrad Norm: 9.999999\tLR: 0.960400\n",
      "Train Epoch: 3 [147456/259852 (56%)]\tLoss: 2.396605\tGrad Norm: 9.999998\tLR: 0.960400\n",
      "Train Epoch: 3 [167936/259852 (64%)]\tLoss: 2.612041\tGrad Norm: 9.999999\tLR: 0.960400\n",
      "Train Epoch: 3 [188416/259852 (72%)]\tLoss: 2.527591\tGrad Norm: 10.000000\tLR: 0.960400\n",
      "Train Epoch: 3 [208896/259852 (80%)]\tLoss: 2.700196\tGrad Norm: 9.999999\tLR: 0.960400\n",
      "Train Epoch: 3 [229376/259852 (88%)]\tLoss: 2.452719\tGrad Norm: 9.999998\tLR: 0.960400\n",
      "Train Epoch: 3 [249856/259852 (95%)]\tLoss: 2.496018\tGrad Norm: 9.999999\tLR: 0.960400\n",
      "Train set: Average loss: 1.7646\n",
      "Test set: Average loss: 2.3911, Average MAE: 1.3161\n",
      "Train Epoch: 4 [4096/259852 (2%)]\tLoss: 2.412590\tGrad Norm: 9.999998\tLR: 0.941192\n",
      "Train Epoch: 4 [24576/259852 (9%)]\tLoss: 1.979895\tGrad Norm: 9.999999\tLR: 0.941192\n",
      "Train Epoch: 4 [45056/259852 (17%)]\tLoss: 2.109583\tGrad Norm: 9.999998\tLR: 0.941192\n",
      "Train Epoch: 4 [65536/259852 (25%)]\tLoss: 2.143910\tGrad Norm: 10.000000\tLR: 0.941192\n",
      "Train Epoch: 4 [86016/259852 (33%)]\tLoss: 2.143283\tGrad Norm: 9.999999\tLR: 0.941192\n",
      "Train Epoch: 4 [106496/259852 (41%)]\tLoss: 1.695788\tGrad Norm: 10.000000\tLR: 0.941192\n",
      "Train Epoch: 4 [126976/259852 (48%)]\tLoss: 2.491054\tGrad Norm: 9.999999\tLR: 0.941192\n",
      "Train Epoch: 4 [147456/259852 (56%)]\tLoss: 1.758034\tGrad Norm: 9.999998\tLR: 0.941192\n",
      "Train Epoch: 4 [167936/259852 (64%)]\tLoss: 2.192203\tGrad Norm: 9.999999\tLR: 0.941192\n",
      "Train Epoch: 4 [188416/259852 (72%)]\tLoss: 1.411267\tGrad Norm: 9.999999\tLR: 0.941192\n",
      "Train Epoch: 4 [208896/259852 (80%)]\tLoss: 2.584059\tGrad Norm: 9.999998\tLR: 0.941192\n",
      "Train Epoch: 4 [229376/259852 (88%)]\tLoss: 1.453334\tGrad Norm: 9.999998\tLR: 0.941192\n",
      "Train Epoch: 4 [249856/259852 (95%)]\tLoss: 2.216061\tGrad Norm: 9.999999\tLR: 0.941192\n",
      "Train set: Average loss: 2.0355\n",
      "Test set: Average loss: 2.5491, Average MAE: 1.4227\n",
      "Train Epoch: 5 [4096/259852 (2%)]\tLoss: 2.553369\tGrad Norm: 10.000000\tLR: 0.922368\n",
      "Train Epoch: 5 [24576/259852 (9%)]\tLoss: 1.248618\tGrad Norm: 10.000000\tLR: 0.922368\n",
      "Train Epoch: 5 [45056/259852 (17%)]\tLoss: 2.518785\tGrad Norm: 9.999999\tLR: 0.922368\n",
      "Train Epoch: 5 [65536/259852 (25%)]\tLoss: 1.234818\tGrad Norm: 9.999999\tLR: 0.922368\n",
      "Train Epoch: 5 [86016/259852 (33%)]\tLoss: 2.028512\tGrad Norm: 10.000000\tLR: 0.922368\n",
      "Train Epoch: 5 [106496/259852 (41%)]\tLoss: 1.505945\tGrad Norm: 9.999998\tLR: 0.922368\n",
      "Train Epoch: 5 [126976/259852 (48%)]\tLoss: 1.667836\tGrad Norm: 10.000000\tLR: 0.922368\n",
      "Train Epoch: 5 [147456/259852 (56%)]\tLoss: 1.666124\tGrad Norm: 9.999999\tLR: 0.922368\n",
      "Train Epoch: 5 [167936/259852 (64%)]\tLoss: 1.725316\tGrad Norm: 9.999999\tLR: 0.922368\n",
      "Train Epoch: 5 [188416/259852 (72%)]\tLoss: 1.460198\tGrad Norm: 9.999999\tLR: 0.922368\n",
      "Train Epoch: 5 [208896/259852 (80%)]\tLoss: 1.592366\tGrad Norm: 10.000000\tLR: 0.922368\n",
      "Train Epoch: 5 [229376/259852 (88%)]\tLoss: 1.399441\tGrad Norm: 9.999999\tLR: 0.922368\n",
      "Train Epoch: 5 [249856/259852 (95%)]\tLoss: 1.614461\tGrad Norm: 9.999999\tLR: 0.922368\n",
      "Train set: Average loss: 1.6152\n",
      "Test set: Average loss: 1.4992, Average MAE: 1.0387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Mean reward = 0.044 +/- 0.001\n",
      "Train Epoch: 6 [4096/259852 (2%)]\tLoss: 1.499727\tGrad Norm: 10.000001\tLR: 0.903921\n",
      "Train Epoch: 6 [24576/259852 (9%)]\tLoss: 1.274030\tGrad Norm: 9.999998\tLR: 0.903921\n",
      "Train Epoch: 6 [45056/259852 (17%)]\tLoss: 1.387603\tGrad Norm: 9.999999\tLR: 0.903921\n",
      "Train Epoch: 6 [65536/259852 (25%)]\tLoss: 1.361368\tGrad Norm: 9.999999\tLR: 0.903921\n",
      "Train Epoch: 6 [86016/259852 (33%)]\tLoss: 1.205142\tGrad Norm: 9.999999\tLR: 0.903921\n",
      "Train Epoch: 6 [106496/259852 (41%)]\tLoss: 1.556188\tGrad Norm: 9.999998\tLR: 0.903921\n",
      "Train Epoch: 6 [126976/259852 (48%)]\tLoss: 1.142905\tGrad Norm: 9.999998\tLR: 0.903921\n",
      "Train Epoch: 6 [147456/259852 (56%)]\tLoss: 1.232325\tGrad Norm: 10.000000\tLR: 0.903921\n",
      "Train Epoch: 6 [167936/259852 (64%)]\tLoss: 1.120810\tGrad Norm: 9.956255\tLR: 0.903921\n",
      "Train Epoch: 6 [188416/259852 (72%)]\tLoss: 1.144179\tGrad Norm: 10.000000\tLR: 0.903921\n",
      "Train Epoch: 6 [208896/259852 (80%)]\tLoss: 1.210618\tGrad Norm: 9.999999\tLR: 0.903921\n",
      "Train Epoch: 6 [229376/259852 (88%)]\tLoss: 1.152442\tGrad Norm: 9.999997\tLR: 0.903921\n",
      "Train Epoch: 6 [249856/259852 (95%)]\tLoss: 1.047059\tGrad Norm: 9.412127\tLR: 0.903921\n",
      "Train set: Average loss: 1.2583\n",
      "Test set: Average loss: 1.0604, Average MAE: 0.8297\n",
      "Train Epoch: 7 [4096/259852 (2%)]\tLoss: 1.060734\tGrad Norm: 9.490155\tLR: 0.885842\n",
      "Train Epoch: 7 [24576/259852 (9%)]\tLoss: 0.858655\tGrad Norm: 7.147196\tLR: 0.885842\n",
      "Train Epoch: 7 [45056/259852 (17%)]\tLoss: 0.451799\tGrad Norm: 1.287606\tLR: 0.885842\n",
      "Train Epoch: 7 [65536/259852 (25%)]\tLoss: 0.425855\tGrad Norm: 0.112107\tLR: 0.885842\n",
      "Train Epoch: 7 [86016/259852 (33%)]\tLoss: 0.426488\tGrad Norm: 0.137750\tLR: 0.885842\n",
      "Train Epoch: 7 [106496/259852 (41%)]\tLoss: 0.415453\tGrad Norm: 0.143143\tLR: 0.885842\n",
      "Train Epoch: 7 [126976/259852 (48%)]\tLoss: 0.419032\tGrad Norm: 0.132640\tLR: 0.885842\n",
      "Train Epoch: 7 [147456/259852 (56%)]\tLoss: 0.415247\tGrad Norm: 0.182406\tLR: 0.885842\n",
      "Train Epoch: 7 [167936/259852 (64%)]\tLoss: 0.402611\tGrad Norm: 0.285855\tLR: 0.885842\n",
      "Train Epoch: 7 [188416/259852 (72%)]\tLoss: 0.407756\tGrad Norm: 0.200602\tLR: 0.885842\n",
      "Train Epoch: 7 [208896/259852 (80%)]\tLoss: 0.397525\tGrad Norm: 0.289373\tLR: 0.885842\n",
      "Train Epoch: 7 [229376/259852 (88%)]\tLoss: 0.409059\tGrad Norm: 0.561586\tLR: 0.885842\n",
      "Train Epoch: 7 [249856/259852 (95%)]\tLoss: 0.411861\tGrad Norm: 0.379856\tLR: 0.885842\n",
      "Train set: Average loss: 0.4864\n",
      "Test set: Average loss: 0.4003, Average MAE: 0.5131\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 8 [4096/259852 (2%)]\tLoss: 0.398332\tGrad Norm: 0.266136\tLR: 0.868126\n",
      "Train Epoch: 8 [24576/259852 (9%)]\tLoss: 0.398810\tGrad Norm: 0.195930\tLR: 0.868126\n",
      "Train Epoch: 8 [45056/259852 (17%)]\tLoss: 0.398581\tGrad Norm: 0.189086\tLR: 0.868126\n",
      "Train Epoch: 8 [65536/259852 (25%)]\tLoss: 0.401947\tGrad Norm: 0.346847\tLR: 0.868126\n",
      "Train Epoch: 8 [86016/259852 (33%)]\tLoss: 0.395645\tGrad Norm: 0.239225\tLR: 0.868126\n",
      "Train Epoch: 8 [106496/259852 (41%)]\tLoss: 0.394305\tGrad Norm: 0.174943\tLR: 0.868126\n",
      "Train Epoch: 8 [126976/259852 (48%)]\tLoss: 0.391503\tGrad Norm: 0.244160\tLR: 0.868126\n",
      "Train Epoch: 8 [147456/259852 (56%)]\tLoss: 0.388513\tGrad Norm: 0.239006\tLR: 0.868126\n",
      "Train Epoch: 8 [167936/259852 (64%)]\tLoss: 0.386954\tGrad Norm: 0.219953\tLR: 0.868126\n",
      "Train Epoch: 8 [188416/259852 (72%)]\tLoss: 0.384063\tGrad Norm: 0.146352\tLR: 0.868126\n",
      "Train Epoch: 8 [208896/259852 (80%)]\tLoss: 0.376708\tGrad Norm: 0.185037\tLR: 0.868126\n",
      "Train Epoch: 8 [229376/259852 (88%)]\tLoss: 0.377015\tGrad Norm: 0.192959\tLR: 0.868126\n",
      "Train Epoch: 8 [249856/259852 (95%)]\tLoss: 0.381987\tGrad Norm: 0.689542\tLR: 0.868126\n",
      "Train set: Average loss: 0.3897\n",
      "Test set: Average loss: 0.3942, Average MAE: 0.5109\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 9 [4096/259852 (2%)]\tLoss: 0.397743\tGrad Norm: 0.783238\tLR: 0.850763\n",
      "Train Epoch: 9 [24576/259852 (9%)]\tLoss: 0.412437\tGrad Norm: 1.120918\tLR: 0.850763\n",
      "Train Epoch: 9 [45056/259852 (17%)]\tLoss: 0.920136\tGrad Norm: 8.088186\tLR: 0.850763\n",
      "Train Epoch: 9 [65536/259852 (25%)]\tLoss: 2.565435\tGrad Norm: 9.999999\tLR: 0.850763\n",
      "Train Epoch: 9 [86016/259852 (33%)]\tLoss: 2.511753\tGrad Norm: 9.999999\tLR: 0.850763\n",
      "Train Epoch: 9 [106496/259852 (41%)]\tLoss: 2.528153\tGrad Norm: 9.999998\tLR: 0.850763\n",
      "Train Epoch: 9 [126976/259852 (48%)]\tLoss: 2.587727\tGrad Norm: 10.000000\tLR: 0.850763\n",
      "Train Epoch: 9 [147456/259852 (56%)]\tLoss: 2.164510\tGrad Norm: 10.000000\tLR: 0.850763\n",
      "Train Epoch: 9 [167936/259852 (64%)]\tLoss: 2.260152\tGrad Norm: 9.999997\tLR: 0.850763\n",
      "Train Epoch: 9 [188416/259852 (72%)]\tLoss: 2.194198\tGrad Norm: 9.999998\tLR: 0.850763\n",
      "Train Epoch: 9 [208896/259852 (80%)]\tLoss: 2.424753\tGrad Norm: 9.999997\tLR: 0.850763\n",
      "Train Epoch: 9 [229376/259852 (88%)]\tLoss: 1.916507\tGrad Norm: 10.000000\tLR: 0.850763\n",
      "Train Epoch: 9 [249856/259852 (95%)]\tLoss: 2.510976\tGrad Norm: 9.999999\tLR: 0.850763\n",
      "Train set: Average loss: 2.0546\n",
      "Test set: Average loss: 2.0448, Average MAE: 1.2805\n",
      "Train Epoch: 10 [4096/259852 (2%)]\tLoss: 2.065165\tGrad Norm: 9.999998\tLR: 0.833748\n",
      "Train Epoch: 10 [24576/259852 (9%)]\tLoss: 2.088850\tGrad Norm: 9.999999\tLR: 0.833748\n",
      "Train Epoch: 10 [45056/259852 (17%)]\tLoss: 1.781094\tGrad Norm: 10.000000\tLR: 0.833748\n",
      "Train Epoch: 10 [65536/259852 (25%)]\tLoss: 1.790758\tGrad Norm: 9.999999\tLR: 0.833748\n",
      "Train Epoch: 10 [86016/259852 (33%)]\tLoss: 1.891586\tGrad Norm: 9.999999\tLR: 0.833748\n",
      "Train Epoch: 10 [106496/259852 (41%)]\tLoss: 1.712549\tGrad Norm: 10.000000\tLR: 0.833748\n",
      "Train Epoch: 10 [126976/259852 (48%)]\tLoss: 1.716285\tGrad Norm: 9.999999\tLR: 0.833748\n",
      "Train Epoch: 10 [147456/259852 (56%)]\tLoss: 1.664957\tGrad Norm: 10.000000\tLR: 0.833748\n",
      "Train Epoch: 10 [167936/259852 (64%)]\tLoss: 1.853217\tGrad Norm: 9.999998\tLR: 0.833748\n",
      "Train Epoch: 10 [188416/259852 (72%)]\tLoss: 1.335229\tGrad Norm: 9.999999\tLR: 0.833748\n",
      "Train Epoch: 10 [208896/259852 (80%)]\tLoss: 2.067017\tGrad Norm: 10.000000\tLR: 0.833748\n",
      "Train Epoch: 10 [229376/259852 (88%)]\tLoss: 1.348469\tGrad Norm: 10.000000\tLR: 0.833748\n",
      "Train Epoch: 10 [249856/259852 (95%)]\tLoss: 2.113257\tGrad Norm: 9.999999\tLR: 0.833748\n",
      "Train set: Average loss: 1.7598\n",
      "Test set: Average loss: 1.7657, Average MAE: 1.1843\n",
      "Epoch 10: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 11 [4096/259852 (2%)]\tLoss: 1.756633\tGrad Norm: 10.000000\tLR: 0.817073\n",
      "Train Epoch: 11 [24576/259852 (9%)]\tLoss: 1.166168\tGrad Norm: 9.999999\tLR: 0.817073\n",
      "Train Epoch: 11 [45056/259852 (17%)]\tLoss: 1.945989\tGrad Norm: 9.999999\tLR: 0.817073\n",
      "Train Epoch: 11 [65536/259852 (25%)]\tLoss: 1.106536\tGrad Norm: 9.999999\tLR: 0.817073\n",
      "Train Epoch: 11 [86016/259852 (33%)]\tLoss: 1.761483\tGrad Norm: 9.999999\tLR: 0.817073\n",
      "Train Epoch: 11 [106496/259852 (41%)]\tLoss: 1.022683\tGrad Norm: 9.999998\tLR: 0.817073\n",
      "Train Epoch: 11 [126976/259852 (48%)]\tLoss: 1.787725\tGrad Norm: 9.999998\tLR: 0.817073\n",
      "Train Epoch: 11 [147456/259852 (56%)]\tLoss: 1.132767\tGrad Norm: 9.999998\tLR: 0.817073\n",
      "Train Epoch: 11 [167936/259852 (64%)]\tLoss: 1.615224\tGrad Norm: 10.000000\tLR: 0.817073\n",
      "Train Epoch: 11 [188416/259852 (72%)]\tLoss: 0.863783\tGrad Norm: 8.347697\tLR: 0.817073\n",
      "Train Epoch: 11 [208896/259852 (80%)]\tLoss: 1.329918\tGrad Norm: 9.999999\tLR: 0.817073\n",
      "Train Epoch: 11 [229376/259852 (88%)]\tLoss: 1.038191\tGrad Norm: 9.999999\tLR: 0.817073\n",
      "Train Epoch: 11 [249856/259852 (95%)]\tLoss: 1.464711\tGrad Norm: 9.999999\tLR: 0.817073\n",
      "Train set: Average loss: 1.3249\n",
      "Test set: Average loss: 1.2252, Average MAE: 0.9759\n",
      "Train Epoch: 12 [4096/259852 (2%)]\tLoss: 1.223114\tGrad Norm: 9.999999\tLR: 0.800731\n",
      "Train Epoch: 12 [24576/259852 (9%)]\tLoss: 0.931634\tGrad Norm: 9.011118\tLR: 0.800731\n",
      "Train Epoch: 12 [45056/259852 (17%)]\tLoss: 1.007600\tGrad Norm: 9.494106\tLR: 0.800731\n",
      "Train Epoch: 12 [65536/259852 (25%)]\tLoss: 0.629306\tGrad Norm: 5.586034\tLR: 0.800731\n",
      "Train Epoch: 12 [86016/259852 (33%)]\tLoss: 0.482738\tGrad Norm: 2.779874\tLR: 0.800731\n",
      "Train Epoch: 12 [106496/259852 (41%)]\tLoss: 0.340287\tGrad Norm: 0.055420\tLR: 0.800731\n",
      "Train Epoch: 12 [126976/259852 (48%)]\tLoss: 0.342920\tGrad Norm: 0.060753\tLR: 0.800731\n",
      "Train Epoch: 12 [147456/259852 (56%)]\tLoss: 0.338192\tGrad Norm: 0.099363\tLR: 0.800731\n",
      "Train Epoch: 12 [167936/259852 (64%)]\tLoss: 0.339003\tGrad Norm: 0.114029\tLR: 0.800731\n",
      "Train Epoch: 12 [188416/259852 (72%)]\tLoss: 0.332090\tGrad Norm: 0.152071\tLR: 0.800731\n",
      "Train Epoch: 12 [208896/259852 (80%)]\tLoss: 0.339475\tGrad Norm: 0.121363\tLR: 0.800731\n",
      "Train Epoch: 12 [229376/259852 (88%)]\tLoss: 0.338647\tGrad Norm: 0.116521\tLR: 0.800731\n",
      "Train Epoch: 12 [249856/259852 (95%)]\tLoss: 0.337307\tGrad Norm: 0.111866\tLR: 0.800731\n",
      "Train set: Average loss: 0.5088\n",
      "Test set: Average loss: 0.3379, Average MAE: 0.4779\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 13 [4096/259852 (2%)]\tLoss: 0.340347\tGrad Norm: 0.215665\tLR: 0.784717\n",
      "Train Epoch: 13 [24576/259852 (9%)]\tLoss: 0.337140\tGrad Norm: 0.292267\tLR: 0.784717\n",
      "Train Epoch: 13 [45056/259852 (17%)]\tLoss: 0.333343\tGrad Norm: 0.337247\tLR: 0.784717\n",
      "Train Epoch: 13 [65536/259852 (25%)]\tLoss: 0.337237\tGrad Norm: 0.167414\tLR: 0.784717\n",
      "Train Epoch: 13 [86016/259852 (33%)]\tLoss: 0.341044\tGrad Norm: 0.259993\tLR: 0.784717\n",
      "Train Epoch: 13 [106496/259852 (41%)]\tLoss: 0.338190\tGrad Norm: 0.198169\tLR: 0.784717\n",
      "Train Epoch: 13 [126976/259852 (48%)]\tLoss: 0.334096\tGrad Norm: 0.130717\tLR: 0.784717\n",
      "Train Epoch: 13 [147456/259852 (56%)]\tLoss: 0.343230\tGrad Norm: 0.497835\tLR: 0.784717\n",
      "Train Epoch: 13 [167936/259852 (64%)]\tLoss: 0.336488\tGrad Norm: 0.131838\tLR: 0.784717\n",
      "Train Epoch: 13 [188416/259852 (72%)]\tLoss: 0.339475\tGrad Norm: 0.079700\tLR: 0.784717\n",
      "Train Epoch: 13 [208896/259852 (80%)]\tLoss: 0.330895\tGrad Norm: 0.074995\tLR: 0.784717\n",
      "Train Epoch: 13 [229376/259852 (88%)]\tLoss: 0.335517\tGrad Norm: 0.276581\tLR: 0.784717\n",
      "Train Epoch: 13 [249856/259852 (95%)]\tLoss: 0.334300\tGrad Norm: 0.068207\tLR: 0.784717\n",
      "Train set: Average loss: 0.3365\n",
      "Test set: Average loss: 0.3318, Average MAE: 0.4770\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 14 [4096/259852 (2%)]\tLoss: 0.334279\tGrad Norm: 0.145815\tLR: 0.769022\n",
      "Train Epoch: 14 [24576/259852 (9%)]\tLoss: 0.334593\tGrad Norm: 0.044130\tLR: 0.769022\n",
      "Train Epoch: 14 [45056/259852 (17%)]\tLoss: 0.334025\tGrad Norm: 0.171188\tLR: 0.769022\n",
      "Train Epoch: 14 [65536/259852 (25%)]\tLoss: 0.331876\tGrad Norm: 0.138303\tLR: 0.769022\n",
      "Train Epoch: 14 [86016/259852 (33%)]\tLoss: 0.336876\tGrad Norm: 0.111803\tLR: 0.769022\n",
      "Train Epoch: 14 [106496/259852 (41%)]\tLoss: 0.331271\tGrad Norm: 0.114919\tLR: 0.769022\n",
      "Train Epoch: 14 [126976/259852 (48%)]\tLoss: 0.330668\tGrad Norm: 0.070274\tLR: 0.769022\n",
      "Train Epoch: 14 [147456/259852 (56%)]\tLoss: 0.333136\tGrad Norm: 0.104729\tLR: 0.769022\n",
      "Train Epoch: 14 [167936/259852 (64%)]\tLoss: 0.331940\tGrad Norm: 0.066446\tLR: 0.769022\n",
      "Train Epoch: 14 [188416/259852 (72%)]\tLoss: 0.333356\tGrad Norm: 0.134923\tLR: 0.769022\n",
      "Train Epoch: 14 [208896/259852 (80%)]\tLoss: 0.332284\tGrad Norm: 0.105026\tLR: 0.769022\n",
      "Train Epoch: 14 [229376/259852 (88%)]\tLoss: 0.333964\tGrad Norm: 0.088852\tLR: 0.769022\n",
      "Train Epoch: 14 [249856/259852 (95%)]\tLoss: 0.325910\tGrad Norm: 0.048030\tLR: 0.769022\n",
      "Train set: Average loss: 0.3322\n",
      "Test set: Average loss: 0.3295, Average MAE: 0.4728\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 15 [4096/259852 (2%)]\tLoss: 0.327743\tGrad Norm: 0.048303\tLR: 0.753642\n",
      "Train Epoch: 15 [24576/259852 (9%)]\tLoss: 0.333680\tGrad Norm: 0.096315\tLR: 0.753642\n",
      "Train Epoch: 15 [45056/259852 (17%)]\tLoss: 0.339081\tGrad Norm: 0.303428\tLR: 0.753642\n",
      "Train Epoch: 15 [65536/259852 (25%)]\tLoss: 0.335618\tGrad Norm: 0.107306\tLR: 0.753642\n",
      "Train Epoch: 15 [86016/259852 (33%)]\tLoss: 0.327992\tGrad Norm: 0.092817\tLR: 0.753642\n",
      "Train Epoch: 15 [106496/259852 (41%)]\tLoss: 0.333052\tGrad Norm: 0.102796\tLR: 0.753642\n",
      "Train Epoch: 15 [126976/259852 (48%)]\tLoss: 0.326804\tGrad Norm: 0.369957\tLR: 0.753642\n",
      "Train Epoch: 15 [147456/259852 (56%)]\tLoss: 0.333513\tGrad Norm: 0.112898\tLR: 0.753642\n",
      "Train Epoch: 15 [167936/259852 (64%)]\tLoss: 0.332894\tGrad Norm: 0.037927\tLR: 0.753642\n",
      "Train Epoch: 15 [188416/259852 (72%)]\tLoss: 0.330245\tGrad Norm: 0.123260\tLR: 0.753642\n",
      "Train Epoch: 15 [208896/259852 (80%)]\tLoss: 0.350809\tGrad Norm: 0.783090\tLR: 0.753642\n",
      "Train Epoch: 15 [229376/259852 (88%)]\tLoss: 0.338023\tGrad Norm: 0.484920\tLR: 0.753642\n",
      "Train Epoch: 15 [249856/259852 (95%)]\tLoss: 0.339528\tGrad Norm: 0.630422\tLR: 0.753642\n",
      "Train set: Average loss: 0.3350\n",
      "Test set: Average loss: 0.3256, Average MAE: 0.4698\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Epoch 15: Mean reward = 0.043 +/- 0.001\n",
      "Train Epoch: 16 [4096/259852 (2%)]\tLoss: 0.329823\tGrad Norm: 0.123569\tLR: 0.738569\n",
      "Train Epoch: 16 [24576/259852 (9%)]\tLoss: 0.339968\tGrad Norm: 0.699662\tLR: 0.738569\n",
      "Train Epoch: 16 [45056/259852 (17%)]\tLoss: 0.324785\tGrad Norm: 0.081868\tLR: 0.738569\n",
      "Train Epoch: 16 [65536/259852 (25%)]\tLoss: 0.337643\tGrad Norm: 0.726495\tLR: 0.738569\n",
      "Train Epoch: 16 [86016/259852 (33%)]\tLoss: 0.322760\tGrad Norm: 0.116911\tLR: 0.738569\n",
      "Train Epoch: 16 [106496/259852 (41%)]\tLoss: 0.322752\tGrad Norm: 0.096469\tLR: 0.738569\n",
      "Train Epoch: 16 [126976/259852 (48%)]\tLoss: 0.335066\tGrad Norm: 0.500655\tLR: 0.738569\n",
      "Train Epoch: 16 [147456/259852 (56%)]\tLoss: 0.312061\tGrad Norm: 0.178221\tLR: 0.738569\n",
      "Train Epoch: 16 [167936/259852 (64%)]\tLoss: 0.328295\tGrad Norm: 0.257800\tLR: 0.738569\n",
      "Train Epoch: 16 [188416/259852 (72%)]\tLoss: 0.311694\tGrad Norm: 0.196234\tLR: 0.738569\n",
      "Train Epoch: 16 [208896/259852 (80%)]\tLoss: 0.324719\tGrad Norm: 0.306850\tLR: 0.738569\n",
      "Train Epoch: 16 [229376/259852 (88%)]\tLoss: 0.321646\tGrad Norm: 0.525054\tLR: 0.738569\n",
      "Train Epoch: 16 [249856/259852 (95%)]\tLoss: 0.315503\tGrad Norm: 0.130277\tLR: 0.738569\n",
      "Train set: Average loss: 0.3276\n",
      "Test set: Average loss: 0.3132, Average MAE: 0.4583\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 17 [4096/259852 (2%)]\tLoss: 0.317426\tGrad Norm: 0.415566\tLR: 0.723798\n",
      "Train Epoch: 17 [24576/259852 (9%)]\tLoss: 0.310880\tGrad Norm: 0.076613\tLR: 0.723798\n",
      "Train Epoch: 17 [45056/259852 (17%)]\tLoss: 0.308247\tGrad Norm: 0.224739\tLR: 0.723798\n",
      "Train Epoch: 17 [65536/259852 (25%)]\tLoss: 0.314893\tGrad Norm: 0.076499\tLR: 0.723798\n",
      "Train Epoch: 17 [86016/259852 (33%)]\tLoss: 0.307007\tGrad Norm: 0.097782\tLR: 0.723798\n",
      "Train Epoch: 17 [106496/259852 (41%)]\tLoss: 0.311338\tGrad Norm: 0.240517\tLR: 0.723798\n",
      "Train Epoch: 17 [126976/259852 (48%)]\tLoss: 0.309533\tGrad Norm: 0.116066\tLR: 0.723798\n",
      "Train Epoch: 17 [147456/259852 (56%)]\tLoss: 0.311525\tGrad Norm: 0.110991\tLR: 0.723798\n",
      "Train Epoch: 17 [167936/259852 (64%)]\tLoss: 0.315930\tGrad Norm: 0.271541\tLR: 0.723798\n",
      "Train Epoch: 17 [188416/259852 (72%)]\tLoss: 0.306055\tGrad Norm: 0.113694\tLR: 0.723798\n",
      "Train Epoch: 17 [208896/259852 (80%)]\tLoss: 0.300378\tGrad Norm: 0.159745\tLR: 0.723798\n",
      "Train Epoch: 17 [229376/259852 (88%)]\tLoss: 0.316152\tGrad Norm: 0.175391\tLR: 0.723798\n",
      "Train Epoch: 17 [249856/259852 (95%)]\tLoss: 0.311449\tGrad Norm: 0.201088\tLR: 0.723798\n",
      "Train set: Average loss: 0.3106\n",
      "Test set: Average loss: 0.3102, Average MAE: 0.4541\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 18 [4096/259852 (2%)]\tLoss: 0.313874\tGrad Norm: 0.196732\tLR: 0.709322\n",
      "Train Epoch: 18 [24576/259852 (9%)]\tLoss: 0.311558\tGrad Norm: 0.091026\tLR: 0.709322\n",
      "Train Epoch: 18 [45056/259852 (17%)]\tLoss: 0.307362\tGrad Norm: 0.120326\tLR: 0.709322\n",
      "Train Epoch: 18 [65536/259852 (25%)]\tLoss: 0.313100\tGrad Norm: 0.143044\tLR: 0.709322\n",
      "Train Epoch: 18 [86016/259852 (33%)]\tLoss: 0.305905\tGrad Norm: 0.282125\tLR: 0.709322\n",
      "Train Epoch: 18 [106496/259852 (41%)]\tLoss: 0.306303\tGrad Norm: 0.130960\tLR: 0.709322\n",
      "Train Epoch: 18 [126976/259852 (48%)]\tLoss: 0.304973\tGrad Norm: 0.057634\tLR: 0.709322\n",
      "Train Epoch: 18 [147456/259852 (56%)]\tLoss: 0.311454\tGrad Norm: 0.140295\tLR: 0.709322\n",
      "Train Epoch: 18 [167936/259852 (64%)]\tLoss: 0.312729\tGrad Norm: 0.240001\tLR: 0.709322\n",
      "Train Epoch: 18 [188416/259852 (72%)]\tLoss: 0.304729\tGrad Norm: 0.154146\tLR: 0.709322\n",
      "Train Epoch: 18 [208896/259852 (80%)]\tLoss: 0.306873\tGrad Norm: 0.063457\tLR: 0.709322\n",
      "Train Epoch: 18 [229376/259852 (88%)]\tLoss: 0.307972\tGrad Norm: 0.071320\tLR: 0.709322\n",
      "Train Epoch: 18 [249856/259852 (95%)]\tLoss: 0.305874\tGrad Norm: 0.058542\tLR: 0.709322\n",
      "Train set: Average loss: 0.3088\n",
      "Test set: Average loss: 0.3124, Average MAE: 0.4541\n",
      "Train Epoch: 19 [4096/259852 (2%)]\tLoss: 0.308933\tGrad Norm: 0.278237\tLR: 0.695135\n",
      "Train Epoch: 19 [24576/259852 (9%)]\tLoss: 0.305704\tGrad Norm: 0.045214\tLR: 0.695135\n",
      "Train Epoch: 19 [45056/259852 (17%)]\tLoss: 0.304056\tGrad Norm: 0.131264\tLR: 0.695135\n",
      "Train Epoch: 19 [65536/259852 (25%)]\tLoss: 0.310780\tGrad Norm: 0.182228\tLR: 0.695135\n",
      "Train Epoch: 19 [86016/259852 (33%)]\tLoss: 0.309143\tGrad Norm: 0.137960\tLR: 0.695135\n",
      "Train Epoch: 19 [106496/259852 (41%)]\tLoss: 0.308293\tGrad Norm: 0.210985\tLR: 0.695135\n",
      "Train Epoch: 19 [126976/259852 (48%)]\tLoss: 0.310396\tGrad Norm: 0.111660\tLR: 0.695135\n",
      "Train Epoch: 19 [147456/259852 (56%)]\tLoss: 0.309407\tGrad Norm: 0.082750\tLR: 0.695135\n",
      "Train Epoch: 19 [167936/259852 (64%)]\tLoss: 0.309571\tGrad Norm: 0.184422\tLR: 0.695135\n",
      "Train Epoch: 19 [188416/259852 (72%)]\tLoss: 0.306005\tGrad Norm: 0.074445\tLR: 0.695135\n",
      "Train Epoch: 19 [208896/259852 (80%)]\tLoss: 0.304354\tGrad Norm: 0.150591\tLR: 0.695135\n",
      "Train Epoch: 19 [229376/259852 (88%)]\tLoss: 0.311518\tGrad Norm: 0.091515\tLR: 0.695135\n",
      "Train Epoch: 19 [249856/259852 (95%)]\tLoss: 0.309059\tGrad Norm: 0.121238\tLR: 0.695135\n",
      "Train set: Average loss: 0.3081\n",
      "Test set: Average loss: 0.3059, Average MAE: 0.4528\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 20 [4096/259852 (2%)]\tLoss: 0.307053\tGrad Norm: 0.058400\tLR: 0.681233\n",
      "Train Epoch: 20 [24576/259852 (9%)]\tLoss: 0.308094\tGrad Norm: 0.045528\tLR: 0.681233\n",
      "Train Epoch: 20 [45056/259852 (17%)]\tLoss: 0.307574\tGrad Norm: 0.101364\tLR: 0.681233\n",
      "Train Epoch: 20 [65536/259852 (25%)]\tLoss: 0.303338\tGrad Norm: 0.091434\tLR: 0.681233\n",
      "Train Epoch: 20 [86016/259852 (33%)]\tLoss: 0.307931\tGrad Norm: 0.053995\tLR: 0.681233\n",
      "Train Epoch: 20 [106496/259852 (41%)]\tLoss: 0.306550\tGrad Norm: 0.051060\tLR: 0.681233\n",
      "Train Epoch: 20 [126976/259852 (48%)]\tLoss: 0.303211\tGrad Norm: 0.071365\tLR: 0.681233\n",
      "Train Epoch: 20 [147456/259852 (56%)]\tLoss: 0.302236\tGrad Norm: 0.114749\tLR: 0.681233\n",
      "Train Epoch: 20 [167936/259852 (64%)]\tLoss: 0.311284\tGrad Norm: 0.180369\tLR: 0.681233\n",
      "Train Epoch: 20 [188416/259852 (72%)]\tLoss: 0.305656\tGrad Norm: 0.100791\tLR: 0.681233\n",
      "Train Epoch: 20 [208896/259852 (80%)]\tLoss: 0.309979\tGrad Norm: 0.250379\tLR: 0.681233\n",
      "Train Epoch: 20 [229376/259852 (88%)]\tLoss: 0.308837\tGrad Norm: 0.133095\tLR: 0.681233\n",
      "Train Epoch: 20 [249856/259852 (95%)]\tLoss: 0.310696\tGrad Norm: 0.162563\tLR: 0.681233\n",
      "Train set: Average loss: 0.3071\n",
      "Test set: Average loss: 0.3061, Average MAE: 0.4521\n",
      "Epoch 20: Mean reward = 0.043 +/- 0.001\n",
      "Train Epoch: 21 [4096/259852 (2%)]\tLoss: 0.308704\tGrad Norm: 0.136015\tLR: 0.667608\n",
      "Train Epoch: 21 [24576/259852 (9%)]\tLoss: 0.305881\tGrad Norm: 0.099659\tLR: 0.667608\n",
      "Train Epoch: 21 [45056/259852 (17%)]\tLoss: 0.306018\tGrad Norm: 0.074236\tLR: 0.667608\n",
      "Train Epoch: 21 [65536/259852 (25%)]\tLoss: 0.304900\tGrad Norm: 0.083493\tLR: 0.667608\n",
      "Train Epoch: 21 [86016/259852 (33%)]\tLoss: 0.307347\tGrad Norm: 0.160916\tLR: 0.667608\n",
      "Train Epoch: 21 [106496/259852 (41%)]\tLoss: 0.308206\tGrad Norm: 0.105343\tLR: 0.667608\n",
      "Train Epoch: 21 [126976/259852 (48%)]\tLoss: 0.315266\tGrad Norm: 0.259270\tLR: 0.667608\n",
      "Train Epoch: 21 [147456/259852 (56%)]\tLoss: 0.300023\tGrad Norm: 0.090174\tLR: 0.667608\n",
      "Train Epoch: 21 [167936/259852 (64%)]\tLoss: 0.308718\tGrad Norm: 0.120129\tLR: 0.667608\n",
      "Train Epoch: 21 [188416/259852 (72%)]\tLoss: 0.307773\tGrad Norm: 0.070404\tLR: 0.667608\n",
      "Train Epoch: 21 [208896/259852 (80%)]\tLoss: 0.308224\tGrad Norm: 0.055909\tLR: 0.667608\n",
      "Train Epoch: 21 [229376/259852 (88%)]\tLoss: 0.302237\tGrad Norm: 0.080907\tLR: 0.667608\n",
      "Train Epoch: 21 [249856/259852 (95%)]\tLoss: 0.307832\tGrad Norm: 0.110278\tLR: 0.667608\n",
      "Train set: Average loss: 0.3064\n",
      "Test set: Average loss: 0.3048, Average MAE: 0.4541\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 22 [4096/259852 (2%)]\tLoss: 0.307147\tGrad Norm: 0.058762\tLR: 0.654256\n",
      "Train Epoch: 22 [24576/259852 (9%)]\tLoss: 0.307265\tGrad Norm: 0.092138\tLR: 0.654256\n",
      "Train Epoch: 22 [45056/259852 (17%)]\tLoss: 0.303470\tGrad Norm: 0.130950\tLR: 0.654256\n",
      "Train Epoch: 22 [65536/259852 (25%)]\tLoss: 0.307571\tGrad Norm: 0.091877\tLR: 0.654256\n",
      "Train Epoch: 22 [86016/259852 (33%)]\tLoss: 0.306882\tGrad Norm: 0.116251\tLR: 0.654256\n",
      "Train Epoch: 22 [106496/259852 (41%)]\tLoss: 0.306075\tGrad Norm: 0.139241\tLR: 0.654256\n",
      "Train Epoch: 22 [126976/259852 (48%)]\tLoss: 0.309913\tGrad Norm: 0.139484\tLR: 0.654256\n",
      "Train Epoch: 22 [147456/259852 (56%)]\tLoss: 0.305522\tGrad Norm: 0.037951\tLR: 0.654256\n",
      "Train Epoch: 22 [167936/259852 (64%)]\tLoss: 0.309013\tGrad Norm: 0.110191\tLR: 0.654256\n",
      "Train Epoch: 22 [188416/259852 (72%)]\tLoss: 0.306078\tGrad Norm: 0.074664\tLR: 0.654256\n",
      "Train Epoch: 22 [208896/259852 (80%)]\tLoss: 0.306614\tGrad Norm: 0.103827\tLR: 0.654256\n",
      "Train Epoch: 22 [229376/259852 (88%)]\tLoss: 0.308362\tGrad Norm: 0.094319\tLR: 0.654256\n",
      "Train Epoch: 22 [249856/259852 (95%)]\tLoss: 0.309498\tGrad Norm: 0.176096\tLR: 0.654256\n",
      "Train set: Average loss: 0.3062\n",
      "Test set: Average loss: 0.3054, Average MAE: 0.4538\n",
      "Train Epoch: 23 [4096/259852 (2%)]\tLoss: 0.303257\tGrad Norm: 0.143501\tLR: 0.641171\n",
      "Train Epoch: 23 [24576/259852 (9%)]\tLoss: 0.303825\tGrad Norm: 0.092992\tLR: 0.641171\n",
      "Train Epoch: 23 [45056/259852 (17%)]\tLoss: 0.304358\tGrad Norm: 0.081474\tLR: 0.641171\n",
      "Train Epoch: 23 [65536/259852 (25%)]\tLoss: 0.308050\tGrad Norm: 0.126301\tLR: 0.641171\n",
      "Train Epoch: 23 [86016/259852 (33%)]\tLoss: 0.305148\tGrad Norm: 0.147519\tLR: 0.641171\n",
      "Train Epoch: 23 [106496/259852 (41%)]\tLoss: 0.302410\tGrad Norm: 0.154221\tLR: 0.641171\n",
      "Train Epoch: 23 [126976/259852 (48%)]\tLoss: 0.300811\tGrad Norm: 0.057520\tLR: 0.641171\n",
      "Train Epoch: 23 [147456/259852 (56%)]\tLoss: 0.305179\tGrad Norm: 0.101047\tLR: 0.641171\n",
      "Train Epoch: 23 [167936/259852 (64%)]\tLoss: 0.309323\tGrad Norm: 0.165108\tLR: 0.641171\n",
      "Train Epoch: 23 [188416/259852 (72%)]\tLoss: 0.309375\tGrad Norm: 0.178835\tLR: 0.641171\n",
      "Train Epoch: 23 [208896/259852 (80%)]\tLoss: 0.304750\tGrad Norm: 0.166771\tLR: 0.641171\n",
      "Train Epoch: 23 [229376/259852 (88%)]\tLoss: 0.307084\tGrad Norm: 0.092588\tLR: 0.641171\n",
      "Train Epoch: 23 [249856/259852 (95%)]\tLoss: 0.304540\tGrad Norm: 0.083971\tLR: 0.641171\n",
      "Train set: Average loss: 0.3062\n",
      "Test set: Average loss: 0.3077, Average MAE: 0.4569\n",
      "Train Epoch: 24 [4096/259852 (2%)]\tLoss: 0.304888\tGrad Norm: 0.211035\tLR: 0.628347\n",
      "Train Epoch: 24 [24576/259852 (9%)]\tLoss: 0.305461\tGrad Norm: 0.080449\tLR: 0.628347\n",
      "Train Epoch: 24 [45056/259852 (17%)]\tLoss: 0.306115\tGrad Norm: 0.106861\tLR: 0.628347\n",
      "Train Epoch: 24 [65536/259852 (25%)]\tLoss: 0.306969\tGrad Norm: 0.081966\tLR: 0.628347\n",
      "Train Epoch: 24 [86016/259852 (33%)]\tLoss: 0.301900\tGrad Norm: 0.109364\tLR: 0.628347\n",
      "Train Epoch: 24 [106496/259852 (41%)]\tLoss: 0.305774\tGrad Norm: 0.138150\tLR: 0.628347\n",
      "Train Epoch: 24 [126976/259852 (48%)]\tLoss: 0.300583\tGrad Norm: 0.064274\tLR: 0.628347\n",
      "Train Epoch: 24 [147456/259852 (56%)]\tLoss: 0.308019\tGrad Norm: 0.191117\tLR: 0.628347\n",
      "Train Epoch: 24 [167936/259852 (64%)]\tLoss: 0.306220\tGrad Norm: 0.116101\tLR: 0.628347\n",
      "Train Epoch: 24 [188416/259852 (72%)]\tLoss: 0.303921\tGrad Norm: 0.077776\tLR: 0.628347\n",
      "Train Epoch: 24 [208896/259852 (80%)]\tLoss: 0.304788\tGrad Norm: 0.123388\tLR: 0.628347\n",
      "Train Epoch: 24 [229376/259852 (88%)]\tLoss: 0.304489\tGrad Norm: 0.108240\tLR: 0.628347\n",
      "Train Epoch: 24 [249856/259852 (95%)]\tLoss: 0.308019\tGrad Norm: 0.082240\tLR: 0.628347\n",
      "Train set: Average loss: 0.3054\n",
      "Test set: Average loss: 0.3061, Average MAE: 0.4506\n",
      "Train Epoch: 25 [4096/259852 (2%)]\tLoss: 0.310497\tGrad Norm: 0.167032\tLR: 0.615780\n",
      "Train Epoch: 25 [24576/259852 (9%)]\tLoss: 0.300108\tGrad Norm: 0.134360\tLR: 0.615780\n",
      "Train Epoch: 25 [45056/259852 (17%)]\tLoss: 0.305181\tGrad Norm: 0.129743\tLR: 0.615780\n",
      "Train Epoch: 25 [65536/259852 (25%)]\tLoss: 0.306570\tGrad Norm: 0.175905\tLR: 0.615780\n",
      "Train Epoch: 25 [86016/259852 (33%)]\tLoss: 0.308691\tGrad Norm: 0.089820\tLR: 0.615780\n",
      "Train Epoch: 25 [106496/259852 (41%)]\tLoss: 0.312585\tGrad Norm: 0.400808\tLR: 0.615780\n",
      "Train Epoch: 25 [126976/259852 (48%)]\tLoss: 0.309487\tGrad Norm: 0.160140\tLR: 0.615780\n",
      "Train Epoch: 25 [147456/259852 (56%)]\tLoss: 0.304249\tGrad Norm: 0.121064\tLR: 0.615780\n",
      "Train Epoch: 25 [167936/259852 (64%)]\tLoss: 0.300095\tGrad Norm: 0.126992\tLR: 0.615780\n",
      "Train Epoch: 25 [188416/259852 (72%)]\tLoss: 0.304801\tGrad Norm: 0.154576\tLR: 0.615780\n",
      "Train Epoch: 25 [208896/259852 (80%)]\tLoss: 0.301842\tGrad Norm: 0.160111\tLR: 0.615780\n",
      "Train Epoch: 25 [229376/259852 (88%)]\tLoss: 0.301200\tGrad Norm: 0.168809\tLR: 0.615780\n",
      "Train Epoch: 25 [249856/259852 (95%)]\tLoss: 0.306873\tGrad Norm: 0.146540\tLR: 0.615780\n",
      "Train set: Average loss: 0.3056\n",
      "Test set: Average loss: 0.3043, Average MAE: 0.4477\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Epoch 25: Mean reward = 0.043 +/- 0.001\n",
      "Train Epoch: 26 [4096/259852 (2%)]\tLoss: 0.307195\tGrad Norm: 0.142309\tLR: 0.603465\n",
      "Train Epoch: 26 [24576/259852 (9%)]\tLoss: 0.305317\tGrad Norm: 0.079567\tLR: 0.603465\n",
      "Train Epoch: 26 [45056/259852 (17%)]\tLoss: 0.309630\tGrad Norm: 0.382411\tLR: 0.603465\n",
      "Train Epoch: 26 [65536/259852 (25%)]\tLoss: 0.301118\tGrad Norm: 0.093457\tLR: 0.603465\n",
      "Train Epoch: 26 [86016/259852 (33%)]\tLoss: 0.302179\tGrad Norm: 0.141246\tLR: 0.603465\n",
      "Train Epoch: 26 [106496/259852 (41%)]\tLoss: 0.308061\tGrad Norm: 0.160400\tLR: 0.603465\n",
      "Train Epoch: 26 [126976/259852 (48%)]\tLoss: 0.305018\tGrad Norm: 0.245472\tLR: 0.603465\n",
      "Train Epoch: 26 [147456/259852 (56%)]\tLoss: 0.307000\tGrad Norm: 0.262956\tLR: 0.603465\n",
      "Train Epoch: 26 [167936/259852 (64%)]\tLoss: 0.301846\tGrad Norm: 0.155493\tLR: 0.603465\n",
      "Train Epoch: 26 [188416/259852 (72%)]\tLoss: 0.303576\tGrad Norm: 0.181290\tLR: 0.603465\n",
      "Train Epoch: 26 [208896/259852 (80%)]\tLoss: 0.301781\tGrad Norm: 0.165897\tLR: 0.603465\n",
      "Train Epoch: 26 [229376/259852 (88%)]\tLoss: 0.310636\tGrad Norm: 0.306937\tLR: 0.603465\n",
      "Train Epoch: 26 [249856/259852 (95%)]\tLoss: 0.299807\tGrad Norm: 0.189093\tLR: 0.603465\n",
      "Train set: Average loss: 0.3060\n",
      "Test set: Average loss: 0.3029, Average MAE: 0.4499\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 27 [4096/259852 (2%)]\tLoss: 0.304157\tGrad Norm: 0.128641\tLR: 0.591395\n",
      "Train Epoch: 27 [24576/259852 (9%)]\tLoss: 0.302130\tGrad Norm: 0.187053\tLR: 0.591395\n",
      "Train Epoch: 27 [45056/259852 (17%)]\tLoss: 0.305897\tGrad Norm: 0.187569\tLR: 0.591395\n",
      "Train Epoch: 27 [65536/259852 (25%)]\tLoss: 0.300397\tGrad Norm: 0.144196\tLR: 0.591395\n",
      "Train Epoch: 27 [86016/259852 (33%)]\tLoss: 0.305514\tGrad Norm: 0.244536\tLR: 0.591395\n",
      "Train Epoch: 27 [106496/259852 (41%)]\tLoss: 0.304298\tGrad Norm: 0.151122\tLR: 0.591395\n",
      "Train Epoch: 27 [126976/259852 (48%)]\tLoss: 0.312424\tGrad Norm: 0.404212\tLR: 0.591395\n",
      "Train Epoch: 27 [147456/259852 (56%)]\tLoss: 0.316111\tGrad Norm: 0.551255\tLR: 0.591395\n",
      "Train Epoch: 27 [167936/259852 (64%)]\tLoss: 0.302310\tGrad Norm: 0.102339\tLR: 0.591395\n",
      "Train Epoch: 27 [188416/259852 (72%)]\tLoss: 0.299535\tGrad Norm: 0.238316\tLR: 0.591395\n",
      "Train Epoch: 27 [208896/259852 (80%)]\tLoss: 0.301890\tGrad Norm: 0.071624\tLR: 0.591395\n",
      "Train Epoch: 27 [229376/259852 (88%)]\tLoss: 0.306003\tGrad Norm: 0.256588\tLR: 0.591395\n",
      "Train Epoch: 27 [249856/259852 (95%)]\tLoss: 0.304777\tGrad Norm: 0.216062\tLR: 0.591395\n",
      "Train set: Average loss: 0.3050\n",
      "Test set: Average loss: 0.3056, Average MAE: 0.4446\n",
      "Train Epoch: 28 [4096/259852 (2%)]\tLoss: 0.302078\tGrad Norm: 0.303323\tLR: 0.579568\n",
      "Train Epoch: 28 [24576/259852 (9%)]\tLoss: 0.325660\tGrad Norm: 0.876465\tLR: 0.579568\n",
      "Train Epoch: 28 [45056/259852 (17%)]\tLoss: 0.306019\tGrad Norm: 0.221742\tLR: 0.579568\n",
      "Train Epoch: 28 [65536/259852 (25%)]\tLoss: 0.296417\tGrad Norm: 0.235946\tLR: 0.579568\n",
      "Train Epoch: 28 [86016/259852 (33%)]\tLoss: 0.304731\tGrad Norm: 0.297564\tLR: 0.579568\n",
      "Train Epoch: 28 [106496/259852 (41%)]\tLoss: 0.309910\tGrad Norm: 0.476436\tLR: 0.579568\n",
      "Train Epoch: 28 [126976/259852 (48%)]\tLoss: 0.320446\tGrad Norm: 0.649305\tLR: 0.579568\n",
      "Train Epoch: 28 [147456/259852 (56%)]\tLoss: 0.304954\tGrad Norm: 0.308230\tLR: 0.579568\n",
      "Train Epoch: 28 [167936/259852 (64%)]\tLoss: 0.311856\tGrad Norm: 0.397185\tLR: 0.579568\n",
      "Train Epoch: 28 [188416/259852 (72%)]\tLoss: 0.300365\tGrad Norm: 0.241973\tLR: 0.579568\n",
      "Train Epoch: 28 [208896/259852 (80%)]\tLoss: 0.308190\tGrad Norm: 0.393114\tLR: 0.579568\n",
      "Train Epoch: 28 [229376/259852 (88%)]\tLoss: 0.297246\tGrad Norm: 0.322308\tLR: 0.579568\n",
      "Train Epoch: 28 [249856/259852 (95%)]\tLoss: 0.299197\tGrad Norm: 0.598427\tLR: 0.579568\n",
      "Train set: Average loss: 0.3046\n",
      "Test set: Average loss: 0.3021, Average MAE: 0.4466\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 29 [4096/259852 (2%)]\tLoss: 0.301081\tGrad Norm: 0.316190\tLR: 0.567976\n",
      "Train Epoch: 29 [24576/259852 (9%)]\tLoss: 0.308354\tGrad Norm: 0.413470\tLR: 0.567976\n",
      "Train Epoch: 29 [45056/259852 (17%)]\tLoss: 0.307650\tGrad Norm: 0.911878\tLR: 0.567976\n",
      "Train Epoch: 29 [65536/259852 (25%)]\tLoss: 0.302661\tGrad Norm: 0.390385\tLR: 0.567976\n",
      "Train Epoch: 29 [86016/259852 (33%)]\tLoss: 0.311299\tGrad Norm: 0.701395\tLR: 0.567976\n",
      "Train Epoch: 29 [106496/259852 (41%)]\tLoss: 0.295223\tGrad Norm: 0.281568\tLR: 0.567976\n",
      "Train Epoch: 29 [126976/259852 (48%)]\tLoss: 0.287470\tGrad Norm: 0.329172\tLR: 0.567976\n",
      "Train Epoch: 29 [147456/259852 (56%)]\tLoss: 0.305840\tGrad Norm: 0.503067\tLR: 0.567976\n",
      "Train Epoch: 29 [167936/259852 (64%)]\tLoss: 0.298682\tGrad Norm: 0.394224\tLR: 0.567976\n",
      "Train Epoch: 29 [188416/259852 (72%)]\tLoss: 0.291123\tGrad Norm: 0.431250\tLR: 0.567976\n",
      "Train Epoch: 29 [208896/259852 (80%)]\tLoss: 0.311956\tGrad Norm: 0.831723\tLR: 0.567976\n",
      "Train Epoch: 29 [229376/259852 (88%)]\tLoss: 0.284525\tGrad Norm: 0.342353\tLR: 0.567976\n",
      "Train Epoch: 29 [249856/259852 (95%)]\tLoss: 0.278874\tGrad Norm: 0.440890\tLR: 0.567976\n",
      "Train set: Average loss: 0.2995\n",
      "Test set: Average loss: 0.3422, Average MAE: 0.4746\n",
      "Train Epoch: 30 [4096/259852 (2%)]\tLoss: 0.345199\tGrad Norm: 1.419656\tLR: 0.556617\n",
      "Train Epoch: 30 [24576/259852 (9%)]\tLoss: 0.311521\tGrad Norm: 0.757884\tLR: 0.556617\n",
      "Train Epoch: 30 [45056/259852 (17%)]\tLoss: 0.275103\tGrad Norm: 0.189749\tLR: 0.556617\n",
      "Train Epoch: 30 [65536/259852 (25%)]\tLoss: 0.274136\tGrad Norm: 0.211757\tLR: 0.556617\n",
      "Train Epoch: 30 [86016/259852 (33%)]\tLoss: 0.276017\tGrad Norm: 0.382687\tLR: 0.556617\n",
      "Train Epoch: 30 [106496/259852 (41%)]\tLoss: 0.293373\tGrad Norm: 0.546766\tLR: 0.556617\n",
      "Train Epoch: 30 [126976/259852 (48%)]\tLoss: 0.284077\tGrad Norm: 0.367786\tLR: 0.556617\n",
      "Train Epoch: 30 [147456/259852 (56%)]\tLoss: 0.275020\tGrad Norm: 0.419276\tLR: 0.556617\n",
      "Train Epoch: 30 [167936/259852 (64%)]\tLoss: 0.263736\tGrad Norm: 0.217160\tLR: 0.556617\n",
      "Train Epoch: 30 [188416/259852 (72%)]\tLoss: 0.289050\tGrad Norm: 0.667292\tLR: 0.556617\n",
      "Train Epoch: 30 [208896/259852 (80%)]\tLoss: 0.279639\tGrad Norm: 0.342687\tLR: 0.556617\n",
      "Train Epoch: 30 [229376/259852 (88%)]\tLoss: 0.284617\tGrad Norm: 0.539244\tLR: 0.556617\n",
      "Train Epoch: 30 [249856/259852 (95%)]\tLoss: 0.270899\tGrad Norm: 0.329680\tLR: 0.556617\n",
      "Train set: Average loss: 0.2841\n",
      "Test set: Average loss: 0.2644, Average MAE: 0.4123\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Epoch 30: Mean reward = 0.044 +/- 0.001\n",
      "Train Epoch: 31 [4096/259852 (2%)]\tLoss: 0.265188\tGrad Norm: 0.297275\tLR: 0.545484\n",
      "Train Epoch: 31 [24576/259852 (9%)]\tLoss: 0.283001\tGrad Norm: 0.569797\tLR: 0.545484\n",
      "Train Epoch: 31 [45056/259852 (17%)]\tLoss: 0.271416\tGrad Norm: 0.371288\tLR: 0.545484\n",
      "Train Epoch: 31 [65536/259852 (25%)]\tLoss: 0.316532\tGrad Norm: 1.621778\tLR: 0.545484\n",
      "Train Epoch: 31 [86016/259852 (33%)]\tLoss: 0.294040\tGrad Norm: 0.749292\tLR: 0.545484\n",
      "Train Epoch: 31 [106496/259852 (41%)]\tLoss: 0.264197\tGrad Norm: 0.144572\tLR: 0.545484\n",
      "Train Epoch: 31 [126976/259852 (48%)]\tLoss: 0.261724\tGrad Norm: 0.376220\tLR: 0.545484\n",
      "Train Epoch: 31 [147456/259852 (56%)]\tLoss: 0.256466\tGrad Norm: 0.144305\tLR: 0.545484\n",
      "Train Epoch: 31 [167936/259852 (64%)]\tLoss: 0.269137\tGrad Norm: 0.542750\tLR: 0.545484\n",
      "Train Epoch: 31 [188416/259852 (72%)]\tLoss: 0.279771\tGrad Norm: 0.688252\tLR: 0.545484\n",
      "Train Epoch: 31 [208896/259852 (80%)]\tLoss: 0.270286\tGrad Norm: 0.530994\tLR: 0.545484\n",
      "Train Epoch: 31 [229376/259852 (88%)]\tLoss: 0.254179\tGrad Norm: 0.186442\tLR: 0.545484\n",
      "Train Epoch: 31 [249856/259852 (95%)]\tLoss: 0.262954\tGrad Norm: 0.343813\tLR: 0.545484\n",
      "Train set: Average loss: 0.2727\n",
      "Test set: Average loss: 0.2540, Average MAE: 0.4000\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 32 [4096/259852 (2%)]\tLoss: 0.259098\tGrad Norm: 0.319978\tLR: 0.534575\n",
      "Train Epoch: 32 [24576/259852 (9%)]\tLoss: 0.265374\tGrad Norm: 0.536358\tLR: 0.534575\n",
      "Train Epoch: 32 [45056/259852 (17%)]\tLoss: 0.261980\tGrad Norm: 0.352985\tLR: 0.534575\n",
      "Train Epoch: 32 [65536/259852 (25%)]\tLoss: 0.260197\tGrad Norm: 0.360257\tLR: 0.534575\n",
      "Train Epoch: 32 [86016/259852 (33%)]\tLoss: 0.255377\tGrad Norm: 0.425124\tLR: 0.534575\n",
      "Train Epoch: 32 [106496/259852 (41%)]\tLoss: 0.270756\tGrad Norm: 0.467610\tLR: 0.534575\n",
      "Train Epoch: 32 [126976/259852 (48%)]\tLoss: 0.248549\tGrad Norm: 0.313855\tLR: 0.534575\n",
      "Train Epoch: 32 [147456/259852 (56%)]\tLoss: 0.275103\tGrad Norm: 0.727385\tLR: 0.534575\n",
      "Train Epoch: 32 [167936/259852 (64%)]\tLoss: 0.273306\tGrad Norm: 0.708531\tLR: 0.534575\n",
      "Train Epoch: 32 [188416/259852 (72%)]\tLoss: 0.269714\tGrad Norm: 0.558101\tLR: 0.534575\n",
      "Train Epoch: 32 [208896/259852 (80%)]\tLoss: 0.259400\tGrad Norm: 0.465944\tLR: 0.534575\n",
      "Train Epoch: 32 [229376/259852 (88%)]\tLoss: 0.243885\tGrad Norm: 0.199915\tLR: 0.534575\n",
      "Train Epoch: 32 [249856/259852 (95%)]\tLoss: 0.250623\tGrad Norm: 0.393187\tLR: 0.534575\n",
      "Train set: Average loss: 0.2630\n",
      "Test set: Average loss: 0.2768, Average MAE: 0.4172\n",
      "Train Epoch: 33 [4096/259852 (2%)]\tLoss: 0.278285\tGrad Norm: 0.882815\tLR: 0.523883\n",
      "Train Epoch: 33 [24576/259852 (9%)]\tLoss: 0.243363\tGrad Norm: 0.295175\tLR: 0.523883\n",
      "Train Epoch: 33 [45056/259852 (17%)]\tLoss: 0.247451\tGrad Norm: 0.326054\tLR: 0.523883\n",
      "Train Epoch: 33 [65536/259852 (25%)]\tLoss: 0.260431\tGrad Norm: 0.626867\tLR: 0.523883\n",
      "Train Epoch: 33 [86016/259852 (33%)]\tLoss: 0.258633\tGrad Norm: 0.557838\tLR: 0.523883\n",
      "Train Epoch: 33 [106496/259852 (41%)]\tLoss: 0.253162\tGrad Norm: 0.613726\tLR: 0.523883\n",
      "Train Epoch: 33 [126976/259852 (48%)]\tLoss: 0.269233\tGrad Norm: 0.690680\tLR: 0.523883\n",
      "Train Epoch: 33 [147456/259852 (56%)]\tLoss: 0.242935\tGrad Norm: 0.437448\tLR: 0.523883\n",
      "Train Epoch: 33 [167936/259852 (64%)]\tLoss: 0.295087\tGrad Norm: 1.129592\tLR: 0.523883\n",
      "Train Epoch: 33 [188416/259852 (72%)]\tLoss: 0.249174\tGrad Norm: 0.448281\tLR: 0.523883\n",
      "Train Epoch: 33 [208896/259852 (80%)]\tLoss: 0.252930\tGrad Norm: 0.426121\tLR: 0.523883\n",
      "Train Epoch: 33 [229376/259852 (88%)]\tLoss: 0.243281\tGrad Norm: 0.320484\tLR: 0.523883\n",
      "Train Epoch: 33 [249856/259852 (95%)]\tLoss: 0.241778\tGrad Norm: 0.430509\tLR: 0.523883\n",
      "Train set: Average loss: 0.2546\n",
      "Test set: Average loss: 0.2401, Average MAE: 0.3902\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 34 [4096/259852 (2%)]\tLoss: 0.240049\tGrad Norm: 0.329431\tLR: 0.513405\n",
      "Train Epoch: 34 [24576/259852 (9%)]\tLoss: 0.287828\tGrad Norm: 1.319224\tLR: 0.513405\n",
      "Train Epoch: 34 [45056/259852 (17%)]\tLoss: 0.245788\tGrad Norm: 0.451063\tLR: 0.513405\n",
      "Train Epoch: 34 [65536/259852 (25%)]\tLoss: 0.249698\tGrad Norm: 0.346357\tLR: 0.513405\n",
      "Train Epoch: 34 [86016/259852 (33%)]\tLoss: 0.228609\tGrad Norm: 0.182972\tLR: 0.513405\n",
      "Train Epoch: 34 [106496/259852 (41%)]\tLoss: 0.243768\tGrad Norm: 0.610266\tLR: 0.513405\n",
      "Train Epoch: 34 [126976/259852 (48%)]\tLoss: 0.258312\tGrad Norm: 0.801748\tLR: 0.513405\n",
      "Train Epoch: 34 [147456/259852 (56%)]\tLoss: 0.249726\tGrad Norm: 0.688558\tLR: 0.513405\n",
      "Train Epoch: 34 [167936/259852 (64%)]\tLoss: 0.229984\tGrad Norm: 0.292551\tLR: 0.513405\n",
      "Train Epoch: 34 [188416/259852 (72%)]\tLoss: 0.234065\tGrad Norm: 0.484977\tLR: 0.513405\n",
      "Train Epoch: 34 [208896/259852 (80%)]\tLoss: 0.230316\tGrad Norm: 0.404996\tLR: 0.513405\n",
      "Train Epoch: 34 [229376/259852 (88%)]\tLoss: 0.258000\tGrad Norm: 0.789138\tLR: 0.513405\n",
      "Train Epoch: 34 [249856/259852 (95%)]\tLoss: 0.220861\tGrad Norm: 0.323971\tLR: 0.513405\n",
      "Train set: Average loss: 0.2432\n",
      "Test set: Average loss: 0.2495, Average MAE: 0.3825\n",
      "Train Epoch: 35 [4096/259852 (2%)]\tLoss: 0.248056\tGrad Norm: 0.840772\tLR: 0.503137\n",
      "Train Epoch: 35 [24576/259852 (9%)]\tLoss: 0.222112\tGrad Norm: 0.288780\tLR: 0.503137\n",
      "Train Epoch: 35 [45056/259852 (17%)]\tLoss: 0.211149\tGrad Norm: 0.346013\tLR: 0.503137\n",
      "Train Epoch: 35 [65536/259852 (25%)]\tLoss: 0.288818\tGrad Norm: 1.274419\tLR: 0.503137\n",
      "Train Epoch: 35 [86016/259852 (33%)]\tLoss: 0.233502\tGrad Norm: 0.467526\tLR: 0.503137\n",
      "Train Epoch: 35 [106496/259852 (41%)]\tLoss: 0.220600\tGrad Norm: 0.290017\tLR: 0.503137\n",
      "Train Epoch: 35 [126976/259852 (48%)]\tLoss: 0.215684\tGrad Norm: 0.444959\tLR: 0.503137\n",
      "Train Epoch: 35 [147456/259852 (56%)]\tLoss: 0.228418\tGrad Norm: 0.486666\tLR: 0.503137\n",
      "Train Epoch: 35 [167936/259852 (64%)]\tLoss: 0.208063\tGrad Norm: 0.268339\tLR: 0.503137\n",
      "Train Epoch: 35 [188416/259852 (72%)]\tLoss: 0.224075\tGrad Norm: 0.610584\tLR: 0.503137\n",
      "Train Epoch: 35 [208896/259852 (80%)]\tLoss: 0.213094\tGrad Norm: 0.374729\tLR: 0.503137\n",
      "Train Epoch: 35 [229376/259852 (88%)]\tLoss: 0.262865\tGrad Norm: 1.044978\tLR: 0.503137\n",
      "Train Epoch: 35 [249856/259852 (95%)]\tLoss: 0.253900\tGrad Norm: 0.994265\tLR: 0.503137\n",
      "Train set: Average loss: 0.2348\n",
      "Test set: Average loss: 0.2499, Average MAE: 0.3899\n",
      "Epoch 35: Mean reward = 0.044 +/- 0.002\n",
      "Train Epoch: 36 [4096/259852 (2%)]\tLoss: 0.249712\tGrad Norm: 0.797251\tLR: 0.493075\n",
      "Train Epoch: 36 [24576/259852 (9%)]\tLoss: 0.225425\tGrad Norm: 0.565057\tLR: 0.493075\n",
      "Train Epoch: 36 [45056/259852 (17%)]\tLoss: 0.209595\tGrad Norm: 0.340193\tLR: 0.493075\n",
      "Train Epoch: 36 [65536/259852 (25%)]\tLoss: 0.212175\tGrad Norm: 0.480964\tLR: 0.493075\n",
      "Train Epoch: 36 [86016/259852 (33%)]\tLoss: 0.207121\tGrad Norm: 0.396752\tLR: 0.493075\n",
      "Train Epoch: 36 [106496/259852 (41%)]\tLoss: 0.204037\tGrad Norm: 0.367036\tLR: 0.493075\n",
      "Train Epoch: 36 [126976/259852 (48%)]\tLoss: 0.254903\tGrad Norm: 1.018153\tLR: 0.493075\n",
      "Train Epoch: 36 [147456/259852 (56%)]\tLoss: 0.200702\tGrad Norm: 0.334211\tLR: 0.493075\n",
      "Train Epoch: 36 [167936/259852 (64%)]\tLoss: 0.187621\tGrad Norm: 0.319566\tLR: 0.493075\n",
      "Train Epoch: 36 [188416/259852 (72%)]\tLoss: 0.233340\tGrad Norm: 0.691007\tLR: 0.493075\n",
      "Train Epoch: 36 [208896/259852 (80%)]\tLoss: 0.199871\tGrad Norm: 0.408878\tLR: 0.493075\n",
      "Train Epoch: 36 [229376/259852 (88%)]\tLoss: 0.212338\tGrad Norm: 0.774799\tLR: 0.493075\n",
      "Train Epoch: 36 [249856/259852 (95%)]\tLoss: 0.193232\tGrad Norm: 0.250321\tLR: 0.493075\n",
      "Train set: Average loss: 0.2137\n",
      "Test set: Average loss: 0.1945, Average MAE: 0.3479\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 37 [4096/259852 (2%)]\tLoss: 0.195622\tGrad Norm: 0.417872\tLR: 0.483213\n",
      "Train Epoch: 37 [24576/259852 (9%)]\tLoss: 0.257804\tGrad Norm: 1.099507\tLR: 0.483213\n",
      "Train Epoch: 37 [45056/259852 (17%)]\tLoss: 0.190629\tGrad Norm: 0.280156\tLR: 0.483213\n",
      "Train Epoch: 37 [65536/259852 (25%)]\tLoss: 0.204821\tGrad Norm: 0.629665\tLR: 0.483213\n",
      "Train Epoch: 37 [86016/259852 (33%)]\tLoss: 0.188261\tGrad Norm: 0.351324\tLR: 0.483213\n",
      "Train Epoch: 37 [106496/259852 (41%)]\tLoss: 0.235386\tGrad Norm: 0.929595\tLR: 0.483213\n",
      "Train Epoch: 37 [126976/259852 (48%)]\tLoss: 0.208562\tGrad Norm: 0.700283\tLR: 0.483213\n",
      "Train Epoch: 37 [147456/259852 (56%)]\tLoss: 0.198112\tGrad Norm: 0.575666\tLR: 0.483213\n",
      "Train Epoch: 37 [167936/259852 (64%)]\tLoss: 0.186874\tGrad Norm: 0.426855\tLR: 0.483213\n",
      "Train Epoch: 37 [188416/259852 (72%)]\tLoss: 0.184473\tGrad Norm: 0.395299\tLR: 0.483213\n",
      "Train Epoch: 37 [208896/259852 (80%)]\tLoss: 0.200247\tGrad Norm: 0.574432\tLR: 0.483213\n",
      "Train Epoch: 37 [229376/259852 (88%)]\tLoss: 0.179560\tGrad Norm: 0.393605\tLR: 0.483213\n",
      "Train Epoch: 37 [249856/259852 (95%)]\tLoss: 0.217486\tGrad Norm: 1.017997\tLR: 0.483213\n",
      "Train set: Average loss: 0.1993\n",
      "Test set: Average loss: 0.1902, Average MAE: 0.3298\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 38 [4096/259852 (2%)]\tLoss: 0.186576\tGrad Norm: 0.441301\tLR: 0.473549\n",
      "Train Epoch: 38 [24576/259852 (9%)]\tLoss: 0.195500\tGrad Norm: 0.601875\tLR: 0.473549\n",
      "Train Epoch: 38 [45056/259852 (17%)]\tLoss: 0.200411\tGrad Norm: 2.365213\tLR: 0.473549\n",
      "Train Epoch: 38 [65536/259852 (25%)]\tLoss: 0.222049\tGrad Norm: 0.751176\tLR: 0.473549\n",
      "Train Epoch: 38 [86016/259852 (33%)]\tLoss: 0.203315\tGrad Norm: 0.616015\tLR: 0.473549\n",
      "Train Epoch: 38 [106496/259852 (41%)]\tLoss: 0.171342\tGrad Norm: 0.307633\tLR: 0.473549\n",
      "Train Epoch: 38 [126976/259852 (48%)]\tLoss: 0.173111\tGrad Norm: 0.390014\tLR: 0.473549\n",
      "Train Epoch: 38 [147456/259852 (56%)]\tLoss: 0.169214\tGrad Norm: 0.328706\tLR: 0.473549\n",
      "Train Epoch: 38 [167936/259852 (64%)]\tLoss: 0.180777\tGrad Norm: 0.545214\tLR: 0.473549\n",
      "Train Epoch: 38 [188416/259852 (72%)]\tLoss: 0.175913\tGrad Norm: 0.504046\tLR: 0.473549\n",
      "Train Epoch: 38 [208896/259852 (80%)]\tLoss: 0.168818\tGrad Norm: 0.302065\tLR: 0.473549\n",
      "Train Epoch: 38 [229376/259852 (88%)]\tLoss: 0.191063\tGrad Norm: 0.684895\tLR: 0.473549\n",
      "Train Epoch: 38 [249856/259852 (95%)]\tLoss: 0.180015\tGrad Norm: 0.551504\tLR: 0.473549\n",
      "Train set: Average loss: 0.1858\n",
      "Test set: Average loss: 0.1927, Average MAE: 0.3295\n",
      "Train Epoch: 39 [4096/259852 (2%)]\tLoss: 0.194676\tGrad Norm: 0.864756\tLR: 0.464078\n",
      "Train Epoch: 39 [24576/259852 (9%)]\tLoss: 0.167628\tGrad Norm: 0.317929\tLR: 0.464078\n",
      "Train Epoch: 39 [45056/259852 (17%)]\tLoss: 0.173235\tGrad Norm: 0.537505\tLR: 0.464078\n",
      "Train Epoch: 39 [65536/259852 (25%)]\tLoss: 0.176751\tGrad Norm: 0.539694\tLR: 0.464078\n",
      "Train Epoch: 39 [86016/259852 (33%)]\tLoss: 0.168239\tGrad Norm: 0.382705\tLR: 0.464078\n",
      "Train Epoch: 39 [106496/259852 (41%)]\tLoss: 0.200446\tGrad Norm: 0.877800\tLR: 0.464078\n",
      "Train Epoch: 39 [126976/259852 (48%)]\tLoss: 0.181948\tGrad Norm: 0.637657\tLR: 0.464078\n",
      "Train Epoch: 39 [147456/259852 (56%)]\tLoss: 0.175846\tGrad Norm: 0.471741\tLR: 0.464078\n",
      "Train Epoch: 39 [167936/259852 (64%)]\tLoss: 0.163188\tGrad Norm: 0.293364\tLR: 0.464078\n",
      "Train Epoch: 39 [188416/259852 (72%)]\tLoss: 0.172643\tGrad Norm: 0.509834\tLR: 0.464078\n",
      "Train Epoch: 39 [208896/259852 (80%)]\tLoss: 0.160938\tGrad Norm: 0.314339\tLR: 0.464078\n",
      "Train Epoch: 39 [229376/259852 (88%)]\tLoss: 0.161235\tGrad Norm: 0.407397\tLR: 0.464078\n",
      "Train Epoch: 39 [249856/259852 (95%)]\tLoss: 0.168512\tGrad Norm: 0.454553\tLR: 0.464078\n",
      "Train set: Average loss: 0.1734\n",
      "Test set: Average loss: 0.1742, Average MAE: 0.3273\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 40 [4096/259852 (2%)]\tLoss: 0.171076\tGrad Norm: 0.514824\tLR: 0.454796\n",
      "Train Epoch: 40 [24576/259852 (9%)]\tLoss: 0.175251\tGrad Norm: 0.718887\tLR: 0.454796\n",
      "Train Epoch: 40 [45056/259852 (17%)]\tLoss: 0.169272\tGrad Norm: 0.521608\tLR: 0.454796\n",
      "Train Epoch: 40 [65536/259852 (25%)]\tLoss: 0.159204\tGrad Norm: 0.326396\tLR: 0.454796\n",
      "Train Epoch: 40 [86016/259852 (33%)]\tLoss: 0.162263\tGrad Norm: 0.457217\tLR: 0.454796\n",
      "Train Epoch: 40 [106496/259852 (41%)]\tLoss: 0.160660\tGrad Norm: 0.407963\tLR: 0.454796\n",
      "Train Epoch: 40 [126976/259852 (48%)]\tLoss: 0.158663\tGrad Norm: 0.374504\tLR: 0.454796\n",
      "Train Epoch: 40 [147456/259852 (56%)]\tLoss: 0.159865\tGrad Norm: 0.375094\tLR: 0.454796\n",
      "Train Epoch: 40 [167936/259852 (64%)]\tLoss: 0.164517\tGrad Norm: 0.498432\tLR: 0.454796\n",
      "Train Epoch: 40 [188416/259852 (72%)]\tLoss: 0.169785\tGrad Norm: 0.629407\tLR: 0.454796\n",
      "Train Epoch: 40 [208896/259852 (80%)]\tLoss: 0.159462\tGrad Norm: 0.462620\tLR: 0.454796\n",
      "Train Epoch: 40 [229376/259852 (88%)]\tLoss: 0.171179\tGrad Norm: 0.686994\tLR: 0.454796\n",
      "Train Epoch: 40 [249856/259852 (95%)]\tLoss: 0.164611\tGrad Norm: 0.584409\tLR: 0.454796\n",
      "Train set: Average loss: 0.1650\n",
      "Test set: Average loss: 0.1522, Average MAE: 0.2957\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Epoch 40: Mean reward = 0.046 +/- 0.001\n",
      "Train Epoch: 41 [4096/259852 (2%)]\tLoss: 0.153683\tGrad Norm: 0.246908\tLR: 0.445700\n",
      "Train Epoch: 41 [24576/259852 (9%)]\tLoss: 0.146399\tGrad Norm: 0.290726\tLR: 0.445700\n",
      "Train Epoch: 41 [45056/259852 (17%)]\tLoss: 0.163871\tGrad Norm: 0.478996\tLR: 0.445700\n",
      "Train Epoch: 41 [65536/259852 (25%)]\tLoss: 0.151345\tGrad Norm: 0.372948\tLR: 0.445700\n",
      "Train Epoch: 41 [86016/259852 (33%)]\tLoss: 0.157067\tGrad Norm: 0.341481\tLR: 0.445700\n",
      "Train Epoch: 41 [106496/259852 (41%)]\tLoss: 0.145583\tGrad Norm: 0.271758\tLR: 0.445700\n",
      "Train Epoch: 41 [126976/259852 (48%)]\tLoss: 0.162047\tGrad Norm: 0.545531\tLR: 0.445700\n",
      "Train Epoch: 41 [147456/259852 (56%)]\tLoss: 0.155292\tGrad Norm: 0.555645\tLR: 0.445700\n",
      "Train Epoch: 41 [167936/259852 (64%)]\tLoss: 0.173648\tGrad Norm: 0.649746\tLR: 0.445700\n",
      "Train Epoch: 41 [188416/259852 (72%)]\tLoss: 0.182422\tGrad Norm: 0.731943\tLR: 0.445700\n",
      "Train Epoch: 41 [208896/259852 (80%)]\tLoss: 0.172885\tGrad Norm: 0.630860\tLR: 0.445700\n",
      "Train Epoch: 41 [229376/259852 (88%)]\tLoss: 0.152362\tGrad Norm: 0.349931\tLR: 0.445700\n",
      "Train Epoch: 41 [249856/259852 (95%)]\tLoss: 0.146413\tGrad Norm: 0.290006\tLR: 0.445700\n",
      "Train set: Average loss: 0.1626\n",
      "Test set: Average loss: 0.1508, Average MAE: 0.2914\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 42 [4096/259852 (2%)]\tLoss: 0.150590\tGrad Norm: 0.340078\tLR: 0.436786\n",
      "Train Epoch: 42 [24576/259852 (9%)]\tLoss: 0.145610\tGrad Norm: 0.338018\tLR: 0.436786\n",
      "Train Epoch: 42 [45056/259852 (17%)]\tLoss: 0.145491\tGrad Norm: 0.327392\tLR: 0.436786\n",
      "Train Epoch: 42 [65536/259852 (25%)]\tLoss: 0.147037\tGrad Norm: 0.280475\tLR: 0.436786\n",
      "Train Epoch: 42 [86016/259852 (33%)]\tLoss: 0.142801\tGrad Norm: 0.354855\tLR: 0.436786\n",
      "Train Epoch: 42 [106496/259852 (41%)]\tLoss: 0.150007\tGrad Norm: 0.411060\tLR: 0.436786\n",
      "Train Epoch: 42 [126976/259852 (48%)]\tLoss: 0.156191\tGrad Norm: 0.453206\tLR: 0.436786\n",
      "Train Epoch: 42 [147456/259852 (56%)]\tLoss: 0.147077\tGrad Norm: 0.378716\tLR: 0.436786\n",
      "Train Epoch: 42 [167936/259852 (64%)]\tLoss: 0.151902\tGrad Norm: 0.449960\tLR: 0.436786\n",
      "Train Epoch: 42 [188416/259852 (72%)]\tLoss: 0.143255\tGrad Norm: 0.332281\tLR: 0.436786\n",
      "Train Epoch: 42 [208896/259852 (80%)]\tLoss: 0.152511\tGrad Norm: 0.491857\tLR: 0.436786\n",
      "Train Epoch: 42 [229376/259852 (88%)]\tLoss: 0.153408\tGrad Norm: 0.449291\tLR: 0.436786\n",
      "Train Epoch: 42 [249856/259852 (95%)]\tLoss: 0.138620\tGrad Norm: 0.271146\tLR: 0.436786\n",
      "Train set: Average loss: 0.1474\n",
      "Test set: Average loss: 0.1459, Average MAE: 0.2828\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 43 [4096/259852 (2%)]\tLoss: 0.142686\tGrad Norm: 0.387822\tLR: 0.428051\n",
      "Train Epoch: 43 [24576/259852 (9%)]\tLoss: 0.141747\tGrad Norm: 0.322148\tLR: 0.428051\n",
      "Train Epoch: 43 [45056/259852 (17%)]\tLoss: 0.152313\tGrad Norm: 0.470878\tLR: 0.428051\n",
      "Train Epoch: 43 [65536/259852 (25%)]\tLoss: 0.147547\tGrad Norm: 0.366713\tLR: 0.428051\n",
      "Train Epoch: 43 [86016/259852 (33%)]\tLoss: 0.151421\tGrad Norm: 0.591768\tLR: 0.428051\n",
      "Train Epoch: 43 [106496/259852 (41%)]\tLoss: 0.140176\tGrad Norm: 0.363622\tLR: 0.428051\n",
      "Train Epoch: 43 [126976/259852 (48%)]\tLoss: 0.135052\tGrad Norm: 0.284955\tLR: 0.428051\n",
      "Train Epoch: 43 [147456/259852 (56%)]\tLoss: 0.140505\tGrad Norm: 0.395744\tLR: 0.428051\n",
      "Train Epoch: 43 [167936/259852 (64%)]\tLoss: 0.143580\tGrad Norm: 0.389623\tLR: 0.428051\n",
      "Train Epoch: 43 [188416/259852 (72%)]\tLoss: 0.167148\tGrad Norm: 0.740650\tLR: 0.428051\n",
      "Train Epoch: 43 [208896/259852 (80%)]\tLoss: 0.143595\tGrad Norm: 0.360061\tLR: 0.428051\n",
      "Train Epoch: 43 [229376/259852 (88%)]\tLoss: 0.135082\tGrad Norm: 0.312230\tLR: 0.428051\n",
      "Train Epoch: 43 [249856/259852 (95%)]\tLoss: 0.148761\tGrad Norm: 0.432260\tLR: 0.428051\n",
      "Train set: Average loss: 0.1454\n",
      "Test set: Average loss: 0.1420, Average MAE: 0.2800\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 44 [4096/259852 (2%)]\tLoss: 0.139557\tGrad Norm: 0.338371\tLR: 0.419490\n",
      "Train Epoch: 44 [24576/259852 (9%)]\tLoss: 0.138663\tGrad Norm: 0.357530\tLR: 0.419490\n",
      "Train Epoch: 44 [45056/259852 (17%)]\tLoss: 0.143918\tGrad Norm: 0.429777\tLR: 0.419490\n",
      "Train Epoch: 44 [65536/259852 (25%)]\tLoss: 0.138534\tGrad Norm: 0.328263\tLR: 0.419490\n",
      "Train Epoch: 44 [86016/259852 (33%)]\tLoss: 0.136496\tGrad Norm: 0.363121\tLR: 0.419490\n",
      "Train Epoch: 44 [106496/259852 (41%)]\tLoss: 0.142330\tGrad Norm: 0.485605\tLR: 0.419490\n",
      "Train Epoch: 44 [126976/259852 (48%)]\tLoss: 0.141592\tGrad Norm: 0.391724\tLR: 0.419490\n",
      "Train Epoch: 44 [147456/259852 (56%)]\tLoss: 0.137613\tGrad Norm: 0.323061\tLR: 0.419490\n",
      "Train Epoch: 44 [167936/259852 (64%)]\tLoss: 0.134696\tGrad Norm: 0.323890\tLR: 0.419490\n",
      "Train Epoch: 44 [188416/259852 (72%)]\tLoss: 0.138724\tGrad Norm: 0.358659\tLR: 0.419490\n",
      "Train Epoch: 44 [208896/259852 (80%)]\tLoss: 0.138044\tGrad Norm: 0.358099\tLR: 0.419490\n",
      "Train Epoch: 44 [229376/259852 (88%)]\tLoss: 0.148224\tGrad Norm: 0.505619\tLR: 0.419490\n",
      "Train Epoch: 44 [249856/259852 (95%)]\tLoss: 0.132569\tGrad Norm: 0.342173\tLR: 0.419490\n",
      "Train set: Average loss: 0.1394\n",
      "Test set: Average loss: 0.1400, Average MAE: 0.2752\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 45 [4096/259852 (2%)]\tLoss: 0.135937\tGrad Norm: 0.379364\tLR: 0.411100\n",
      "Train Epoch: 45 [24576/259852 (9%)]\tLoss: 0.141354\tGrad Norm: 0.345036\tLR: 0.411100\n",
      "Train Epoch: 45 [45056/259852 (17%)]\tLoss: 0.130882\tGrad Norm: 0.221809\tLR: 0.411100\n",
      "Train Epoch: 45 [65536/259852 (25%)]\tLoss: 0.141761\tGrad Norm: 0.390855\tLR: 0.411100\n",
      "Train Epoch: 45 [86016/259852 (33%)]\tLoss: 0.136715\tGrad Norm: 0.440131\tLR: 0.411100\n",
      "Train Epoch: 45 [106496/259852 (41%)]\tLoss: 0.140559\tGrad Norm: 0.447813\tLR: 0.411100\n",
      "Train Epoch: 45 [126976/259852 (48%)]\tLoss: 0.134240\tGrad Norm: 0.355981\tLR: 0.411100\n",
      "Train Epoch: 45 [147456/259852 (56%)]\tLoss: 0.128720\tGrad Norm: 0.310348\tLR: 0.411100\n",
      "Train Epoch: 45 [167936/259852 (64%)]\tLoss: 0.136352\tGrad Norm: 0.383695\tLR: 0.411100\n",
      "Train Epoch: 45 [188416/259852 (72%)]\tLoss: 0.133551\tGrad Norm: 0.335577\tLR: 0.411100\n",
      "Train Epoch: 45 [208896/259852 (80%)]\tLoss: 0.130566\tGrad Norm: 0.310951\tLR: 0.411100\n",
      "Train Epoch: 45 [229376/259852 (88%)]\tLoss: 0.134642\tGrad Norm: 0.395767\tLR: 0.411100\n",
      "Train Epoch: 45 [249856/259852 (95%)]\tLoss: 0.129874\tGrad Norm: 0.377380\tLR: 0.411100\n",
      "Train set: Average loss: 0.1352\n",
      "Test set: Average loss: 0.1369, Average MAE: 0.2732\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Epoch 45: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 46 [4096/259852 (2%)]\tLoss: 0.134273\tGrad Norm: 0.382357\tLR: 0.402878\n",
      "Train Epoch: 46 [24576/259852 (9%)]\tLoss: 0.137651\tGrad Norm: 0.456553\tLR: 0.402878\n",
      "Train Epoch: 46 [45056/259852 (17%)]\tLoss: 0.127808\tGrad Norm: 0.270732\tLR: 0.402878\n",
      "Train Epoch: 46 [65536/259852 (25%)]\tLoss: 0.137494\tGrad Norm: 0.454986\tLR: 0.402878\n",
      "Train Epoch: 46 [86016/259852 (33%)]\tLoss: 0.136090\tGrad Norm: 0.420140\tLR: 0.402878\n",
      "Train Epoch: 46 [106496/259852 (41%)]\tLoss: 0.136222\tGrad Norm: 0.462825\tLR: 0.402878\n",
      "Train Epoch: 46 [126976/259852 (48%)]\tLoss: 0.125686\tGrad Norm: 0.266258\tLR: 0.402878\n",
      "Train Epoch: 46 [147456/259852 (56%)]\tLoss: 0.127109\tGrad Norm: 0.288669\tLR: 0.402878\n",
      "Train Epoch: 46 [167936/259852 (64%)]\tLoss: 0.129022\tGrad Norm: 0.341044\tLR: 0.402878\n",
      "Train Epoch: 46 [188416/259852 (72%)]\tLoss: 0.124754\tGrad Norm: 0.318071\tLR: 0.402878\n",
      "Train Epoch: 46 [208896/259852 (80%)]\tLoss: 0.132232\tGrad Norm: 0.403720\tLR: 0.402878\n",
      "Train Epoch: 46 [229376/259852 (88%)]\tLoss: 0.126844\tGrad Norm: 0.285521\tLR: 0.402878\n",
      "Train Epoch: 46 [249856/259852 (95%)]\tLoss: 0.127355\tGrad Norm: 0.342485\tLR: 0.402878\n",
      "Train set: Average loss: 0.1312\n",
      "Test set: Average loss: 0.1384, Average MAE: 0.2790\n",
      "Train Epoch: 47 [4096/259852 (2%)]\tLoss: 0.134565\tGrad Norm: 0.453092\tLR: 0.394820\n",
      "Train Epoch: 47 [24576/259852 (9%)]\tLoss: 0.130428\tGrad Norm: 0.341827\tLR: 0.394820\n",
      "Train Epoch: 47 [45056/259852 (17%)]\tLoss: 0.133383\tGrad Norm: 0.418761\tLR: 0.394820\n",
      "Train Epoch: 47 [65536/259852 (25%)]\tLoss: 0.133611\tGrad Norm: 0.409833\tLR: 0.394820\n",
      "Train Epoch: 47 [86016/259852 (33%)]\tLoss: 0.139349\tGrad Norm: 0.501300\tLR: 0.394820\n",
      "Train Epoch: 47 [106496/259852 (41%)]\tLoss: 0.124395\tGrad Norm: 0.258637\tLR: 0.394820\n",
      "Train Epoch: 47 [126976/259852 (48%)]\tLoss: 0.129020\tGrad Norm: 0.313390\tLR: 0.394820\n",
      "Train Epoch: 47 [147456/259852 (56%)]\tLoss: 0.126685\tGrad Norm: 0.333636\tLR: 0.394820\n",
      "Train Epoch: 47 [167936/259852 (64%)]\tLoss: 0.132385\tGrad Norm: 0.354311\tLR: 0.394820\n",
      "Train Epoch: 47 [188416/259852 (72%)]\tLoss: 0.123509\tGrad Norm: 0.222521\tLR: 0.394820\n",
      "Train Epoch: 47 [208896/259852 (80%)]\tLoss: 0.130277\tGrad Norm: 0.443594\tLR: 0.394820\n",
      "Train Epoch: 47 [229376/259852 (88%)]\tLoss: 0.125170\tGrad Norm: 0.292302\tLR: 0.394820\n",
      "Train Epoch: 47 [249856/259852 (95%)]\tLoss: 0.127508\tGrad Norm: 0.376812\tLR: 0.394820\n",
      "Train set: Average loss: 0.1290\n",
      "Test set: Average loss: 0.1868, Average MAE: 0.3196\n",
      "Train Epoch: 48 [4096/259852 (2%)]\tLoss: 0.180225\tGrad Norm: 7.748863\tLR: 0.386924\n",
      "Train Epoch: 48 [24576/259852 (9%)]\tLoss: 0.121710\tGrad Norm: 0.220821\tLR: 0.386924\n",
      "Train Epoch: 48 [45056/259852 (17%)]\tLoss: 0.123309\tGrad Norm: 0.293568\tLR: 0.386924\n",
      "Train Epoch: 48 [65536/259852 (25%)]\tLoss: 0.123124\tGrad Norm: 0.293962\tLR: 0.386924\n",
      "Train Epoch: 48 [86016/259852 (33%)]\tLoss: 0.124017\tGrad Norm: 0.281218\tLR: 0.386924\n",
      "Train Epoch: 48 [106496/259852 (41%)]\tLoss: 0.129782\tGrad Norm: 0.406701\tLR: 0.386924\n",
      "Train Epoch: 48 [126976/259852 (48%)]\tLoss: 0.118945\tGrad Norm: 0.236802\tLR: 0.386924\n",
      "Train Epoch: 48 [147456/259852 (56%)]\tLoss: 0.121908\tGrad Norm: 0.264881\tLR: 0.386924\n",
      "Train Epoch: 48 [167936/259852 (64%)]\tLoss: 0.119976\tGrad Norm: 0.257743\tLR: 0.386924\n",
      "Train Epoch: 48 [188416/259852 (72%)]\tLoss: 0.119095\tGrad Norm: 0.254630\tLR: 0.386924\n",
      "Train Epoch: 48 [208896/259852 (80%)]\tLoss: 0.130025\tGrad Norm: 0.448202\tLR: 0.386924\n",
      "Train Epoch: 48 [229376/259852 (88%)]\tLoss: 0.127054\tGrad Norm: 0.314134\tLR: 0.386924\n",
      "Train Epoch: 48 [249856/259852 (95%)]\tLoss: 0.125051\tGrad Norm: 0.377932\tLR: 0.386924\n",
      "Train set: Average loss: 0.1281\n",
      "Test set: Average loss: 0.1302, Average MAE: 0.2654\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 49 [4096/259852 (2%)]\tLoss: 0.126682\tGrad Norm: 0.427608\tLR: 0.379185\n",
      "Train Epoch: 49 [24576/259852 (9%)]\tLoss: 0.119684\tGrad Norm: 0.281011\tLR: 0.379185\n",
      "Train Epoch: 49 [45056/259852 (17%)]\tLoss: 0.122094\tGrad Norm: 0.372106\tLR: 0.379185\n",
      "Train Epoch: 49 [65536/259852 (25%)]\tLoss: 0.127379\tGrad Norm: 0.362419\tLR: 0.379185\n",
      "Train Epoch: 49 [86016/259852 (33%)]\tLoss: 0.123677\tGrad Norm: 0.399198\tLR: 0.379185\n",
      "Train Epoch: 49 [106496/259852 (41%)]\tLoss: 0.125557\tGrad Norm: 0.361554\tLR: 0.379185\n",
      "Train Epoch: 49 [126976/259852 (48%)]\tLoss: 0.120293\tGrad Norm: 0.312370\tLR: 0.379185\n",
      "Train Epoch: 49 [147456/259852 (56%)]\tLoss: 0.123140\tGrad Norm: 0.301884\tLR: 0.379185\n",
      "Train Epoch: 49 [167936/259852 (64%)]\tLoss: 0.119586\tGrad Norm: 0.251821\tLR: 0.379185\n",
      "Train Epoch: 49 [188416/259852 (72%)]\tLoss: 0.130166\tGrad Norm: 0.435328\tLR: 0.379185\n",
      "Train Epoch: 49 [208896/259852 (80%)]\tLoss: 0.132177\tGrad Norm: 0.617524\tLR: 0.379185\n",
      "Train Epoch: 49 [229376/259852 (88%)]\tLoss: 0.118487\tGrad Norm: 0.326620\tLR: 0.379185\n",
      "Train Epoch: 49 [249856/259852 (95%)]\tLoss: 0.124816\tGrad Norm: 0.306232\tLR: 0.379185\n",
      "Train set: Average loss: 0.1240\n",
      "Test set: Average loss: 0.1300, Average MAE: 0.2714\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 50 [4096/259852 (2%)]\tLoss: 0.127872\tGrad Norm: 0.416822\tLR: 0.371602\n",
      "Train Epoch: 50 [24576/259852 (9%)]\tLoss: 0.117606\tGrad Norm: 0.300669\tLR: 0.371602\n",
      "Train Epoch: 50 [45056/259852 (17%)]\tLoss: 0.119420\tGrad Norm: 0.309790\tLR: 0.371602\n",
      "Train Epoch: 50 [65536/259852 (25%)]\tLoss: 0.120267\tGrad Norm: 0.338811\tLR: 0.371602\n",
      "Train Epoch: 50 [86016/259852 (33%)]\tLoss: 0.120559\tGrad Norm: 0.304371\tLR: 0.371602\n",
      "Train Epoch: 50 [106496/259852 (41%)]\tLoss: 0.120722\tGrad Norm: 0.330464\tLR: 0.371602\n",
      "Train Epoch: 50 [126976/259852 (48%)]\tLoss: 0.119237\tGrad Norm: 0.340160\tLR: 0.371602\n",
      "Train Epoch: 50 [147456/259852 (56%)]\tLoss: 0.124895\tGrad Norm: 0.417128\tLR: 0.371602\n",
      "Train Epoch: 50 [167936/259852 (64%)]\tLoss: 0.119737\tGrad Norm: 0.275537\tLR: 0.371602\n",
      "Train Epoch: 50 [188416/259852 (72%)]\tLoss: 0.119980\tGrad Norm: 0.298050\tLR: 0.371602\n",
      "Train Epoch: 50 [208896/259852 (80%)]\tLoss: 0.116348\tGrad Norm: 0.301211\tLR: 0.371602\n",
      "Train Epoch: 50 [229376/259852 (88%)]\tLoss: 0.140318\tGrad Norm: 0.952895\tLR: 0.371602\n",
      "Train Epoch: 50 [249856/259852 (95%)]\tLoss: 0.118562\tGrad Norm: 0.227098\tLR: 0.371602\n",
      "Train set: Average loss: 0.1210\n",
      "Test set: Average loss: 0.1209, Average MAE: 0.2547\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Epoch 50: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 51 [4096/259852 (2%)]\tLoss: 0.113763\tGrad Norm: 0.284747\tLR: 0.364170\n",
      "Train Epoch: 51 [24576/259852 (9%)]\tLoss: 0.114971\tGrad Norm: 0.258335\tLR: 0.364170\n",
      "Train Epoch: 51 [45056/259852 (17%)]\tLoss: 0.117900\tGrad Norm: 0.329381\tLR: 0.364170\n",
      "Train Epoch: 51 [65536/259852 (25%)]\tLoss: 0.115989\tGrad Norm: 0.275449\tLR: 0.364170\n",
      "Train Epoch: 51 [86016/259852 (33%)]\tLoss: 0.115970\tGrad Norm: 0.360441\tLR: 0.364170\n",
      "Train Epoch: 51 [106496/259852 (41%)]\tLoss: 0.115867\tGrad Norm: 0.212476\tLR: 0.364170\n",
      "Train Epoch: 51 [126976/259852 (48%)]\tLoss: 0.115273\tGrad Norm: 0.320713\tLR: 0.364170\n",
      "Train Epoch: 51 [147456/259852 (56%)]\tLoss: 0.117491\tGrad Norm: 0.319208\tLR: 0.364170\n",
      "Train Epoch: 51 [167936/259852 (64%)]\tLoss: 0.114811\tGrad Norm: 0.336400\tLR: 0.364170\n",
      "Train Epoch: 51 [188416/259852 (72%)]\tLoss: 0.118219\tGrad Norm: 0.364016\tLR: 0.364170\n",
      "Train Epoch: 51 [208896/259852 (80%)]\tLoss: 0.118670\tGrad Norm: 0.411086\tLR: 0.364170\n",
      "Train Epoch: 51 [229376/259852 (88%)]\tLoss: 0.119207\tGrad Norm: 0.321721\tLR: 0.364170\n",
      "Train Epoch: 51 [249856/259852 (95%)]\tLoss: 0.113803\tGrad Norm: 0.222056\tLR: 0.364170\n",
      "Train set: Average loss: 0.1172\n",
      "Test set: Average loss: 0.1187, Average MAE: 0.2517\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 52 [4096/259852 (2%)]\tLoss: 0.113021\tGrad Norm: 0.283758\tLR: 0.356886\n",
      "Train Epoch: 52 [24576/259852 (9%)]\tLoss: 0.113591\tGrad Norm: 0.302785\tLR: 0.356886\n",
      "Train Epoch: 52 [45056/259852 (17%)]\tLoss: 0.120043\tGrad Norm: 0.411948\tLR: 0.356886\n",
      "Train Epoch: 52 [65536/259852 (25%)]\tLoss: 0.115239\tGrad Norm: 0.315565\tLR: 0.356886\n",
      "Train Epoch: 52 [86016/259852 (33%)]\tLoss: 0.119500\tGrad Norm: 0.340338\tLR: 0.356886\n",
      "Train Epoch: 52 [106496/259852 (41%)]\tLoss: 0.111345\tGrad Norm: 0.254904\tLR: 0.356886\n",
      "Train Epoch: 52 [126976/259852 (48%)]\tLoss: 0.116679\tGrad Norm: 0.276478\tLR: 0.356886\n",
      "Train Epoch: 52 [147456/259852 (56%)]\tLoss: 0.118226\tGrad Norm: 0.353743\tLR: 0.356886\n",
      "Train Epoch: 52 [167936/259852 (64%)]\tLoss: 0.117919\tGrad Norm: 0.343312\tLR: 0.356886\n",
      "Train Epoch: 52 [188416/259852 (72%)]\tLoss: 0.114307\tGrad Norm: 0.291494\tLR: 0.356886\n",
      "Train Epoch: 52 [208896/259852 (80%)]\tLoss: 0.114860\tGrad Norm: 0.318207\tLR: 0.356886\n",
      "Train Epoch: 52 [229376/259852 (88%)]\tLoss: 0.120714\tGrad Norm: 0.389755\tLR: 0.356886\n",
      "Train Epoch: 52 [249856/259852 (95%)]\tLoss: 0.116941\tGrad Norm: 0.373021\tLR: 0.356886\n",
      "Train set: Average loss: 0.1162\n",
      "Test set: Average loss: 0.1170, Average MAE: 0.2492\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 53 [4096/259852 (2%)]\tLoss: 0.110532\tGrad Norm: 0.241098\tLR: 0.349749\n",
      "Train Epoch: 53 [24576/259852 (9%)]\tLoss: 0.112215\tGrad Norm: 0.263696\tLR: 0.349749\n",
      "Train Epoch: 53 [45056/259852 (17%)]\tLoss: 0.113041\tGrad Norm: 0.305514\tLR: 0.349749\n",
      "Train Epoch: 53 [65536/259852 (25%)]\tLoss: 0.118049\tGrad Norm: 0.364057\tLR: 0.349749\n",
      "Train Epoch: 53 [86016/259852 (33%)]\tLoss: 0.114280\tGrad Norm: 0.313046\tLR: 0.349749\n",
      "Train Epoch: 53 [106496/259852 (41%)]\tLoss: 0.119421\tGrad Norm: 0.404615\tLR: 0.349749\n",
      "Train Epoch: 53 [126976/259852 (48%)]\tLoss: 0.121079\tGrad Norm: 0.380665\tLR: 0.349749\n",
      "Train Epoch: 53 [147456/259852 (56%)]\tLoss: 0.112795\tGrad Norm: 0.321534\tLR: 0.349749\n",
      "Train Epoch: 53 [167936/259852 (64%)]\tLoss: 0.112456\tGrad Norm: 0.283402\tLR: 0.349749\n",
      "Train Epoch: 53 [188416/259852 (72%)]\tLoss: 0.118401\tGrad Norm: 0.395585\tLR: 0.349749\n",
      "Train Epoch: 53 [208896/259852 (80%)]\tLoss: 0.115498\tGrad Norm: 0.354598\tLR: 0.349749\n",
      "Train Epoch: 53 [229376/259852 (88%)]\tLoss: 0.110679\tGrad Norm: 0.296611\tLR: 0.349749\n",
      "Train Epoch: 53 [249856/259852 (95%)]\tLoss: 0.115700\tGrad Norm: 0.370691\tLR: 0.349749\n",
      "Train set: Average loss: 0.1152\n",
      "Test set: Average loss: 0.1388, Average MAE: 0.2868\n",
      "Train Epoch: 54 [4096/259852 (2%)]\tLoss: 0.132702\tGrad Norm: 0.630543\tLR: 0.342754\n",
      "Train Epoch: 54 [24576/259852 (9%)]\tLoss: 0.111084\tGrad Norm: 0.247814\tLR: 0.342754\n",
      "Train Epoch: 54 [45056/259852 (17%)]\tLoss: 0.109956\tGrad Norm: 0.257060\tLR: 0.342754\n",
      "Train Epoch: 54 [65536/259852 (25%)]\tLoss: 0.108752\tGrad Norm: 0.263100\tLR: 0.342754\n",
      "Train Epoch: 54 [86016/259852 (33%)]\tLoss: 0.111158\tGrad Norm: 0.329220\tLR: 0.342754\n",
      "Train Epoch: 54 [106496/259852 (41%)]\tLoss: 0.112831\tGrad Norm: 0.272087\tLR: 0.342754\n",
      "Train Epoch: 54 [126976/259852 (48%)]\tLoss: 0.110540\tGrad Norm: 0.295546\tLR: 0.342754\n",
      "Train Epoch: 54 [147456/259852 (56%)]\tLoss: 0.110145\tGrad Norm: 0.316664\tLR: 0.342754\n",
      "Train Epoch: 54 [167936/259852 (64%)]\tLoss: 0.115680\tGrad Norm: 0.354534\tLR: 0.342754\n",
      "Train Epoch: 54 [188416/259852 (72%)]\tLoss: 0.109777\tGrad Norm: 0.205172\tLR: 0.342754\n",
      "Train Epoch: 54 [208896/259852 (80%)]\tLoss: 0.107870\tGrad Norm: 0.235317\tLR: 0.342754\n",
      "Train Epoch: 54 [229376/259852 (88%)]\tLoss: 0.111281\tGrad Norm: 0.269884\tLR: 0.342754\n",
      "Train Epoch: 54 [249856/259852 (95%)]\tLoss: 0.111437\tGrad Norm: 0.342829\tLR: 0.342754\n",
      "Train set: Average loss: 0.1118\n",
      "Test set: Average loss: 0.1176, Average MAE: 0.2507\n",
      "Train Epoch: 55 [4096/259852 (2%)]\tLoss: 0.109975\tGrad Norm: 0.341429\tLR: 0.335899\n",
      "Train Epoch: 55 [24576/259852 (9%)]\tLoss: 0.114411\tGrad Norm: 0.433517\tLR: 0.335899\n",
      "Train Epoch: 55 [45056/259852 (17%)]\tLoss: 0.113667\tGrad Norm: 0.324036\tLR: 0.335899\n",
      "Train Epoch: 55 [65536/259852 (25%)]\tLoss: 0.106835\tGrad Norm: 0.267784\tLR: 0.335899\n",
      "Train Epoch: 55 [86016/259852 (33%)]\tLoss: 0.111584\tGrad Norm: 0.386671\tLR: 0.335899\n",
      "Train Epoch: 55 [106496/259852 (41%)]\tLoss: 0.109706\tGrad Norm: 0.218631\tLR: 0.335899\n",
      "Train Epoch: 55 [126976/259852 (48%)]\tLoss: 0.106311\tGrad Norm: 0.207222\tLR: 0.335899\n",
      "Train Epoch: 55 [147456/259852 (56%)]\tLoss: 0.110282\tGrad Norm: 0.301466\tLR: 0.335899\n",
      "Train Epoch: 55 [167936/259852 (64%)]\tLoss: 0.107915\tGrad Norm: 0.241155\tLR: 0.335899\n",
      "Train Epoch: 55 [188416/259852 (72%)]\tLoss: 0.116880\tGrad Norm: 0.450597\tLR: 0.335899\n",
      "Train Epoch: 55 [208896/259852 (80%)]\tLoss: 0.110871\tGrad Norm: 0.311948\tLR: 0.335899\n",
      "Train Epoch: 55 [229376/259852 (88%)]\tLoss: 0.110248\tGrad Norm: 0.277656\tLR: 0.335899\n",
      "Train Epoch: 55 [249856/259852 (95%)]\tLoss: 0.108015\tGrad Norm: 0.322396\tLR: 0.335899\n",
      "Train set: Average loss: 0.1105\n",
      "Test set: Average loss: 0.1194, Average MAE: 0.2480\n",
      "Epoch 55: Mean reward = 0.046 +/- 0.040\n",
      "Train Epoch: 56 [4096/259852 (2%)]\tLoss: 0.114360\tGrad Norm: 0.423155\tLR: 0.329181\n",
      "Train Epoch: 56 [24576/259852 (9%)]\tLoss: 0.108051\tGrad Norm: 0.317624\tLR: 0.329181\n",
      "Train Epoch: 56 [45056/259852 (17%)]\tLoss: 0.107723\tGrad Norm: 0.271800\tLR: 0.329181\n",
      "Train Epoch: 56 [65536/259852 (25%)]\tLoss: 0.108054\tGrad Norm: 0.402446\tLR: 0.329181\n",
      "Train Epoch: 56 [86016/259852 (33%)]\tLoss: 0.114585\tGrad Norm: 0.349725\tLR: 0.329181\n",
      "Train Epoch: 56 [106496/259852 (41%)]\tLoss: 0.106578\tGrad Norm: 0.251982\tLR: 0.329181\n",
      "Train Epoch: 56 [126976/259852 (48%)]\tLoss: 0.108000\tGrad Norm: 0.398162\tLR: 0.329181\n",
      "Train Epoch: 56 [147456/259852 (56%)]\tLoss: 0.107558\tGrad Norm: 0.273176\tLR: 0.329181\n",
      "Train Epoch: 56 [167936/259852 (64%)]\tLoss: 0.106697\tGrad Norm: 0.287234\tLR: 0.329181\n",
      "Train Epoch: 56 [188416/259852 (72%)]\tLoss: 0.110865\tGrad Norm: 0.330551\tLR: 0.329181\n",
      "Train Epoch: 56 [208896/259852 (80%)]\tLoss: 0.108939\tGrad Norm: 0.314594\tLR: 0.329181\n",
      "Train Epoch: 56 [229376/259852 (88%)]\tLoss: 0.104227\tGrad Norm: 0.269008\tLR: 0.329181\n",
      "Train Epoch: 56 [249856/259852 (95%)]\tLoss: 0.112913\tGrad Norm: 0.472212\tLR: 0.329181\n",
      "Train set: Average loss: 0.1096\n",
      "Test set: Average loss: 0.1146, Average MAE: 0.2452\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 57 [4096/259852 (2%)]\tLoss: 0.108031\tGrad Norm: 0.290862\tLR: 0.322597\n",
      "Train Epoch: 57 [24576/259852 (9%)]\tLoss: 0.103112\tGrad Norm: 0.214087\tLR: 0.322597\n",
      "Train Epoch: 57 [45056/259852 (17%)]\tLoss: 0.106855\tGrad Norm: 0.298074\tLR: 0.322597\n",
      "Train Epoch: 57 [65536/259852 (25%)]\tLoss: 0.109169\tGrad Norm: 0.322450\tLR: 0.322597\n",
      "Train Epoch: 57 [86016/259852 (33%)]\tLoss: 0.101272\tGrad Norm: 0.290490\tLR: 0.322597\n",
      "Train Epoch: 57 [106496/259852 (41%)]\tLoss: 0.110916\tGrad Norm: 0.356882\tLR: 0.322597\n",
      "Train Epoch: 57 [126976/259852 (48%)]\tLoss: 0.108757\tGrad Norm: 0.291184\tLR: 0.322597\n",
      "Train Epoch: 57 [147456/259852 (56%)]\tLoss: 0.110546\tGrad Norm: 0.362863\tLR: 0.322597\n",
      "Train Epoch: 57 [167936/259852 (64%)]\tLoss: 0.110342\tGrad Norm: 0.493662\tLR: 0.322597\n",
      "Train Epoch: 57 [188416/259852 (72%)]\tLoss: 0.108116\tGrad Norm: 0.307834\tLR: 0.322597\n",
      "Train Epoch: 57 [208896/259852 (80%)]\tLoss: 0.109071\tGrad Norm: 0.353403\tLR: 0.322597\n",
      "Train Epoch: 57 [229376/259852 (88%)]\tLoss: 0.111119\tGrad Norm: 0.326784\tLR: 0.322597\n",
      "Train Epoch: 57 [249856/259852 (95%)]\tLoss: 0.106944\tGrad Norm: 0.286395\tLR: 0.322597\n",
      "Train set: Average loss: 0.1074\n",
      "Test set: Average loss: 0.1123, Average MAE: 0.2461\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 58 [4096/259852 (2%)]\tLoss: 0.104319\tGrad Norm: 0.286412\tLR: 0.316145\n",
      "Train Epoch: 58 [24576/259852 (9%)]\tLoss: 0.100015\tGrad Norm: 0.194000\tLR: 0.316145\n",
      "Train Epoch: 58 [45056/259852 (17%)]\tLoss: 0.106774\tGrad Norm: 0.322335\tLR: 0.316145\n",
      "Train Epoch: 58 [65536/259852 (25%)]\tLoss: 0.103482\tGrad Norm: 0.292459\tLR: 0.316145\n",
      "Train Epoch: 58 [86016/259852 (33%)]\tLoss: 0.101826\tGrad Norm: 0.284139\tLR: 0.316145\n",
      "Train Epoch: 58 [106496/259852 (41%)]\tLoss: 0.105120\tGrad Norm: 0.340239\tLR: 0.316145\n",
      "Train Epoch: 58 [126976/259852 (48%)]\tLoss: 0.106160\tGrad Norm: 0.233612\tLR: 0.316145\n",
      "Train Epoch: 58 [147456/259852 (56%)]\tLoss: 0.109837\tGrad Norm: 0.356888\tLR: 0.316145\n",
      "Train Epoch: 58 [167936/259852 (64%)]\tLoss: 0.108413\tGrad Norm: 0.312073\tLR: 0.316145\n",
      "Train Epoch: 58 [188416/259852 (72%)]\tLoss: 0.105635\tGrad Norm: 0.314138\tLR: 0.316145\n",
      "Train Epoch: 58 [208896/259852 (80%)]\tLoss: 0.107626\tGrad Norm: 0.366560\tLR: 0.316145\n",
      "Train Epoch: 58 [229376/259852 (88%)]\tLoss: 0.113703\tGrad Norm: 0.390087\tLR: 0.316145\n",
      "Train Epoch: 58 [249856/259852 (95%)]\tLoss: 0.103988\tGrad Norm: 0.304620\tLR: 0.316145\n",
      "Train set: Average loss: 0.1059\n",
      "Test set: Average loss: 0.1098, Average MAE: 0.2409\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 59 [4096/259852 (2%)]\tLoss: 0.101860\tGrad Norm: 0.260813\tLR: 0.309822\n",
      "Train Epoch: 59 [24576/259852 (9%)]\tLoss: 0.103193\tGrad Norm: 0.276338\tLR: 0.309822\n",
      "Train Epoch: 59 [45056/259852 (17%)]\tLoss: 0.105028\tGrad Norm: 0.224862\tLR: 0.309822\n",
      "Train Epoch: 59 [65536/259852 (25%)]\tLoss: 0.107393\tGrad Norm: 0.292826\tLR: 0.309822\n",
      "Train Epoch: 59 [86016/259852 (33%)]\tLoss: 0.107949\tGrad Norm: 0.424997\tLR: 0.309822\n",
      "Train Epoch: 59 [106496/259852 (41%)]\tLoss: 0.106790\tGrad Norm: 0.355766\tLR: 0.309822\n",
      "Train Epoch: 59 [126976/259852 (48%)]\tLoss: 0.102055\tGrad Norm: 0.246484\tLR: 0.309822\n",
      "Train Epoch: 59 [147456/259852 (56%)]\tLoss: 0.100957\tGrad Norm: 0.274960\tLR: 0.309822\n",
      "Train Epoch: 59 [167936/259852 (64%)]\tLoss: 0.103213\tGrad Norm: 0.335660\tLR: 0.309822\n",
      "Train Epoch: 59 [188416/259852 (72%)]\tLoss: 0.104928\tGrad Norm: 0.324206\tLR: 0.309822\n",
      "Train Epoch: 59 [208896/259852 (80%)]\tLoss: 0.104056\tGrad Norm: 0.297086\tLR: 0.309822\n",
      "Train Epoch: 59 [229376/259852 (88%)]\tLoss: 0.102457\tGrad Norm: 0.227345\tLR: 0.309822\n",
      "Train Epoch: 59 [249856/259852 (95%)]\tLoss: 0.100470\tGrad Norm: 0.249730\tLR: 0.309822\n",
      "Train set: Average loss: 0.1040\n",
      "Test set: Average loss: 0.1102, Average MAE: 0.2412\n",
      "Train Epoch: 60 [4096/259852 (2%)]\tLoss: 0.102012\tGrad Norm: 0.311510\tLR: 0.303626\n",
      "Train Epoch: 60 [24576/259852 (9%)]\tLoss: 0.102758\tGrad Norm: 0.327189\tLR: 0.303626\n",
      "Train Epoch: 60 [45056/259852 (17%)]\tLoss: 0.103751\tGrad Norm: 0.370892\tLR: 0.303626\n",
      "Train Epoch: 60 [65536/259852 (25%)]\tLoss: 0.104001\tGrad Norm: 0.384594\tLR: 0.303626\n",
      "Train Epoch: 60 [86016/259852 (33%)]\tLoss: 0.103673\tGrad Norm: 0.280384\tLR: 0.303626\n",
      "Train Epoch: 60 [106496/259852 (41%)]\tLoss: 0.102604\tGrad Norm: 0.333342\tLR: 0.303626\n",
      "Train Epoch: 60 [126976/259852 (48%)]\tLoss: 0.100452\tGrad Norm: 0.233471\tLR: 0.303626\n",
      "Train Epoch: 60 [147456/259852 (56%)]\tLoss: 0.099315\tGrad Norm: 0.233343\tLR: 0.303626\n",
      "Train Epoch: 60 [167936/259852 (64%)]\tLoss: 0.098763\tGrad Norm: 0.301037\tLR: 0.303626\n",
      "Train Epoch: 60 [188416/259852 (72%)]\tLoss: 0.100765\tGrad Norm: 0.287121\tLR: 0.303626\n",
      "Train Epoch: 60 [208896/259852 (80%)]\tLoss: 0.104285\tGrad Norm: 0.322539\tLR: 0.303626\n",
      "Train Epoch: 60 [229376/259852 (88%)]\tLoss: 0.105190\tGrad Norm: 0.398127\tLR: 0.303626\n",
      "Train Epoch: 60 [249856/259852 (95%)]\tLoss: 0.102279\tGrad Norm: 0.292016\tLR: 0.303626\n",
      "Train set: Average loss: 0.1032\n",
      "Test set: Average loss: 0.1085, Average MAE: 0.2411\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Epoch 60: Mean reward = 0.051 +/- 0.033\n",
      "Train Epoch: 61 [4096/259852 (2%)]\tLoss: 0.098928\tGrad Norm: 0.288435\tLR: 0.297553\n",
      "Train Epoch: 61 [24576/259852 (9%)]\tLoss: 0.096905\tGrad Norm: 0.209559\tLR: 0.297553\n",
      "Train Epoch: 61 [45056/259852 (17%)]\tLoss: 0.098577\tGrad Norm: 0.179584\tLR: 0.297553\n",
      "Train Epoch: 61 [65536/259852 (25%)]\tLoss: 0.100888\tGrad Norm: 0.259141\tLR: 0.297553\n",
      "Train Epoch: 61 [86016/259852 (33%)]\tLoss: 0.105714\tGrad Norm: 0.359934\tLR: 0.297553\n",
      "Train Epoch: 61 [106496/259852 (41%)]\tLoss: 0.101009\tGrad Norm: 0.301772\tLR: 0.297553\n",
      "Train Epoch: 61 [126976/259852 (48%)]\tLoss: 0.102636\tGrad Norm: 0.324236\tLR: 0.297553\n",
      "Train Epoch: 61 [147456/259852 (56%)]\tLoss: 0.103275\tGrad Norm: 0.340001\tLR: 0.297553\n",
      "Train Epoch: 61 [167936/259852 (64%)]\tLoss: 0.102903\tGrad Norm: 0.354331\tLR: 0.297553\n",
      "Train Epoch: 61 [188416/259852 (72%)]\tLoss: 0.097350\tGrad Norm: 0.247506\tLR: 0.297553\n",
      "Train Epoch: 61 [208896/259852 (80%)]\tLoss: 0.102728\tGrad Norm: 0.273482\tLR: 0.297553\n",
      "Train Epoch: 61 [229376/259852 (88%)]\tLoss: 0.103959\tGrad Norm: 0.344521\tLR: 0.297553\n",
      "Train Epoch: 61 [249856/259852 (95%)]\tLoss: 0.099743\tGrad Norm: 0.266064\tLR: 0.297553\n",
      "Train set: Average loss: 0.1014\n",
      "Test set: Average loss: 0.1086, Average MAE: 0.2398\n",
      "Train Epoch: 62 [4096/259852 (2%)]\tLoss: 0.101246\tGrad Norm: 0.292806\tLR: 0.291602\n",
      "Train Epoch: 62 [24576/259852 (9%)]\tLoss: 0.101873\tGrad Norm: 0.304758\tLR: 0.291602\n",
      "Train Epoch: 62 [45056/259852 (17%)]\tLoss: 0.097850\tGrad Norm: 0.240282\tLR: 0.291602\n",
      "Train Epoch: 62 [65536/259852 (25%)]\tLoss: 0.099671\tGrad Norm: 0.290658\tLR: 0.291602\n",
      "Train Epoch: 62 [86016/259852 (33%)]\tLoss: 0.097298\tGrad Norm: 0.277356\tLR: 0.291602\n",
      "Train Epoch: 62 [106496/259852 (41%)]\tLoss: 0.100424\tGrad Norm: 0.260673\tLR: 0.291602\n",
      "Train Epoch: 62 [126976/259852 (48%)]\tLoss: 0.099912\tGrad Norm: 0.302361\tLR: 0.291602\n",
      "Train Epoch: 62 [147456/259852 (56%)]\tLoss: 0.097701\tGrad Norm: 0.292333\tLR: 0.291602\n",
      "Train Epoch: 62 [167936/259852 (64%)]\tLoss: 0.102511\tGrad Norm: 0.269851\tLR: 0.291602\n",
      "Train Epoch: 62 [188416/259852 (72%)]\tLoss: 0.103123\tGrad Norm: 0.382940\tLR: 0.291602\n",
      "Train Epoch: 62 [208896/259852 (80%)]\tLoss: 0.099150\tGrad Norm: 0.270202\tLR: 0.291602\n",
      "Train Epoch: 62 [229376/259852 (88%)]\tLoss: 0.099688\tGrad Norm: 0.320290\tLR: 0.291602\n",
      "Train Epoch: 62 [249856/259852 (95%)]\tLoss: 0.100974\tGrad Norm: 0.386449\tLR: 0.291602\n",
      "Train set: Average loss: 0.1005\n",
      "Test set: Average loss: 0.1104, Average MAE: 0.2409\n",
      "Train Epoch: 63 [4096/259852 (2%)]\tLoss: 0.102080\tGrad Norm: 0.360104\tLR: 0.285770\n",
      "Train Epoch: 63 [24576/259852 (9%)]\tLoss: 0.099656\tGrad Norm: 0.290277\tLR: 0.285770\n",
      "Train Epoch: 63 [45056/259852 (17%)]\tLoss: 0.095553\tGrad Norm: 0.229297\tLR: 0.285770\n",
      "Train Epoch: 63 [65536/259852 (25%)]\tLoss: 0.095314\tGrad Norm: 0.231831\tLR: 0.285770\n",
      "Train Epoch: 63 [86016/259852 (33%)]\tLoss: 0.102268\tGrad Norm: 0.302987\tLR: 0.285770\n",
      "Train Epoch: 63 [106496/259852 (41%)]\tLoss: 0.097519\tGrad Norm: 0.194059\tLR: 0.285770\n",
      "Train Epoch: 63 [126976/259852 (48%)]\tLoss: 0.094746\tGrad Norm: 0.216028\tLR: 0.285770\n",
      "Train Epoch: 63 [147456/259852 (56%)]\tLoss: 0.098860\tGrad Norm: 0.294574\tLR: 0.285770\n",
      "Train Epoch: 63 [167936/259852 (64%)]\tLoss: 0.095981\tGrad Norm: 0.340130\tLR: 0.285770\n",
      "Train Epoch: 63 [188416/259852 (72%)]\tLoss: 0.100443\tGrad Norm: 0.349400\tLR: 0.285770\n",
      "Train Epoch: 63 [208896/259852 (80%)]\tLoss: 0.097966\tGrad Norm: 0.323502\tLR: 0.285770\n",
      "Train Epoch: 63 [229376/259852 (88%)]\tLoss: 0.096932\tGrad Norm: 0.244336\tLR: 0.285770\n",
      "Train Epoch: 63 [249856/259852 (95%)]\tLoss: 0.099798\tGrad Norm: 0.311655\tLR: 0.285770\n",
      "Train set: Average loss: 0.0989\n",
      "Test set: Average loss: 0.1089, Average MAE: 0.2410\n",
      "Train Epoch: 64 [4096/259852 (2%)]\tLoss: 0.100161\tGrad Norm: 0.334413\tLR: 0.280055\n",
      "Train Epoch: 64 [24576/259852 (9%)]\tLoss: 0.096382\tGrad Norm: 0.243384\tLR: 0.280055\n",
      "Train Epoch: 64 [45056/259852 (17%)]\tLoss: 0.092896\tGrad Norm: 0.238972\tLR: 0.280055\n",
      "Train Epoch: 64 [65536/259852 (25%)]\tLoss: 0.102376\tGrad Norm: 0.462235\tLR: 0.280055\n",
      "Train Epoch: 64 [86016/259852 (33%)]\tLoss: 0.101078\tGrad Norm: 0.360758\tLR: 0.280055\n",
      "Train Epoch: 64 [106496/259852 (41%)]\tLoss: 0.095751\tGrad Norm: 0.288453\tLR: 0.280055\n",
      "Train Epoch: 64 [126976/259852 (48%)]\tLoss: 0.099300\tGrad Norm: 0.334561\tLR: 0.280055\n",
      "Train Epoch: 64 [147456/259852 (56%)]\tLoss: 0.099479\tGrad Norm: 0.279579\tLR: 0.280055\n",
      "Train Epoch: 64 [167936/259852 (64%)]\tLoss: 0.095898\tGrad Norm: 0.279563\tLR: 0.280055\n",
      "Train Epoch: 64 [188416/259852 (72%)]\tLoss: 0.096714\tGrad Norm: 0.227766\tLR: 0.280055\n",
      "Train Epoch: 64 [208896/259852 (80%)]\tLoss: 0.095917\tGrad Norm: 0.275410\tLR: 0.280055\n",
      "Train Epoch: 64 [229376/259852 (88%)]\tLoss: 0.098753\tGrad Norm: 0.291049\tLR: 0.280055\n",
      "Train Epoch: 64 [249856/259852 (95%)]\tLoss: 0.095224\tGrad Norm: 0.224202\tLR: 0.280055\n",
      "Train set: Average loss: 0.0981\n",
      "Test set: Average loss: 0.1052, Average MAE: 0.2359\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 65 [4096/259852 (2%)]\tLoss: 0.096426\tGrad Norm: 0.300641\tLR: 0.274454\n",
      "Train Epoch: 65 [24576/259852 (9%)]\tLoss: 0.094737\tGrad Norm: 0.236908\tLR: 0.274454\n",
      "Train Epoch: 65 [45056/259852 (17%)]\tLoss: 0.095903\tGrad Norm: 0.283866\tLR: 0.274454\n",
      "Train Epoch: 65 [65536/259852 (25%)]\tLoss: 0.101415\tGrad Norm: 0.367078\tLR: 0.274454\n",
      "Train Epoch: 65 [86016/259852 (33%)]\tLoss: 0.096338\tGrad Norm: 0.308533\tLR: 0.274454\n",
      "Train Epoch: 65 [106496/259852 (41%)]\tLoss: 0.092899\tGrad Norm: 0.223280\tLR: 0.274454\n",
      "Train Epoch: 65 [126976/259852 (48%)]\tLoss: 0.097963\tGrad Norm: 0.309920\tLR: 0.274454\n",
      "Train Epoch: 65 [147456/259852 (56%)]\tLoss: 0.095406\tGrad Norm: 0.276673\tLR: 0.274454\n",
      "Train Epoch: 65 [167936/259852 (64%)]\tLoss: 0.100672\tGrad Norm: 0.431488\tLR: 0.274454\n",
      "Train Epoch: 65 [188416/259852 (72%)]\tLoss: 0.100950\tGrad Norm: 0.307577\tLR: 0.274454\n",
      "Train Epoch: 65 [208896/259852 (80%)]\tLoss: 0.095506\tGrad Norm: 0.204029\tLR: 0.274454\n",
      "Train Epoch: 65 [229376/259852 (88%)]\tLoss: 0.100305\tGrad Norm: 0.378462\tLR: 0.274454\n",
      "Train Epoch: 65 [249856/259852 (95%)]\tLoss: 0.098112\tGrad Norm: 0.347209\tLR: 0.274454\n",
      "Train set: Average loss: 0.0975\n",
      "Test set: Average loss: 0.1047, Average MAE: 0.2323\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Epoch 65: Mean reward = 0.047 +/- 0.038\n",
      "Train Epoch: 66 [4096/259852 (2%)]\tLoss: 0.097011\tGrad Norm: 0.286209\tLR: 0.268964\n",
      "Train Epoch: 66 [24576/259852 (9%)]\tLoss: 0.096990\tGrad Norm: 0.305717\tLR: 0.268964\n",
      "Train Epoch: 66 [45056/259852 (17%)]\tLoss: 0.095937\tGrad Norm: 0.314828\tLR: 0.268964\n",
      "Train Epoch: 66 [65536/259852 (25%)]\tLoss: 0.094423\tGrad Norm: 0.280116\tLR: 0.268964\n",
      "Train Epoch: 66 [86016/259852 (33%)]\tLoss: 0.095842\tGrad Norm: 0.243398\tLR: 0.268964\n",
      "Train Epoch: 66 [106496/259852 (41%)]\tLoss: 0.096359\tGrad Norm: 0.245020\tLR: 0.268964\n",
      "Train Epoch: 66 [126976/259852 (48%)]\tLoss: 0.095726\tGrad Norm: 0.230607\tLR: 0.268964\n",
      "Train Epoch: 66 [147456/259852 (56%)]\tLoss: 0.092325\tGrad Norm: 0.219130\tLR: 0.268964\n",
      "Train Epoch: 66 [167936/259852 (64%)]\tLoss: 0.097616\tGrad Norm: 0.266159\tLR: 0.268964\n",
      "Train Epoch: 66 [188416/259852 (72%)]\tLoss: 0.096319\tGrad Norm: 0.278346\tLR: 0.268964\n",
      "Train Epoch: 66 [208896/259852 (80%)]\tLoss: 0.096121\tGrad Norm: 0.308154\tLR: 0.268964\n",
      "Train Epoch: 66 [229376/259852 (88%)]\tLoss: 0.095099\tGrad Norm: 0.314592\tLR: 0.268964\n",
      "Train Epoch: 66 [249856/259852 (95%)]\tLoss: 0.096492\tGrad Norm: 0.352882\tLR: 0.268964\n",
      "Train set: Average loss: 0.0953\n",
      "Test set: Average loss: 0.1073, Average MAE: 0.2418\n",
      "Train Epoch: 67 [4096/259852 (2%)]\tLoss: 0.099103\tGrad Norm: 0.372449\tLR: 0.263585\n",
      "Train Epoch: 67 [24576/259852 (9%)]\tLoss: 0.093817\tGrad Norm: 0.292504\tLR: 0.263585\n",
      "Train Epoch: 67 [45056/259852 (17%)]\tLoss: 0.091676\tGrad Norm: 0.299649\tLR: 0.263585\n",
      "Train Epoch: 67 [65536/259852 (25%)]\tLoss: 0.096303\tGrad Norm: 0.271520\tLR: 0.263585\n",
      "Train Epoch: 67 [86016/259852 (33%)]\tLoss: 0.091151\tGrad Norm: 0.217952\tLR: 0.263585\n",
      "Train Epoch: 67 [106496/259852 (41%)]\tLoss: 0.091658\tGrad Norm: 0.241540\tLR: 0.263585\n",
      "Train Epoch: 67 [126976/259852 (48%)]\tLoss: 0.098493\tGrad Norm: 0.361028\tLR: 0.263585\n",
      "Train Epoch: 67 [147456/259852 (56%)]\tLoss: 0.099913\tGrad Norm: 0.377767\tLR: 0.263585\n",
      "Train Epoch: 67 [167936/259852 (64%)]\tLoss: 0.093287\tGrad Norm: 0.265146\tLR: 0.263585\n",
      "Train Epoch: 67 [188416/259852 (72%)]\tLoss: 0.092584\tGrad Norm: 0.229538\tLR: 0.263585\n",
      "Train Epoch: 67 [208896/259852 (80%)]\tLoss: 0.099041\tGrad Norm: 0.425314\tLR: 0.263585\n",
      "Train Epoch: 67 [229376/259852 (88%)]\tLoss: 0.102765\tGrad Norm: 0.553192\tLR: 0.263585\n",
      "Train Epoch: 67 [249856/259852 (95%)]\tLoss: 0.098096\tGrad Norm: 0.322538\tLR: 0.263585\n",
      "Train set: Average loss: 0.0956\n",
      "Test set: Average loss: 0.1029, Average MAE: 0.2339\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 68 [4096/259852 (2%)]\tLoss: 0.093626\tGrad Norm: 0.301786\tLR: 0.258313\n",
      "Train Epoch: 68 [24576/259852 (9%)]\tLoss: 0.091374\tGrad Norm: 0.232531\tLR: 0.258313\n",
      "Train Epoch: 68 [45056/259852 (17%)]\tLoss: 0.094486\tGrad Norm: 0.292723\tLR: 0.258313\n",
      "Train Epoch: 68 [65536/259852 (25%)]\tLoss: 0.094951\tGrad Norm: 0.302320\tLR: 0.258313\n",
      "Train Epoch: 68 [86016/259852 (33%)]\tLoss: 0.095589\tGrad Norm: 0.364318\tLR: 0.258313\n",
      "Train Epoch: 68 [106496/259852 (41%)]\tLoss: 0.092287\tGrad Norm: 0.279076\tLR: 0.258313\n",
      "Train Epoch: 68 [126976/259852 (48%)]\tLoss: 0.091914\tGrad Norm: 0.320234\tLR: 0.258313\n",
      "Train Epoch: 68 [147456/259852 (56%)]\tLoss: 0.094020\tGrad Norm: 0.289220\tLR: 0.258313\n",
      "Train Epoch: 68 [167936/259852 (64%)]\tLoss: 0.097956\tGrad Norm: 0.388269\tLR: 0.258313\n",
      "Train Epoch: 68 [188416/259852 (72%)]\tLoss: 0.095594\tGrad Norm: 0.290256\tLR: 0.258313\n",
      "Train Epoch: 68 [208896/259852 (80%)]\tLoss: 0.093340\tGrad Norm: 0.231589\tLR: 0.258313\n",
      "Train Epoch: 68 [229376/259852 (88%)]\tLoss: 0.091751\tGrad Norm: 0.222584\tLR: 0.258313\n",
      "Train Epoch: 68 [249856/259852 (95%)]\tLoss: 0.091869\tGrad Norm: 0.255180\tLR: 0.258313\n",
      "Train set: Average loss: 0.0936\n",
      "Test set: Average loss: 0.1020, Average MAE: 0.2315\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 69 [4096/259852 (2%)]\tLoss: 0.095526\tGrad Norm: 0.296284\tLR: 0.253147\n",
      "Train Epoch: 69 [24576/259852 (9%)]\tLoss: 0.092612\tGrad Norm: 0.297042\tLR: 0.253147\n",
      "Train Epoch: 69 [45056/259852 (17%)]\tLoss: 0.093474\tGrad Norm: 0.283689\tLR: 0.253147\n",
      "Train Epoch: 69 [65536/259852 (25%)]\tLoss: 0.091101\tGrad Norm: 0.276161\tLR: 0.253147\n",
      "Train Epoch: 69 [86016/259852 (33%)]\tLoss: 0.092617\tGrad Norm: 0.223917\tLR: 0.253147\n",
      "Train Epoch: 69 [106496/259852 (41%)]\tLoss: 0.091353\tGrad Norm: 0.215151\tLR: 0.253147\n",
      "Train Epoch: 69 [126976/259852 (48%)]\tLoss: 0.091840\tGrad Norm: 0.271411\tLR: 0.253147\n",
      "Train Epoch: 69 [147456/259852 (56%)]\tLoss: 0.095042\tGrad Norm: 0.325566\tLR: 0.253147\n",
      "Train Epoch: 69 [167936/259852 (64%)]\tLoss: 0.092391\tGrad Norm: 0.299059\tLR: 0.253147\n",
      "Train Epoch: 69 [188416/259852 (72%)]\tLoss: 0.091056\tGrad Norm: 0.286611\tLR: 0.253147\n",
      "Train Epoch: 69 [208896/259852 (80%)]\tLoss: 0.094624\tGrad Norm: 0.281770\tLR: 0.253147\n",
      "Train Epoch: 69 [229376/259852 (88%)]\tLoss: 0.098040\tGrad Norm: 0.431581\tLR: 0.253147\n",
      "Train Epoch: 69 [249856/259852 (95%)]\tLoss: 0.092189\tGrad Norm: 0.321540\tLR: 0.253147\n",
      "Train set: Average loss: 0.0930\n",
      "Test set: Average loss: 0.1033, Average MAE: 0.2314\n",
      "Train Epoch: 70 [4096/259852 (2%)]\tLoss: 0.092595\tGrad Norm: 0.326661\tLR: 0.248084\n",
      "Train Epoch: 70 [24576/259852 (9%)]\tLoss: 0.090648\tGrad Norm: 0.248724\tLR: 0.248084\n",
      "Train Epoch: 70 [45056/259852 (17%)]\tLoss: 0.091461\tGrad Norm: 0.236828\tLR: 0.248084\n",
      "Train Epoch: 70 [65536/259852 (25%)]\tLoss: 0.089873\tGrad Norm: 0.252009\tLR: 0.248084\n",
      "Train Epoch: 70 [86016/259852 (33%)]\tLoss: 0.088225\tGrad Norm: 0.252095\tLR: 0.248084\n",
      "Train Epoch: 70 [106496/259852 (41%)]\tLoss: 0.092440\tGrad Norm: 0.263164\tLR: 0.248084\n",
      "Train Epoch: 70 [126976/259852 (48%)]\tLoss: 0.091988\tGrad Norm: 0.287651\tLR: 0.248084\n",
      "Train Epoch: 70 [147456/259852 (56%)]\tLoss: 0.093392\tGrad Norm: 0.349086\tLR: 0.248084\n",
      "Train Epoch: 70 [167936/259852 (64%)]\tLoss: 0.090979\tGrad Norm: 0.229284\tLR: 0.248084\n",
      "Train Epoch: 70 [188416/259852 (72%)]\tLoss: 0.094179\tGrad Norm: 0.394468\tLR: 0.248084\n",
      "Train Epoch: 70 [208896/259852 (80%)]\tLoss: 0.096256\tGrad Norm: 0.389531\tLR: 0.248084\n",
      "Train Epoch: 70 [229376/259852 (88%)]\tLoss: 0.096517\tGrad Norm: 0.317352\tLR: 0.248084\n",
      "Train Epoch: 70 [249856/259852 (95%)]\tLoss: 0.096458\tGrad Norm: 0.285149\tLR: 0.248084\n",
      "Train set: Average loss: 0.0921\n",
      "Test set: Average loss: 0.1001, Average MAE: 0.2298\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Epoch 70: Mean reward = 0.055 +/- 0.046\n",
      "Train Epoch: 71 [4096/259852 (2%)]\tLoss: 0.091496\tGrad Norm: 0.257228\tLR: 0.243123\n",
      "Train Epoch: 71 [24576/259852 (9%)]\tLoss: 0.090333\tGrad Norm: 0.211549\tLR: 0.243123\n",
      "Train Epoch: 71 [45056/259852 (17%)]\tLoss: 0.090170\tGrad Norm: 0.192145\tLR: 0.243123\n",
      "Train Epoch: 71 [65536/259852 (25%)]\tLoss: 0.091190\tGrad Norm: 0.443444\tLR: 0.243123\n",
      "Train Epoch: 71 [86016/259852 (33%)]\tLoss: 0.089025\tGrad Norm: 0.258473\tLR: 0.243123\n",
      "Train Epoch: 71 [106496/259852 (41%)]\tLoss: 0.091414\tGrad Norm: 0.301666\tLR: 0.243123\n",
      "Train Epoch: 71 [126976/259852 (48%)]\tLoss: 0.090138\tGrad Norm: 0.320670\tLR: 0.243123\n",
      "Train Epoch: 71 [147456/259852 (56%)]\tLoss: 0.090320\tGrad Norm: 0.284017\tLR: 0.243123\n",
      "Train Epoch: 71 [167936/259852 (64%)]\tLoss: 0.091737\tGrad Norm: 0.427701\tLR: 0.243123\n",
      "Train Epoch: 71 [188416/259852 (72%)]\tLoss: 0.094832\tGrad Norm: 0.307750\tLR: 0.243123\n",
      "Train Epoch: 71 [208896/259852 (80%)]\tLoss: 0.089693\tGrad Norm: 0.251893\tLR: 0.243123\n",
      "Train Epoch: 71 [229376/259852 (88%)]\tLoss: 0.091668\tGrad Norm: 0.269065\tLR: 0.243123\n",
      "Train Epoch: 71 [249856/259852 (95%)]\tLoss: 0.092039\tGrad Norm: 0.324412\tLR: 0.243123\n",
      "Train set: Average loss: 0.0907\n",
      "Test set: Average loss: 0.1005, Average MAE: 0.2278\n",
      "Train Epoch: 72 [4096/259852 (2%)]\tLoss: 0.091024\tGrad Norm: 0.312840\tLR: 0.238260\n",
      "Train Epoch: 72 [24576/259852 (9%)]\tLoss: 0.089054\tGrad Norm: 0.179726\tLR: 0.238260\n",
      "Train Epoch: 72 [45056/259852 (17%)]\tLoss: 0.088020\tGrad Norm: 0.174138\tLR: 0.238260\n",
      "Train Epoch: 72 [65536/259852 (25%)]\tLoss: 0.091094\tGrad Norm: 0.270296\tLR: 0.238260\n",
      "Train Epoch: 72 [86016/259852 (33%)]\tLoss: 0.089202\tGrad Norm: 0.290209\tLR: 0.238260\n",
      "Train Epoch: 72 [106496/259852 (41%)]\tLoss: 0.093094\tGrad Norm: 0.340071\tLR: 0.238260\n",
      "Train Epoch: 72 [126976/259852 (48%)]\tLoss: 0.090978\tGrad Norm: 0.372167\tLR: 0.238260\n",
      "Train Epoch: 72 [147456/259852 (56%)]\tLoss: 0.092150\tGrad Norm: 0.319732\tLR: 0.238260\n",
      "Train Epoch: 72 [167936/259852 (64%)]\tLoss: 0.091195\tGrad Norm: 0.315719\tLR: 0.238260\n",
      "Train Epoch: 72 [188416/259852 (72%)]\tLoss: 0.091298\tGrad Norm: 0.285917\tLR: 0.238260\n",
      "Train Epoch: 72 [208896/259852 (80%)]\tLoss: 0.090003\tGrad Norm: 0.310327\tLR: 0.238260\n",
      "Train Epoch: 72 [229376/259852 (88%)]\tLoss: 0.096204\tGrad Norm: 0.360084\tLR: 0.238260\n",
      "Train Epoch: 72 [249856/259852 (95%)]\tLoss: 0.088280\tGrad Norm: 0.296776\tLR: 0.238260\n",
      "Train set: Average loss: 0.0904\n",
      "Test set: Average loss: 0.1002, Average MAE: 0.2298\n",
      "Train Epoch: 73 [4096/259852 (2%)]\tLoss: 0.087066\tGrad Norm: 0.298028\tLR: 0.233495\n",
      "Train Epoch: 73 [24576/259852 (9%)]\tLoss: 0.089829\tGrad Norm: 0.238160\tLR: 0.233495\n",
      "Train Epoch: 73 [45056/259852 (17%)]\tLoss: 0.088537\tGrad Norm: 0.195943\tLR: 0.233495\n",
      "Train Epoch: 73 [65536/259852 (25%)]\tLoss: 0.084231\tGrad Norm: 0.194427\tLR: 0.233495\n",
      "Train Epoch: 73 [86016/259852 (33%)]\tLoss: 0.084659\tGrad Norm: 0.231867\tLR: 0.233495\n",
      "Train Epoch: 73 [106496/259852 (41%)]\tLoss: 0.089196\tGrad Norm: 0.214174\tLR: 0.233495\n",
      "Train Epoch: 73 [126976/259852 (48%)]\tLoss: 0.088416\tGrad Norm: 0.273875\tLR: 0.233495\n",
      "Train Epoch: 73 [147456/259852 (56%)]\tLoss: 0.091437\tGrad Norm: 0.375877\tLR: 0.233495\n",
      "Train Epoch: 73 [167936/259852 (64%)]\tLoss: 0.084526\tGrad Norm: 0.270984\tLR: 0.233495\n",
      "Train Epoch: 73 [188416/259852 (72%)]\tLoss: 0.085451\tGrad Norm: 0.301057\tLR: 0.233495\n",
      "Train Epoch: 73 [208896/259852 (80%)]\tLoss: 0.089011\tGrad Norm: 0.241093\tLR: 0.233495\n",
      "Train Epoch: 73 [229376/259852 (88%)]\tLoss: 0.088526\tGrad Norm: 0.280503\tLR: 0.233495\n",
      "Train Epoch: 73 [249856/259852 (95%)]\tLoss: 0.088700\tGrad Norm: 0.303225\tLR: 0.233495\n",
      "Train set: Average loss: 0.0887\n",
      "Test set: Average loss: 0.0980, Average MAE: 0.2250\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 74 [4096/259852 (2%)]\tLoss: 0.088732\tGrad Norm: 0.264446\tLR: 0.228825\n",
      "Train Epoch: 74 [24576/259852 (9%)]\tLoss: 0.086461\tGrad Norm: 0.271921\tLR: 0.228825\n",
      "Train Epoch: 74 [45056/259852 (17%)]\tLoss: 0.090188\tGrad Norm: 0.296803\tLR: 0.228825\n",
      "Train Epoch: 74 [65536/259852 (25%)]\tLoss: 0.085197\tGrad Norm: 0.200770\tLR: 0.228825\n",
      "Train Epoch: 74 [86016/259852 (33%)]\tLoss: 0.088576\tGrad Norm: 0.252202\tLR: 0.228825\n",
      "Train Epoch: 74 [106496/259852 (41%)]\tLoss: 0.086851\tGrad Norm: 0.226543\tLR: 0.228825\n",
      "Train Epoch: 74 [126976/259852 (48%)]\tLoss: 0.089638\tGrad Norm: 0.275831\tLR: 0.228825\n",
      "Train Epoch: 74 [147456/259852 (56%)]\tLoss: 0.094762\tGrad Norm: 0.403256\tLR: 0.228825\n",
      "Train Epoch: 74 [167936/259852 (64%)]\tLoss: 0.089502\tGrad Norm: 0.317281\tLR: 0.228825\n",
      "Train Epoch: 74 [188416/259852 (72%)]\tLoss: 0.090372\tGrad Norm: 0.307236\tLR: 0.228825\n",
      "Train Epoch: 74 [208896/259852 (80%)]\tLoss: 0.089947\tGrad Norm: 0.323359\tLR: 0.228825\n",
      "Train Epoch: 74 [229376/259852 (88%)]\tLoss: 0.089519\tGrad Norm: 0.303701\tLR: 0.228825\n",
      "Train Epoch: 74 [249856/259852 (95%)]\tLoss: 0.087832\tGrad Norm: 0.278090\tLR: 0.228825\n",
      "Train set: Average loss: 0.0882\n",
      "Test set: Average loss: 0.1011, Average MAE: 0.2269\n",
      "Train Epoch: 75 [4096/259852 (2%)]\tLoss: 0.088884\tGrad Norm: 0.378387\tLR: 0.224249\n",
      "Train Epoch: 75 [24576/259852 (9%)]\tLoss: 0.089171\tGrad Norm: 0.321318\tLR: 0.224249\n",
      "Train Epoch: 75 [45056/259852 (17%)]\tLoss: 0.086237\tGrad Norm: 0.314723\tLR: 0.224249\n",
      "Train Epoch: 75 [65536/259852 (25%)]\tLoss: 0.087483\tGrad Norm: 0.252304\tLR: 0.224249\n",
      "Train Epoch: 75 [86016/259852 (33%)]\tLoss: 0.085307\tGrad Norm: 0.201018\tLR: 0.224249\n",
      "Train Epoch: 75 [106496/259852 (41%)]\tLoss: 0.085577\tGrad Norm: 0.252676\tLR: 0.224249\n",
      "Train Epoch: 75 [126976/259852 (48%)]\tLoss: 0.089533\tGrad Norm: 0.347382\tLR: 0.224249\n",
      "Train Epoch: 75 [147456/259852 (56%)]\tLoss: 0.088514\tGrad Norm: 0.295569\tLR: 0.224249\n",
      "Train Epoch: 75 [167936/259852 (64%)]\tLoss: 0.088820\tGrad Norm: 0.273572\tLR: 0.224249\n",
      "Train Epoch: 75 [188416/259852 (72%)]\tLoss: 0.085483\tGrad Norm: 0.254773\tLR: 0.224249\n",
      "Train Epoch: 75 [208896/259852 (80%)]\tLoss: 0.087049\tGrad Norm: 0.261921\tLR: 0.224249\n",
      "Train Epoch: 75 [229376/259852 (88%)]\tLoss: 0.084322\tGrad Norm: 0.210389\tLR: 0.224249\n",
      "Train Epoch: 75 [249856/259852 (95%)]\tLoss: 0.085485\tGrad Norm: 0.253756\tLR: 0.224249\n",
      "Train set: Average loss: 0.0871\n",
      "Test set: Average loss: 0.0999, Average MAE: 0.2265\n",
      "Epoch 75: Mean reward = 0.075 +/- 0.074\n",
      "Train Epoch: 76 [4096/259852 (2%)]\tLoss: 0.088637\tGrad Norm: 0.374270\tLR: 0.219764\n",
      "Train Epoch: 76 [24576/259852 (9%)]\tLoss: 0.088715\tGrad Norm: 0.320073\tLR: 0.219764\n",
      "Train Epoch: 76 [45056/259852 (17%)]\tLoss: 0.085995\tGrad Norm: 0.234137\tLR: 0.219764\n",
      "Train Epoch: 76 [65536/259852 (25%)]\tLoss: 0.088207\tGrad Norm: 0.333797\tLR: 0.219764\n",
      "Train Epoch: 76 [86016/259852 (33%)]\tLoss: 0.086272\tGrad Norm: 0.325202\tLR: 0.219764\n",
      "Train Epoch: 76 [106496/259852 (41%)]\tLoss: 0.083811\tGrad Norm: 0.248030\tLR: 0.219764\n",
      "Train Epoch: 76 [126976/259852 (48%)]\tLoss: 0.085399\tGrad Norm: 0.313384\tLR: 0.219764\n",
      "Train Epoch: 76 [147456/259852 (56%)]\tLoss: 0.087302\tGrad Norm: 0.322361\tLR: 0.219764\n",
      "Train Epoch: 76 [167936/259852 (64%)]\tLoss: 0.082184\tGrad Norm: 0.206622\tLR: 0.219764\n",
      "Train Epoch: 76 [188416/259852 (72%)]\tLoss: 0.085502\tGrad Norm: 0.246638\tLR: 0.219764\n",
      "Train Epoch: 76 [208896/259852 (80%)]\tLoss: 0.086893\tGrad Norm: 0.298428\tLR: 0.219764\n",
      "Train Epoch: 76 [229376/259852 (88%)]\tLoss: 0.086783\tGrad Norm: 0.243151\tLR: 0.219764\n",
      "Train Epoch: 76 [249856/259852 (95%)]\tLoss: 0.086940\tGrad Norm: 0.290023\tLR: 0.219764\n",
      "Train set: Average loss: 0.0867\n",
      "Test set: Average loss: 0.0974, Average MAE: 0.2244\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 77 [4096/259852 (2%)]\tLoss: 0.086610\tGrad Norm: 0.278286\tLR: 0.215368\n",
      "Train Epoch: 77 [24576/259852 (9%)]\tLoss: 0.086492\tGrad Norm: 0.263992\tLR: 0.215368\n",
      "Train Epoch: 77 [45056/259852 (17%)]\tLoss: 0.085773\tGrad Norm: 0.282929\tLR: 0.215368\n",
      "Train Epoch: 77 [65536/259852 (25%)]\tLoss: 0.085820\tGrad Norm: 0.275602\tLR: 0.215368\n",
      "Train Epoch: 77 [86016/259852 (33%)]\tLoss: 0.085595\tGrad Norm: 0.259137\tLR: 0.215368\n",
      "Train Epoch: 77 [106496/259852 (41%)]\tLoss: 0.086119\tGrad Norm: 0.311209\tLR: 0.215368\n",
      "Train Epoch: 77 [126976/259852 (48%)]\tLoss: 0.086717\tGrad Norm: 0.298957\tLR: 0.215368\n",
      "Train Epoch: 77 [147456/259852 (56%)]\tLoss: 0.084392\tGrad Norm: 0.303037\tLR: 0.215368\n",
      "Train Epoch: 77 [167936/259852 (64%)]\tLoss: 0.085766\tGrad Norm: 0.281835\tLR: 0.215368\n",
      "Train Epoch: 77 [188416/259852 (72%)]\tLoss: 0.083163\tGrad Norm: 0.281721\tLR: 0.215368\n",
      "Train Epoch: 77 [208896/259852 (80%)]\tLoss: 0.086896\tGrad Norm: 0.311152\tLR: 0.215368\n",
      "Train Epoch: 77 [229376/259852 (88%)]\tLoss: 0.087657\tGrad Norm: 0.320176\tLR: 0.215368\n",
      "Train Epoch: 77 [249856/259852 (95%)]\tLoss: 0.086637\tGrad Norm: 0.323582\tLR: 0.215368\n",
      "Train set: Average loss: 0.0860\n",
      "Test set: Average loss: 0.0986, Average MAE: 0.2257\n",
      "Train Epoch: 78 [4096/259852 (2%)]\tLoss: 0.085891\tGrad Norm: 0.355996\tLR: 0.211061\n",
      "Train Epoch: 78 [24576/259852 (9%)]\tLoss: 0.084371\tGrad Norm: 0.274038\tLR: 0.211061\n",
      "Train Epoch: 78 [45056/259852 (17%)]\tLoss: 0.082876\tGrad Norm: 0.241657\tLR: 0.211061\n",
      "Train Epoch: 78 [65536/259852 (25%)]\tLoss: 0.083217\tGrad Norm: 0.224383\tLR: 0.211061\n",
      "Train Epoch: 78 [86016/259852 (33%)]\tLoss: 0.082231\tGrad Norm: 0.217846\tLR: 0.211061\n",
      "Train Epoch: 78 [106496/259852 (41%)]\tLoss: 0.083871\tGrad Norm: 0.196796\tLR: 0.211061\n",
      "Train Epoch: 78 [126976/259852 (48%)]\tLoss: 0.080973\tGrad Norm: 0.233249\tLR: 0.211061\n",
      "Train Epoch: 78 [147456/259852 (56%)]\tLoss: 0.085862\tGrad Norm: 0.280438\tLR: 0.211061\n",
      "Train Epoch: 78 [167936/259852 (64%)]\tLoss: 0.085398\tGrad Norm: 0.290810\tLR: 0.211061\n",
      "Train Epoch: 78 [188416/259852 (72%)]\tLoss: 0.082511\tGrad Norm: 0.229344\tLR: 0.211061\n",
      "Train Epoch: 78 [208896/259852 (80%)]\tLoss: 0.081404\tGrad Norm: 0.188438\tLR: 0.211061\n",
      "Train Epoch: 78 [229376/259852 (88%)]\tLoss: 0.087789\tGrad Norm: 0.342637\tLR: 0.211061\n",
      "Train Epoch: 78 [249856/259852 (95%)]\tLoss: 0.086515\tGrad Norm: 0.367319\tLR: 0.211061\n",
      "Train set: Average loss: 0.0846\n",
      "Test set: Average loss: 0.0978, Average MAE: 0.2294\n",
      "Train Epoch: 79 [4096/259852 (2%)]\tLoss: 0.086396\tGrad Norm: 0.339643\tLR: 0.206840\n",
      "Train Epoch: 79 [24576/259852 (9%)]\tLoss: 0.085156\tGrad Norm: 0.305132\tLR: 0.206840\n",
      "Train Epoch: 79 [45056/259852 (17%)]\tLoss: 0.084585\tGrad Norm: 0.274787\tLR: 0.206840\n",
      "Train Epoch: 79 [65536/259852 (25%)]\tLoss: 0.079927\tGrad Norm: 0.219243\tLR: 0.206840\n",
      "Train Epoch: 79 [86016/259852 (33%)]\tLoss: 0.083729\tGrad Norm: 0.257375\tLR: 0.206840\n",
      "Train Epoch: 79 [106496/259852 (41%)]\tLoss: 0.085422\tGrad Norm: 0.288440\tLR: 0.206840\n",
      "Train Epoch: 79 [126976/259852 (48%)]\tLoss: 0.082468\tGrad Norm: 0.229190\tLR: 0.206840\n",
      "Train Epoch: 79 [147456/259852 (56%)]\tLoss: 0.083570\tGrad Norm: 0.231408\tLR: 0.206840\n",
      "Train Epoch: 79 [167936/259852 (64%)]\tLoss: 0.084905\tGrad Norm: 0.282309\tLR: 0.206840\n",
      "Train Epoch: 79 [188416/259852 (72%)]\tLoss: 0.084968\tGrad Norm: 0.270063\tLR: 0.206840\n",
      "Train Epoch: 79 [208896/259852 (80%)]\tLoss: 0.081335\tGrad Norm: 0.211310\tLR: 0.206840\n",
      "Train Epoch: 79 [229376/259852 (88%)]\tLoss: 0.083798\tGrad Norm: 0.218677\tLR: 0.206840\n",
      "Train Epoch: 79 [249856/259852 (95%)]\tLoss: 0.081833\tGrad Norm: 0.159971\tLR: 0.206840\n",
      "Train set: Average loss: 0.0836\n",
      "Test set: Average loss: 0.0946, Average MAE: 0.2224\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 80 [4096/259852 (2%)]\tLoss: 0.080667\tGrad Norm: 0.241237\tLR: 0.202703\n",
      "Train Epoch: 80 [24576/259852 (9%)]\tLoss: 0.081585\tGrad Norm: 0.238401\tLR: 0.202703\n",
      "Train Epoch: 80 [45056/259852 (17%)]\tLoss: 0.083021\tGrad Norm: 0.263892\tLR: 0.202703\n",
      "Train Epoch: 80 [65536/259852 (25%)]\tLoss: 0.085872\tGrad Norm: 0.340495\tLR: 0.202703\n",
      "Train Epoch: 80 [86016/259852 (33%)]\tLoss: 0.088267\tGrad Norm: 0.381493\tLR: 0.202703\n",
      "Train Epoch: 80 [106496/259852 (41%)]\tLoss: 0.083250\tGrad Norm: 0.312344\tLR: 0.202703\n",
      "Train Epoch: 80 [126976/259852 (48%)]\tLoss: 0.083251\tGrad Norm: 0.290409\tLR: 0.202703\n",
      "Train Epoch: 80 [147456/259852 (56%)]\tLoss: 0.082674\tGrad Norm: 0.263987\tLR: 0.202703\n",
      "Train Epoch: 80 [167936/259852 (64%)]\tLoss: 0.084866\tGrad Norm: 0.314748\tLR: 0.202703\n",
      "Train Epoch: 80 [188416/259852 (72%)]\tLoss: 0.083273\tGrad Norm: 0.308825\tLR: 0.202703\n",
      "Train Epoch: 80 [208896/259852 (80%)]\tLoss: 0.084340\tGrad Norm: 0.338341\tLR: 0.202703\n",
      "Train Epoch: 80 [229376/259852 (88%)]\tLoss: 0.084095\tGrad Norm: 0.303154\tLR: 0.202703\n",
      "Train Epoch: 80 [249856/259852 (95%)]\tLoss: 0.080392\tGrad Norm: 0.236290\tLR: 0.202703\n",
      "Train set: Average loss: 0.0838\n",
      "Test set: Average loss: 0.0941, Average MAE: 0.2223\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Epoch 80: Mean reward = 0.077 +/- 0.075\n",
      "Train Epoch: 81 [4096/259852 (2%)]\tLoss: 0.083035\tGrad Norm: 0.232368\tLR: 0.198649\n",
      "Train Epoch: 81 [24576/259852 (9%)]\tLoss: 0.082656\tGrad Norm: 0.295718\tLR: 0.198649\n",
      "Train Epoch: 81 [45056/259852 (17%)]\tLoss: 0.082119\tGrad Norm: 0.222922\tLR: 0.198649\n",
      "Train Epoch: 81 [65536/259852 (25%)]\tLoss: 0.083948\tGrad Norm: 0.315985\tLR: 0.198649\n",
      "Train Epoch: 81 [86016/259852 (33%)]\tLoss: 0.081803\tGrad Norm: 0.280649\tLR: 0.198649\n",
      "Train Epoch: 81 [106496/259852 (41%)]\tLoss: 0.081427\tGrad Norm: 0.283315\tLR: 0.198649\n",
      "Train Epoch: 81 [126976/259852 (48%)]\tLoss: 0.080595\tGrad Norm: 0.276000\tLR: 0.198649\n",
      "Train Epoch: 81 [147456/259852 (56%)]\tLoss: 0.084123\tGrad Norm: 0.271565\tLR: 0.198649\n",
      "Train Epoch: 81 [167936/259852 (64%)]\tLoss: 0.084258\tGrad Norm: 0.275089\tLR: 0.198649\n",
      "Train Epoch: 81 [188416/259852 (72%)]\tLoss: 0.083175\tGrad Norm: 0.313023\tLR: 0.198649\n",
      "Train Epoch: 81 [208896/259852 (80%)]\tLoss: 0.084272\tGrad Norm: 0.316307\tLR: 0.198649\n",
      "Train Epoch: 81 [229376/259852 (88%)]\tLoss: 0.084928\tGrad Norm: 0.342098\tLR: 0.198649\n",
      "Train Epoch: 81 [249856/259852 (95%)]\tLoss: 0.082965\tGrad Norm: 0.296475\tLR: 0.198649\n",
      "Train set: Average loss: 0.0829\n",
      "Test set: Average loss: 0.0952, Average MAE: 0.2205\n",
      "Train Epoch: 82 [4096/259852 (2%)]\tLoss: 0.079895\tGrad Norm: 0.333855\tLR: 0.194676\n",
      "Train Epoch: 82 [24576/259852 (9%)]\tLoss: 0.080720\tGrad Norm: 0.212015\tLR: 0.194676\n",
      "Train Epoch: 82 [45056/259852 (17%)]\tLoss: 0.079082\tGrad Norm: 0.187723\tLR: 0.194676\n",
      "Train Epoch: 82 [65536/259852 (25%)]\tLoss: 0.081157\tGrad Norm: 0.247786\tLR: 0.194676\n",
      "Train Epoch: 82 [86016/259852 (33%)]\tLoss: 0.082575\tGrad Norm: 0.287076\tLR: 0.194676\n",
      "Train Epoch: 82 [106496/259852 (41%)]\tLoss: 0.080775\tGrad Norm: 0.260802\tLR: 0.194676\n",
      "Train Epoch: 82 [126976/259852 (48%)]\tLoss: 0.079560\tGrad Norm: 0.228514\tLR: 0.194676\n",
      "Train Epoch: 82 [147456/259852 (56%)]\tLoss: 0.078751\tGrad Norm: 0.189130\tLR: 0.194676\n",
      "Train Epoch: 82 [167936/259852 (64%)]\tLoss: 0.080846\tGrad Norm: 0.229354\tLR: 0.194676\n",
      "Train Epoch: 82 [188416/259852 (72%)]\tLoss: 0.082621\tGrad Norm: 0.235656\tLR: 0.194676\n",
      "Train Epoch: 82 [208896/259852 (80%)]\tLoss: 0.085292\tGrad Norm: 0.340255\tLR: 0.194676\n",
      "Train Epoch: 82 [229376/259852 (88%)]\tLoss: 0.083998\tGrad Norm: 0.293339\tLR: 0.194676\n",
      "Train Epoch: 82 [249856/259852 (95%)]\tLoss: 0.082421\tGrad Norm: 0.266779\tLR: 0.194676\n",
      "Train set: Average loss: 0.0814\n",
      "Test set: Average loss: 0.0935, Average MAE: 0.2207\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 83 [4096/259852 (2%)]\tLoss: 0.081923\tGrad Norm: 0.249403\tLR: 0.190782\n",
      "Train Epoch: 83 [24576/259852 (9%)]\tLoss: 0.080290\tGrad Norm: 0.235710\tLR: 0.190782\n",
      "Train Epoch: 83 [45056/259852 (17%)]\tLoss: 0.081656\tGrad Norm: 0.235949\tLR: 0.190782\n",
      "Train Epoch: 83 [65536/259852 (25%)]\tLoss: 0.080588\tGrad Norm: 0.256258\tLR: 0.190782\n",
      "Train Epoch: 83 [86016/259852 (33%)]\tLoss: 0.081897\tGrad Norm: 0.247707\tLR: 0.190782\n",
      "Train Epoch: 83 [106496/259852 (41%)]\tLoss: 0.083996\tGrad Norm: 0.351932\tLR: 0.190782\n",
      "Train Epoch: 83 [126976/259852 (48%)]\tLoss: 0.081108\tGrad Norm: 0.249309\tLR: 0.190782\n",
      "Train Epoch: 83 [147456/259852 (56%)]\tLoss: 0.080233\tGrad Norm: 0.409997\tLR: 0.190782\n",
      "Train Epoch: 83 [167936/259852 (64%)]\tLoss: 0.082133\tGrad Norm: 0.329453\tLR: 0.190782\n",
      "Train Epoch: 83 [188416/259852 (72%)]\tLoss: 0.083157\tGrad Norm: 0.382827\tLR: 0.190782\n",
      "Train Epoch: 83 [208896/259852 (80%)]\tLoss: 0.080309\tGrad Norm: 0.272311\tLR: 0.190782\n",
      "Train Epoch: 83 [229376/259852 (88%)]\tLoss: 0.080881\tGrad Norm: 0.325251\tLR: 0.190782\n",
      "Train Epoch: 83 [249856/259852 (95%)]\tLoss: 0.080754\tGrad Norm: 0.295860\tLR: 0.190782\n",
      "Train set: Average loss: 0.0817\n",
      "Test set: Average loss: 0.0950, Average MAE: 0.2245\n",
      "Train Epoch: 84 [4096/259852 (2%)]\tLoss: 0.083429\tGrad Norm: 0.310061\tLR: 0.186967\n",
      "Train Epoch: 84 [24576/259852 (9%)]\tLoss: 0.080327\tGrad Norm: 0.315087\tLR: 0.186967\n",
      "Train Epoch: 84 [45056/259852 (17%)]\tLoss: 0.079138\tGrad Norm: 0.213287\tLR: 0.186967\n",
      "Train Epoch: 84 [65536/259852 (25%)]\tLoss: 0.080236\tGrad Norm: 0.245422\tLR: 0.186967\n",
      "Train Epoch: 84 [86016/259852 (33%)]\tLoss: 0.080062\tGrad Norm: 0.245454\tLR: 0.186967\n",
      "Train Epoch: 84 [106496/259852 (41%)]\tLoss: 0.079809\tGrad Norm: 0.199595\tLR: 0.186967\n",
      "Train Epoch: 84 [126976/259852 (48%)]\tLoss: 0.078805\tGrad Norm: 0.179137\tLR: 0.186967\n",
      "Train Epoch: 84 [147456/259852 (56%)]\tLoss: 0.080235\tGrad Norm: 0.263411\tLR: 0.186967\n",
      "Train Epoch: 84 [167936/259852 (64%)]\tLoss: 0.080997\tGrad Norm: 0.312877\tLR: 0.186967\n",
      "Train Epoch: 84 [188416/259852 (72%)]\tLoss: 0.085452\tGrad Norm: 0.515618\tLR: 0.186967\n",
      "Train Epoch: 84 [208896/259852 (80%)]\tLoss: 0.079219\tGrad Norm: 0.265342\tLR: 0.186967\n",
      "Train Epoch: 84 [229376/259852 (88%)]\tLoss: 0.080069\tGrad Norm: 0.191814\tLR: 0.186967\n",
      "Train Epoch: 84 [249856/259852 (95%)]\tLoss: 0.081720\tGrad Norm: 0.152701\tLR: 0.186967\n",
      "Train set: Average loss: 0.0801\n",
      "Test set: Average loss: 0.0925, Average MAE: 0.2200\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 85 [4096/259852 (2%)]\tLoss: 0.078398\tGrad Norm: 0.238251\tLR: 0.183227\n",
      "Train Epoch: 85 [24576/259852 (9%)]\tLoss: 0.076399\tGrad Norm: 0.182704\tLR: 0.183227\n",
      "Train Epoch: 85 [45056/259852 (17%)]\tLoss: 0.083384\tGrad Norm: 0.268329\tLR: 0.183227\n",
      "Train Epoch: 85 [65536/259852 (25%)]\tLoss: 0.080684\tGrad Norm: 0.250398\tLR: 0.183227\n",
      "Train Epoch: 85 [86016/259852 (33%)]\tLoss: 0.079348\tGrad Norm: 0.235637\tLR: 0.183227\n",
      "Train Epoch: 85 [106496/259852 (41%)]\tLoss: 0.081243\tGrad Norm: 0.284287\tLR: 0.183227\n",
      "Train Epoch: 85 [126976/259852 (48%)]\tLoss: 0.079748\tGrad Norm: 0.247248\tLR: 0.183227\n",
      "Train Epoch: 85 [147456/259852 (56%)]\tLoss: 0.077681\tGrad Norm: 0.189131\tLR: 0.183227\n",
      "Train Epoch: 85 [167936/259852 (64%)]\tLoss: 0.078977\tGrad Norm: 0.245594\tLR: 0.183227\n",
      "Train Epoch: 85 [188416/259852 (72%)]\tLoss: 0.079059\tGrad Norm: 0.341253\tLR: 0.183227\n",
      "Train Epoch: 85 [208896/259852 (80%)]\tLoss: 0.078569\tGrad Norm: 0.174875\tLR: 0.183227\n",
      "Train Epoch: 85 [229376/259852 (88%)]\tLoss: 0.079991\tGrad Norm: 0.241482\tLR: 0.183227\n",
      "Train Epoch: 85 [249856/259852 (95%)]\tLoss: 0.080499\tGrad Norm: 0.252285\tLR: 0.183227\n",
      "Train set: Average loss: 0.0794\n",
      "Test set: Average loss: 0.0938, Average MAE: 0.2238\n",
      "Epoch 85: Mean reward = 0.044 +/- 0.017\n",
      "Train Epoch: 86 [4096/259852 (2%)]\tLoss: 0.078441\tGrad Norm: 0.282316\tLR: 0.179563\n",
      "Train Epoch: 86 [24576/259852 (9%)]\tLoss: 0.080382\tGrad Norm: 0.210024\tLR: 0.179563\n",
      "Train Epoch: 86 [45056/259852 (17%)]\tLoss: 0.076240\tGrad Norm: 0.240825\tLR: 0.179563\n",
      "Train Epoch: 86 [65536/259852 (25%)]\tLoss: 0.077628\tGrad Norm: 0.193561\tLR: 0.179563\n",
      "Train Epoch: 86 [86016/259852 (33%)]\tLoss: 0.077862\tGrad Norm: 0.254174\tLR: 0.179563\n",
      "Train Epoch: 86 [106496/259852 (41%)]\tLoss: 0.081074\tGrad Norm: 0.333877\tLR: 0.179563\n",
      "Train Epoch: 86 [126976/259852 (48%)]\tLoss: 0.082558\tGrad Norm: 0.368652\tLR: 0.179563\n",
      "Train Epoch: 86 [147456/259852 (56%)]\tLoss: 0.080901\tGrad Norm: 0.303523\tLR: 0.179563\n",
      "Train Epoch: 86 [167936/259852 (64%)]\tLoss: 0.080380\tGrad Norm: 0.262209\tLR: 0.179563\n",
      "Train Epoch: 86 [188416/259852 (72%)]\tLoss: 0.079125\tGrad Norm: 0.273862\tLR: 0.179563\n",
      "Train Epoch: 86 [208896/259852 (80%)]\tLoss: 0.076118\tGrad Norm: 0.169851\tLR: 0.179563\n",
      "Train Epoch: 86 [229376/259852 (88%)]\tLoss: 0.078696\tGrad Norm: 0.176752\tLR: 0.179563\n",
      "Train Epoch: 86 [249856/259852 (95%)]\tLoss: 0.079986\tGrad Norm: 0.188889\tLR: 0.179563\n",
      "Train set: Average loss: 0.0789\n",
      "Test set: Average loss: 0.0919, Average MAE: 0.2168\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 87 [4096/259852 (2%)]\tLoss: 0.078934\tGrad Norm: 0.248447\tLR: 0.175972\n",
      "Train Epoch: 87 [24576/259852 (9%)]\tLoss: 0.080188\tGrad Norm: 0.309626\tLR: 0.175972\n",
      "Train Epoch: 87 [45056/259852 (17%)]\tLoss: 0.082056\tGrad Norm: 0.411220\tLR: 0.175972\n",
      "Train Epoch: 87 [65536/259852 (25%)]\tLoss: 0.080637\tGrad Norm: 0.366242\tLR: 0.175972\n",
      "Train Epoch: 87 [86016/259852 (33%)]\tLoss: 0.080395\tGrad Norm: 0.301953\tLR: 0.175972\n",
      "Train Epoch: 87 [106496/259852 (41%)]\tLoss: 0.078015\tGrad Norm: 0.252228\tLR: 0.175972\n",
      "Train Epoch: 87 [126976/259852 (48%)]\tLoss: 0.076759\tGrad Norm: 0.275344\tLR: 0.175972\n",
      "Train Epoch: 87 [147456/259852 (56%)]\tLoss: 0.082223\tGrad Norm: 0.351545\tLR: 0.175972\n",
      "Train Epoch: 87 [167936/259852 (64%)]\tLoss: 0.077042\tGrad Norm: 0.222821\tLR: 0.175972\n",
      "Train Epoch: 87 [188416/259852 (72%)]\tLoss: 0.078734\tGrad Norm: 0.327037\tLR: 0.175972\n",
      "Train Epoch: 87 [208896/259852 (80%)]\tLoss: 0.082389\tGrad Norm: 0.321697\tLR: 0.175972\n",
      "Train Epoch: 87 [229376/259852 (88%)]\tLoss: 0.080567\tGrad Norm: 0.254956\tLR: 0.175972\n",
      "Train Epoch: 87 [249856/259852 (95%)]\tLoss: 0.079386\tGrad Norm: 0.343557\tLR: 0.175972\n",
      "Train set: Average loss: 0.0795\n",
      "Test set: Average loss: 0.0942, Average MAE: 0.2189\n",
      "Train Epoch: 88 [4096/259852 (2%)]\tLoss: 0.078724\tGrad Norm: 0.372913\tLR: 0.172452\n",
      "Train Epoch: 88 [24576/259852 (9%)]\tLoss: 0.079160\tGrad Norm: 0.237719\tLR: 0.172452\n",
      "Train Epoch: 88 [45056/259852 (17%)]\tLoss: 0.077335\tGrad Norm: 0.261322\tLR: 0.172452\n",
      "Train Epoch: 88 [65536/259852 (25%)]\tLoss: 0.077183\tGrad Norm: 0.208824\tLR: 0.172452\n",
      "Train Epoch: 88 [86016/259852 (33%)]\tLoss: 0.075675\tGrad Norm: 0.224273\tLR: 0.172452\n",
      "Train Epoch: 88 [106496/259852 (41%)]\tLoss: 0.077741\tGrad Norm: 0.229107\tLR: 0.172452\n",
      "Train Epoch: 88 [126976/259852 (48%)]\tLoss: 0.079580\tGrad Norm: 0.215368\tLR: 0.172452\n",
      "Train Epoch: 88 [147456/259852 (56%)]\tLoss: 0.076664\tGrad Norm: 0.240217\tLR: 0.172452\n",
      "Train Epoch: 88 [167936/259852 (64%)]\tLoss: 0.077445\tGrad Norm: 0.234293\tLR: 0.172452\n",
      "Train Epoch: 88 [188416/259852 (72%)]\tLoss: 0.078167\tGrad Norm: 0.256170\tLR: 0.172452\n",
      "Train Epoch: 88 [208896/259852 (80%)]\tLoss: 0.079831\tGrad Norm: 0.274872\tLR: 0.172452\n",
      "Train Epoch: 88 [229376/259852 (88%)]\tLoss: 0.076722\tGrad Norm: 0.294698\tLR: 0.172452\n",
      "Train Epoch: 88 [249856/259852 (95%)]\tLoss: 0.078092\tGrad Norm: 0.287487\tLR: 0.172452\n",
      "Train set: Average loss: 0.0776\n",
      "Test set: Average loss: 0.0915, Average MAE: 0.2191\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Train Epoch: 89 [4096/259852 (2%)]\tLoss: 0.075782\tGrad Norm: 0.250245\tLR: 0.169003\n",
      "Train Epoch: 89 [24576/259852 (9%)]\tLoss: 0.076617\tGrad Norm: 0.238946\tLR: 0.169003\n",
      "Train Epoch: 89 [45056/259852 (17%)]\tLoss: 0.075608\tGrad Norm: 0.178694\tLR: 0.169003\n",
      "Train Epoch: 89 [65536/259852 (25%)]\tLoss: 0.074759\tGrad Norm: 0.144814\tLR: 0.169003\n",
      "Train Epoch: 89 [86016/259852 (33%)]\tLoss: 0.076215\tGrad Norm: 0.196775\tLR: 0.169003\n",
      "Train Epoch: 89 [106496/259852 (41%)]\tLoss: 0.075679\tGrad Norm: 0.185057\tLR: 0.169003\n",
      "Train Epoch: 89 [126976/259852 (48%)]\tLoss: 0.078271\tGrad Norm: 0.300208\tLR: 0.169003\n",
      "Train Epoch: 89 [147456/259852 (56%)]\tLoss: 0.077239\tGrad Norm: 0.327369\tLR: 0.169003\n",
      "Train Epoch: 89 [167936/259852 (64%)]\tLoss: 0.076849\tGrad Norm: 0.225339\tLR: 0.169003\n",
      "Train Epoch: 89 [188416/259852 (72%)]\tLoss: 0.076905\tGrad Norm: 0.209629\tLR: 0.169003\n",
      "Train Epoch: 89 [208896/259852 (80%)]\tLoss: 0.074289\tGrad Norm: 0.187576\tLR: 0.169003\n",
      "Train Epoch: 89 [229376/259852 (88%)]\tLoss: 0.077399\tGrad Norm: 0.206987\tLR: 0.169003\n",
      "Train Epoch: 89 [249856/259852 (95%)]\tLoss: 0.077626\tGrad Norm: 0.307306\tLR: 0.169003\n",
      "Train set: Average loss: 0.0769\n",
      "Test set: Average loss: 0.0932, Average MAE: 0.2221\n",
      "Train Epoch: 90 [4096/259852 (2%)]\tLoss: 0.081782\tGrad Norm: 0.368398\tLR: 0.165623\n",
      "Train Epoch: 90 [24576/259852 (9%)]\tLoss: 0.076055\tGrad Norm: 0.309063\tLR: 0.165623\n",
      "Train Epoch: 90 [45056/259852 (17%)]\tLoss: 0.075594\tGrad Norm: 0.237039\tLR: 0.165623\n",
      "Train Epoch: 90 [65536/259852 (25%)]\tLoss: 0.077191\tGrad Norm: 0.286213\tLR: 0.165623\n",
      "Train Epoch: 90 [86016/259852 (33%)]\tLoss: 0.077722\tGrad Norm: 0.330396\tLR: 0.165623\n",
      "Train Epoch: 90 [106496/259852 (41%)]\tLoss: 0.076987\tGrad Norm: 0.390242\tLR: 0.165623\n",
      "Train Epoch: 90 [126976/259852 (48%)]\tLoss: 0.077005\tGrad Norm: 0.266821\tLR: 0.165623\n",
      "Train Epoch: 90 [147456/259852 (56%)]\tLoss: 0.078500\tGrad Norm: 0.241383\tLR: 0.165623\n",
      "Train Epoch: 90 [167936/259852 (64%)]\tLoss: 0.075625\tGrad Norm: 0.230148\tLR: 0.165623\n",
      "Train Epoch: 90 [188416/259852 (72%)]\tLoss: 0.079370\tGrad Norm: 0.227073\tLR: 0.165623\n",
      "Train Epoch: 90 [208896/259852 (80%)]\tLoss: 0.076448\tGrad Norm: 0.266916\tLR: 0.165623\n",
      "Train Epoch: 90 [229376/259852 (88%)]\tLoss: 0.075935\tGrad Norm: 0.237844\tLR: 0.165623\n",
      "Train Epoch: 90 [249856/259852 (95%)]\tLoss: 0.072534\tGrad Norm: 0.154687\tLR: 0.165623\n",
      "Train set: Average loss: 0.0770\n",
      "Test set: Average loss: 0.0900, Average MAE: 0.2172\n",
      "Saved best model to checkpoints/imitation_PPO\n",
      "Epoch 90: Mean reward = 0.066 +/- 0.066\n",
      "Train Epoch: 91 [4096/259852 (2%)]\tLoss: 0.075173\tGrad Norm: 0.205384\tLR: 0.162311\n",
      "Train Epoch: 91 [24576/259852 (9%)]\tLoss: 0.075382\tGrad Norm: 0.241609\tLR: 0.162311\n",
      "Train Epoch: 91 [45056/259852 (17%)]\tLoss: 0.075051\tGrad Norm: 0.217733\tLR: 0.162311\n",
      "Train Epoch: 91 [65536/259852 (25%)]\tLoss: 0.075559\tGrad Norm: 0.195844\tLR: 0.162311\n",
      "Train Epoch: 91 [86016/259852 (33%)]\tLoss: 0.074918\tGrad Norm: 0.150675\tLR: 0.162311\n",
      "Train Epoch: 91 [106496/259852 (41%)]\tLoss: 0.074856\tGrad Norm: 0.195809\tLR: 0.162311\n",
      "Train Epoch: 91 [126976/259852 (48%)]\tLoss: 0.075316\tGrad Norm: 0.240778\tLR: 0.162311\n",
      "Train Epoch: 91 [147456/259852 (56%)]\tLoss: 0.075537\tGrad Norm: 0.214089\tLR: 0.162311\n",
      "Train Epoch: 91 [167936/259852 (64%)]\tLoss: 0.078671\tGrad Norm: 0.288066\tLR: 0.162311\n",
      "Train Epoch: 91 [188416/259852 (72%)]\tLoss: 0.076258\tGrad Norm: 0.219941\tLR: 0.162311\n",
      "Train Epoch: 91 [208896/259852 (80%)]\tLoss: 0.073218\tGrad Norm: 0.200859\tLR: 0.162311\n",
      "Train Epoch: 91 [229376/259852 (88%)]\tLoss: 0.076121\tGrad Norm: 0.227515\tLR: 0.162311\n",
      "Train Epoch: 91 [249856/259852 (95%)]\tLoss: 0.075950\tGrad Norm: 0.214980\tLR: 0.162311\n",
      "Train set: Average loss: 0.0754\n",
      "Test set: Average loss: 0.0907, Average MAE: 0.2177\n",
      "Train Epoch: 92 [4096/259852 (2%)]\tLoss: 0.074558\tGrad Norm: 0.263258\tLR: 0.159064\n",
      "Train Epoch: 92 [24576/259852 (9%)]\tLoss: 0.077266\tGrad Norm: 0.325384\tLR: 0.159064\n",
      "Train Epoch: 92 [45056/259852 (17%)]\tLoss: 0.078594\tGrad Norm: 0.313852\tLR: 0.159064\n",
      "Train Epoch: 92 [65536/259852 (25%)]\tLoss: 0.077992\tGrad Norm: 0.325887\tLR: 0.159064\n",
      "Train Epoch: 92 [86016/259852 (33%)]\tLoss: 0.075475\tGrad Norm: 0.243216\tLR: 0.159064\n",
      "Train Epoch: 92 [106496/259852 (41%)]\tLoss: 0.073847\tGrad Norm: 0.231418\tLR: 0.159064\n",
      "Train Epoch: 92 [126976/259852 (48%)]\tLoss: 0.073331\tGrad Norm: 0.212402\tLR: 0.159064\n",
      "Train Epoch: 92 [147456/259852 (56%)]\tLoss: 0.072394\tGrad Norm: 0.198534\tLR: 0.159064\n",
      "Train Epoch: 92 [167936/259852 (64%)]\tLoss: 0.075697\tGrad Norm: 0.253269\tLR: 0.159064\n",
      "Train Epoch: 92 [188416/259852 (72%)]\tLoss: 0.079230\tGrad Norm: 0.361055\tLR: 0.159064\n",
      "Train Epoch: 92 [208896/259852 (80%)]\tLoss: 0.076408\tGrad Norm: 0.313170\tLR: 0.159064\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpretrain_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpert_observations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpert_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_env\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_gamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.98\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_cuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot_curves\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensorboard_log_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtb_logs/imitation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoints/imitation_PPO\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomet_ml_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNo20MKxPKu7vWLOUQCFBRO8mo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomet_ml_project_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpretraining_rl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomet_ml_experiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPPO_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/thomas/GitHub/sogym_v2/sogym/pretraining.py:272\u001b[0m, in \u001b[0;36mpretrain_agent\u001b[0;34m(student, expert_observations, expert_actions, env, test_env, batch_size, epochs, scheduler_gamma, learning_rate, log_interval, no_cuda, seed, test_batch_size, early_stopping_patience, plot_curves, tensorboard_log_dir, verbose, save_path, comet_ml_api_key, comet_ml_project_name, comet_ml_experiment_name, n_eval_episodes, eval_freq, l2_reg_strength)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    271\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train(model, device, train_loader, optimizer, epoch, max_grad_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10.0\u001b[39m)\n\u001b[0;32m--> 272\u001b[0m     test_loss,test_mae \u001b[38;5;241m=\u001b[39m test(model, device, test_loader)\n\u001b[1;32m    274\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m    275\u001b[0m     test_losses\u001b[38;5;241m.\u001b[39mappend(test_loss)\n",
      "File \u001b[0;32m/scratch/thomas/GitHub/sogym_v2/sogym/pretraining.py:109\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch, max_grad_norm)\u001b[0m\n\u001b[1;32m    107\u001b[0m if isinstance(student, (A2C, PPO)):\n\u001b[1;32m    108\u001b[0m     action, _, _ = model(data)\n\u001b[0;32m--> 109\u001b[0m else:\n\u001b[1;32m    110\u001b[0m     # SAC/TD3:\n\u001b[1;32m    111\u001b[0m     action = model(data)\n\u001b[1;32m    112\u001b[0m action_prediction = action.double()\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/policies.py:649\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    647\u001b[0m     pi_features, vf_features \u001b[38;5;241m=\u001b[39m features\n\u001b[1;32m    648\u001b[0m     latent_pi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor\u001b[38;5;241m.\u001b[39mforward_actor(pi_features)\n\u001b[0;32m--> 649\u001b[0m     latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvf_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;66;03m# Evaluate the values for the given observations\u001b[39;00m\n\u001b[1;32m    651\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(latent_vf)\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/torch_layers.py:228\u001b[0m, in \u001b[0;36mMlpExtractor.forward_critic\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_critic\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/torch/nn/modules/activation.py:358\u001b[0m, in \u001b[0;36mTanh.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/torch/fx/traceback.py:57\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m current_stack\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/traceback.py:213\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m format_list(\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/traceback.py:227\u001b[0m, in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[0;32m--> 227\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/traceback.py:379\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    376\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(FrameSummary(\n\u001b[1;32m    377\u001b[0m         filename, lineno, name, lookup_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39mf_locals))\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[0;32m--> 379\u001b[0m     \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m   \u001b[38;5;66;03m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     stat \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     cache\u001b[38;5;241m.\u001b[39mpop(filename, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrain_agent(\n",
    "    model,\n",
    "    expert_observations,\n",
    "    expert_actions,\n",
    "    env,\n",
    "    test_env = eval_env,\n",
    "    batch_size=4096,\n",
    "    epochs=500,\n",
    "    scheduler_gamma=0.98,\n",
    "    learning_rate=1.0,\n",
    "    log_interval=5,\n",
    "    no_cuda=False,\n",
    "    seed=1,\n",
    "    verbose=True,\n",
    "    test_batch_size=512,\n",
    "    early_stopping_patience=300,\n",
    "    plot_curves=True,\n",
    "    tensorboard_log_dir=\"tb_logs/imitation\",\n",
    "    save_path=\"checkpoints/imitation_PPO\",\n",
    "    comet_ml_api_key=\"No20MKxPKu7vWLOUQCFBRO8mo\",\n",
    "    comet_ml_project_name=\"pretraining_rl\",\n",
    "    comet_ml_experiment_name=\"PPO_1\",\n",
    "    eval_freq = 5,\n",
    "    l2_reg_strength=0.0000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820005a-be57-4236-b66b-0b34ed558aff",
   "metadata": {},
   "source": [
    "---\n",
    "### Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d3e210c1-817a-42a8-ab28-2ea437ed2c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 64 cpus!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from transformers import AutoTokenizer, AutoModel\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecCheckNan\n",
    "import multiprocessing\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"huggingface/CodeBERTa-small-v1\")\n",
    "#model = AutoModel.from_pretrained(\"huggingface/CodeBERTa-small-v1\").to('cuda')\n",
    "\n",
    "# Set number of cpus to use automatically:\n",
    "num_cpu = multiprocessing.cpu_count()\n",
    "print(\"Using {} cpus!\".format(num_cpu))\n",
    "observation_type = \"topopt_game\"\n",
    "\n",
    "train_env = sogym(mode='train',observation_type=observation_type,vol_constraint_type = 'hard',resolution=50,check_connectivity=True)#,model=model,tokenizer=tokenizer)\n",
    "env= make_vec_env(lambda:train_env, n_envs=num_cpu,vec_env_cls=SubprocVecEnv)\n",
    "env = VecCheckNan(env, raise_exception=True)\n",
    "#env=VecNormalize(env,gamma=1.0)\n",
    "\n",
    "eval_env = sogym(mode='test',observation_type=observation_type,vol_constraint_type='hard',resolution=50,check_connectivity=True)#,model=model,tokenizer=tokenizer)\n",
    "eval_env = make_vec_env(lambda:eval_env, n_envs=1,vec_env_cls=SubprocVecEnv)\n",
    "#eval_env =VecNormalize(eval_env,gamma=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b6f2e-5ceb-447f-ac10-a52f0379cb31",
   "metadata": {},
   "source": [
    "--- \n",
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a1132db-34f4-4a72-bb12-1339a9e00a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "# The noise objects for TD3\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.5 * np.ones(n_actions))\n",
    "\n",
    "chosen_policy = \"MlpPolicy\" if observation_type == 'box_dense' else \"MultiInputPolicy\"\n",
    "\n",
    "feature_extractor = ImageDictExtractor if observation_type == 'image' or observation_type==\"topopt_game\" else CustomBoxDense\n",
    "\n",
    "# Load the YAML file\n",
    "\n",
    "with open(\"algorithms.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Extract the parameters for the desired algorithm\n",
    "algorithm_name = \"PPO\"  # or \"TD3\"\n",
    "algorithm_params = config[algorithm_name]\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=feature_extractor,\n",
    "    net_arch = config['common']['net_arch'],\n",
    "    share_features_extractor = False,\n",
    ")\n",
    "# Create the model based on the algorithm name and parameters\n",
    "if algorithm_name == \"SAC\":\n",
    "    model = SAC(env=env,\n",
    "                policy = chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                #action_noise = action_noise,\n",
    "                ent_coef = 0.0,\n",
    "                device=device, \n",
    "                **algorithm_params)\n",
    "\n",
    "elif algorithm_name == \"PPO\":\n",
    "    model = PPO(env=env, \n",
    "                policy = chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                device = device, \n",
    "                **algorithm_params)\n",
    "\n",
    "elif algorithm_name == \"TD3\":\n",
    "    # Create the action noise object\n",
    "    n_actions = env.action_space.shape[-1]\n",
    "    action_noise_params = algorithm_params.pop(\"action_noise\")\n",
    "    action_noise = NormalActionNoise(mean=action_noise_params[\"mean\"] * np.ones(n_actions),\n",
    "                                     \n",
    "                                     sigma=action_noise_params[\"sigma\"] * np.ones(n_actions))\n",
    "    model = TD3(env=env,\n",
    "                policy =chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                action_noise=action_noise,\n",
    "                device=device, \n",
    "                **algorithm_params)\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Create the tb_log_name string\n",
    "tb_log_name = f\"{algorithm_name}_{current_datetime}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ec8bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a checkpoint every 1000 steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=50000//num_cpu,\n",
    "  save_path=\"./checkpoints/\",\n",
    "  name_prefix=tb_log_name,\n",
    "  save_replay_buffer=True,\n",
    "  save_vecnormalize=True,\n",
    ")\n",
    "\n",
    "eval_callback = EvalCallback(eval_env,\n",
    "                             log_path='tb_logs',\n",
    "                             eval_freq=5000//num_cpu,\n",
    "                             deterministic=True,\n",
    "                            n_eval_episodes=10,\n",
    "                             render=False,\n",
    "                             best_model_save_path='./checkpoints',\n",
    "                             verbose=0)\n",
    "\n",
    "callback_list = CallbackList([eval_callback,\n",
    "                         checkpoint_callback,\n",
    "                         MaxRewardCallback(verbose=1),\n",
    "                         GradientClippingCallback(clip_value=1.0, verbose=1),\n",
    "                         GradientNormCallback(verbose=1),\n",
    "                         FigureRecorderCallback(check_freq=5000//num_cpu,eval_env=eval_env),\n",
    "                         StopTrainingOnNoModelImprovement(max_no_improvement_evals=50, min_evals=100, verbose=1)\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f70cf1-e15e-4fed-ab75-12bdca996f2d",
   "metadata": {},
   "source": [
    "--- \n",
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a84e3f",
   "metadata": {},
   "source": [
    "Save the model:\n",
    "\n",
    "If model is on-policy:\n",
    "#model.save(\"sac_pendulum\")\n",
    "#loaded_model = SAC.load(\"sac_pendulum\")\n",
    "\n",
    "if model is off-policy, we also need to save the replay buffer:\n",
    "#model.save_replay_buffer(\"sac_replay_buffer\")\n",
    "#loaded_model.load_replay_buffer(\"sac_replay_buffer\")\n",
    "\n",
    "If the environment is normalized:\n",
    "#env.save('env_saved.pkl')\n",
    "#env = VecNormalize.load('env_saved.pkl',env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "978d7768-b7bb-4c14-b620-28a278392b6b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "            \n",
    "#model = SAC.load(\"checkpoints/imitation_SAC\",env =env) \n",
    "model.set_parameters(\"checkpoints/imitation_PPO.zip\")\n",
    "model.set_parameters(\"imitation_PPO_critic\")\n",
    "\n",
    "train_critic_only = False\n",
    "if train_critic_only:\n",
    "    #Freeze everything:\n",
    "    for name, param in model.policy.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            param.requires_grad=False\n",
    "\n",
    "    if algorithm_name =='SAC'\n",
    "        # Unfreeze critic:\n",
    "        for param in model.policy.critic.parameters():\n",
    "            if param.requires_grad==False:\n",
    "                param.requires_grad=True\n",
    "\n",
    "        for param in model.policy.critic_target.parameters():\n",
    "            if param.requires_grad==False:\n",
    "                param.requires_grad=True\n",
    "        \n",
    "        #Reset critic networks:\n",
    "        if hasattr(model.policy.critic_target, 'reset_parameters'):\n",
    "            print(' resetting')\n",
    "            model.policy.critic_target.reset_parameters()\n",
    "        else:\n",
    "            model.policy.critic_target.apply(init_weights)\n",
    "\n",
    "            \n",
    "        if hasattr(model.policy.critic, 'reset_parameters'):\n",
    "            print('resetting')\n",
    "            model.policy.critic_target.reset_parameters() \n",
    "        else:\n",
    "            model.policy.critic.apply(init_weights)\n",
    "\n",
    "\n",
    "    if algorithm_name == 'PPO':\n",
    "        for param in model.policy.mlp_extractor.value_net.parameters():\n",
    "            if param.requires_grad==False:\n",
    "                param.requires_grad=True\n",
    "            \n",
    "        for param in model.policy.value_net.parameters():\n",
    "            if param.requires_grad==False:\n",
    "                param.requires_grad=True\n",
    "        \n",
    "\n",
    "        if hasattr(model.policy.value_net, 'reset_parameters'):\n",
    "            print(' resetting')\n",
    "            model.policy.value_net.reset_parameters()\n",
    "        else:\n",
    "            model.policy.value_net.apply(init_weights)\n",
    "            \n",
    "        if hasattr(model.policy.mlp_extractor.value_net, 'reset_parameters'):\n",
    "            print(' resetting')\n",
    "            model.policy.mlp_extractor.value_net.reset_parameters() \n",
    "        else:\n",
    "            model.policy.mlp_extractor.value_net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c876f635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:414: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_check_nan.VecCheckNan object at 0x7fc140458340> != <stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fbc2204fee0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "Process ForkServerProcess-249:\n",
      "Process ForkServerProcess-312:\n",
      "Process ForkServerProcess-311:\n",
      "Process ForkServerProcess-310:\n",
      "Process ForkServerProcess-309:\n",
      "Process ForkServerProcess-308:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "Traceback (most recent call last):\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-307:\n",
      "Process ForkServerProcess-306:\n",
      "Process ForkServerProcess-304:\n",
      "Process ForkServerProcess-291:\n",
      "Process ForkServerProcess-301:\n",
      "Process ForkServerProcess-303:\n",
      "Process ForkServerProcess-279:\n",
      "Process ForkServerProcess-276:\n",
      "Process ForkServerProcess-271:\n",
      "Process ForkServerProcess-297:\n",
      "Process ForkServerProcess-277:\n",
      "Process ForkServerProcess-284:\n",
      "Process ForkServerProcess-268:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process ForkServerProcess-299:\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "Process ForkServerProcess-273:\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-262:\n",
      "Process ForkServerProcess-275:\n",
      "Process ForkServerProcess-256:\n",
      "Process ForkServerProcess-257:\n",
      "Process ForkServerProcess-295:\n",
      "Process ForkServerProcess-264:\n",
      "Process ForkServerProcess-251:\n",
      "Process ForkServerProcess-287:\n",
      "Process ForkServerProcess-258:\n",
      "Process ForkServerProcess-250:\n",
      "Process ForkServerProcess-288:\n",
      "Process ForkServerProcess-282:\n",
      "Process ForkServerProcess-281:\n",
      "Process ForkServerProcess-296:\n",
      "Process ForkServerProcess-263:\n",
      "Process ForkServerProcess-252:\n",
      "Process ForkServerProcess-302:\n",
      "Process ForkServerProcess-292:\n",
      "Process ForkServerProcess-300:\n",
      "Process ForkServerProcess-267:\n",
      "Process ForkServerProcess-294:\n",
      "Process ForkServerProcess-289:\n",
      "Process ForkServerProcess-272:\n",
      "Process ForkServerProcess-261:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-293:\n",
      "Process ForkServerProcess-285:\n",
      "Process ForkServerProcess-283:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "Traceback (most recent call last):\n",
      "Process ForkServerProcess-274:\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "Process ForkServerProcess-305:\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "Traceback (most recent call last):\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-290:\n",
      "Process ForkServerProcess-254:\n",
      "Process ForkServerProcess-260:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-278:\n",
      "Process ForkServerProcess-265:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-259:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "Process ForkServerProcess-286:\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "Process ForkServerProcess-269:\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkServerProcess-255:\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-280:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-266:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-298:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-253:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-270:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#print(model.batch_size)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#model.load_replay_buffer(\"sac_replay_buffer\")\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20000000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#model.save('model_saved_march15',)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#model.save_replay_buffer(\"sac_replay_buffer_march15\")\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#env.save('env_saved.pkl')\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:277\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 277\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:200\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n\u001b[1;32m    199\u001b[0m callback\u001b[38;5;241m.\u001b[39mupdate_locals(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_info_buffer(infos)\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:114\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnum_timesteps\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:219\u001b[0m, in \u001b[0;36mCallbackList._on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    216\u001b[0m continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# Return False (stop training) if at least one callback returns False\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m continue_training\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m continue_training\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:114\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnum_timesteps\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:460\u001b[0m, in \u001b[0;36mEvalCallback._on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;66;03m# Reset success rate buffer\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_success_buffer \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 460\u001b[0m episode_rewards, episode_lengths \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_episode_rewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_success_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(episode_rewards, \u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:94\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (episode_counts \u001b[38;5;241m<\u001b[39m episode_count_targets)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m     88\u001b[0m     actions, states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m     89\u001b[0m         observations,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m     90\u001b[0m         state\u001b[38;5;241m=\u001b[39mstates,\n\u001b[1;32m     91\u001b[0m         episode_start\u001b[38;5;241m=\u001b[39mepisode_starts,\n\u001b[1;32m     92\u001b[0m         deterministic\u001b[38;5;241m=\u001b[39mdeterministic,\n\u001b[1;32m     93\u001b[0m     )\n\u001b[0;32m---> 94\u001b[0m     new_observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     current_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n\u001b[1;32m     96\u001b[0m     current_lengths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py:129\u001b[0m, in \u001b[0;36mSubprocVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m--> 129\u001b[0m     results \u001b[38;5;241m=\u001b[39m [remote\u001b[38;5;241m.\u001b[39mrecv() \u001b[38;5;28;01mfor\u001b[39;00m remote \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremotes]\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     obs, rews, dones, infos, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py:129\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m--> 129\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\u001b[43mremote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m remote \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremotes]\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     obs, rews, dones, infos, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py:255\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 255\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(buf\u001b[38;5;241m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py:419\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 419\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py:384\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    382\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 384\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#print(model.batch_size)\n",
    "#model.load_replay_buffer(\"sac_replay_buffer\")\n",
    "model.learn(20000000,\n",
    "            callback=callback_list, \n",
    "            tb_log_name=tb_log_name\n",
    "            )\n",
    "#model.save('model_saved_march15',)\n",
    "#model.save_replay_buffer(\"sac_replay_buffer_march15\")\n",
    "\n",
    "#env.save('env_saved.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c27da122",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('checkpoints/imitation_PPO_critic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2827c8cc-028d-4448-b109-d6542816df2a",
   "metadata": {},
   "source": [
    "---\n",
    "### Let's visualize the agent's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6728408e-71c2-4f7b-a8f1-f0cc6b43dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=sogym(mode='train',observation_type='image',vol_constraint_type='hard' ,resolution = 50)\n",
    "#env= make_vec_env(lambda:env, n_envs=1,vec_env_cls=SubprocVecEnv)\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5cbb153d-80e3-4d6c-ada3-f4d323360b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.96462613 -0.99048704  0.38009977  0.08957338  0.9785042   0.96510744]\n",
      "[-0.57749254  0.55976856 -0.8036035  -0.97631305  0.4027611   0.51337385]\n",
      "[-0.63751614 -0.5484587  -0.68173546  0.99026537  0.60249984  0.7280123 ]\n",
      "[-0.7092295   0.9814949   0.23871887 -0.2769655   0.8502177   0.86372375]\n",
      "[-0.44792038 -0.94188035  0.49378872  0.09349489  0.5592005   0.39041722]\n",
      "[-0.27786326 -0.2657057   0.4812293   0.17316115  0.08975875 -0.43019438]\n",
      "[ 0.2240442  -0.05230045  0.78103995  0.9819428   0.7970079   0.7638916 ]\n",
      "[ 0.83928895  0.9800999   0.31479597 -0.41921186  0.8269007   0.76901615]\n",
      "Desired volume: 0.49 Obtained volume: 0.3428822942241449\n",
      "Env reward: 0.2637187488365308\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs,info=env.reset()\n",
    "dones=False\n",
    "saved_conditions = env.conditions\n",
    "saved_nelx, saved_nely = env.nelx, env.nely\n",
    "saved_dx, saved_dy = env.dx, env.dy\n",
    "#use deepcopy to save \n",
    "while dones== False:\n",
    "    action, _states = model.predict(obs,deterministic=True)\n",
    "    print(action)\n",
    "    obs, rewards, dones,truncated, info = env.step(action)\n",
    "print(\"Desired volume:\",saved_conditions['volfrac'],\"Obtained volume:\",env.volume)\n",
    "print(\"Env reward:\",rewards)\n",
    "env.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d2de1f-dd90-40b2-acc9-b3227fd2888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xval, f0val,it, H, Phimax, allPhi, den, N, cfg = run_mmc(saved_conditions,saved_nelx,saved_nely,saved_dx,saved_dy,plotting='contour')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('SB3_update')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e21ef5adabae340b8408649b4e28a9d7d4d8eaab8fdd4faf01af585df564eed2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
