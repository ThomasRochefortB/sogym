{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "94e6157f-d831-41f7-b27c-f949e2253f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "SB3 version: 2.2.1\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'       #Disactivate multiprocessing for numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import gymnasium as gym\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "import stable_baselines3\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize, SubprocVecEnv\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList, CheckpointCallback, StopTrainingOnNoModelImprovement\n",
    "\n",
    "from sogym.mmc_optim import run_mmc\n",
    "from sogym.env import sogym\n",
    "from sogym.expert_generation import generate_expert_dataset, generate_mmc_solutions, generate_dataset\n",
    "from sogym.utils import profile_and_analyze,ImageDictExtractor, CustomBoxDense\n",
    "from sogym.callbacks import FigureRecorderCallback, MaxRewardCallback, GradientNormCallback, GradientClippingCallback\n",
    "from sogym.pretraining import pretrain_agent, ExpertDataSet\n",
    "\n",
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import random_split, Dataset\n",
    "from IPython.display import display\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print('SB3 version:', stable_baselines3.__version__)\n",
    "# Let's make the code device agnostic:\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02b8cad-4629-49cf-a15e-ebb7c9a5b6c1",
   "metadata": {},
   "source": [
    "---\n",
    "### Environment test and visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "85a4374a-b735-4474-8427-30b1d55dfe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the environment using the check_env util from SB3:\n",
    "observation_type = 'topopt_game'\n",
    "train_env = sogym(mode='train',observation_type=observation_type,vol_constraint_type='hard',resolution=50,check_connectivity = True)\n",
    "eval_env = sogym(mode='test',observation_type=observation_type,vol_constraint_type='hard',resolution=50,check_connectivity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "27032137",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dones:\n\u001b[1;32m      8\u001b[0m         action \u001b[38;5;241m=\u001b[39m train_env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m----> 9\u001b[0m         obs, reward, dones, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     fig \u001b[38;5;241m=\u001b[39m train_env\u001b[38;5;241m.\u001b[39mplot()\n\u001b[1;32m     12\u001b[0m fig\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menv_test.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/scratch/thomas/GitHub/sogym_v2/sogym/env.py:251\u001b[0m, in \u001b[0;36msogym.step\u001b[0;34m(self, action, evaluate)\u001b[0m\n\u001b[1;32m    249\u001b[0m     terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type \u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopopt_game\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_compliance_and_stress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_connected\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;66;03m# At the end of the episode\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/thomas/GitHub/sogym_v2/sogym/env.py:270\u001b[0m, in \u001b[0;36msogym.calculate_compliance_and_stress\u001b[0;34m(self, is_connected)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_compliance_and_stress\u001b[39m(\u001b[38;5;28mself\u001b[39m, is_connected):\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompliance, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvolume, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mF \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_compliance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconditions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnelx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnely\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_connected:\n\u001b[1;32m    274\u001b[0m         nDof \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/scratch/thomas/GitHub/sogym_v2/sogym/struct.py:110\u001b[0m, in \u001b[0;36mcalculate_compliance\u001b[0;34m(H, conditions, DW, DH, nelx, nely)\u001b[0m\n\u001b[1;32m    108\u001b[0m K \u001b[38;5;241m=\u001b[39m csc_matrix((sK\u001b[38;5;241m.\u001b[39mflatten(order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m), (Iar0[:,\u001b[38;5;241m0\u001b[39m], Iar0[:,\u001b[38;5;241m1\u001b[39m])), shape\u001b[38;5;241m=\u001b[39m(nDof, nDof))\n\u001b[1;32m    109\u001b[0m K \u001b[38;5;241m=\u001b[39m  K \u001b[38;5;241m+\u001b[39m K\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m-\u001b[39m diags((K\u001b[38;5;241m.\u001b[39mdiagonal()))\n\u001b[0;32m--> 110\u001b[0m U[freeDof] \u001b[38;5;241m=\u001b[39m\u001b[43mspsolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfreeDof\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfreeDof\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfreeDof\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;28mlen\u001b[39m(freeDof),\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    112\u001b[0m f0val \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m*\u001b[39mU\n\u001b[1;32m    114\u001b[0m Comp\u001b[38;5;241m=\u001b[39mf0val\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/scipy/sparse/linalg/_dsolve/linsolve.py:273\u001b[0m, in \u001b[0;36mspsolve\u001b[0;34m(A, b, permc_spec, use_umfpack)\u001b[0m\n\u001b[1;32m    270\u001b[0m     flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# CSR format\u001b[39;00m\n\u001b[1;32m    272\u001b[0m options \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(ColPerm\u001b[38;5;241m=\u001b[39mpermc_spec)\n\u001b[0;32m--> 273\u001b[0m x, info \u001b[38;5;241m=\u001b[39m \u001b[43m_superlu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgssv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnnz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    276\u001b[0m     warn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatrix is exactly singular\u001b[39m\u001b[38;5;124m\"\u001b[39m, MatrixRankWarning)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reward = 0.0\n",
    "while reward == 0.0:\n",
    "    obs, info = train_env.reset()\n",
    "    dones = False\n",
    "    while not dones:\n",
    "        action = train_env.action_space.sample()\n",
    "        obs, reward, dones, truncated, info = train_env.step(action)\n",
    "\n",
    "    fig = train_env.plot()\n",
    "fig.savefig('env_test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edcde2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will check your custom environment and output additional warnings if needed\n",
    "check_env(sogym(mode='train',observation_type='topopt_game'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10de06f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Specify the number of episodes to run\n",
    "num_episodes = 20\n",
    "# Call the profile_and_analyze function\n",
    "result_df = profile_and_analyze(num_episodes, train_env)\n",
    "# Print the resulting DataFrame\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f888890",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = train_env.reset()\n",
    "cfg = {\n",
    "            'optimizer':'mma', #optimiser choice\n",
    "            'xInt':0.25, #initial interval of components in x\n",
    "            'yInt':0.25, #initial interval of components in y\n",
    "            'E':1.0, #Young's modulus\n",
    "            'nu':0.3, #Poisson ratio\n",
    "            'h':1, #thickness\n",
    "            'dgt0':5, #significant digit of sens.\n",
    "            'scl':1, #scale factor for obj\n",
    "            'p':6,  #power of super ellipsoid\n",
    "            'lmd':100, #power of KS aggregation   \n",
    "            'maxiter':500, # maximum number of outer iterations\n",
    "            'alpha':1e-9, # This is the threshold level in the Heaviside function\n",
    "            'epsilon':0.2, #This is the regularization term in the Heaviside function\n",
    "            'maxinnerinit':1, # This is the maximum number of inner iterations for GCMMA\n",
    "            'switch':-0.000002, # This is the switch criteria for the hybrid optimizer\n",
    "            'convergence_threshold':2e-4, #This is the threshold for the relative change in the objective function\n",
    "            'xmin':(0.0, 0.0, 0.0, 0.00, 0.00, -np.pi),\n",
    "            'xmax':(train_env.dx, train_env.dy, 0.7*min(train_env.dx,train_env.dy), 0.05*min(train_env.dx,train_env.dy),0.05*min(train_env.dx,train_env.dy), np.pi)\n",
    "        }\n",
    "\n",
    "#run_mmc(train_env.conditions,train_env.nelx,train_env.nely,train_env.dx,train_env.dy,plotting='contour',verbose=0,cfg=cfg)\n",
    "dataset_folder = \"/home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/mmc\"\n",
    "#generate_mmc_solutions(key=0,dataset_folder=\"/home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/mmc\")\n",
    "generate_dataset(dataset_folder= dataset_folder, num_threads=32, num_samples=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a5035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the training environment on a random problem statement and visualize a 'successful' solution:\n",
    "reward = 0.0\n",
    "while reward==0.0:\n",
    "    obs = train_env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = train_env.action_space.sample()\n",
    "        obs, reward, done,truncated, info = train_env.step(action)\n",
    "        \n",
    "# print(\"Volume: \", train_env.volume)\n",
    "print(\"Reward \",reward)\n",
    "\n",
    "train_env.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a21145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axes for the subplots\n",
    "fig, axes = plt.subplots(nrows=5, ncols=5, figsize=(10, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Initialize the index for the current subplot\n",
    "subplot_index = 0\n",
    "\n",
    "# Let's visualize the training environment on a random problem statement and visualize a 'successful' solution:\n",
    "reward = 0.0\n",
    "while reward == 0.0:\n",
    "    obs = train_env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = train_env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = train_env.step(action)\n",
    "        \n",
    "        # Plot the current observation image\n",
    "        axes[subplot_index].imshow(obs['strain_energy'].T, cmap='gray')\n",
    "        axes[subplot_index].axis('off')\n",
    "        axes[subplot_index].set_title(f\"Timestep {subplot_index+1}\")\n",
    "        \n",
    "        # Increment the subplot index\n",
    "        subplot_index += 1\n",
    "        \n",
    "        # If all subplots are filled, display the plot and reset the index\n",
    "        if subplot_index == len(axes):\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            subplot_index = 0\n",
    "\n",
    "# Print the reward\n",
    "print(\"Reward:\", reward)\n",
    "\n",
    "# Plot the final state of the training environment\n",
    "train_env.plot()\n",
    "\n",
    "# Display any remaining subplots\n",
    "if subplot_index > 0:\n",
    "    for i in range(subplot_index, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b3f4a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 38855/38855 [3:42:43<00:00,  2.91file/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# Specify the number of permutations to generate\n",
    "num_permutations = None\n",
    "observation_type = \"topopt_game\"\n",
    "\n",
    "# Specify the environment configuration (optional)\n",
    "env_kwargs = {\n",
    "    'mode': 'train',\n",
    "    'observation_type': observation_type,\n",
    "    'vol_constraint_type': 'hard',\n",
    "    'seed': 42,\n",
    "    'resolution' : 50, \n",
    "    'check_connectivity':True\n",
    "}\n",
    "\n",
    "directory_path = \"/home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies_narval\"\n",
    "expert_observations, expert_actions = generate_expert_dataset(directory_path,env_kwargs, plot_terminated=False,num_permutations = num_permutations, file_fraction=1.0)\n",
    "# Save the dataset\n",
    "import pickle\n",
    "\n",
    "# Save the data using pickle\n",
    "with open('expert_dataset_narval_topoptgame.pkl', 'wb') as f:\n",
    "    pickle.dump({'expert_observations': expert_observations, 'expert_actions': expert_actions}, f, protocol=4)\n",
    "print(len(expert_observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5ae43524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the data using pickle\n",
    "with open('expert_dataset_narval_topoptgame.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "expert_observations = data['expert_observations']\n",
    "expert_actions = data['expert_actions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "baea7b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0581, -0.9009,  0.8737,  0.7976, -1.3026, -1.3027],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\\\n",
    "# Assuming you have the expert_dataset defined\n",
    "expert_dataset = ExpertDataSet(expert_observations, expert_actions, train_env)\n",
    "# Get a random sample from the dataset\n",
    "sample_idx = np.random.randint(len(expert_dataset))\n",
    "sample = expert_dataset[sample_idx]\n",
    "\n",
    "# Extract the observation and reward from the sample\n",
    "observation, action = sample\n",
    "\n",
    "# Subplot with image, strain_energy, and structure_strain_energy observations:\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot image observation\n",
    "axes[0].imshow(observation['image'].T, cmap='gray', origin='lower')\n",
    "axes[0].axis('on')\n",
    "axes[0].set_title(\"Image Observation\")\n",
    "\n",
    "# Plot strain_energy observation\n",
    "axes[1].imshow(observation['structure_strain_energy'].T, origin='lower')\n",
    "axes[1].axis('on')\n",
    "axes[1].set_title(\"Structure Strain Energy Observation\")\n",
    "\n",
    "print(action)\n",
    "plt.tight_layout()\n",
    "plt.savefig('expert_observation.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "62c7c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = train_env.reset()\n",
    "\n",
    "#use action and plot the result\n",
    "obs, rewards, dones,truncated, info = train_env.step(np.array(action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4bedb95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(obs['image'].T,origin='lower')\n",
    "plt.savefig('expert_action.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "73ad778e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:155: UserWarning: You have specified a mini-batch size of 4096, but because the `RolloutBuffer` is of size `n_steps * n_envs = 64`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 64\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=64 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "chosen_policy = \"MlpPolicy\" if observation_type == 'box_dense' else \"MultiInputPolicy\"\n",
    "\n",
    "feature_extractor = ImageDictExtractor if observation_type == 'image' or observation_type == 'topopt_game' else CustomBoxDense\n",
    "\n",
    "# Load the YAML file\n",
    "env=train_env\n",
    "\n",
    "with open(\"algorithms.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Extract the parameters for the desired algorithm\n",
    "algorithm_name = \"PPO\"  # or \"TD3\"\n",
    "algorithm_params = config[algorithm_name]\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=feature_extractor,\n",
    "    net_arch = config['common']['net_arch'],\n",
    "    share_features_extractor = False\n",
    ")\n",
    "\n",
    "# Create the model based on the algorithm name and parameters\n",
    "if algorithm_name == \"SAC\":\n",
    "    model = SAC(env=env,\n",
    "                policy = chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                device=device, \n",
    "                **algorithm_params)\n",
    "\n",
    "elif algorithm_name == \"PPO\":\n",
    "    model = PPO(env=env, \n",
    "                policy = chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                device = device, \n",
    "                **algorithm_params)\n",
    "\n",
    "elif algorithm_name == \"TD3\":\n",
    "    # Create the action noise object\n",
    "    n_actions = env.action_space.shape[-1]\n",
    "    action_noise_params = algorithm_params.pop(\"action_noise\")\n",
    "    action_noise = NormalActionNoise(mean=action_noise_params[\"mean\"] * np.ones(n_actions),\n",
    "                                     sigma=action_noise_params[\"sigma\"] * np.ones(n_actions))\n",
    "    model = TD3(env=env,\n",
    "                policy =chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                action_noise=action_noise,\n",
    "                device=device, \n",
    "                **algorithm_params)\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Create the tb_log_name string\n",
    "tb_log_name = f\"{algorithm_name}_{current_datetime}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "090eedd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 85,996,045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MultiInputActorCriticPolicy              6\n",
       "├─ImageDictExtractor: 1-1                --\n",
       "│    └─ReLU: 2-1                         --\n",
       "│    └─ModuleDict: 2-2                   --\n",
       "│    │    └─Sequential: 3-1              22,784\n",
       "│    │    └─Sequential: 3-2              93,248\n",
       "│    │    └─Sequential: 3-3              16,768\n",
       "│    │    └─Sequential: 3-4              16,768\n",
       "│    │    └─Sequential: 3-5              93,248\n",
       "│    │    └─Sequential: 3-6              16,768\n",
       "├─ImageDictExtractor: 1-2                (recursive)\n",
       "│    └─ReLU: 2-3                         --\n",
       "│    └─ModuleDict: 2-4                   (recursive)\n",
       "│    │    └─Sequential: 3-7              (recursive)\n",
       "│    │    └─Sequential: 3-8              (recursive)\n",
       "│    │    └─Sequential: 3-9              (recursive)\n",
       "│    │    └─Sequential: 3-10             (recursive)\n",
       "│    │    └─Sequential: 3-11             (recursive)\n",
       "│    │    └─Sequential: 3-12             (recursive)\n",
       "├─ImageDictExtractor: 1-3                --\n",
       "│    └─ReLU: 2-5                         --\n",
       "│    └─ModuleDict: 2-6                   --\n",
       "│    │    └─Sequential: 3-13             22,784\n",
       "│    │    └─Sequential: 3-14             93,248\n",
       "│    │    └─Sequential: 3-15             16,768\n",
       "│    │    └─Sequential: 3-16             16,768\n",
       "│    │    └─Sequential: 3-17             93,248\n",
       "│    │    └─Sequential: 3-18             16,768\n",
       "├─MlpExtractor: 1-4                      --\n",
       "│    └─Sequential: 2-7                   --\n",
       "│    │    └─Linear: 3-19                 34,605,056\n",
       "│    │    └─Tanh: 3-20                   --\n",
       "│    │    └─Linear: 3-21                 4,196,352\n",
       "│    │    └─Tanh: 3-22                   --\n",
       "│    │    └─Linear: 3-23                 2,098,176\n",
       "│    │    └─Tanh: 3-24                   --\n",
       "│    │    └─Linear: 3-25                 1,049,600\n",
       "│    │    └─Tanh: 3-26                   --\n",
       "│    │    └─Linear: 3-27                 524,800\n",
       "│    │    └─Tanh: 3-28                   --\n",
       "│    │    └─Linear: 3-29                 262,656\n",
       "│    │    └─Tanh: 3-30                   --\n",
       "│    └─Sequential: 2-8                   --\n",
       "│    │    └─Linear: 3-31                 34,605,056\n",
       "│    │    └─Tanh: 3-32                   --\n",
       "│    │    └─Linear: 3-33                 4,196,352\n",
       "│    │    └─Tanh: 3-34                   --\n",
       "│    │    └─Linear: 3-35                 2,098,176\n",
       "│    │    └─Tanh: 3-36                   --\n",
       "│    │    └─Linear: 3-37                 1,049,600\n",
       "│    │    └─Tanh: 3-38                   --\n",
       "│    │    └─Linear: 3-39                 524,800\n",
       "│    │    └─Tanh: 3-40                   --\n",
       "│    │    └─Linear: 3-41                 262,656\n",
       "│    │    └─Tanh: 3-42                   --\n",
       "├─Linear: 1-5                            3,078\n",
       "├─Linear: 1-6                            513\n",
       "=================================================================\n",
       "Total params: 85,996,045\n",
       "Trainable params: 85,996,045\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "total_params = sum(p.numel() for p in model.policy.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "data = {k: v for k, v in observation.items()}\n",
    "# Assuming you have a PyTorch model named 'model' and the input size is (3, 224, 224)\n",
    "summary(model.policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0166b0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: tensorboard, torch.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/thomasrb/pretraining-rl/97a45188a1b74315b4f5973f5585e0a2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     grad_norm [8]  : (9.999997824963602, 10.000000433511623)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mae [8]        : (0.6573815548862346, 1.7594083989552685)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_reward    : 0.042885462567210195\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     std_reward     : 0.0011408701136307441\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     test_loss [8]  : (0.8659590560230338, 5.786423800384693)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     test_mae [8]   : (0.6573815548862346, 1.7594083989552685)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss [8] : (1.0927965177950392, 3.6893997174110282)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : PPO_1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed)     : 1 (21.79 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: tensorboard, torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/thomasrb/pretraining-rl/8169cc7a7c2b431e99d3e4ef9db25df7\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [4096/248672 (2%)]\tLoss: 2.518659\tGrad Norm: 4.628726\tLR: 1.000000\n",
      "Train Epoch: 1 [24576/248672 (10%)]\tLoss: 2.919998\tGrad Norm: 9.999999\tLR: 1.000000\n",
      "Train Epoch: 1 [45056/248672 (18%)]\tLoss: 2.779126\tGrad Norm: 9.999999\tLR: 1.000000\n",
      "Train Epoch: 1 [65536/248672 (26%)]\tLoss: 3.073122\tGrad Norm: 9.999998\tLR: 1.000000\n",
      "Train Epoch: 1 [86016/248672 (34%)]\tLoss: 3.257378\tGrad Norm: 9.999999\tLR: 1.000000\n",
      "Train Epoch: 1 [106496/248672 (43%)]\tLoss: 2.565800\tGrad Norm: 8.192236\tLR: 1.000000\n",
      "Train Epoch: 1 [126976/248672 (51%)]\tLoss: 2.866419\tGrad Norm: 9.999999\tLR: 1.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpretrain_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpert_observations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpert_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_env\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_gamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.98\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_cuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot_curves\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensorboard_log_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtb_logs/imitation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoints/imitation_PPO\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomet_ml_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNo20MKxPKu7vWLOUQCFBRO8mo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomet_ml_project_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpretraining_rl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomet_ml_experiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPPO_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/thomas/GitHub/sogym_v2/sogym/pretraining.py:271\u001b[0m, in \u001b[0;36mpretrain_agent\u001b[0;34m(student, expert_observations, expert_actions, env, test_env, batch_size, epochs, scheduler_gamma, learning_rate, log_interval, no_cuda, seed, test_batch_size, early_stopping_patience, plot_curves, tensorboard_log_dir, verbose, save_path, comet_ml_api_key, comet_ml_project_name, comet_ml_experiment_name, n_eval_episodes, eval_freq, l2_reg_strength)\u001b[0m\n\u001b[1;32m    268\u001b[0m     experiment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 271\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     test_loss,test_mae \u001b[38;5;241m=\u001b[39m test(model, device, test_loader)\n\u001b[1;32m    274\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "File \u001b[0;32m/scratch/thomas/GitHub/sogym_v2/sogym/pretraining.py:128\u001b[0m, in \u001b[0;36mpretrain_agent.<locals>.train\u001b[0;34m(model, device, train_loader, optimizer, epoch, max_grad_norm)\u001b[0m\n\u001b[1;32m    126\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l2_reg_strength \u001b[38;5;241m*\u001b[39m l2_reg_loss\n\u001b[1;32m    127\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 128\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Clip gradients\u001b[39;00m\n\u001b[1;32m    131\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_grad_norm)\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pretrain_agent(\n",
    "    model,\n",
    "    expert_observations,\n",
    "    expert_actions,\n",
    "    env,\n",
    "    test_env = eval_env,\n",
    "    batch_size=4096,\n",
    "    epochs=500,\n",
    "    scheduler_gamma=0.98,\n",
    "    learning_rate=1.0,\n",
    "    log_interval=5,\n",
    "    no_cuda=False,\n",
    "    seed=1,\n",
    "    verbose=True,\n",
    "    test_batch_size=512,\n",
    "    early_stopping_patience=300,\n",
    "    plot_curves=True,\n",
    "    tensorboard_log_dir=\"tb_logs/imitation\",\n",
    "    save_path=\"checkpoints/imitation_PPO\",\n",
    "    comet_ml_api_key=\"No20MKxPKu7vWLOUQCFBRO8mo\",\n",
    "    comet_ml_project_name=\"pretraining_rl\",\n",
    "    comet_ml_experiment_name=\"PPO_1\",\n",
    "    eval_freq = 5,\n",
    "    l2_reg_strength=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820005a-be57-4236-b66b-0b34ed558aff",
   "metadata": {},
   "source": [
    "---\n",
    "### Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e210c1-817a-42a8-ab28-2ea437ed2c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoTokenizer, AutoModel\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecCheckNan\n",
    "import multiprocessing\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"huggingface/CodeBERTa-small-v1\")\n",
    "#model = AutoModel.from_pretrained(\"huggingface/CodeBERTa-small-v1\").to('cuda')\n",
    "\n",
    "# Set number of cpus to use automatically:\n",
    "num_cpu = multiprocessing.cpu_count()\n",
    "print(\"Using {} cpus!\".format(num_cpu))\n",
    "observation_type = \"topopt_game\"\n",
    "\n",
    "train_env = sogym(mode='train',observation_type=observation_type,vol_constraint_type = 'hard',resolution=50,check_connectivity=True)#,model=model,tokenizer=tokenizer)\n",
    "env= make_vec_env(lambda:train_env, n_envs=num_cpu,vec_env_cls=SubprocVecEnv)\n",
    "env = VecCheckNan(env, raise_exception=True)\n",
    "#env=VecNormalize(env,gamma=1.0)\n",
    "\n",
    "eval_env = sogym(mode='test',observation_type=observation_type,vol_constraint_type='hard',resolution=50,check_connectivity=True)#,model=model,tokenizer=tokenizer)\n",
    "eval_env = make_vec_env(lambda:eval_env, n_envs=1,vec_env_cls=SubprocVecEnv)\n",
    "#eval_env =VecNormalize(eval_env,gamma=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b6f2e-5ceb-447f-ac10-a52f0379cb31",
   "metadata": {},
   "source": [
    "--- \n",
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1132db-34f4-4a72-bb12-1339a9e00a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "# The noise objects for TD3\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.5 * np.ones(n_actions))\n",
    "\n",
    "chosen_policy = \"MlpPolicy\" if observation_type == 'box_dense' else \"MultiInputPolicy\"\n",
    "\n",
    "feature_extractor = ImageDictExtractor if observation_type == 'image' or observation_type==\"topopt_game\" else CustomBoxDense\n",
    "\n",
    "# Load the YAML file\n",
    "\n",
    "with open(\"algorithms.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Extract the parameters for the desired algorithm\n",
    "algorithm_name = \"PPO\"  # or \"TD3\"\n",
    "algorithm_params = config[algorithm_name]\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=feature_extractor,\n",
    "    net_arch = config['common']['net_arch'],\n",
    "    share_features_extractor = False,\n",
    ")\n",
    "# Create the model based on the algorithm name and parameters\n",
    "if algorithm_name == \"SAC\":\n",
    "    model = SAC(env=env,\n",
    "                policy = chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                #action_noise = action_noise,\n",
    "                ent_coef = 0.0,\n",
    "                device=device, \n",
    "                **algorithm_params)\n",
    "\n",
    "elif algorithm_name == \"PPO\":\n",
    "    model = PPO(env=env, \n",
    "                policy = chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                device = device, \n",
    "                **algorithm_params)\n",
    "\n",
    "elif algorithm_name == \"TD3\":\n",
    "    # Create the action noise object\n",
    "    n_actions = env.action_space.shape[-1]\n",
    "    action_noise_params = algorithm_params.pop(\"action_noise\")\n",
    "    action_noise = NormalActionNoise(mean=action_noise_params[\"mean\"] * np.ones(n_actions),\n",
    "                                     \n",
    "                                     sigma=action_noise_params[\"sigma\"] * np.ones(n_actions))\n",
    "    model = TD3(env=env,\n",
    "                policy =chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                action_noise=action_noise,\n",
    "                device=device, \n",
    "                **algorithm_params)\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Create the tb_log_name string\n",
    "tb_log_name = f\"{algorithm_name}_{current_datetime}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec8bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a checkpoint every 1000 steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=50000//num_cpu,\n",
    "  save_path=\"./checkpoints/\",\n",
    "  name_prefix=tb_log_name,\n",
    "  save_replay_buffer=True,\n",
    "  save_vecnormalize=True,\n",
    ")\n",
    "\n",
    "eval_callback = EvalCallback(eval_env,\n",
    "                             log_path='tb_logs',\n",
    "                             eval_freq=5000//num_cpu,\n",
    "                             deterministic=True,\n",
    "                            n_eval_episodes=10,\n",
    "                             render=False,\n",
    "                             best_model_save_path='./checkpoints',\n",
    "                             verbose=0)\n",
    "\n",
    "callback_list = CallbackList([eval_callback,\n",
    "                         checkpoint_callback,\n",
    "                         MaxRewardCallback(verbose=1),\n",
    "                         GradientClippingCallback(clip_value=1.0, verbose=1),\n",
    "                         GradientNormCallback(verbose=1),\n",
    "                         FigureRecorderCallback(check_freq=5000//num_cpu,eval_env=eval_env),\n",
    "                         StopTrainingOnNoModelImprovement(max_no_improvement_evals=50, min_evals=100, verbose=1)\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f70cf1-e15e-4fed-ab75-12bdca996f2d",
   "metadata": {},
   "source": [
    "--- \n",
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a84e3f",
   "metadata": {},
   "source": [
    "Save the model:\n",
    "\n",
    "If model is on-policy:\n",
    "#model.save(\"sac_pendulum\")\n",
    "#loaded_model = SAC.load(\"sac_pendulum\")\n",
    "\n",
    "if model is off-policy, we also need to save the replay buffer:\n",
    "#model.save_replay_buffer(\"sac_replay_buffer\")\n",
    "#loaded_model.load_replay_buffer(\"sac_replay_buffer\")\n",
    "\n",
    "If the environment is normalized:\n",
    "#env.save('env_saved.pkl')\n",
    "#env = VecNormalize.load('env_saved.pkl',env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d7768-b7bb-4c14-b620-28a278392b6b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "            \n",
    "#model = SAC.load(\"checkpoints/imitation_SAC\",env =env) \n",
    "model.set_parameters(\"checkpoints/imitation_PPO.zip\")\n",
    "model.set_parameters(\"imitation_PPO_critic\")\n",
    "\n",
    "train_critic_only = False\n",
    "if train_critic_only:\n",
    "    #Freeze everything:\n",
    "    for name, param in model.policy.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            param.requires_grad=False\n",
    "\n",
    "    if algorithm_name =='SAC'\n",
    "        # Unfreeze critic:\n",
    "        for param in model.policy.critic.parameters():\n",
    "            if param.requires_grad==False:\n",
    "                param.requires_grad=True\n",
    "\n",
    "        for param in model.policy.critic_target.parameters():\n",
    "            if param.requires_grad==False:\n",
    "                param.requires_grad=True\n",
    "        \n",
    "        #Reset critic networks:\n",
    "        if hasattr(model.policy.critic_target, 'reset_parameters'):\n",
    "            print(' resetting')\n",
    "            model.policy.critic_target.reset_parameters()\n",
    "        else:\n",
    "            model.policy.critic_target.apply(init_weights)\n",
    "\n",
    "            \n",
    "        if hasattr(model.policy.critic, 'reset_parameters'):\n",
    "            print('resetting')\n",
    "            model.policy.critic_target.reset_parameters() \n",
    "        else:\n",
    "            model.policy.critic.apply(init_weights)\n",
    "\n",
    "\n",
    "    if algorithm_name == 'PPO':\n",
    "        for param in model.policy.mlp_extractor.value_net.parameters():\n",
    "            if param.requires_grad==False:\n",
    "                param.requires_grad=True\n",
    "            \n",
    "        for param in model.policy.value_net.parameters():\n",
    "            if param.requires_grad==False:\n",
    "                param.requires_grad=True\n",
    "        \n",
    "\n",
    "        if hasattr(model.policy.value_net, 'reset_parameters'):\n",
    "            print(' resetting')\n",
    "            model.policy.value_net.reset_parameters()\n",
    "        else:\n",
    "            model.policy.value_net.apply(init_weights)\n",
    "            \n",
    "        if hasattr(model.policy.mlp_extractor.value_net, 'reset_parameters'):\n",
    "            print(' resetting')\n",
    "            model.policy.mlp_extractor.value_net.reset_parameters() \n",
    "        else:\n",
    "            model.policy.mlp_extractor.value_net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c876f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(model.batch_size)\n",
    "#model.load_replay_buffer(\"sac_replay_buffer\")\n",
    "model.learn(20000000,\n",
    "            callback=callback_list, \n",
    "            tb_log_name=tb_log_name\n",
    "            )\n",
    "#model.save('model_saved_march15',)\n",
    "#model.save_replay_buffer(\"sac_replay_buffer_march15\")\n",
    "\n",
    "#env.save('env_saved.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27da122",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('checkpoints/imitation_PPO_critic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2827c8cc-028d-4448-b109-d6542816df2a",
   "metadata": {},
   "source": [
    "---\n",
    "### Let's visualize the agent's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6728408e-71c2-4f7b-a8f1-f0cc6b43dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=sogym(mode='train',observation_type='image',vol_constraint_type='hard' ,resolution = 50)\n",
    "#env= make_vec_env(lambda:env, n_envs=1,vec_env_cls=SubprocVecEnv)\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5cbb153d-80e3-4d6c-ada3-f4d323360b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.99356264 -0.88954514  0.9281974  -0.01584718  0.8976205   1.        ]\n",
      "[-1.         -0.16608484  0.29554093 -0.54767656  0.5325971   0.55634016]\n",
      "[-1.          0.15776648  0.8151289   0.12417474  0.9366684   0.6389875 ]\n",
      "[-0.82835186  0.5015193   0.96246237 -0.62295455  0.96702075  0.96023256]\n",
      "[-1.         -1.          1.         -0.14496084  0.9393588   1.        ]\n",
      "[-0.9223096  -0.3790284   1.         -0.35445184  1.          1.        ]\n",
      "[-1.         -0.2980384   1.          0.00239816  0.6195617   0.7502269 ]\n",
      "[-0.9283385  -0.21001871  0.93565124  0.03575581  0.91625756  1.        ]\n",
      "Desired volume: 0.49 Obtained volume: 0.3380188606665687\n",
      "Env reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "obs,info=env.reset()\n",
    "dones=False\n",
    "saved_conditions = env.conditions\n",
    "saved_nelx, saved_nely = env.nelx, env.nely\n",
    "saved_dx, saved_dy = env.dx, env.dy\n",
    "#use deepcopy to save \n",
    "while dones== False:\n",
    "    action, _states = model.predict(obs,deterministic=True)\n",
    "    print(action)\n",
    "    obs, rewards, dones,truncated, info = env.step(action)\n",
    "print(\"Desired volume:\",saved_conditions['volfrac'],\"Obtained volume:\",env.volume)\n",
    "print(\"Env reward:\",rewards)\n",
    "fig = env.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "10b8b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('trained_agent.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d2de1f-dd90-40b2-acc9-b3227fd2888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xval, f0val,it, H, Phimax, allPhi, den, N, cfg = run_mmc(saved_conditions,saved_nelx,saved_nely,saved_dx,saved_dy,plotting='contour')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('SB3_update')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e21ef5adabae340b8408649b4e28a9d7d4d8eaab8fdd4faf01af585df564eed2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
