{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94e6157f-d831-41f7-b27c-f949e2253f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sogym.env import sogym\n",
    "import numpy as np\n",
    "import stable_baselines3\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize, SubprocVecEnv\n",
    "import torch\n",
    "from sogym.mmc_optim import run_mmc\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "from sogym.env import sogym\n",
    "from sogym.mmc_optim import run_mmc\n",
    "from sogym.expert_generation import generate_expert_dataset\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print('SB3 version:', stable_baselines3.__version__)\n",
    "# Let's make the code device agnostic:\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02b8cad-4629-49cf-a15e-ebb7c9a5b6c1",
   "metadata": {},
   "source": [
    "---\n",
    "### Environment test and visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edcde2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "check_env(sogym(mode='train',observation_type='image'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f888890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sogym.expert_generation import generate_mmc_solutions,generate_dataset\n",
    "\n",
    "dataset_folder = \"/home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/mmc\"\n",
    "#generate_mmc_solutions(key=0,dataset_folder=\"/home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/mmc\")\n",
    "generate_dataset(dataset_folder= dataset_folder, num_threads=28, num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a4374a-b735-4474-8427-30b1d55dfe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the environment using the check_env util from SB3:\n",
    "train_env = sogym(mode='train',observation_type='image',vol_constraint_type='hard',resolution=50,check_connectivity = False)\n",
    "eval_env = sogym(mode='test',observation_type='image',vol_constraint_type='hard',resolution=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f4a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the number of permutations to generate\n",
    "num_permutations = 1\n",
    "observation_type = \"image\"\n",
    "\n",
    "# Specify the environment configuration (optional)\n",
    "env_kwargs = {\n",
    "    'mode': 'train',\n",
    "    'observation_type': 'image',\n",
    "    'vol_constraint_type': 'hard',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "directory_path = \"/home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/mmc\"\n",
    "expert_observations, expert_actions = generate_expert_dataset(directory_path,env_kwargs, plot_terminated=False,num_processes = 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3491cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpertDataSet(Dataset):\n",
    "    def __init__(self, expert_observations, expert_actions, env):\n",
    "        if isinstance(env.observation_space, gym.spaces.Dict):\n",
    "            # Handle Dict observation space\n",
    "            self.observations = {}\n",
    "            for key in env.observation_space.spaces.keys():\n",
    "                self.observations[key] = th.from_numpy(np.stack([obs[key] for obs in expert_observations]).astype(np.float32))\n",
    "        else:\n",
    "            # Handle Box observation space\n",
    "            self.observations = th.from_numpy(expert_observations.astype(np.float32))  # Convert to float32\n",
    "        \n",
    "        if isinstance(env.action_space, gym.spaces.Box):\n",
    "            self.actions = th.from_numpy(expert_actions).double()\n",
    "        else:\n",
    "            self.actions = th.from_numpy(expert_actions).long()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(self.observations, dict):\n",
    "            # Handle Dict observation space\n",
    "            obs_dict = {k: v[index] for k, v in self.observations.items()}\n",
    "            return obs_dict, self.actions[index]\n",
    "        else:\n",
    "            # Handle Box observation space\n",
    "            return self.observations[index], self.actions[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ac77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_agent(\n",
    "    student,\n",
    "    expert_observations,\n",
    "    expert_actions,\n",
    "    env,\n",
    "    batch_size=64,\n",
    "    epochs=1000,\n",
    "    scheduler_gamma=0.7,\n",
    "    learning_rate=1.0,\n",
    "    log_interval=100,\n",
    "    no_cuda=True,\n",
    "    seed=1,\n",
    "    test_batch_size=64,\n",
    "    early_stopping_patience=10,\n",
    "    plot_curves=True,\n",
    "):\n",
    "    use_cuda = not no_cuda and th.cuda.is_available()\n",
    "    th.manual_seed(seed)\n",
    "    device = th.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
    "\n",
    "    if isinstance(env.action_space, gym.spaces.Box):\n",
    "        criterion = nn.MSELoss()\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Extract initial policy\n",
    "    model = student.policy.to(device)\n",
    "\n",
    "    def train(model, device, train_loader, optimizer, epoch, max_grad_norm):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        num_batches = len(train_loader)\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            if isinstance(data, dict):\n",
    "                # Handle Dict observation space\n",
    "                data = {k: v.to(device) for k, v in data.items()}\n",
    "            else:\n",
    "                # Handle Box observation space\n",
    "                data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if isinstance(env.action_space, gym.spaces.Box):\n",
    "                # A2C/PPO policy outputs actions, values, log_prob\n",
    "                # SAC/TD3 policy outputs actions only\n",
    "                if isinstance(student, (A2C, PPO)):\n",
    "                    action, _, _ = model(data)\n",
    "                else:\n",
    "                    # SAC/TD3:\n",
    "                    action = model(data)\n",
    "                action_prediction = action.double()\n",
    "            else:\n",
    "                # Retrieve the logits for A2C/PPO when using discrete actions\n",
    "                dist = model.get_distribution(data)\n",
    "                action_prediction = dist.distribution.logits\n",
    "                target = target.long()\n",
    "\n",
    "            loss = criterion(action_prediction, target)\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "            # Calculate gradient norm after clipping\n",
    "            grad_norm = 0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    grad_norm += param_norm.item() ** 2\n",
    "            grad_norm = grad_norm ** 0.5\n",
    "\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                print(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tGrad Norm: {:.6f}\\tLR: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(data),\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                        grad_norm,\n",
    "                        current_lr,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        train_loss /= num_batches\n",
    "        print(f\"Train set: Average loss: {train_loss:.4f}\")\n",
    "        return train_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def test(model, device, test_loader):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        num_batches = len(test_loader)\n",
    "        with th.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                if isinstance(data, dict):\n",
    "                    # Handle Dict observation space\n",
    "                    data = {k: v.to(device) for k, v in data.items()}\n",
    "                else:\n",
    "                    # Handle Box observation space\n",
    "                    data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                if isinstance(env.action_space, gym.spaces.Box):\n",
    "                    # A2C/PPO policy outputs actions, values, log_prob\n",
    "                    # SAC/TD3 policy outputs actions only\n",
    "                    if isinstance(student, (A2C, PPO)):\n",
    "                        action, _, _ = model(data)\n",
    "                    else:\n",
    "                        # SAC/TD3:\n",
    "                        action = model(data)\n",
    "                    action_prediction = action.double()\n",
    "                else:\n",
    "                    # Retrieve the logits for A2C/PPO when using discrete actions\n",
    "                    dist = model.get_distribution(data)\n",
    "                    action_prediction = dist.distribution.logits\n",
    "                    target = target.long()\n",
    "\n",
    "                test_loss += criterion(action_prediction, target).item()\n",
    "        test_loss /= num_batches\n",
    "        print(f\"Test set: Average loss: {test_loss:.4f}\")\n",
    "        return test_loss\n",
    "\n",
    "\n",
    "    expert_dataset = ExpertDataSet(expert_observations, expert_actions, env)\n",
    "\n",
    "    train_size = int(0.8 * len(expert_dataset))\n",
    "    test_size = len(expert_dataset) - train_size\n",
    "    train_expert_dataset, test_expert_dataset = random_split(\n",
    "        expert_dataset, [train_size, test_size]\n",
    "    )\n",
    "\n",
    "    train_loader = th.utils.data.DataLoader(\n",
    "        dataset=train_expert_dataset, batch_size=batch_size, shuffle=True, **kwargs\n",
    "    )\n",
    "    test_loader = th.utils.data.DataLoader(\n",
    "        dataset=test_expert_dataset,\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=True,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=scheduler_gamma)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    if plot_curves:\n",
    "        plt.ion()\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_title('Training and Test Loss Curves')\n",
    "        train_line, = ax.plot([], [], label='Train Loss')\n",
    "        test_line, = ax.plot([], [], label='Test Loss')\n",
    "        ax.legend()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train(model, device, train_loader, optimizer, epoch,max_grad_norm = 0.5)\n",
    "        test_loss = test(model, device, test_loader)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        if plot_curves:\n",
    "            train_line.set_data(range(1, epoch + 1), train_losses)\n",
    "            test_line.set_data(range(1, epoch + 1), test_losses)\n",
    "            ax.relim()\n",
    "            # add grid lines:\n",
    "            ax.grid(True)\n",
    "            ax.autoscale_view(True, True, True)\n",
    "            fig.canvas.draw()\n",
    "            fig.canvas.flush_events()\n",
    "\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            no_improvement_count = 0\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "\n",
    "        if no_improvement_count >= early_stopping_patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    student.policy = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ad778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from stable_baselines3 import SAC, TD3, PPO\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from sogym.utils import ImageDictExtractor, CustomBoxDense\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "chosen_policy = \"MlpPolicy\" if observation_type == 'box_dense' else \"MultiInputPolicy\"\n",
    "\n",
    "feature_extractor = ImageDictExtractor if observation_type == 'image' else CustomBoxDense\n",
    "\n",
    "# Load the YAML file\n",
    "env=train_env\n",
    "\n",
    "with open(\"algorithms.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Extract the parameters for the desired algorithm\n",
    "algorithm_name = \"PPO\"  # or \"TD3\"\n",
    "algorithm_params = config[algorithm_name]\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    #features_extractor_class=feature_extractor,\n",
    "    net_arch = config['common']['net_arch']\n",
    ")\n",
    "\n",
    "# Create the model based on the algorithm name and parameters\n",
    "if algorithm_name == \"SAC\":\n",
    "    model = SAC(env=env,\n",
    "                policy = chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                device=device, \n",
    "                **algorithm_params)\n",
    "\n",
    "elif algorithm_name == \"PPO\":\n",
    "    model = PPO(env=env, \n",
    "                policy = chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                device = device, \n",
    "                **algorithm_params)\n",
    "\n",
    "elif algorithm_name == \"TD3\":\n",
    "    # Create the action noise object\n",
    "    n_actions = env.action_space.shape[-1]\n",
    "    action_noise_params = algorithm_params.pop(\"action_noise\")\n",
    "    action_noise = NormalActionNoise(mean=action_noise_params[\"mean\"] * np.ones(n_actions),\n",
    "                                     sigma=action_noise_params[\"sigma\"] * np.ones(n_actions))\n",
    "    model = TD3(env=env,\n",
    "                policy =chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                action_noise=action_noise,\n",
    "                device=device, \n",
    "                **algorithm_params)\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Create the tb_log_name string\n",
    "tb_log_name = f\"{algorithm_name}_{current_datetime}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df482b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.policy.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0166b0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_agent(\n",
    "    model,\n",
    "    expert_observations,\n",
    "    expert_actions,\n",
    "    env,\n",
    "    batch_size=256,\n",
    "    epochs=100,\n",
    "    scheduler_gamma=0.98,\n",
    "    learning_rate=3e-3,\n",
    "    log_interval=5,\n",
    "    no_cuda=False,\n",
    "    seed=1,\n",
    "    test_batch_size=128,\n",
    "    early_stopping_patience=300,\n",
    "    plot_curves=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8983b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.policy.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221e6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the training environment on a random problem statement and visualize a 'successful' solution:\n",
    "reward = 0.0\n",
    "while reward==0.0:\n",
    "    obs = train_env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = train_env.action_space.sample()\n",
    "        obs, reward, done,truncated, info = train_env.step(action)\n",
    "        \n",
    "# print(\"Volume: \", train_env.volume)\n",
    "print(\"Reward \",reward)\n",
    "\n",
    "train_env.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0634399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sogym.utils import profile_and_analyze\n",
    "# Example usage\n",
    "# Specify the number of episodes to run\n",
    "num_episodes = 20\n",
    "# Call the profile_and_analyze function\n",
    "result_df = profile_and_analyze(num_episodes, train_env)\n",
    "# Print the resulting DataFrame\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820005a-be57-4236-b66b-0b34ed558aff",
   "metadata": {},
   "source": [
    "---\n",
    "### Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e210c1-817a-42a8-ab28-2ea437ed2c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoTokenizer, AutoModel\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecCheckNan\n",
    "import multiprocessing\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"huggingface/CodeBERTa-small-v1\")\n",
    "#model = AutoModel.from_pretrained(\"huggingface/CodeBERTa-small-v1\").to('cuda')\n",
    "\n",
    "# Set number of cpus to use automatically:\n",
    "num_cpu = multiprocessing.cpu_count()\n",
    "# num_cpu = \n",
    "print(\"Using {} cpus!\".format(num_cpu))\n",
    "observation_type = \"image\"\n",
    "\n",
    "train_env = sogym(mode='train',observation_type=observation_type,vol_constraint_type = 'hard',resolution=50,check_connectivity=True)#,model=model,tokenizer=tokenizer)\n",
    "env= make_vec_env(lambda:train_env, n_envs=num_cpu,vec_env_cls=SubprocVecEnv)\n",
    "env = VecCheckNan(env, raise_exception=True)\n",
    "#env=VecNormalize(env,gamma=1.0)\n",
    "\n",
    "eval_env = sogym(mode='test',observation_type=observation_type,vol_constraint_type='hard',resolution=50,check_connectivity=True)#,model=model,tokenizer=tokenizer)\n",
    "eval_env = make_vec_env(lambda:eval_env, n_envs=1,vec_env_cls=SubprocVecEnv)\n",
    "#eval_env =VecNormalize(eval_env,gamma=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b6f2e-5ceb-447f-ac10-a52f0379cb31",
   "metadata": {},
   "source": [
    "--- \n",
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1132db-34f4-4a72-bb12-1339a9e00a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from stable_baselines3 import SAC, TD3, PPO\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from sogym.utils import ImageDictExtractor, CustomBoxDense\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "chosen_policy = \"MlpPolicy\" if observation_type == 'box_dense' else \"MultiInputPolicy\"\n",
    "\n",
    "feature_extractor = ImageDictExtractor if observation_type == 'image' else CustomBoxDense\n",
    "\n",
    "# Load the YAML file\n",
    "\n",
    "\n",
    "with open(\"algorithms.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Extract the parameters for the desired algorithm\n",
    "algorithm_name = \"PPO\"  # or \"TD3\"\n",
    "algorithm_params = config[algorithm_name]\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=feature_extractor,\n",
    "    net_arch = config['common']['net_arch']\n",
    ")\n",
    "\n",
    "# Create the model based on the algorithm name and parameters\n",
    "if algorithm_name == \"SAC\":\n",
    "    model = SAC(env=env,\n",
    "                policy = chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                device=device, \n",
    "                **algorithm_params)\n",
    "\n",
    "elif algorithm_name == \"PPO\":\n",
    "    model = PPO(env=env, \n",
    "                policy = chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                device = device, \n",
    "                **algorithm_params)\n",
    "\n",
    "elif algorithm_name == \"TD3\":\n",
    "    # Create the action noise object\n",
    "    n_actions = env.action_space.shape[-1]\n",
    "    action_noise_params = algorithm_params.pop(\"action_noise\")\n",
    "    action_noise = NormalActionNoise(mean=action_noise_params[\"mean\"] * np.ones(n_actions),\n",
    "                                     sigma=action_noise_params[\"sigma\"] * np.ones(n_actions))\n",
    "    model = TD3(env=env,\n",
    "                policy =chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                action_noise=action_noise,\n",
    "                device=device, \n",
    "                **algorithm_params)\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Create the tb_log_name string\n",
    "tb_log_name = f\"{algorithm_name}_{current_datetime}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec8bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's  test that an eval callback actually works:\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList\n",
    "from sogym.callbacks import FigureRecorderCallback, MaxRewardCallback, GradientNormCallback\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "\n",
    "# Save a checkpoint every 1000 steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=5000//num_cpu,\n",
    "  save_path=\"./checkpoints/\",\n",
    "  name_prefix=tb_log_name,\n",
    "  save_replay_buffer=True,\n",
    "  save_vecnormalize=True,\n",
    ")\n",
    "\n",
    "eval_callback = EvalCallback(eval_env,\n",
    "                             log_path='tb_logs',\n",
    "                             eval_freq=5000//num_cpu,\n",
    "                             deterministic=True,\n",
    "                            n_eval_episodes=10,\n",
    "                             render=False,\n",
    "                             best_model_save_path='./checkpoints',\n",
    "                             verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "callback_list = CallbackList([eval_callback,\n",
    "                         checkpoint_callback,\n",
    "                         MaxRewardCallback(verbose=1),\n",
    "                         GradientNormCallback(verbose=1),\n",
    "                         FigureRecorderCallback(check_freq=5000//num_cpu,eval_env=eval_env)\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f70cf1-e15e-4fed-ab75-12bdca996f2d",
   "metadata": {},
   "source": [
    "--- \n",
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a84e3f",
   "metadata": {},
   "source": [
    "Save the model:\n",
    "\n",
    "If model is on-policy:\n",
    "#model.save(\"sac_pendulum\")\n",
    "#loaded_model = SAC.load(\"sac_pendulum\")\n",
    "\n",
    "if model is off-policy, we also need to save the replay buffer:\n",
    "#model.save_replay_buffer(\"sac_replay_buffer\")\n",
    "#loaded_model.load_replay_buffer(\"sac_replay_buffer\")\n",
    "\n",
    "If the environment is normalized:\n",
    "#env.save('env_saved.pkl')\n",
    "#env = VecNormalize.load('env_saved.pkl',env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d7768-b7bb-4c14-b620-28a278392b6b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = SAC.load(\"model_saved_march15\",env =env) #Saved model is with soft volume constraint and 75 r\n",
    "#model.set_parameters(\"model_saved_march15\")\n",
    "#print(model.batch_size)\n",
    "#model.load_replay_buffer(\"sac_replay_buffer\")\n",
    "model.learn(20000000,\n",
    "            callback=callback_list, \n",
    "            tb_log_name=tb_log_name\n",
    "            )\n",
    "#model.save('model_saved_march15',)\n",
    "#model.save_replay_buffer(\"sac_replay_buffer_march15\")\n",
    "\n",
    "#env.save('env_saved.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2827c8cc-028d-4448-b109-d6542816df2a",
   "metadata": {},
   "source": [
    "---\n",
    "### Let's visualize the agent's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6728408e-71c2-4f7b-a8f1-f0cc6b43dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=sogym(mode='train',observation_type='image',vol_constraint_type='hard' ,resolution = 50)\n",
    "#env= make_vec_env(lambda:env, n_envs=1,vec_env_cls=SubprocVecEnv)\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb153d-80e3-4d6c-ada3-f4d323360b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs,info=env.reset()\n",
    "dones=False\n",
    "saved_conditions = env.conditions\n",
    "saved_nelx, saved_nely = env.nelx, env.nely\n",
    "saved_dx, saved_dy = env.dx, env.dy\n",
    "#use deepcopy to save \n",
    "while dones== False:\n",
    "    action, _states = model.predict(obs,deterministic=True)\n",
    "    # print(action)\n",
    "    obs, rewards, dones,truncated, info = env.step(action)\n",
    "print(\"Desired volume:\",saved_conditions['volfrac'],\"Obtained volume:\",env.volume)\n",
    "print(\"Env reward:\",rewards, \"Compliance:\",np.exp(1/rewards))\n",
    "env.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d2de1f-dd90-40b2-acc9-b3227fd2888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xval, f0val,it, H, Phimax, allPhi, den, N, cfg = run_mmc(saved_conditions,saved_nelx,saved_nely,saved_dx,saved_dy,plotting='contour')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb3_update",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
