{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94e6157f-d831-41f7-b27c-f949e2253f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'       #Disactivate multiprocessing for numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import gymnasium as gym\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "import stable_baselines3\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize, SubprocVecEnv\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList, CheckpointCallback, StopTrainingOnNoModelImprovement\n",
    "\n",
    "from sogym.mmc_optim import run_mmc\n",
    "from sogym.env import sogym\n",
    "from sogym.expert_generation import generate_expert_dataset, generate_mmc_solutions, generate_dataset\n",
    "from sogym.utils import profile_and_analyze,ImageDictExtractor, CustomBoxDense\n",
    "from sogym.callbacks import FigureRecorderCallback, MaxRewardCallback, GradientNormCallback, GradientClippingCallback\n",
    "from sogym.pretraining import pretrain_agent, ExpertDataSet\n",
    "\n",
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import random_split, Dataset\n",
    "from IPython.display import display\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print('SB3 version:', stable_baselines3.__version__)\n",
    "# Let's make the code device agnostic:\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02b8cad-4629-49cf-a15e-ebb7c9a5b6c1",
   "metadata": {},
   "source": [
    "---\n",
    "### Environment test and visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85a4374a-b735-4474-8427-30b1d55dfe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the environment using the check_env util from SB3:\n",
    "observation_type = 'topopt_game'\n",
    "train_env = sogym(mode='train',observation_type=observation_type,vol_constraint_type='hard',resolution=50,check_connectivity = True)\n",
    "eval_env = sogym(mode='test',observation_type=observation_type,vol_constraint_type='hard',resolution=50,check_connectivity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27032137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reward = 0.0\n",
    "while reward == 0.0:\n",
    "    obs, info = train_env.reset()\n",
    "    dones = False\n",
    "    while not dones:\n",
    "        action = train_env.action_space.sample()\n",
    "        obs, reward, dones, truncated, info = train_env.step(action)\n",
    "\n",
    "    fig = train_env.plot()\n",
    "fig.savefig('env_test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "413948af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sogym.utils import visualize_expert_trajectory\n",
    "#visualize an expert trajectory:\n",
    "\n",
    "file_path = '/home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/unique_narval/20240414-174038-790212.json'\n",
    "visualize_expert_trajectory(train_env, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6edcde2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will check your custom environment and output additional warnings if needed\n",
    "check_env(sogym(mode='train',observation_type='topopt_game'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10de06f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Specify the number of episodes to run\n",
    "num_episodes = 20\n",
    "# Call the profile_and_analyze function\n",
    "result_df = profile_and_analyze(num_episodes, train_env)\n",
    "# Print the resulting DataFrame\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f888890",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = train_env.reset()\n",
    "cfg = {\n",
    "            'optimizer':'mma', #optimiser choice\n",
    "            'xInt':0.25, #initial interval of components in x\n",
    "            'yInt':0.25, #initial interval of components in y\n",
    "            'E':1.0, #Young's modulus\n",
    "            'nu':0.3, #Poisson ratio\n",
    "            'h':1, #thickness\n",
    "            'dgt0':5, #significant digit of sens.\n",
    "            'scl':1, #scale factor for obj\n",
    "            'p':6,  #power of super ellipsoid\n",
    "            'lmd':100, #power of KS aggregation   \n",
    "            'maxiter':500, # maximum number of outer iterations\n",
    "            'alpha':1e-9, # This is the threshold level in the Heaviside function\n",
    "            'epsilon':0.2, #This is the regularization term in the Heaviside function\n",
    "            'maxinnerinit':1, # This is the maximum number of inner iterations for GCMMA\n",
    "            'switch':-0.000002, # This is the switch criteria for the hybrid optimizer\n",
    "            'convergence_threshold':2e-4, #This is the threshold for the relative change in the objective function\n",
    "            'xmin':(0.0, 0.0, 0.0, 0.00, 0.00, -np.pi),\n",
    "            'xmax':(train_env.dx, train_env.dy, 0.7*min(train_env.dx,train_env.dy), 0.05*min(train_env.dx,train_env.dy),0.05*min(train_env.dx,train_env.dy), np.pi)\n",
    "        }\n",
    "\n",
    "#run_mmc(train_env.conditions,train_env.nelx,train_env.nely,train_env.dx,train_env.dy,plotting='contour',verbose=0,cfg=cfg)\n",
    "dataset_folder = \"/home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/mmc\"\n",
    "#generate_mmc_solutions(key=0,dataset_folder=\"/home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/mmc\")\n",
    "generate_dataset(dataset_folder= dataset_folder, num_threads=32, num_samples=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a21145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axes for the subplots\n",
    "fig, axes = plt.subplots(nrows=5, ncols=5, figsize=(10, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Initialize the index for the current subplot\n",
    "subplot_index = 0\n",
    "\n",
    "# Let's visualize the training environment on a random problem statement and visualize a 'successful' solution:\n",
    "reward = 0.0\n",
    "while reward == 0.0:\n",
    "    obs = train_env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = train_env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = train_env.step(action)\n",
    "        \n",
    "        # Plot the current observation image\n",
    "        axes[subplot_index].imshow(obs['strain_energy'].T, cmap='gray')\n",
    "        axes[subplot_index].axis('off')\n",
    "        axes[subplot_index].set_title(f\"Timestep {subplot_index+1}\")\n",
    "        \n",
    "        # Increment the subplot index\n",
    "        subplot_index += 1\n",
    "        \n",
    "        # If all subplots are filled, display the plot and reset the index\n",
    "        if subplot_index == len(axes):\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            subplot_index = 0\n",
    "\n",
    "# Print the reward\n",
    "print(\"Reward:\", reward)\n",
    "\n",
    "# Plot the final state of the training environment\n",
    "train_env.plot()\n",
    "\n",
    "# Display any remaining subplots\n",
    "if subplot_index > 0:\n",
    "    for i in range(subplot_index, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b3f4a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 38855/38855 [3:42:43<00:00,  2.91file/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# Specify the number of permutations to generate\n",
    "num_permutations = None\n",
    "observation_type = \"topopt_game\"\n",
    "\n",
    "# Specify the environment configuration (optional)\n",
    "env_kwargs = {\n",
    "    'mode': 'train',\n",
    "    'observation_type': observation_type,\n",
    "    'vol_constraint_type': 'hard',\n",
    "    'seed': 42,\n",
    "    'resolution' : 50, \n",
    "    'check_connectivity':True\n",
    "}\n",
    "\n",
    "directory_path = \"/home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies_narval\"\n",
    "generate_expert_dataset(directory_path,env_kwargs, plot_terminated=False,num_permutations = num_permutations, file_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40f0e710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    }
   ],
   "source": [
    "# Copy the files in /home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/unique_narval \n",
    "# and the files in /home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/holodeck_may12\n",
    "# to: /home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/combined\n",
    "!find /home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/cortex_may13 -name \"*.json\" -print0 | xargs -0 -I {} cp {} /home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/combined/\n",
    "!find /home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/holodeck_may12 -name \"*.json\" -print0 | xargs -0 -I {} cp {} /home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/combined/\n",
    "!find /home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/narval_may13 -name \"*.json\" -print0 | xargs -0 -I {} cp {} /home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/combined/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "309429d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Processing files: 100%|█████████▉| 37859/37860 [03:57<00:00, 159.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates found. Check 'duplicate.txt' for the list of duplicate files.\n",
      "Unique files listed in 'unique_files.txt'.\n",
      "Unique files copied to '/home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/unique_combined'.\n"
     ]
    }
   ],
   "source": [
    "from sogym.expert_generation import check_duplicates, copy_unique_files\n",
    "# Specify the folder path containing the .json files\n",
    "folder_path = '/home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/combined'\n",
    "\n",
    "# Adjust the percentage as needed, e.g., 50 for 50%\n",
    "check_duplicates(folder_path, percentage=100)\n",
    "\n",
    "\n",
    "# Specify the path to the unique_files.txt file\n",
    "unique_files_file = 'unique_files.txt'\n",
    "# Specify the destination folder for the unique files\n",
    "destination_folder = '/home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/unique_combined'\n",
    "\n",
    "# Copy the unique files to the destination folder\n",
    "copy_unique_files(unique_files_file, destination_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7996845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  28%|██▊       | 10530/37419 [07:10<14:16, 31.40file/s]  /scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  28%|██▊       | 10547/37419 [07:10<19:31, 22.95file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  28%|██▊       | 10592/37419 [07:12<20:02, 22.30file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  28%|██▊       | 10605/37419 [07:13<21:15, 21.02file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  28%|██▊       | 10632/37419 [07:14<16:29, 27.08file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  28%|██▊       | 10640/37419 [07:14<17:10, 25.99file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  28%|██▊       | 10654/37419 [07:15<13:27, 33.15file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  28%|██▊       | 10662/37419 [07:15<14:47, 30.15file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▊       | 10672/37419 [07:15<18:48, 23.69file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▊       | 10679/37419 [07:15<13:26, 33.14file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▊       | 10687/37419 [07:16<15:02, 29.61file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▊       | 10701/37419 [07:16<20:37, 21.58file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▊       | 10717/37419 [07:17<23:15, 19.13file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▊       | 10721/37419 [07:17<19:19, 23.03file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▊       | 10730/37419 [07:18<20:16, 21.94file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▊       | 10748/37419 [07:19<21:44, 20.44file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▊       | 10751/37419 [07:19<20:24, 21.77file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▉       | 10770/37419 [07:20<22:26, 19.79file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▉       | 10822/37419 [07:22<16:22, 27.07file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▉       | 10835/37419 [07:22<20:24, 21.71file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▉       | 10854/37419 [07:23<20:57, 21.13file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▉       | 10857/37419 [07:23<22:35, 19.60file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▉       | 10861/37419 [07:23<19:24, 22.81file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▉       | 10910/37419 [07:26<18:27, 23.93file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▉       | 10916/37419 [07:26<18:55, 23.33file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▉       | 10932/37419 [07:26<14:16, 30.92file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▉       | 10940/37419 [07:27<20:47, 21.23file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▉       | 10945/37419 [07:27<17:11, 25.66file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▉       | 10952/37419 [07:27<19:23, 22.75file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▉       | 10960/37419 [07:28<20:33, 21.46file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▉       | 10964/37419 [07:28<19:27, 22.66file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▉       | 10968/37419 [07:28<18:57, 23.26file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▉       | 10978/37419 [07:28<19:45, 22.31file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▉       | 10983/37419 [07:29<16:37, 26.49file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  29%|██▉       | 10997/37419 [07:29<20:14, 21.75file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|██▉       | 11055/37419 [07:32<14:12, 30.91file/s]  /scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|██▉       | 11065/37419 [07:32<14:04, 31.19file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|██▉       | 11088/37419 [07:33<15:11, 28.88file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|██▉       | 11095/37419 [07:33<18:24, 23.82file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|██▉       | 11103/37419 [07:33<15:04, 29.10file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|██▉       | 11110/37419 [07:34<21:38, 20.26file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|██▉       | 11118/37419 [07:34<20:47, 21.09file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|██▉       | 11147/37419 [07:35<12:30, 34.99file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|██▉       | 11167/37419 [07:36<17:04, 25.61file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|██▉       | 11179/37419 [07:36<20:50, 20.98file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|██▉       | 11205/37419 [07:38<18:15, 23.94file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|██▉       | 11209/37419 [07:38<15:48, 27.62file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|██▉       | 11212/37419 [07:38<16:22, 26.68file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|███       | 11241/37419 [07:39<18:29, 23.60file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|███       | 11251/37419 [07:39<19:05, 22.84file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|███       | 11262/37419 [07:40<20:08, 21.65file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|███       | 11265/37419 [07:40<24:56, 17.48file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|███       | 11281/37419 [07:41<16:30, 26.38file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|███       | 11287/37419 [07:41<19:01, 22.90file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|███       | 11296/37419 [07:41<14:35, 29.84file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|███       | 11300/37419 [07:41<20:51, 20.87file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|███       | 11336/37419 [07:43<17:58, 24.18file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|███       | 11339/37419 [07:43<17:52, 24.32file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|███       | 11354/37419 [07:44<18:03, 24.07file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|███       | 11357/37419 [07:44<17:32, 24.77file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|███       | 11368/37419 [07:44<16:32, 26.25file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|███       | 11381/37419 [07:45<20:53, 20.77file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|███       | 11384/37419 [07:45<20:30, 21.15file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|███       | 11394/37419 [07:45<17:16, 25.11file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  30%|███       | 11407/37419 [07:46<21:24, 20.26file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11413/37419 [07:46<15:48, 27.41file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11421/37419 [07:46<19:41, 22.00file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11442/37419 [07:47<18:21, 23.58file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11456/37419 [07:48<16:28, 26.27file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11460/37419 [07:48<15:35, 27.76file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11471/37419 [07:48<18:10, 23.80file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11478/37419 [07:49<19:24, 22.27file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11484/37419 [07:49<14:38, 29.53file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11497/37419 [07:49<15:21, 28.13file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11507/37419 [07:50<16:33, 26.08file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11520/37419 [07:50<18:36, 23.19file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11527/37419 [07:51<12:50, 33.62file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11536/37419 [07:51<14:18, 30.16file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11548/37419 [07:51<19:18, 22.34file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11553/37419 [07:52<18:52, 22.84file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11558/37419 [07:52<15:36, 27.60file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11562/37419 [07:52<19:14, 22.40file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11565/37419 [07:52<19:13, 22.41file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11589/37419 [07:53<17:38, 24.39file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11592/37419 [07:53<21:08, 20.36file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11598/37419 [07:53<15:16, 28.17file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11602/37419 [07:54<15:20, 28.03file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11615/37419 [07:54<22:21, 19.24file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11623/37419 [07:55<20:14, 21.25file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11626/37419 [07:55<22:23, 19.20file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11632/37419 [07:55<17:22, 24.74file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11640/37419 [07:55<15:13, 28.21file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11651/37419 [07:56<16:16, 26.40file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11660/37419 [07:56<13:57, 30.74file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11668/37419 [07:56<16:01, 26.78file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11671/37419 [07:56<16:23, 26.19file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11678/37419 [07:57<19:22, 22.14file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11681/37419 [07:57<20:01, 21.42file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11688/37419 [07:57<19:06, 22.44file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███       | 11693/37419 [07:57<17:28, 24.54file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███▏      | 11696/37419 [07:57<18:11, 23.57file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███▏      | 11699/37419 [07:58<21:52, 19.60file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███▏      | 11705/37419 [07:58<17:46, 24.10file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███▏      | 11711/37419 [07:58<19:06, 22.42file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███▏      | 11714/37419 [07:58<22:01, 19.45file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███▏      | 11720/37419 [07:59<22:50, 18.75file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███▏      | 11725/37419 [07:59<17:32, 24.42file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███▏      | 11728/37419 [07:59<19:40, 21.77file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███▏      | 11731/37419 [07:59<18:54, 22.64file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███▏      | 11735/37419 [07:59<16:38, 25.72file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███▏      | 11738/37419 [07:59<17:40, 24.21file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███▏      | 11749/37419 [08:00<21:52, 19.55file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███▏      | 11760/37419 [08:00<16:19, 26.21file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███▏      | 11763/37419 [08:00<16:16, 26.27file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███▏      | 11766/37419 [08:00<19:08, 22.33file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███▏      | 11769/37419 [08:01<19:22, 22.07file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███▏      | 11778/37419 [08:01<17:55, 23.84file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  31%|███▏      | 11781/37419 [08:01<17:53, 23.88file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11790/37419 [08:01<14:01, 30.46file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11798/37419 [08:02<16:51, 25.32file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11801/37419 [08:02<17:25, 24.50file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11804/37419 [08:02<21:08, 20.19file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11807/37419 [08:02<19:16, 22.15file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11815/37419 [08:03<25:06, 17.00file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11825/37419 [08:03<17:23, 24.53file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11832/37419 [08:03<15:31, 27.46file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11835/37419 [08:03<15:21, 27.75file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11841/37419 [08:03<12:36, 33.81file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11845/37419 [08:03<12:04, 35.31file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11850/37419 [08:04<12:02, 35.39file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11854/37419 [08:04<12:40, 33.61file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11858/37419 [08:04<19:39, 21.67file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11864/37419 [08:04<22:07, 19.26file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11867/37419 [08:05<23:16, 18.29file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11875/37419 [08:05<19:27, 21.87file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11882/37419 [08:05<19:33, 21.75file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11887/37419 [08:05<15:59, 26.60file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11906/37419 [08:06<12:25, 34.20file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11910/37419 [08:06<13:40, 31.08file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11917/37419 [08:06<13:47, 30.83file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11924/37419 [08:07<19:19, 21.99file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11937/37419 [08:07<23:51, 17.80file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11947/37419 [08:08<17:21, 24.45file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11957/37419 [08:08<12:55, 32.83file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11961/37419 [08:08<14:42, 28.84file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11965/37419 [08:08<15:14, 27.83file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11980/37419 [08:09<21:07, 20.07file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11987/37419 [08:09<23:40, 17.90file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11990/37419 [08:10<21:54, 19.35file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 11996/37419 [08:10<21:12, 19.99file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 12051/37419 [08:12<12:45, 33.15file/s]  /scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 12067/37419 [08:13<15:18, 27.61file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 12079/37419 [08:13<14:57, 28.25file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 12095/37419 [08:14<16:30, 25.56file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 12101/37419 [08:14<16:16, 25.91file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 12106/37419 [08:14<17:45, 23.75file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 12110/37419 [08:15<20:10, 20.91file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 12117/37419 [08:15<19:51, 21.24file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 12122/37419 [08:15<17:10, 24.54file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 12125/37419 [08:15<19:52, 21.21file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 12130/37419 [08:16<17:38, 23.89file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 12139/37419 [08:16<16:22, 25.74file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 12149/37419 [08:16<15:19, 27.48file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  32%|███▏      | 12158/37419 [08:17<16:01, 26.27file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12163/37419 [08:17<16:24, 25.65file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12166/37419 [08:17<19:15, 21.86file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12169/37419 [08:17<18:47, 22.39file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12172/37419 [08:17<17:40, 23.81file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12179/37419 [08:18<17:33, 23.96file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12183/37419 [08:18<16:25, 25.61file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12193/37419 [08:18<16:42, 25.17file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12196/37419 [08:18<17:03, 24.65file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12200/37419 [08:18<15:09, 27.74file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12203/37419 [08:18<16:02, 26.20file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12213/37419 [08:19<13:24, 31.34file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12217/37419 [08:19<12:34, 33.41file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12221/37419 [08:19<15:21, 27.35file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12230/37419 [08:20<19:31, 21.50file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12237/37419 [08:20<15:09, 27.70file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12244/37419 [08:20<17:32, 23.91file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12250/37419 [08:20<14:31, 28.88file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12254/37419 [08:20<14:52, 28.20file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12258/37419 [08:20<14:17, 29.35file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12262/37419 [08:21<14:28, 28.96file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12272/37419 [08:21<16:33, 25.31file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12278/37419 [08:21<14:53, 28.13file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12282/37419 [08:21<15:00, 27.91file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12288/37419 [08:22<20:30, 20.43file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12291/37419 [08:22<19:21, 21.64file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12295/37419 [08:22<17:13, 24.32file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12305/37419 [08:22<16:33, 25.28file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12311/37419 [08:22<13:01, 32.11file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12318/37419 [08:23<17:16, 24.22file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12323/37419 [08:23<14:02, 29.78file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12339/37419 [08:24<17:18, 24.16file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12363/37419 [08:25<20:45, 20.11file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12366/37419 [08:25<19:22, 21.55file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12370/37419 [08:25<17:00, 24.54file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12377/37419 [08:25<12:37, 33.07file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12386/37419 [08:25<14:19, 29.13file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12401/37419 [08:26<16:21, 25.50file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12407/37419 [08:27<25:27, 16.38file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12410/37419 [08:27<23:36, 17.66file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12416/37419 [08:27<17:04, 24.41file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12423/37419 [08:27<16:43, 24.91file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12427/37419 [08:27<21:01, 19.81file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12435/37419 [08:28<16:35, 25.09file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12439/37419 [08:28<16:59, 24.51file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12442/37419 [08:28<17:06, 24.33file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12445/37419 [08:28<22:23, 18.58file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12452/37419 [08:28<14:46, 28.17file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12456/37419 [08:28<16:08, 25.77file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12460/37419 [08:29<18:04, 23.01file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12474/37419 [08:29<17:18, 24.02file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12481/37419 [08:30<18:24, 22.57file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12489/37419 [08:30<12:34, 33.02file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12494/37419 [08:30<15:40, 26.50file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12504/37419 [08:30<16:45, 24.79file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12515/37419 [08:31<16:42, 24.84file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12525/37419 [08:31<15:53, 26.11file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12528/37419 [08:31<19:06, 21.72file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12531/37419 [08:32<25:04, 16.54file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  33%|███▎      | 12534/37419 [08:32<23:00, 18.02file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▎      | 12539/37419 [08:32<17:14, 24.05file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▎      | 12543/37419 [08:32<15:46, 26.28file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▎      | 12551/37419 [08:32<14:06, 29.37file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▎      | 12555/37419 [08:33<18:12, 22.75file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▎      | 12559/37419 [08:33<16:02, 25.83file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▎      | 12562/37419 [08:33<15:47, 26.24file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▎      | 12568/37419 [08:33<21:30, 19.25file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▎      | 12578/37419 [08:33<15:51, 26.10file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▎      | 12597/37419 [08:34<16:36, 24.90file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▎      | 12603/37419 [08:34<12:54, 32.03file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▎      | 12611/37419 [08:35<15:09, 27.28file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▎      | 12618/37419 [08:35<15:27, 26.73file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12631/37419 [08:35<14:10, 29.14file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12637/37419 [08:36<13:08, 31.44file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12647/37419 [08:36<19:48, 20.84file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12653/37419 [08:36<19:35, 21.08file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12659/37419 [08:37<15:49, 26.09file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12663/37419 [08:37<24:06, 17.12file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12688/37419 [08:38<17:18, 23.82file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12691/37419 [08:38<21:15, 19.39file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12694/37419 [08:38<23:29, 17.54file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12697/37419 [08:39<22:51, 18.03file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12703/37419 [08:39<16:41, 24.67file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12710/37419 [08:39<13:47, 29.85file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12714/37419 [08:39<13:11, 31.21file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12718/37419 [08:39<15:09, 27.16file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12727/37419 [08:40<19:33, 21.05file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12734/37419 [08:40<14:22, 28.63file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12738/37419 [08:40<15:18, 26.87file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12742/37419 [08:40<16:08, 25.49file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12751/37419 [08:40<16:07, 25.49file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12757/37419 [08:41<19:13, 21.38file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12764/37419 [08:41<17:05, 24.03file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12767/37419 [08:41<20:38, 19.90file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12770/37419 [08:41<20:21, 20.18file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12783/37419 [08:42<20:23, 20.14file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12786/37419 [08:42<19:10, 21.42file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12790/37419 [08:42<16:58, 24.19file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12795/37419 [08:42<14:28, 28.34file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12798/37419 [08:42<14:41, 27.92file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12806/37419 [08:43<14:06, 29.07file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12810/37419 [08:43<13:44, 29.86file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12822/37419 [08:43<13:42, 29.90file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12829/37419 [08:44<18:14, 22.47file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12842/37419 [08:44<20:12, 20.27file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12847/37419 [08:45<19:26, 21.06file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12853/37419 [08:45<14:49, 27.61file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12860/37419 [08:45<11:23, 35.91file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12865/37419 [08:45<11:35, 35.31file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12869/37419 [08:45<12:30, 32.71file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12873/37419 [08:45<15:54, 25.72file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12877/37419 [08:45<15:05, 27.09file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  34%|███▍      | 12886/37419 [08:46<14:17, 28.62file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▍      | 12910/37419 [08:47<18:29, 22.10file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▍      | 12923/37419 [08:47<13:01, 31.33file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▍      | 12932/37419 [08:48<16:09, 25.25file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▍      | 12939/37419 [08:48<12:05, 33.75file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▍      | 12943/37419 [08:48<14:12, 28.72file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▍      | 12947/37419 [08:48<16:41, 24.44file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▍      | 12950/37419 [08:48<17:35, 23.19file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▍      | 12954/37419 [08:49<15:43, 25.93file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▍      | 12958/37419 [08:49<16:05, 25.34file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▍      | 12972/37419 [08:49<14:46, 27.58file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▍      | 12975/37419 [08:49<18:29, 22.02file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▍      | 12978/37419 [08:50<23:14, 17.53file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▍      | 12984/37419 [08:50<16:44, 24.33file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▍      | 12988/37419 [08:50<14:59, 27.17file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▍      | 12999/37419 [08:50<11:09, 36.45file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▍      | 13003/37419 [08:52<59:42,  6.82file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▍      | 13045/37419 [08:52<13:01, 31.18file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▍      | 13058/37419 [08:53<12:36, 32.22file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▍      | 13068/37419 [08:53<12:45, 31.81file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▍      | 13076/37419 [08:53<14:08, 28.68file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▍      | 13083/37419 [08:54<15:07, 26.82file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▍      | 13092/37419 [08:54<17:35, 23.05file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▍      | 13096/37419 [08:54<17:19, 23.39file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▌      | 13100/37419 [08:54<17:46, 22.80file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▌      | 13109/37419 [08:55<15:36, 25.95file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▌      | 13123/37419 [08:55<19:05, 21.21file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▌      | 13132/37419 [08:56<19:23, 20.87file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▌      | 13136/37419 [08:56<17:23, 23.27file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▌      | 13147/37419 [08:56<15:46, 25.65file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▌      | 13153/37419 [08:57<16:42, 24.22file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▌      | 13166/37419 [08:57<16:33, 24.42file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▌      | 13175/37419 [08:58<20:19, 19.88file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▌      | 13185/37419 [08:58<16:56, 23.85file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▌      | 13188/37419 [08:58<16:10, 24.96file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▌      | 13196/37419 [08:58<12:59, 31.09file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▌      | 13209/37419 [08:59<18:34, 21.72file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▌      | 13217/37419 [08:59<17:18, 23.30file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▌      | 13222/37419 [08:59<15:00, 26.87file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▌      | 13233/37419 [09:00<14:44, 27.34file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▌      | 13237/37419 [09:00<13:50, 29.13file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▌      | 13241/37419 [09:00<13:27, 29.93file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▌      | 13245/37419 [09:00<20:41, 19.47file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▌      | 13249/37419 [09:00<19:05, 21.10file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▌      | 13264/37419 [09:01<18:16, 22.03file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▌      | 13269/37419 [09:01<17:01, 23.63file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  35%|███▌      | 13272/37419 [09:01<20:49, 19.33file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13289/37419 [09:02<17:44, 22.66file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13297/37419 [09:02<15:10, 26.48file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13306/37419 [09:03<13:42, 29.33file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13311/37419 [09:03<13:34, 29.58file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13315/37419 [09:03<13:32, 29.65file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13319/37419 [09:03<16:33, 24.26file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13323/37419 [09:03<16:16, 24.68file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13333/37419 [09:04<18:54, 21.23file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13342/37419 [09:04<22:21, 17.95file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13347/37419 [09:05<16:50, 23.83file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13352/37419 [09:05<13:58, 28.72file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13359/37419 [09:05<16:08, 24.86file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13363/37419 [09:05<15:45, 25.44file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13371/37419 [09:05<13:13, 30.29file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13382/37419 [09:06<19:09, 20.90file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13385/37419 [09:06<17:57, 22.31file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13389/37419 [09:06<17:42, 22.62file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13392/37419 [09:06<20:08, 19.88file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13419/37419 [09:07<15:12, 26.30file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13425/37419 [09:08<15:27, 25.88file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13433/37419 [09:08<17:23, 22.99file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13444/37419 [09:09<16:15, 24.56file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13454/37419 [09:09<16:02, 24.89file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13457/37419 [09:09<22:16, 17.93file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13467/37419 [09:10<17:20, 23.02file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13475/37419 [09:10<13:35, 29.38file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13479/37419 [09:10<14:26, 27.63file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13487/37419 [09:10<13:49, 28.84file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13494/37419 [09:11<16:20, 24.41file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13497/37419 [09:11<16:48, 23.71file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13500/37419 [09:11<16:01, 24.88file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13508/37419 [09:11<18:48, 21.19file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13511/37419 [09:11<19:15, 20.70file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13517/37419 [09:12<19:07, 20.83file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13523/37419 [09:12<13:43, 29.03file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13530/37419 [09:12<18:00, 22.10file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13533/37419 [09:12<21:26, 18.57file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13541/37419 [09:13<13:57, 28.52file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13545/37419 [09:13<14:38, 27.17file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13549/37419 [09:13<21:13, 18.74file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13553/37419 [09:13<18:28, 21.53file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▌      | 13561/37419 [09:13<12:35, 31.60file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▋      | 13566/37419 [09:14<17:32, 22.67file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▋      | 13570/37419 [09:14<16:15, 24.46file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▋      | 13574/37419 [09:14<16:37, 23.90file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▋      | 13580/37419 [09:14<14:38, 27.13file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▋      | 13590/37419 [09:15<20:03, 19.79file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▋      | 13596/37419 [09:15<16:04, 24.69file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▋      | 13607/37419 [09:15<14:57, 26.54file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▋      | 13611/37419 [09:16<16:26, 24.13file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▋      | 13615/37419 [09:16<15:34, 25.46file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▋      | 13634/37419 [09:17<17:14, 22.99file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▋      | 13637/37419 [09:17<16:59, 23.32file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▋      | 13645/37419 [09:17<16:19, 24.28file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▋      | 13650/37419 [09:17<13:22, 29.60file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  36%|███▋      | 13655/37419 [09:17<13:05, 30.25file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13659/37419 [09:18<18:50, 21.02file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13663/37419 [09:18<17:53, 22.13file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13669/37419 [09:18<20:01, 19.76file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13674/37419 [09:18<16:20, 24.23file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13677/37419 [09:18<20:16, 19.52file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13683/37419 [09:19<19:24, 20.38file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13686/37419 [09:19<19:32, 20.24file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13691/37419 [09:19<16:08, 24.50file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13697/37419 [09:19<15:38, 25.27file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13704/37419 [09:20<15:18, 25.83file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13710/37419 [09:20<15:50, 24.95file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13716/37419 [09:20<17:45, 22.25file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13719/37419 [09:20<16:30, 23.92file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13723/37419 [09:20<15:14, 25.92file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13730/37419 [09:20<11:20, 34.83file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13734/37419 [09:21<15:23, 25.66file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13743/37419 [09:21<13:26, 29.34file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13747/37419 [09:21<14:42, 26.84file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13753/37419 [09:21<15:49, 24.92file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13764/37419 [09:22<17:59, 21.90file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13770/37419 [09:22<20:56, 18.83file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13781/37419 [09:23<17:44, 22.20file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13787/37419 [09:23<13:14, 29.73file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13796/37419 [09:23<19:46, 19.91file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13808/37419 [09:24<17:02, 23.08file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13815/37419 [09:24<19:14, 20.44file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13826/37419 [09:25<14:58, 26.25file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13829/37419 [09:25<17:32, 22.42file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13838/37419 [09:25<20:50, 18.86file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13845/37419 [09:25<14:42, 26.71file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13853/37419 [09:26<15:26, 25.44file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13877/37419 [09:27<16:46, 23.38file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13880/37419 [09:27<17:15, 22.73file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13883/37419 [09:27<18:23, 21.33file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13886/37419 [09:27<20:45, 18.89file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13892/37419 [09:27<18:54, 20.74file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13895/37419 [09:28<21:08, 18.54file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13915/37419 [09:28<16:02, 24.42file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13923/37419 [09:29<11:05, 35.30file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13962/37419 [09:30<17:34, 22.25file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13965/37419 [09:30<18:27, 21.17file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13983/37419 [09:31<14:27, 27.00file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13993/37419 [09:31<18:50, 20.71file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  37%|███▋      | 13996/37419 [09:32<18:33, 21.04file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14043/37419 [09:34<13:39, 28.53file/s]  /scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14049/37419 [09:34<13:18, 29.26file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14063/37419 [09:34<13:14, 29.41file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14068/37419 [09:34<12:29, 31.15file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14077/37419 [09:35<14:36, 26.63file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14089/37419 [09:35<19:15, 20.19file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14097/37419 [09:36<16:45, 23.20file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14110/37419 [09:36<13:10, 29.50file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14118/37419 [09:36<12:22, 31.39file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14122/37419 [09:37<14:00, 27.72file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14125/37419 [09:37<18:46, 20.67file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14132/37419 [09:37<16:42, 23.24file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14136/37419 [09:37<15:01, 25.82file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14139/37419 [09:37<15:18, 25.35file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14145/37419 [09:38<16:26, 23.59file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14152/37419 [09:38<14:16, 27.18file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14156/37419 [09:38<13:44, 28.20file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14160/37419 [09:38<12:46, 30.33file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14164/37419 [09:38<20:26, 18.96file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14172/37419 [09:39<19:09, 20.23file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14182/37419 [09:39<16:01, 24.16file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14185/37419 [09:39<15:48, 24.50file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14188/37419 [09:39<15:26, 25.08file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14199/37419 [09:40<15:23, 25.15file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14202/37419 [09:40<18:00, 21.49file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14206/37419 [09:40<17:08, 22.56file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14215/37419 [09:40<11:18, 34.18file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14219/37419 [09:41<12:41, 30.47file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14226/37419 [09:41<12:43, 30.39file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14230/37419 [09:41<14:32, 26.59file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14234/37419 [09:41<13:21, 28.93file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14238/37419 [09:41<14:11, 27.23file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14241/37419 [09:41<13:59, 27.62file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14244/37419 [09:42<16:56, 22.79file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14250/37419 [09:42<13:33, 28.47file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14257/37419 [09:42<15:10, 25.44file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14260/37419 [09:42<17:22, 22.22file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14268/37419 [09:43<17:22, 22.22file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14279/37419 [09:43<19:09, 20.13file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14282/37419 [09:43<21:51, 17.64file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14287/37419 [09:43<16:31, 23.34file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14290/37419 [09:44<16:26, 23.45file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14296/37419 [09:44<12:56, 29.78file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14300/37419 [09:44<17:49, 21.62file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14303/37419 [09:44<19:09, 20.10file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14306/37419 [09:44<19:10, 20.08file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14309/37419 [09:45<22:01, 17.49file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14313/37419 [09:45<23:18, 16.53file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14316/37419 [09:45<19:58, 19.27file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14321/37419 [09:45<16:48, 22.91file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14325/37419 [09:45<15:46, 24.39file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14328/37419 [09:45<16:53, 22.78file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14334/37419 [09:45<12:37, 30.49file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14340/37419 [09:46<10:21, 37.13file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14345/37419 [09:46<11:42, 32.83file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14355/37419 [09:46<10:35, 36.32file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14362/37419 [09:46<08:51, 43.34file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14367/37419 [09:47<16:10, 23.75file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14374/37419 [09:47<18:20, 20.93file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14377/37419 [09:47<17:30, 21.94file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14380/37419 [09:47<16:25, 23.38file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14384/37419 [09:47<14:20, 26.77file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14388/37419 [09:47<13:19, 28.80file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14392/37419 [09:48<12:57, 29.60file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14399/37419 [09:48<16:01, 23.95file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  38%|███▊      | 14402/37419 [09:48<16:49, 22.79file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▊      | 14409/37419 [09:48<15:04, 25.43file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▊      | 14416/37419 [09:49<17:36, 21.78file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▊      | 14421/37419 [09:49<15:48, 24.25file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▊      | 14426/37419 [09:49<13:09, 29.12file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▊      | 14445/37419 [09:50<13:20, 28.69file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▊      | 14452/37419 [09:50<16:31, 23.16file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▊      | 14460/37419 [09:50<19:16, 19.86file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▊      | 14466/37419 [09:51<14:25, 26.53file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▊      | 14475/37419 [09:51<16:20, 23.41file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▊      | 14485/37419 [09:51<14:59, 25.49file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▊      | 14498/37419 [09:52<18:36, 20.54file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14504/37419 [09:52<19:33, 19.53file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14507/37419 [09:52<19:02, 20.05file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14512/37419 [09:53<16:19, 23.38file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14520/37419 [09:53<14:21, 26.57file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14530/37419 [09:53<11:10, 34.15file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14535/37419 [09:53<10:18, 37.00file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14539/37419 [09:53<10:44, 35.49file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14547/37419 [09:54<16:09, 23.60file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14550/37419 [09:54<18:09, 20.99file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14553/37419 [09:54<17:51, 21.34file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14563/37419 [09:55<17:05, 22.30file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14569/37419 [09:55<13:18, 28.61file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14583/37419 [09:55<13:02, 29.17file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14587/37419 [09:56<21:19, 17.85file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14602/37419 [09:56<16:35, 22.93file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14606/37419 [09:56<15:02, 25.28file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14615/37419 [09:56<12:23, 30.67file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14622/37419 [09:57<14:54, 25.49file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14644/37419 [09:58<19:39, 19.31file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14652/37419 [09:58<13:46, 27.55file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14656/37419 [09:58<15:18, 24.78file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14672/37419 [09:59<17:13, 22.00file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14680/37419 [09:59<15:47, 24.01file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14683/37419 [09:59<15:39, 24.20file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14688/37419 [09:59<13:29, 28.07file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14691/37419 [10:00<13:38, 27.75file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14695/37419 [10:00<12:27, 30.41file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14699/37419 [10:00<15:40, 24.17file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14703/37419 [10:00<14:18, 26.47file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14706/37419 [10:00<14:43, 25.70file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14714/37419 [10:00<15:40, 24.14file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14718/37419 [10:01<14:00, 27.01file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14721/37419 [10:01<15:15, 24.80file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14724/37419 [10:01<15:41, 24.11file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14730/37419 [10:01<18:18, 20.65file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14740/37419 [10:02<16:05, 23.49file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14749/37419 [10:02<14:27, 26.14file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14753/37419 [10:02<13:04, 28.89file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14757/37419 [10:02<16:44, 22.57file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14763/37419 [10:02<15:12, 24.84file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14770/37419 [10:03<15:51, 23.80file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  39%|███▉      | 14777/37419 [10:03<16:32, 22.81file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14783/37419 [10:03<17:52, 21.11file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14788/37419 [10:04<16:50, 22.39file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14791/37419 [10:04<18:08, 20.78file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14795/37419 [10:04<16:31, 22.83file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14798/37419 [10:04<15:43, 23.98file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14813/37419 [10:04<12:09, 30.98file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14818/37419 [10:05<11:08, 33.81file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14822/37419 [10:05<10:56, 34.39file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14826/37419 [10:05<10:42, 35.15file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14830/37419 [10:05<18:01, 20.89file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14840/37419 [10:05<16:22, 22.99file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14843/37419 [10:06<19:04, 19.72file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14850/37419 [10:06<20:37, 18.24file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14854/37419 [10:06<17:11, 21.87file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14859/37419 [10:06<14:23, 26.14file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14863/37419 [10:06<13:11, 28.49file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14867/37419 [10:07<14:09, 26.54file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14870/37419 [10:07<18:44, 20.04file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14877/37419 [10:07<16:37, 22.61file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14883/37419 [10:08<23:18, 16.12file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14888/37419 [10:08<17:19, 21.67file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14891/37419 [10:08<16:33, 22.67file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14894/37419 [10:08<21:08, 17.75file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14899/37419 [10:08<15:54, 23.59file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14905/37419 [10:08<12:39, 29.63file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14918/37419 [10:09<12:59, 28.86file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14926/37419 [10:09<13:18, 28.18file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14930/37419 [10:09<13:56, 26.88file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14933/37419 [10:10<18:36, 20.14file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14943/37419 [10:10<14:03, 26.66file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14947/37419 [10:10<16:15, 23.04file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14950/37419 [10:10<16:11, 23.13file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14960/37419 [10:11<14:11, 26.37file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|███▉      | 14967/37419 [10:11<12:09, 30.76file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|████      | 14971/37419 [10:11<11:29, 32.55file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|████      | 14978/37419 [10:11<15:30, 24.12file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|████      | 14989/37419 [10:12<17:50, 20.96file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|████      | 14999/37419 [10:12<13:14, 28.22file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|████      | 15066/37419 [10:15<12:14, 30.44file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|████      | 15081/37419 [10:15<13:14, 28.12file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|████      | 15087/37419 [10:16<16:05, 23.12file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|████      | 15091/37419 [10:16<16:31, 22.52file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|████      | 15099/37419 [10:16<15:21, 24.22file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|████      | 15109/37419 [10:17<16:42, 22.25file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|████      | 15114/37419 [10:17<16:12, 22.94file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|████      | 15132/37419 [10:18<16:11, 22.94file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|████      | 15136/37419 [10:18<14:37, 25.40file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|████      | 15141/37419 [10:18<14:39, 25.32file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  40%|████      | 15150/37419 [10:18<12:18, 30.15file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15156/37419 [10:18<10:47, 34.36file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15160/37419 [10:19<13:46, 26.95file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15170/37419 [10:19<11:58, 30.96file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15182/37419 [10:19<15:00, 24.70file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15186/37419 [10:20<15:53, 23.32file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15190/37419 [10:20<14:49, 25.00file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15193/37419 [10:20<17:48, 20.80file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15196/37419 [10:20<22:17, 16.61file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15204/37419 [10:21<25:13, 14.68file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15220/37419 [10:21<15:52, 23.30file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15223/37419 [10:22<16:12, 22.81file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15235/37419 [10:22<17:25, 21.22file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15241/37419 [10:22<14:08, 26.13file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15259/37419 [10:23<14:44, 25.06file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15263/37419 [10:23<16:57, 21.77file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15266/37419 [10:23<17:10, 21.50file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15269/37419 [10:23<16:19, 22.61file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15275/37419 [10:24<18:42, 19.72file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15281/37419 [10:24<13:49, 26.70file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15286/37419 [10:24<12:20, 29.88file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15291/37419 [10:24<11:10, 32.98file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15295/37419 [10:24<13:14, 27.83file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15299/37419 [10:25<17:13, 21.39file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15305/37419 [10:25<17:27, 21.12file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15308/37419 [10:25<18:26, 19.98file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15315/37419 [10:25<17:01, 21.64file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15322/37419 [10:26<13:28, 27.34file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15328/37419 [10:26<11:06, 33.13file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15343/37419 [10:26<14:34, 25.25file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15348/37419 [10:27<13:52, 26.52file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15351/37419 [10:27<15:57, 23.04file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15355/37419 [10:27<13:58, 26.32file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15358/37419 [10:27<19:32, 18.81file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15362/37419 [10:27<16:40, 22.05file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15368/37419 [10:27<12:55, 28.42file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15372/37419 [10:28<14:49, 24.80file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15375/37419 [10:28<16:08, 22.76file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15379/37419 [10:28<16:11, 22.69file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15384/37419 [10:28<15:12, 24.14file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15387/37419 [10:28<17:07, 21.44file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15391/37419 [10:28<16:57, 21.65file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15394/37419 [10:29<16:59, 21.61file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15399/37419 [10:29<13:49, 26.54file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15404/37419 [10:29<15:12, 24.12file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15407/37419 [10:29<15:02, 24.40file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15421/37419 [10:30<16:06, 22.75file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15427/37419 [10:30<14:39, 25.00file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████      | 15430/37419 [10:30<14:17, 25.64file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████▏     | 15436/37419 [10:30<15:57, 22.95file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████▏     | 15439/37419 [10:30<17:03, 21.48file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████▏     | 15447/37419 [10:31<12:19, 29.73file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████▏     | 15457/37419 [10:31<12:40, 28.89file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████▏     | 15465/37419 [10:31<14:59, 24.40file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████▏     | 15476/37419 [10:32<16:26, 22.24file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████▏     | 15482/37419 [10:32<15:00, 24.36file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████▏     | 15485/37419 [10:32<15:30, 23.58file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████▏     | 15489/37419 [10:32<14:49, 24.66file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████▏     | 15492/37419 [10:33<16:55, 21.60file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████▏     | 15503/37419 [10:33<12:33, 29.10file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████▏     | 15514/37419 [10:33<13:04, 27.93file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  41%|████▏     | 15521/37419 [10:34<17:49, 20.47file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15536/37419 [10:34<14:36, 24.96file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15539/37419 [10:34<14:06, 25.85file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15543/37419 [10:34<12:51, 28.37file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15547/37419 [10:35<13:02, 27.94file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15560/37419 [10:35<16:30, 22.06file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15567/37419 [10:35<11:14, 32.42file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15571/37419 [10:36<14:42, 24.75file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15590/37419 [10:36<12:32, 29.00file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15597/37419 [10:37<18:23, 19.77file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15607/37419 [10:37<19:42, 18.44file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15610/37419 [10:37<18:17, 19.87file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15637/37419 [10:38<12:34, 28.87file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15641/37419 [10:39<13:31, 26.85file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15651/37419 [10:39<20:07, 18.02file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15659/37419 [10:40<16:20, 22.20file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15667/37419 [10:40<13:41, 26.49file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15671/37419 [10:40<14:49, 24.44file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15681/37419 [10:40<12:13, 29.63file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15694/37419 [10:41<16:21, 22.13file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15697/37419 [10:41<19:12, 18.85file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15700/37419 [10:41<18:48, 19.25file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15726/37419 [10:42<11:27, 31.56file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15738/37419 [10:43<15:05, 23.95file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15741/37419 [10:43<19:56, 18.12file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15747/37419 [10:43<20:33, 17.57file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15754/37419 [10:44<17:24, 20.74file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15769/37419 [10:44<13:34, 26.57file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15774/37419 [10:44<11:29, 31.40file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15794/37419 [10:45<14:17, 25.21file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15807/37419 [10:46<13:19, 27.02file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15813/37419 [10:46<15:39, 23.00file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15820/37419 [10:46<16:09, 22.27file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15831/37419 [10:47<18:19, 19.64file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15840/37419 [10:47<13:58, 25.72file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15843/37419 [10:47<14:16, 25.18file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15846/37419 [10:47<14:34, 24.66file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15849/37419 [10:48<15:47, 22.76file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15870/37419 [10:49<16:26, 21.83file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15880/37419 [10:49<13:23, 26.79file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15885/37419 [10:49<15:42, 22.85file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15888/37419 [10:49<16:27, 21.80file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  42%|████▏     | 15896/37419 [10:49<12:05, 29.67file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 15907/37419 [10:50<12:02, 29.78file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 15911/37419 [10:50<14:18, 25.05file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 15917/37419 [10:50<18:42, 19.16file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 15924/37419 [10:51<18:15, 19.62file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 15932/37419 [10:51<12:25, 28.83file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 15937/37419 [10:51<11:09, 32.11file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 15942/37419 [10:51<12:12, 29.33file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 15946/37419 [10:52<16:03, 22.28file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 15952/37419 [10:52<12:58, 27.57file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 15961/37419 [10:52<13:54, 25.73file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 15969/37419 [10:52<12:59, 27.52file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 15973/37419 [10:53<15:52, 22.53file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 15978/37419 [10:53<13:06, 27.26file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 15982/37419 [10:53<12:16, 29.11file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 15986/37419 [10:53<13:09, 27.15file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 15991/37419 [10:53<11:28, 31.12file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 15995/37419 [10:53<16:50, 21.20file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 15998/37419 [10:54<17:16, 20.67file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16055/37419 [10:56<10:24, 34.20file/s]  /scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16083/37419 [10:57<13:59, 25.42file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16092/37419 [10:57<13:27, 26.42file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16100/37419 [10:58<12:31, 28.37file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16113/37419 [10:58<11:54, 29.81file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16118/37419 [10:58<12:37, 28.12file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16123/37419 [10:58<13:32, 26.22file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16127/37419 [10:59<18:18, 19.39file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16133/37419 [10:59<15:09, 23.40file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16137/37419 [10:59<14:17, 24.82file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16143/37419 [10:59<12:24, 28.56file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16147/37419 [10:59<12:57, 27.37file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16151/37419 [11:00<14:02, 25.25file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16162/37419 [11:00<13:36, 26.03file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16165/37419 [11:00<14:17, 24.78file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16170/37419 [11:00<13:13, 26.77file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16176/37419 [11:01<13:25, 26.39file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16179/37419 [11:01<13:34, 26.08file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16187/37419 [11:01<13:53, 25.48file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16195/37419 [11:01<15:08, 23.37file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16200/37419 [11:01<12:26, 28.41file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16207/37419 [11:02<19:37, 18.02file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16218/37419 [11:03<15:24, 22.94file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16233/37419 [11:03<11:33, 30.57file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16237/37419 [11:03<11:03, 31.95file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16245/37419 [11:03<12:50, 27.48file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16255/37419 [11:04<16:35, 21.26file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16258/37419 [11:04<15:49, 22.29file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16261/37419 [11:04<15:21, 22.95file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16267/37419 [11:05<18:29, 19.07file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  43%|████▎     | 16275/37419 [11:05<19:10, 18.37file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▎     | 16281/37419 [11:05<14:10, 24.85file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▎     | 16288/37419 [11:05<10:54, 32.28file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▎     | 16299/37419 [11:06<15:09, 23.22file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▎     | 16306/37419 [11:06<14:42, 23.92file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▎     | 16309/37419 [11:06<16:48, 20.93file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▎     | 16317/37419 [11:07<14:23, 24.43file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▎     | 16321/37419 [11:07<14:05, 24.96file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▎     | 16325/37419 [11:07<13:22, 26.30file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▎     | 16335/37419 [11:07<16:13, 21.66file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▎     | 16344/37419 [11:08<14:18, 24.55file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▎     | 16349/37419 [11:08<12:41, 27.66file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▎     | 16353/37419 [11:08<14:59, 23.42file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▎     | 16369/37419 [11:09<16:40, 21.03file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16373/37419 [11:09<14:06, 24.87file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16379/37419 [11:09<14:38, 23.96file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16384/37419 [11:09<11:50, 29.61file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16388/37419 [11:09<11:43, 29.92file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16392/37419 [11:10<11:54, 29.45file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16398/37419 [11:10<09:37, 36.41file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16414/37419 [11:10<12:59, 26.96file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16424/37419 [11:11<14:33, 24.05file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16427/37419 [11:11<18:25, 18.98file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16432/37419 [11:11<15:30, 22.55file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16440/37419 [11:11<12:57, 26.97file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16455/37419 [11:12<19:47, 17.65file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16465/37419 [11:12<13:03, 26.73file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16469/37419 [11:13<16:17, 21.43file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16476/37419 [11:13<15:50, 22.03file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16482/37419 [11:13<12:25, 28.10file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16488/37419 [11:13<12:23, 28.14file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16495/37419 [11:14<16:25, 21.23file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16502/37419 [11:14<19:42, 17.70file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16510/37419 [11:15<16:25, 21.22file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16516/37419 [11:15<14:33, 23.92file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16519/37419 [11:15<15:15, 22.83file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16523/37419 [11:15<14:40, 23.73file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16533/37419 [11:15<09:41, 35.95file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16542/37419 [11:16<11:26, 30.39file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16553/37419 [11:16<09:04, 38.35file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16558/37419 [11:16<10:19, 33.69file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16568/37419 [11:17<19:09, 18.15file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16571/37419 [11:17<18:29, 18.78file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16577/37419 [11:17<16:32, 21.00file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16581/37419 [11:17<14:08, 24.56file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16584/37419 [11:17<17:09, 20.24file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16593/37419 [11:18<12:11, 28.46file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16601/37419 [11:18<14:13, 24.39file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16604/37419 [11:18<14:04, 24.65file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16610/37419 [11:18<10:41, 32.42file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16614/37419 [11:18<12:34, 27.59file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16622/37419 [11:19<13:13, 26.20file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16628/37419 [11:19<10:46, 32.14file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16635/37419 [11:19<09:41, 35.72file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16639/37419 [11:19<12:03, 28.74file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  44%|████▍     | 16646/37419 [11:20<14:49, 23.35file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16652/37419 [11:20<18:11, 19.03file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16655/37419 [11:20<17:22, 19.91file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16663/37419 [11:20<11:46, 29.38file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16667/37419 [11:20<11:16, 30.69file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16671/37419 [11:21<14:48, 23.34file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16680/37419 [11:21<13:29, 25.63file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16683/37419 [11:21<15:11, 22.75file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16687/37419 [11:21<15:24, 22.43file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16691/37419 [11:21<13:36, 25.39file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16700/37419 [11:22<14:45, 23.40file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16703/37419 [11:22<14:20, 24.06file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16707/37419 [11:22<12:40, 27.23file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16717/37419 [11:22<13:57, 24.71file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16720/37419 [11:23<16:17, 21.17file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16723/37419 [11:23<18:03, 19.10file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16730/37419 [11:23<12:19, 27.98file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16739/37419 [11:23<11:35, 29.74file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16744/37419 [11:23<11:12, 30.74file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16748/37419 [11:24<11:57, 28.80file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16755/37419 [11:24<14:41, 23.44file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16763/37419 [11:24<15:28, 22.24file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16766/37419 [11:25<17:37, 19.54file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16784/37419 [11:25<13:06, 26.25file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16792/37419 [11:25<11:49, 29.09file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16796/37419 [11:26<11:06, 30.93file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16800/37419 [11:26<12:04, 28.47file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16818/37419 [11:27<16:06, 21.32file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16824/37419 [11:27<14:34, 23.56file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16829/37419 [11:27<12:26, 27.58file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▍     | 16833/37419 [11:27<14:18, 23.97file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▌     | 16846/37419 [11:28<15:29, 22.13file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▌     | 16852/37419 [11:28<17:36, 19.47file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▌     | 16855/37419 [11:28<19:17, 17.76file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▌     | 16861/37419 [11:29<22:40, 15.11file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▌     | 16863/37419 [11:29<25:15, 13.57file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▌     | 16869/37419 [11:29<15:02, 22.76file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▌     | 16886/37419 [11:30<12:42, 26.92file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▌     | 16891/37419 [11:30<11:13, 30.50file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▌     | 16916/37419 [11:31<14:10, 24.10file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▌     | 16919/37419 [11:31<17:24, 19.62file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▌     | 16935/37419 [11:32<17:15, 19.79file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▌     | 16951/37419 [11:33<15:46, 21.63file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▌     | 16959/37419 [11:33<10:21, 32.90file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▌     | 16968/37419 [11:33<12:42, 26.82file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▌     | 16972/37419 [11:33<14:49, 22.99file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▌     | 16975/37419 [11:34<17:20, 19.64file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▌     | 16987/37419 [11:34<13:14, 25.73file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  45%|████▌     | 16993/37419 [11:34<15:36, 21.80file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17050/37419 [11:36<10:53, 31.19file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17059/37419 [11:37<10:50, 31.31file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17066/37419 [11:37<09:57, 34.06file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17073/37419 [11:37<09:21, 36.26file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17079/37419 [11:37<09:54, 34.23file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17093/37419 [11:38<15:01, 22.54file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17097/37419 [11:38<13:46, 24.59file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17101/37419 [11:38<14:21, 23.59file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17110/37419 [11:39<13:04, 25.90file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17114/37419 [11:39<13:19, 25.39file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17121/37419 [11:39<12:39, 26.71file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17126/37419 [11:39<12:18, 27.49file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17129/37419 [11:39<13:06, 25.78file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17133/37419 [11:40<14:47, 22.85file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17136/37419 [11:40<17:39, 19.15file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17142/37419 [11:40<17:14, 19.60file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17146/37419 [11:40<15:10, 22.28file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17152/37419 [11:41<12:03, 28.03file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17161/37419 [11:41<14:29, 23.29file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17165/37419 [11:41<14:33, 23.19file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17169/37419 [11:41<13:11, 25.57file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17176/37419 [11:42<14:41, 22.96file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17179/37419 [11:42<13:56, 24.19file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17182/37419 [11:42<19:02, 17.71file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17186/37419 [11:42<18:41, 18.04file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17189/37419 [11:42<17:26, 19.34file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17193/37419 [11:42<16:12, 20.79file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17197/37419 [11:43<14:03, 23.97file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17202/37419 [11:43<11:58, 28.14file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17207/37419 [11:43<10:49, 31.10file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17217/37419 [11:43<13:59, 24.06file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17221/37419 [11:43<12:41, 26.51file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17228/37419 [11:44<11:52, 28.35file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17234/37419 [11:44<10:27, 32.14file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17238/37419 [11:44<10:52, 30.95file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17246/37419 [11:45<15:42, 21.41file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17252/37419 [11:45<14:37, 22.99file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17258/37419 [11:45<18:53, 17.79file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17261/37419 [11:45<19:21, 17.36file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17268/37419 [11:46<13:20, 25.16file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17271/37419 [11:46<13:07, 25.58file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17274/37419 [11:46<13:45, 24.39file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17283/37419 [11:46<12:09, 27.62file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17289/37419 [11:47<19:53, 16.86file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17295/37419 [11:47<16:17, 20.60file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▌     | 17301/37419 [11:47<13:49, 24.24file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▋     | 17310/37419 [11:47<11:27, 29.26file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▋     | 17319/37419 [11:48<15:14, 21.97file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▋     | 17326/37419 [11:48<14:36, 22.92file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▋     | 17331/37419 [11:48<12:57, 25.84file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▋     | 17339/37419 [11:49<17:11, 19.46file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▋     | 17342/37419 [11:49<15:51, 21.10file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▋     | 17351/37419 [11:49<13:34, 24.65file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▋     | 17356/37419 [11:49<11:16, 29.64file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▋     | 17360/37419 [11:49<11:55, 28.03file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▋     | 17364/37419 [11:49<11:24, 29.31file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▋     | 17372/37419 [11:50<13:01, 25.66file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▋     | 17385/37419 [11:51<18:15, 18.29file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  46%|████▋     | 17394/37419 [11:51<17:19, 19.27file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17405/37419 [11:51<15:08, 22.02file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17410/37419 [11:52<16:56, 19.69file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17434/37419 [11:53<14:05, 23.65file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17444/37419 [11:53<14:27, 23.03file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17451/37419 [11:53<13:35, 24.48file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17458/37419 [11:53<10:07, 32.84file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17462/37419 [11:54<14:48, 22.47file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17466/37419 [11:54<16:05, 20.67file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17485/37419 [11:55<16:19, 20.35file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17497/37419 [11:55<11:53, 27.92file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17512/37419 [11:56<07:52, 42.17file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17517/37419 [11:56<10:03, 32.99file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17525/37419 [11:56<14:54, 22.25file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17528/37419 [11:57<15:17, 21.67file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17566/37419 [11:58<09:59, 33.10file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17583/37419 [11:59<18:45, 17.62file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17607/37419 [12:00<13:37, 24.23file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17610/37419 [12:00<13:47, 23.95file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17613/37419 [12:00<13:45, 23.98file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17621/37419 [12:00<13:53, 23.76file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17635/37419 [12:01<11:43, 28.13file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17640/37419 [12:01<09:55, 33.22file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17644/37419 [12:01<09:58, 33.06file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17651/37419 [12:01<09:01, 36.54file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17655/37419 [12:01<11:00, 29.91file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17661/37419 [12:01<09:29, 34.67file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17665/37419 [12:02<13:45, 23.93file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17668/37419 [12:02<13:13, 24.90file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17675/37419 [12:02<13:19, 24.68file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17678/37419 [12:02<14:45, 22.30file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17689/37419 [12:03<25:35, 12.85file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17696/37419 [12:04<17:50, 18.43file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17700/37419 [12:04<14:51, 22.12file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17708/37419 [12:04<12:09, 27.02file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17715/37419 [12:04<13:31, 24.27file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17719/37419 [12:04<11:56, 27.51file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17731/37419 [12:05<10:32, 31.13file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17740/37419 [12:05<14:54, 21.99file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17745/37419 [12:05<12:16, 26.73file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17749/37419 [12:06<16:00, 20.48file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17752/37419 [12:06<15:36, 21.00file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17755/37419 [12:06<16:18, 20.09file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17758/37419 [12:06<20:25, 16.05file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  47%|████▋     | 17768/37419 [12:06<13:26, 24.36file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17777/37419 [12:07<11:51, 27.61file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17781/37419 [12:07<12:29, 26.20file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17794/37419 [12:07<09:49, 33.27file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17798/37419 [12:07<10:07, 32.32file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17802/37419 [12:08<14:51, 22.01file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17807/37419 [12:08<13:05, 24.98file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17811/37419 [12:08<14:25, 22.66file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17817/37419 [12:08<11:23, 28.68file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17821/37419 [12:08<13:26, 24.30file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17826/37419 [12:08<11:28, 28.46file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17847/37419 [12:09<12:35, 25.91file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17850/37419 [12:09<13:10, 24.75file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17853/37419 [12:10<12:44, 25.60file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17861/37419 [12:10<11:39, 27.98file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17868/37419 [12:10<08:28, 38.43file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17873/37419 [12:10<10:44, 30.32file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17877/37419 [12:11<16:25, 19.83file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17883/37419 [12:11<17:05, 19.04file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17888/37419 [12:11<14:29, 22.46file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17891/37419 [12:11<15:05, 21.57file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17898/37419 [12:11<13:59, 23.27file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17901/37419 [12:12<13:49, 23.52file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17906/37419 [12:12<13:31, 24.05file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17910/37419 [12:12<12:34, 25.85file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17913/37419 [12:12<14:24, 22.57file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17919/37419 [12:12<15:29, 20.98file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17924/37419 [12:13<12:59, 25.00file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17929/37419 [12:13<10:54, 29.78file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17933/37419 [12:13<12:03, 26.92file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17942/37419 [12:13<12:58, 25.02file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17948/37419 [12:13<10:16, 31.58file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17957/37419 [12:14<09:51, 32.91file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17961/37419 [12:14<12:37, 25.70file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17991/37419 [12:15<13:11, 24.54file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 17999/37419 [12:16<11:24, 28.35file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 18047/37419 [12:18<10:12, 31.64file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 18055/37419 [12:18<10:15, 31.44file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 18066/37419 [12:18<11:10, 28.87file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 18071/37419 [12:18<11:07, 28.99file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 18080/37419 [12:19<12:45, 25.25file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 18084/37419 [12:19<11:41, 27.58file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 18109/37419 [12:20<14:53, 21.61file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 18113/37419 [12:20<13:03, 24.64file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 18140/37419 [12:21<16:21, 19.64file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  48%|████▊     | 18143/37419 [12:22<15:11, 21.14file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▊     | 18151/37419 [12:22<13:29, 23.81file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▊     | 18154/37419 [12:22<15:28, 20.75file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▊     | 18158/37419 [12:22<16:44, 19.17file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▊     | 18163/37419 [12:22<13:24, 23.94file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▊     | 18181/37419 [12:23<13:17, 24.13file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▊     | 18191/37419 [12:23<09:52, 32.47file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▊     | 18201/37419 [12:24<13:59, 22.90file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▊     | 18209/37419 [12:24<12:57, 24.72file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▊     | 18219/37419 [12:25<13:02, 24.54file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▊     | 18234/37419 [12:25<15:06, 21.17file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▊     | 18238/37419 [12:25<13:04, 24.43file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18242/37419 [12:25<11:48, 27.08file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18246/37419 [12:26<13:37, 23.44file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18252/37419 [12:26<10:27, 30.55file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18256/37419 [12:26<11:33, 27.64file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18266/37419 [12:26<12:30, 25.53file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18269/37419 [12:27<13:16, 24.03file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18272/37419 [12:27<13:08, 24.29file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18277/37419 [12:27<16:20, 19.51file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18281/37419 [12:27<14:14, 22.40file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18289/37419 [12:27<11:02, 28.85file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18296/37419 [12:28<13:14, 24.06file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18299/37419 [12:28<15:36, 20.41file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18317/37419 [12:29<15:53, 20.04file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18320/37419 [12:29<18:34, 17.14file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18325/37419 [12:29<14:44, 21.58file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18329/37419 [12:29<12:39, 25.14file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18335/37419 [12:29<09:58, 31.89file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18342/37419 [12:30<11:12, 28.35file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18346/37419 [12:30<14:37, 21.74file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18350/37419 [12:30<13:39, 23.26file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18353/37419 [12:30<14:58, 21.21file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18358/37419 [12:30<12:36, 25.19file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18367/37419 [12:31<11:18, 28.06file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18371/37419 [12:31<12:02, 26.37file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18374/37419 [12:31<13:19, 23.81file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18379/37419 [12:31<11:26, 27.73file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18382/37419 [12:31<11:26, 27.73file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18385/37419 [12:31<11:15, 28.19file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18394/37419 [12:32<12:17, 25.79file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18397/37419 [12:32<13:46, 23.02file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18403/37419 [12:32<10:19, 30.71file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18411/37419 [12:32<12:20, 25.66file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18414/37419 [12:33<12:56, 24.46file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18425/37419 [12:33<15:04, 21.00file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18433/37419 [12:33<10:19, 30.64file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18439/37419 [12:33<09:29, 33.34file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18443/37419 [12:33<09:57, 31.73file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18447/37419 [12:34<15:23, 20.54file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18450/37419 [12:34<14:35, 21.67file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18454/37419 [12:34<17:30, 18.05file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18457/37419 [12:34<16:19, 19.37file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18460/37419 [12:35<16:26, 19.23file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18463/37419 [12:35<16:39, 18.97file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18475/37419 [12:35<10:45, 29.33file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18479/37419 [12:35<13:28, 23.43file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18484/37419 [12:35<11:38, 27.12file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18491/37419 [12:36<13:19, 23.67file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18507/37419 [12:37<16:19, 19.31file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18516/37419 [12:37<11:18, 27.85file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  49%|████▉     | 18520/37419 [12:37<13:06, 24.04file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|████▉     | 18530/37419 [12:37<11:02, 28.52file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|████▉     | 18534/37419 [12:37<10:27, 30.10file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|████▉     | 18544/37419 [12:38<16:13, 19.39file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|████▉     | 18554/37419 [12:38<14:02, 22.40file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|████▉     | 18564/37419 [12:39<11:37, 27.02file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|████▉     | 18569/37419 [12:39<10:50, 28.99file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|████▉     | 18573/37419 [12:39<11:47, 26.65file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|████▉     | 18612/37419 [12:41<12:40, 24.72file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|████▉     | 18619/37419 [12:41<13:24, 23.36file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|████▉     | 18622/37419 [12:41<14:52, 21.06file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|████▉     | 18626/37419 [12:41<15:03, 20.79file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|████▉     | 18636/37419 [12:42<11:28, 27.28file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|████▉     | 18639/37419 [12:42<11:27, 27.30file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|████▉     | 18642/37419 [12:42<16:47, 18.64file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|████▉     | 18655/37419 [12:43<13:07, 23.84file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|████▉     | 18658/37419 [12:43<12:55, 24.19file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|████▉     | 18671/37419 [12:43<10:36, 29.46file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|████▉     | 18683/37419 [12:43<07:12, 43.29file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|████▉     | 18688/37419 [12:44<12:01, 25.95file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|████▉     | 18699/37419 [12:44<09:33, 32.62file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|████▉     | 18708/37419 [12:44<11:15, 27.71file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|█████     | 18719/37419 [12:45<09:45, 31.96file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|█████     | 18733/37419 [12:45<11:31, 27.02file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|█████     | 18739/37419 [12:46<12:39, 24.60file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|█████     | 18743/37419 [12:46<12:20, 25.21file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|█████     | 18746/37419 [12:46<12:59, 23.94file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|█████     | 18749/37419 [12:46<13:30, 23.02file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|█████     | 18769/37419 [12:47<18:15, 17.02file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|█████     | 18776/37419 [12:47<14:02, 22.12file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|█████     | 18780/37419 [12:47<12:06, 25.66file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|█████     | 18811/37419 [12:48<11:28, 27.03file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|█████     | 18814/37419 [12:49<11:28, 27.02file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|█████     | 18826/37419 [12:49<09:12, 33.67file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|█████     | 18834/37419 [12:49<12:44, 24.30file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|█████     | 18837/37419 [12:50<14:08, 21.90file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|█████     | 18840/37419 [12:50<15:01, 20.61file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|█████     | 18867/37419 [12:51<15:01, 20.57file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|█████     | 18874/37419 [12:51<15:30, 19.94file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  50%|█████     | 18879/37419 [12:51<11:59, 25.76file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 18897/37419 [12:52<15:46, 19.57file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 18904/37419 [12:52<16:28, 18.73file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 18907/37419 [12:53<15:01, 20.53file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 18912/37419 [12:53<13:18, 23.18file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 18918/37419 [12:53<10:07, 30.43file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 18922/37419 [12:53<09:57, 30.98file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 18926/37419 [12:53<09:38, 31.97file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 18930/37419 [12:53<13:16, 23.22file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 18933/37419 [12:54<17:39, 17.45file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 18940/37419 [12:54<15:21, 20.05file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 18944/37419 [12:54<13:41, 22.50file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 18947/37419 [12:54<13:23, 22.99file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 18952/37419 [12:54<11:00, 27.95file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 18956/37419 [12:55<12:02, 25.56file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 18961/37419 [12:55<11:44, 26.19file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 18964/37419 [12:55<12:18, 24.99file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 18967/37419 [12:55<13:09, 23.38file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 18978/37419 [12:56<17:21, 17.70file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 18984/37419 [12:56<13:00, 23.63file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 18987/37419 [12:56<12:39, 24.25file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 18993/37419 [12:56<13:30, 22.72file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 19046/37419 [12:58<09:56, 30.80file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 19056/37419 [12:59<09:40, 31.66file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 19064/37419 [12:59<09:53, 30.91file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 19071/37419 [12:59<09:31, 32.13file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 19077/37419 [12:59<09:37, 31.75file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 19082/37419 [13:00<10:09, 30.10file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 19087/37419 [13:00<10:21, 29.49file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 19091/37419 [13:00<10:17, 29.68file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 19095/37419 [13:00<12:44, 23.95file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 19098/37419 [13:00<15:37, 19.54file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 19104/37419 [13:01<14:34, 20.95file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 19107/37419 [13:01<15:40, 19.46file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 19112/37419 [13:01<13:07, 23.25file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 19119/37419 [13:01<10:13, 29.83file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 19123/37419 [13:01<10:13, 29.80file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 19127/37419 [13:02<12:36, 24.19file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 19130/37419 [13:02<12:20, 24.70file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 19140/37419 [13:02<07:48, 39.05file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 19145/37419 [13:02<11:56, 25.52file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 19149/37419 [13:02<12:05, 25.19file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 19153/37419 [13:02<11:17, 26.95file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 19167/37419 [13:03<14:08, 21.52file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████     | 19176/37419 [13:04<11:46, 25.82file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████▏    | 19179/37419 [13:04<14:33, 20.89file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████▏    | 19193/37419 [13:04<13:36, 22.32file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████▏    | 19209/37419 [13:05<09:33, 31.77file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████▏    | 19223/37419 [13:05<13:05, 23.17file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████▏    | 19229/37419 [13:06<10:16, 29.48file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████▏    | 19248/37419 [13:06<09:47, 30.94file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████▏    | 19261/37419 [13:07<09:52, 30.65file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  51%|█████▏    | 19265/37419 [13:07<11:03, 27.35file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19278/37419 [13:08<14:08, 21.38file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19288/37419 [13:08<11:26, 26.40file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19305/37419 [13:09<13:07, 23.00file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19310/37419 [13:09<11:29, 26.28file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19316/37419 [13:09<13:57, 21.60file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19321/37419 [13:09<11:15, 26.79file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19332/37419 [13:10<12:12, 24.70file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19335/37419 [13:10<14:17, 21.08file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19351/37419 [13:10<11:16, 26.70file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19359/37419 [13:11<08:42, 34.59file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19363/37419 [13:11<09:37, 31.27file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19371/37419 [13:11<16:59, 17.70file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19378/37419 [13:12<14:04, 21.36file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19392/37419 [13:12<15:48, 19.00file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19400/37419 [13:13<10:51, 27.67file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19404/37419 [13:13<13:40, 21.96file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19418/37419 [13:14<15:40, 19.13file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19424/37419 [13:14<14:20, 20.91file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19427/37419 [13:14<14:17, 20.98file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19430/37419 [13:14<13:48, 21.70file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19441/37419 [13:14<11:42, 25.58file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19446/37419 [13:15<10:57, 27.35file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19451/37419 [13:15<09:58, 30.01file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19464/37419 [13:15<11:08, 26.84file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19480/37419 [13:16<12:07, 24.66file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19484/37419 [13:16<11:58, 24.96file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19494/37419 [13:17<12:51, 23.23file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19504/37419 [13:17<10:57, 27.24file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19508/37419 [13:17<11:52, 25.13file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19511/37419 [13:17<12:04, 24.71file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19521/37419 [13:18<10:29, 28.43file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19533/37419 [13:18<13:13, 22.53file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19537/37419 [13:18<11:18, 26.35file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19541/37419 [13:18<10:07, 29.44file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19545/37419 [13:19<10:03, 29.60file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19549/37419 [13:19<13:00, 22.90file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19557/37419 [13:19<11:06, 26.80file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19566/37419 [13:19<10:05, 29.47file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19573/37419 [13:20<16:57, 17.54file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19579/37419 [13:20<16:33, 17.97file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19582/37419 [13:20<15:03, 19.74file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19590/37419 [13:21<11:54, 24.95file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19596/37419 [13:21<10:17, 28.85file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19606/37419 [13:21<09:00, 32.94file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19610/37419 [13:21<09:03, 32.79file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19614/37419 [13:21<09:08, 32.45file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19618/37419 [13:22<09:39, 30.71file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19625/37419 [13:22<16:46, 17.67file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19631/37419 [13:22<14:22, 20.62file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19635/37419 [13:23<14:11, 20.89file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19638/37419 [13:23<14:48, 20.02file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  52%|█████▏    | 19641/37419 [13:23<15:29, 19.13file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19649/37419 [13:23<11:57, 24.75file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19657/37419 [13:23<08:18, 35.60file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19671/37419 [13:24<08:12, 36.04file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19679/37419 [13:24<10:09, 29.13file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19690/37419 [13:25<14:12, 20.81file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19696/37419 [13:25<17:17, 17.08file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19702/37419 [13:25<13:23, 22.05file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19705/37419 [13:25<12:40, 23.28file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19713/37419 [13:26<12:00, 24.56file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19717/37419 [13:26<10:51, 27.17file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19721/37419 [13:26<10:44, 27.47file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19724/37419 [13:26<10:43, 27.48file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19727/37419 [13:26<10:39, 27.67file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19730/37419 [13:26<12:06, 24.35file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19733/37419 [13:27<12:22, 23.82file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19736/37419 [13:27<11:51, 24.86file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19755/37419 [13:27<11:47, 24.97file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19761/37419 [13:28<14:16, 20.62file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19773/37419 [13:28<12:42, 23.16file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19776/37419 [13:28<12:07, 24.26file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19779/37419 [13:29<12:51, 22.85file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19788/37419 [13:29<15:20, 19.16file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19791/37419 [13:29<15:21, 19.14file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19793/37419 [13:29<16:18, 18.02file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19800/37419 [13:29<10:43, 27.40file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19804/37419 [13:30<10:22, 28.28file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19807/37419 [13:30<11:09, 26.29file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19815/37419 [13:30<09:45, 30.06file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19825/37419 [13:31<13:30, 21.70file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19829/37419 [13:31<11:28, 25.54file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19833/37419 [13:31<10:09, 28.86file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19840/37419 [13:31<12:08, 24.12file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19843/37419 [13:31<11:38, 25.17file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19850/37419 [13:31<10:54, 26.83file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19853/37419 [13:32<12:43, 23.00file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19862/37419 [13:32<10:13, 28.64file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19876/37419 [13:33<15:12, 19.23file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19881/37419 [13:33<12:36, 23.18file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19884/37419 [13:33<12:13, 23.91file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19891/37419 [13:33<11:09, 26.18file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19895/37419 [13:33<10:21, 28.20file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19902/37419 [13:34<14:13, 20.53file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19907/37419 [13:34<11:18, 25.80file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19910/37419 [13:34<12:59, 22.47file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19913/37419 [13:34<13:06, 22.27file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19927/37419 [13:35<11:33, 25.22file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19941/37419 [13:35<08:44, 33.32file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19949/37419 [13:36<11:43, 24.82file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19960/37419 [13:36<13:53, 20.95file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19964/37419 [13:36<15:22, 18.92file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19969/37419 [13:37<12:21, 23.52file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19972/37419 [13:37<12:59, 22.39file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19977/37419 [13:37<11:07, 26.14file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19986/37419 [13:37<11:10, 26.00file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19992/37419 [13:37<09:01, 32.17file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:419: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(dpi=100)\n",
      "Processing files:  53%|█████▎    | 19996/37419 [13:37<10:01, 28.96file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▎    | 20058/37419 [13:40<10:10, 28.45file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▎    | 20071/37419 [13:41<11:00, 26.25file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▎    | 20084/37419 [13:41<11:20, 25.47file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▎    | 20096/37419 [13:42<09:27, 30.54file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▎    | 20108/37419 [13:42<09:46, 29.51file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20118/37419 [13:42<09:28, 30.42file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20122/37419 [13:43<10:31, 27.37file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20135/37419 [13:43<13:34, 21.22file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20138/37419 [13:44<13:38, 21.13file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20147/37419 [13:44<13:01, 22.10file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20151/37419 [13:44<13:04, 22.01file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20174/37419 [13:45<10:49, 26.54file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20181/37419 [13:45<13:17, 21.61file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20188/37419 [13:46<12:49, 22.40file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20193/37419 [13:46<11:10, 25.69file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20204/37419 [13:46<11:21, 25.26file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20207/37419 [13:46<11:10, 25.67file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20211/37419 [13:47<10:07, 28.33file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20218/37419 [13:47<10:02, 28.57file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20221/37419 [13:47<15:44, 18.22file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20231/37419 [13:47<10:24, 27.53file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20256/37419 [13:49<12:33, 22.77file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20265/37419 [13:49<12:11, 23.45file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20282/37419 [13:49<09:03, 31.55file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20290/37419 [13:50<09:27, 30.18file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20319/37419 [13:51<10:44, 26.55file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20325/37419 [13:51<11:47, 24.17file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20339/37419 [13:52<09:08, 31.15file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20343/37419 [13:52<08:38, 32.91file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20363/37419 [13:53<13:56, 20.39file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20371/37419 [13:53<11:26, 24.83file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  54%|█████▍    | 20386/37419 [13:53<08:10, 34.74file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▍    | 20401/37419 [13:54<08:47, 32.28file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▍    | 20407/37419 [13:54<08:28, 33.45file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▍    | 20414/37419 [13:54<10:30, 26.96file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▍    | 20424/37419 [13:55<12:16, 23.09file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▍    | 20444/37419 [13:56<09:40, 29.26file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▍    | 20454/37419 [13:56<08:33, 33.06file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▍    | 20473/37419 [13:57<14:30, 19.48file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▍    | 20477/37419 [13:57<13:20, 21.15file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▍    | 20495/37419 [13:58<10:10, 27.74file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▍    | 20515/37419 [13:59<10:54, 25.84file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▍    | 20522/37419 [13:59<11:25, 24.65file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▍    | 20525/37419 [13:59<11:02, 25.52file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▍    | 20547/37419 [14:00<12:48, 21.96file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▍    | 20553/37419 [14:00<13:19, 21.09file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▍    | 20573/37419 [14:01<10:57, 25.63file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▌    | 20584/37419 [14:02<11:21, 24.69file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▌    | 20591/37419 [14:02<08:35, 32.67file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▌    | 20599/37419 [14:02<12:34, 22.30file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▌    | 20606/37419 [14:03<12:57, 21.62file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▌    | 20616/37419 [14:03<10:13, 27.38file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▌    | 20620/37419 [14:03<09:20, 29.96file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▌    | 20629/37419 [14:03<09:59, 28.00file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▌    | 20637/37419 [14:04<10:36, 26.36file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▌    | 20640/37419 [14:04<11:08, 25.10file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▌    | 20646/37419 [14:04<12:58, 21.56file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▌    | 20649/37419 [14:04<12:25, 22.50file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▌    | 20660/37419 [14:05<11:31, 24.25file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▌    | 20700/37419 [14:06<08:32, 32.62file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▌    | 20705/37419 [14:06<07:47, 35.76file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▌    | 20716/37419 [14:07<12:12, 22.79file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▌    | 20719/37419 [14:07<12:13, 22.77file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▌    | 20732/37419 [14:08<10:59, 25.31file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▌    | 20751/37419 [14:08<11:10, 24.85file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  55%|█████▌    | 20759/37419 [14:09<09:45, 28.46file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  56%|█████▌    | 20778/37419 [14:10<12:11, 22.76file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  56%|█████▌    | 20782/37419 [14:10<11:21, 24.40file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  56%|█████▌    | 20804/37419 [14:11<10:29, 26.39file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  56%|█████▌    | 20827/37419 [14:12<10:57, 25.23file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  56%|█████▌    | 20838/37419 [14:12<10:10, 27.18file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  56%|█████▌    | 20841/37419 [14:12<10:14, 27.00file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  56%|█████▌    | 20873/37419 [14:14<10:23, 26.54file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  56%|█████▌    | 20902/37419 [14:15<12:06, 22.74file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  56%|█████▌    | 20911/37419 [14:15<12:30, 22.00file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  56%|█████▌    | 20922/37419 [14:16<11:49, 23.26file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  56%|█████▌    | 20931/37419 [14:16<14:23, 19.09file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  56%|█████▌    | 20936/37419 [14:16<10:52, 25.25file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  56%|█████▌    | 20940/37419 [14:17<12:26, 22.07file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  56%|█████▌    | 20960/37419 [14:17<12:10, 22.54file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  56%|█████▌    | 20965/37419 [14:18<10:43, 25.57file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  56%|█████▌    | 20969/37419 [14:18<09:38, 28.44file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  56%|█████▌    | 20974/37419 [14:18<09:59, 27.41file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  56%|█████▌    | 20982/37419 [14:18<10:18, 26.60file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files:  56%|█████▌    | 20997/37419 [14:19<08:49, 30.99file/s]/scratch/thomas/GitHub/sogym_v2/sogym/env.py:37: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  self.fig = plt.figure(dpi=100)\n",
      "Processing files: 100%|██████████| 37419/37419 [25:35<00:00, 24.37file/s]\n"
     ]
    }
   ],
   "source": [
    "from sogym.expert_generation import generate_expert_dataset\n",
    "import pickle\n",
    "\n",
    "# Specify the number of permutations to generate\n",
    "num_permutations = None\n",
    "observation_type = \"topopt_game\"\n",
    "\n",
    "# Specify the environment configuration (optional)\n",
    "env_kwargs = {\n",
    "    'mode': 'train',\n",
    "    'observation_type': observation_type,\n",
    "    'vol_constraint_type': 'hard',\n",
    "    'seed': 42,\n",
    "    'resolution' : 50,\n",
    "    'check_connectivity':True\n",
    "}\n",
    "\n",
    "directory_path = \"/home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/topologies/unique_combined\"\n",
    "generate_expert_dataset(directory_path,env_kwargs,observation_type=observation_type, plot_terminated=False,num_permutations = num_permutations, file_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ae43524",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msogym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpretraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_expert_dataset\n\u001b[1;32m      2\u001b[0m chunk_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/expert/unique_combined_topopt_game_20240515_071024\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m expert_dataset \u001b[38;5;241m=\u001b[39m load_expert_dataset(chunk_dir, \u001b[43mtrain_env\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#print length of expertdataset:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(expert_dataset)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m8\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_env' is not defined"
     ]
    }
   ],
   "source": [
    "from sogym.pretraining import load_expert_dataset\n",
    "chunk_dir = '/home/thomas/Documents/scratch_thomas/GitHub/sogym_v2/dataset/expert/unique_combined_topopt_game_20240515_071024'\n",
    "expert_dataset = load_expert_dataset(chunk_dir, train_env)\n",
    "#print length of expertdataset:\n",
    "print(len(expert_dataset)/8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baea7b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2148453/3307617055.py:12: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3277.)\n",
      "  axes[0].imshow(observation['image'].T, cmap='gray', origin='lower')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8048,  1.0000,  0.3147, -0.3081,  1.0000,  1.0000],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get a random sample from the dataset\n",
    "sample_idx = np.random.randint(len(expert_dataset))\n",
    "sample = expert_dataset[sample_idx]\n",
    "\n",
    "# Extract the observation and reward from the sample\n",
    "observation, action = sample\n",
    "\n",
    "# Subplot with image, strain_energy, and structure_strain_energy observations:\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot image observation\n",
    "axes[0].imshow(observation['image'].T, cmap='gray', origin='lower')\n",
    "axes[0].axis('on')\n",
    "axes[0].set_title(\"Image Observation\")\n",
    "\n",
    "# Plot strain_energy observation\n",
    "axes[1].imshow(observation['structure_strain_energy'].T, origin='lower')\n",
    "axes[1].axis('on')\n",
    "axes[1].set_title(\"Structure Strain Energy Observation\")\n",
    "\n",
    "print(action)\n",
    "plt.tight_layout()\n",
    "plt.savefig('expert_observation.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62c7c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = train_env.reset()\n",
    "\n",
    "#use action and plot the result\n",
    "obs, rewards, dones,truncated, info = train_env.step(np.array(action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bedb95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(obs['image'].T,origin='lower')\n",
    "plt.savefig('expert_action.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73ad778e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'datetime' has no attribute 'now'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 52\u001b[0m\n\u001b[1;32m     44\u001b[0m     model \u001b[38;5;241m=\u001b[39m TD3(env\u001b[38;5;241m=\u001b[39menv,\n\u001b[1;32m     45\u001b[0m                 policy \u001b[38;5;241m=\u001b[39mchosen_policy, \n\u001b[1;32m     46\u001b[0m                 policy_kwargs\u001b[38;5;241m=\u001b[39mpolicy_kwargs,\n\u001b[1;32m     47\u001b[0m                 action_noise\u001b[38;5;241m=\u001b[39maction_noise,\n\u001b[1;32m     48\u001b[0m                 device\u001b[38;5;241m=\u001b[39mdevice, \n\u001b[1;32m     49\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39malgorithm_params)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Get the current date and time\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m current_datetime \u001b[38;5;241m=\u001b[39m \u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnow\u001b[49m()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Create the tb_log_name string\u001b[39;00m\n\u001b[1;32m     55\u001b[0m tb_log_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malgorithm_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_datetime\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'datetime' has no attribute 'now'"
     ]
    }
   ],
   "source": [
    "chosen_policy = \"MlpPolicy\" if observation_type == 'box_dense' else \"MultiInputPolicy\"\n",
    "\n",
    "feature_extractor = ImageDictExtractor if observation_type == 'image' or observation_type == 'topopt_game' else CustomBoxDense\n",
    "\n",
    "# Load the YAML file\n",
    "env=train_env\n",
    "\n",
    "with open(\"algorithms.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Extract the parameters for the desired algorithm\n",
    "algorithm_name = \"PPO\"  # or \"TD3\"\n",
    "algorithm_params = config[algorithm_name]\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=feature_extractor,\n",
    "    net_arch = config['common']['net_arch'],\n",
    "    share_features_extractor = False\n",
    ")\n",
    "\n",
    "# Create the model based on the algorithm name and parameters\n",
    "if algorithm_name == \"SAC\":\n",
    "    model = SAC(env=env,\n",
    "                policy = chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                device=device, \n",
    "                **algorithm_params)\n",
    "\n",
    "elif algorithm_name == \"PPO\":\n",
    "    model = PPO(env=env, \n",
    "                policy = chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                device = device, \n",
    "                **algorithm_params)\n",
    "    \n",
    "    \n",
    "\n",
    "elif algorithm_name == \"TD3\":\n",
    "    # Create the action noise object\n",
    "    n_actions = env.action_space.shape[-1]\n",
    "    action_noise_params = algorithm_params.pop(\"action_noise\")\n",
    "    action_noise = NormalActionNoise(mean=action_noise_params[\"mean\"] * np.ones(n_actions),\n",
    "                                     sigma=action_noise_params[\"sigma\"] * np.ones(n_actions))\n",
    "    model = TD3(env=env,\n",
    "                policy =chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                action_noise=action_noise,\n",
    "                device=device, \n",
    "                **algorithm_params)\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Create the tb_log_name string\n",
    "tb_log_name = f\"{algorithm_name}_{current_datetime}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090eedd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 18,875,917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MultiInputActorCriticPolicy              6\n",
       "├─ImageDictExtractor: 1-1                --\n",
       "│    └─ReLU: 2-1                         --\n",
       "│    └─ModuleDict: 2-2                   --\n",
       "│    │    └─Sequential: 3-1              22,784\n",
       "│    │    └─Sequential: 3-2              93,248\n",
       "│    │    └─Sequential: 3-3              16,768\n",
       "│    │    └─Sequential: 3-4              16,768\n",
       "│    │    └─Sequential: 3-5              93,248\n",
       "│    │    └─Sequential: 3-6              16,768\n",
       "├─ImageDictExtractor: 1-2                (recursive)\n",
       "│    └─ReLU: 2-3                         --\n",
       "│    └─ModuleDict: 2-4                   (recursive)\n",
       "│    │    └─Sequential: 3-7              (recursive)\n",
       "│    │    └─Sequential: 3-8              (recursive)\n",
       "│    │    └─Sequential: 3-9              (recursive)\n",
       "│    │    └─Sequential: 3-10             (recursive)\n",
       "│    │    └─Sequential: 3-11             (recursive)\n",
       "│    │    └─Sequential: 3-12             (recursive)\n",
       "├─ImageDictExtractor: 1-3                --\n",
       "│    └─ReLU: 2-5                         --\n",
       "│    └─ModuleDict: 2-6                   --\n",
       "│    │    └─Sequential: 3-13             22,784\n",
       "│    │    └─Sequential: 3-14             93,248\n",
       "│    │    └─Sequential: 3-15             16,768\n",
       "│    │    └─Sequential: 3-16             16,768\n",
       "│    │    └─Sequential: 3-17             93,248\n",
       "│    │    └─Sequential: 3-18             16,768\n",
       "├─MlpExtractor: 1-4                      --\n",
       "│    └─Sequential: 2-7                   --\n",
       "│    │    └─Linear: 3-19                 8,651,264\n",
       "│    │    └─Tanh: 3-20                   --\n",
       "│    │    └─Linear: 3-21                 262,656\n",
       "│    │    └─Tanh: 3-22                   --\n",
       "│    │    └─Linear: 3-23                 262,656\n",
       "│    │    └─Tanh: 3-24                   --\n",
       "│    └─Sequential: 2-8                   --\n",
       "│    │    └─Linear: 3-25                 8,651,264\n",
       "│    │    └─Tanh: 3-26                   --\n",
       "│    │    └─Linear: 3-27                 262,656\n",
       "│    │    └─Tanh: 3-28                   --\n",
       "│    │    └─Linear: 3-29                 262,656\n",
       "│    │    └─Tanh: 3-30                   --\n",
       "├─Linear: 1-5                            3,078\n",
       "├─Linear: 1-6                            513\n",
       "=================================================================\n",
       "Total params: 18,875,917\n",
       "Trainable params: 18,875,917\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "total_params = sum(p.numel() for p in model.policy.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "data = {k: v for k, v in observation.items()}\n",
    "# Assuming you have a PyTorch model named 'model' and the input size is (3, 224, 224)\n",
    "summary(model.policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0166b0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, tensorboard.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/thomasrb/pretraining-rl/a5ffd4bd210a4d489ca9f6d633d6c3bb\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     grad_norm [20]  : (0.8937201855982214, 1.7579312044124606)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mae [20]        : (0.8019486623404353, 0.9296369073108344)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mean_reward [3] : (0.04329246059060097, 0.0435519840568304)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     std_reward [3]  : (0.0011471042911364835, 0.0013665396855924601)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     test_loss [20]  : (1.031089244450825, 1.3671609398306774)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     test_mae [20]   : (0.8019486623404353, 0.9296369073108344)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss [20] : (1.5140182760401668, 1.8570199370762988)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : PPO_1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed)     : 1 (129.92 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, tensorboard.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/thomasrb/pretraining-rl/7edb867b080a4f00a583fee300ee1c6b\n",
      "\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [4096/194182 (2%)]\tLoss: 2.160648\tGrad Norm: 4.577515\tLR: 0.030000\n",
      "Train Epoch: 1 [24576/194182 (12%)]\tLoss: 2.073319\tGrad Norm: 3.667909\tLR: 0.030000\n",
      "Train Epoch: 1 [45056/194182 (23%)]\tLoss: 1.973014\tGrad Norm: 2.675155\tLR: 0.030000\n",
      "Train Epoch: 1 [65536/194182 (33%)]\tLoss: 1.976953\tGrad Norm: 1.580951\tLR: 0.030000\n",
      "Train Epoch: 1 [86016/194182 (44%)]\tLoss: 1.920007\tGrad Norm: 1.185111\tLR: 0.030000\n",
      "Train Epoch: 1 [106496/194182 (54%)]\tLoss: 1.926742\tGrad Norm: 0.883217\tLR: 0.030000\n",
      "Train Epoch: 1 [126976/194182 (65%)]\tLoss: 1.907687\tGrad Norm: 0.878565\tLR: 0.030000\n",
      "Train Epoch: 1 [147456/194182 (75%)]\tLoss: 1.927369\tGrad Norm: 0.875123\tLR: 0.030000\n",
      "Train Epoch: 1 [167936/194182 (85%)]\tLoss: 1.924447\tGrad Norm: 0.856896\tLR: 0.030000\n",
      "Train Epoch: 1 [188416/194182 (96%)]\tLoss: 1.938705\tGrad Norm: 0.890139\tLR: 0.030000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Average loss: 1.9590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 1.4309, Average MAE: 0.9517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:283: UserWarning: Path 'checkpoints/imitation_PPO_20240514' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 2 [4096/194182 (2%)]\tLoss: 1.900907\tGrad Norm: 0.849075\tLR: 0.030000\n",
      "Train Epoch: 2 [24576/194182 (12%)]\tLoss: 1.932526\tGrad Norm: 0.876250\tLR: 0.030000\n",
      "Train Epoch: 2 [45056/194182 (23%)]\tLoss: 1.891913\tGrad Norm: 0.822363\tLR: 0.030000\n",
      "Train Epoch: 2 [65536/194182 (33%)]\tLoss: 1.941382\tGrad Norm: 0.899777\tLR: 0.030000\n",
      "Train Epoch: 2 [86016/194182 (44%)]\tLoss: 1.904559\tGrad Norm: 0.857332\tLR: 0.030000\n",
      "Train Epoch: 2 [106496/194182 (54%)]\tLoss: 1.886916\tGrad Norm: 0.845536\tLR: 0.030000\n",
      "Train Epoch: 2 [126976/194182 (65%)]\tLoss: 1.902973\tGrad Norm: 0.837618\tLR: 0.030000\n",
      "Train Epoch: 2 [147456/194182 (75%)]\tLoss: 1.937067\tGrad Norm: 0.902733\tLR: 0.030000\n",
      "Train Epoch: 2 [167936/194182 (85%)]\tLoss: 1.887563\tGrad Norm: 0.834090\tLR: 0.030000\n",
      "Train Epoch: 2 [188416/194182 (96%)]\tLoss: 1.884206\tGrad Norm: 0.872362\tLR: 0.030000\n",
      "Train set: Average loss: 1.9026\n",
      "Test set: Average loss: 1.4074, Average MAE: 0.9434\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 3 [4096/194182 (2%)]\tLoss: 1.905300\tGrad Norm: 0.916991\tLR: 0.030000\n",
      "Train Epoch: 3 [24576/194182 (12%)]\tLoss: 1.887032\tGrad Norm: 0.868953\tLR: 0.030000\n",
      "Train Epoch: 3 [45056/194182 (23%)]\tLoss: 1.902989\tGrad Norm: 0.902055\tLR: 0.030000\n",
      "Train Epoch: 3 [65536/194182 (33%)]\tLoss: 1.901816\tGrad Norm: 0.863805\tLR: 0.030000\n",
      "Train Epoch: 3 [86016/194182 (44%)]\tLoss: 1.911526\tGrad Norm: 0.842206\tLR: 0.030000\n",
      "Train Epoch: 3 [106496/194182 (54%)]\tLoss: 1.858106\tGrad Norm: 0.839347\tLR: 0.030000\n",
      "Train Epoch: 3 [126976/194182 (65%)]\tLoss: 1.894552\tGrad Norm: 0.856238\tLR: 0.030000\n",
      "Train Epoch: 3 [147456/194182 (75%)]\tLoss: 1.863844\tGrad Norm: 0.844431\tLR: 0.030000\n",
      "Train Epoch: 3 [167936/194182 (85%)]\tLoss: 1.874046\tGrad Norm: 0.922572\tLR: 0.030000\n",
      "Train Epoch: 3 [188416/194182 (96%)]\tLoss: 1.861079\tGrad Norm: 0.813419\tLR: 0.030000\n",
      "Train set: Average loss: 1.8860\n",
      "Test set: Average loss: 1.3909, Average MAE: 0.9378\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 4 [4096/194182 (2%)]\tLoss: 1.881025\tGrad Norm: 1.010961\tLR: 0.030000\n",
      "Train Epoch: 4 [24576/194182 (12%)]\tLoss: 1.871706\tGrad Norm: 0.913992\tLR: 0.030000\n",
      "Train Epoch: 4 [45056/194182 (23%)]\tLoss: 1.866827\tGrad Norm: 0.953945\tLR: 0.030000\n",
      "Train Epoch: 4 [65536/194182 (33%)]\tLoss: 1.850085\tGrad Norm: 0.790033\tLR: 0.030000\n",
      "Train Epoch: 4 [86016/194182 (44%)]\tLoss: 1.870010\tGrad Norm: 0.864223\tLR: 0.030000\n",
      "Train Epoch: 4 [106496/194182 (54%)]\tLoss: 1.892248\tGrad Norm: 1.299179\tLR: 0.030000\n",
      "Train Epoch: 4 [126976/194182 (65%)]\tLoss: 1.859638\tGrad Norm: 0.867680\tLR: 0.030000\n",
      "Train Epoch: 4 [147456/194182 (75%)]\tLoss: 1.864684\tGrad Norm: 1.434162\tLR: 0.030000\n",
      "Train Epoch: 4 [167936/194182 (85%)]\tLoss: 1.864604\tGrad Norm: 0.915059\tLR: 0.030000\n",
      "Train Epoch: 4 [188416/194182 (96%)]\tLoss: 1.862749\tGrad Norm: 1.145096\tLR: 0.030000\n",
      "Train set: Average loss: 1.8628\n",
      "Test set: Average loss: 1.3770, Average MAE: 0.9334\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 5 [4096/194182 (2%)]\tLoss: 1.867790\tGrad Norm: 1.611221\tLR: 0.030000\n",
      "Train Epoch: 5 [24576/194182 (12%)]\tLoss: 1.864925\tGrad Norm: 0.983519\tLR: 0.030000\n",
      "Train Epoch: 5 [45056/194182 (23%)]\tLoss: 1.838402\tGrad Norm: 0.827902\tLR: 0.030000\n",
      "Train Epoch: 5 [65536/194182 (33%)]\tLoss: 1.828217\tGrad Norm: 1.420263\tLR: 0.030000\n",
      "Train Epoch: 5 [86016/194182 (44%)]\tLoss: 1.858537\tGrad Norm: 1.426191\tLR: 0.030000\n",
      "Train Epoch: 5 [106496/194182 (54%)]\tLoss: 1.845391\tGrad Norm: 1.366249\tLR: 0.030000\n",
      "Train Epoch: 5 [126976/194182 (65%)]\tLoss: 1.839639\tGrad Norm: 0.968527\tLR: 0.030000\n",
      "Train Epoch: 5 [147456/194182 (75%)]\tLoss: 1.850496\tGrad Norm: 0.921731\tLR: 0.030000\n",
      "Train Epoch: 5 [167936/194182 (85%)]\tLoss: 1.868066\tGrad Norm: 1.315301\tLR: 0.030000\n",
      "Train Epoch: 5 [188416/194182 (96%)]\tLoss: 1.840816\tGrad Norm: 1.278813\tLR: 0.030000\n",
      "Train set: Average loss: 1.8451\n",
      "Test set: Average loss: 1.3569, Average MAE: 0.9255\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Mean reward = 0.043 +/- 0.001\n",
      "Train Epoch: 6 [4096/194182 (2%)]\tLoss: 1.827989\tGrad Norm: 1.502147\tLR: 0.030000\n",
      "Train Epoch: 6 [24576/194182 (12%)]\tLoss: 1.833435\tGrad Norm: 1.413443\tLR: 0.030000\n",
      "Train Epoch: 6 [45056/194182 (23%)]\tLoss: 1.823013\tGrad Norm: 1.501365\tLR: 0.030000\n",
      "Train Epoch: 6 [65536/194182 (33%)]\tLoss: 1.827443\tGrad Norm: 0.856202\tLR: 0.030000\n",
      "Train Epoch: 6 [86016/194182 (44%)]\tLoss: 1.826959\tGrad Norm: 0.886554\tLR: 0.030000\n",
      "Train Epoch: 6 [106496/194182 (54%)]\tLoss: 1.796588\tGrad Norm: 0.980739\tLR: 0.030000\n",
      "Train Epoch: 6 [126976/194182 (65%)]\tLoss: 1.827485\tGrad Norm: 1.772264\tLR: 0.030000\n",
      "Train Epoch: 6 [147456/194182 (75%)]\tLoss: 1.841626\tGrad Norm: 1.423901\tLR: 0.030000\n",
      "Train Epoch: 6 [167936/194182 (85%)]\tLoss: 1.815033\tGrad Norm: 1.446026\tLR: 0.030000\n",
      "Train Epoch: 6 [188416/194182 (96%)]\tLoss: 1.823656\tGrad Norm: 1.364411\tLR: 0.030000\n",
      "Train set: Average loss: 1.8244\n",
      "Test set: Average loss: 1.3326, Average MAE: 0.9183\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 7 [4096/194182 (2%)]\tLoss: 1.826189\tGrad Norm: 1.394226\tLR: 0.030000\n",
      "Train Epoch: 7 [24576/194182 (12%)]\tLoss: 1.816713\tGrad Norm: 1.279190\tLR: 0.030000\n",
      "Train Epoch: 7 [45056/194182 (23%)]\tLoss: 1.792685\tGrad Norm: 1.191845\tLR: 0.030000\n",
      "Train Epoch: 7 [65536/194182 (33%)]\tLoss: 1.799801\tGrad Norm: 1.356409\tLR: 0.030000\n",
      "Train Epoch: 7 [86016/194182 (44%)]\tLoss: 1.814556\tGrad Norm: 1.269151\tLR: 0.030000\n",
      "Train Epoch: 7 [106496/194182 (54%)]\tLoss: 1.801710\tGrad Norm: 1.211699\tLR: 0.030000\n",
      "Train Epoch: 7 [126976/194182 (65%)]\tLoss: 1.784874\tGrad Norm: 1.377833\tLR: 0.030000\n",
      "Train Epoch: 7 [147456/194182 (75%)]\tLoss: 1.817101\tGrad Norm: 2.040298\tLR: 0.030000\n",
      "Train Epoch: 7 [167936/194182 (85%)]\tLoss: 1.798471\tGrad Norm: 1.212355\tLR: 0.030000\n",
      "Train Epoch: 7 [188416/194182 (96%)]\tLoss: 1.770860\tGrad Norm: 1.315980\tLR: 0.030000\n",
      "Train set: Average loss: 1.8038\n",
      "Test set: Average loss: 1.3101, Average MAE: 0.9072\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 8 [4096/194182 (2%)]\tLoss: 1.794392\tGrad Norm: 1.389322\tLR: 0.030000\n",
      "Train Epoch: 8 [24576/194182 (12%)]\tLoss: 1.780001\tGrad Norm: 1.369722\tLR: 0.030000\n",
      "Train Epoch: 8 [45056/194182 (23%)]\tLoss: 1.806547\tGrad Norm: 1.155636\tLR: 0.030000\n",
      "Train Epoch: 8 [65536/194182 (33%)]\tLoss: 1.787196\tGrad Norm: 1.185304\tLR: 0.030000\n",
      "Train Epoch: 8 [86016/194182 (44%)]\tLoss: 1.780211\tGrad Norm: 1.385403\tLR: 0.030000\n",
      "Train Epoch: 8 [106496/194182 (54%)]\tLoss: 1.792336\tGrad Norm: 1.494418\tLR: 0.030000\n",
      "Train Epoch: 8 [126976/194182 (65%)]\tLoss: 1.799640\tGrad Norm: 1.400565\tLR: 0.030000\n",
      "Train Epoch: 8 [147456/194182 (75%)]\tLoss: 1.783947\tGrad Norm: 1.324531\tLR: 0.030000\n",
      "Train Epoch: 8 [167936/194182 (85%)]\tLoss: 1.777803\tGrad Norm: 1.361896\tLR: 0.030000\n",
      "Train Epoch: 8 [188416/194182 (96%)]\tLoss: 1.810462\tGrad Norm: 1.431548\tLR: 0.030000\n",
      "Train set: Average loss: 1.7890\n",
      "Test set: Average loss: 1.2952, Average MAE: 0.9020\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 9 [4096/194182 (2%)]\tLoss: 1.762981\tGrad Norm: 1.360382\tLR: 0.030000\n",
      "Train Epoch: 9 [24576/194182 (12%)]\tLoss: 1.769045\tGrad Norm: 1.712500\tLR: 0.030000\n",
      "Train Epoch: 9 [45056/194182 (23%)]\tLoss: 1.780984\tGrad Norm: 1.553580\tLR: 0.030000\n",
      "Train Epoch: 9 [65536/194182 (33%)]\tLoss: 1.789789\tGrad Norm: 1.380874\tLR: 0.030000\n",
      "Train Epoch: 9 [86016/194182 (44%)]\tLoss: 1.763864\tGrad Norm: 1.338462\tLR: 0.030000\n",
      "Train Epoch: 9 [106496/194182 (54%)]\tLoss: 1.763221\tGrad Norm: 1.465822\tLR: 0.030000\n",
      "Train Epoch: 9 [126976/194182 (65%)]\tLoss: 1.777327\tGrad Norm: 1.567607\tLR: 0.030000\n",
      "Train Epoch: 9 [147456/194182 (75%)]\tLoss: 1.737129\tGrad Norm: 1.146961\tLR: 0.030000\n",
      "Train Epoch: 9 [167936/194182 (85%)]\tLoss: 1.757338\tGrad Norm: 1.476737\tLR: 0.030000\n",
      "Train Epoch: 9 [188416/194182 (96%)]\tLoss: 1.749885\tGrad Norm: 1.084300\tLR: 0.030000\n",
      "Train set: Average loss: 1.7679\n",
      "Test set: Average loss: 1.2740, Average MAE: 0.8980\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 10 [4096/194182 (2%)]\tLoss: 1.769253\tGrad Norm: 0.975296\tLR: 0.030000\n",
      "Train Epoch: 10 [24576/194182 (12%)]\tLoss: 1.770911\tGrad Norm: 1.248902\tLR: 0.030000\n",
      "Train Epoch: 10 [45056/194182 (23%)]\tLoss: 1.733680\tGrad Norm: 1.510344\tLR: 0.030000\n",
      "Train Epoch: 10 [65536/194182 (33%)]\tLoss: 1.743270\tGrad Norm: 1.002045\tLR: 0.030000\n",
      "Train Epoch: 10 [86016/194182 (44%)]\tLoss: 1.772159\tGrad Norm: 1.500090\tLR: 0.030000\n",
      "Train Epoch: 10 [106496/194182 (54%)]\tLoss: 1.728231\tGrad Norm: 1.035781\tLR: 0.030000\n",
      "Train Epoch: 10 [126976/194182 (65%)]\tLoss: 1.738982\tGrad Norm: 1.649494\tLR: 0.030000\n",
      "Train Epoch: 10 [147456/194182 (75%)]\tLoss: 1.741372\tGrad Norm: 1.341466\tLR: 0.030000\n",
      "Train Epoch: 10 [167936/194182 (85%)]\tLoss: 1.738523\tGrad Norm: 0.900771\tLR: 0.030000\n",
      "Train Epoch: 10 [188416/194182 (96%)]\tLoss: 1.740744\tGrad Norm: 1.669049\tLR: 0.030000\n",
      "Train set: Average loss: 1.7454\n",
      "Test set: Average loss: 1.2719, Average MAE: 0.8955\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 10: Mean reward = 0.043 +/- 0.001\n",
      "Train Epoch: 11 [4096/194182 (2%)]\tLoss: 1.764326\tGrad Norm: 2.245006\tLR: 0.030000\n",
      "Train Epoch: 11 [24576/194182 (12%)]\tLoss: 1.720478\tGrad Norm: 1.083872\tLR: 0.030000\n",
      "Train Epoch: 11 [45056/194182 (23%)]\tLoss: 1.734799\tGrad Norm: 1.510068\tLR: 0.030000\n",
      "Train Epoch: 11 [65536/194182 (33%)]\tLoss: 1.723508\tGrad Norm: 0.904805\tLR: 0.030000\n",
      "Train Epoch: 11 [86016/194182 (44%)]\tLoss: 1.753489\tGrad Norm: 1.646385\tLR: 0.030000\n",
      "Train Epoch: 11 [106496/194182 (54%)]\tLoss: 1.729498\tGrad Norm: 1.519382\tLR: 0.030000\n",
      "Train Epoch: 11 [126976/194182 (65%)]\tLoss: 1.729522\tGrad Norm: 1.566272\tLR: 0.030000\n",
      "Train Epoch: 11 [147456/194182 (75%)]\tLoss: 1.716372\tGrad Norm: 1.309420\tLR: 0.030000\n",
      "Train Epoch: 11 [167936/194182 (85%)]\tLoss: 1.718971\tGrad Norm: 1.657474\tLR: 0.030000\n",
      "Train Epoch: 11 [188416/194182 (96%)]\tLoss: 1.704807\tGrad Norm: 1.662744\tLR: 0.030000\n",
      "Train set: Average loss: 1.7309\n",
      "Test set: Average loss: 1.2390, Average MAE: 0.8849\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 12 [4096/194182 (2%)]\tLoss: 1.719006\tGrad Norm: 1.566524\tLR: 0.030000\n",
      "Train Epoch: 12 [24576/194182 (12%)]\tLoss: 1.716619\tGrad Norm: 0.909381\tLR: 0.030000\n",
      "Train Epoch: 12 [45056/194182 (23%)]\tLoss: 1.708320\tGrad Norm: 1.134912\tLR: 0.030000\n",
      "Train Epoch: 12 [65536/194182 (33%)]\tLoss: 1.695576\tGrad Norm: 1.300493\tLR: 0.030000\n",
      "Train Epoch: 12 [86016/194182 (44%)]\tLoss: 1.709697\tGrad Norm: 1.404707\tLR: 0.030000\n",
      "Train Epoch: 12 [106496/194182 (54%)]\tLoss: 1.707170\tGrad Norm: 1.848675\tLR: 0.030000\n",
      "Train Epoch: 12 [126976/194182 (65%)]\tLoss: 1.711682\tGrad Norm: 1.298863\tLR: 0.030000\n",
      "Train Epoch: 12 [147456/194182 (75%)]\tLoss: 1.683574\tGrad Norm: 1.519559\tLR: 0.030000\n",
      "Train Epoch: 12 [167936/194182 (85%)]\tLoss: 1.721223\tGrad Norm: 1.645282\tLR: 0.030000\n",
      "Train Epoch: 12 [188416/194182 (96%)]\tLoss: 1.681801\tGrad Norm: 1.200863\tLR: 0.030000\n",
      "Train set: Average loss: 1.7086\n",
      "Test set: Average loss: 1.2151, Average MAE: 0.8757\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 13 [4096/194182 (2%)]\tLoss: 1.676733\tGrad Norm: 1.186686\tLR: 0.030000\n",
      "Train Epoch: 13 [24576/194182 (12%)]\tLoss: 1.713564\tGrad Norm: 1.496598\tLR: 0.030000\n",
      "Train Epoch: 13 [45056/194182 (23%)]\tLoss: 1.703152\tGrad Norm: 1.893175\tLR: 0.030000\n",
      "Train Epoch: 13 [65536/194182 (33%)]\tLoss: 1.699946\tGrad Norm: 1.387157\tLR: 0.030000\n",
      "Train Epoch: 13 [86016/194182 (44%)]\tLoss: 1.710743\tGrad Norm: 1.786567\tLR: 0.030000\n",
      "Train Epoch: 13 [106496/194182 (54%)]\tLoss: 1.693396\tGrad Norm: 1.249256\tLR: 0.030000\n",
      "Train Epoch: 13 [126976/194182 (65%)]\tLoss: 1.702985\tGrad Norm: 1.711317\tLR: 0.030000\n",
      "Train Epoch: 13 [147456/194182 (75%)]\tLoss: 1.686655\tGrad Norm: 1.456181\tLR: 0.030000\n",
      "Train Epoch: 13 [167936/194182 (85%)]\tLoss: 1.677374\tGrad Norm: 1.284999\tLR: 0.030000\n",
      "Train Epoch: 13 [188416/194182 (96%)]\tLoss: 1.683011\tGrad Norm: 1.685890\tLR: 0.030000\n",
      "Train set: Average loss: 1.6922\n",
      "Test set: Average loss: 1.2031, Average MAE: 0.8730\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 14 [4096/194182 (2%)]\tLoss: 1.675609\tGrad Norm: 1.130678\tLR: 0.030000\n",
      "Train Epoch: 14 [24576/194182 (12%)]\tLoss: 1.679100\tGrad Norm: 1.012843\tLR: 0.030000\n",
      "Train Epoch: 14 [45056/194182 (23%)]\tLoss: 1.680248\tGrad Norm: 1.223234\tLR: 0.030000\n",
      "Train Epoch: 14 [65536/194182 (33%)]\tLoss: 1.688571\tGrad Norm: 1.205026\tLR: 0.030000\n",
      "Train Epoch: 14 [86016/194182 (44%)]\tLoss: 1.665736\tGrad Norm: 1.077061\tLR: 0.030000\n",
      "Train Epoch: 14 [106496/194182 (54%)]\tLoss: 1.672874\tGrad Norm: 1.517053\tLR: 0.030000\n",
      "Train Epoch: 14 [126976/194182 (65%)]\tLoss: 1.653328\tGrad Norm: 1.240079\tLR: 0.030000\n",
      "Train Epoch: 14 [147456/194182 (75%)]\tLoss: 1.666823\tGrad Norm: 1.177881\tLR: 0.030000\n",
      "Train Epoch: 14 [167936/194182 (85%)]\tLoss: 1.646624\tGrad Norm: 1.025228\tLR: 0.030000\n",
      "Train Epoch: 14 [188416/194182 (96%)]\tLoss: 1.661366\tGrad Norm: 1.464113\tLR: 0.030000\n",
      "Train set: Average loss: 1.6657\n",
      "Test set: Average loss: 1.1999, Average MAE: 0.8698\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 15 [4096/194182 (2%)]\tLoss: 1.682660\tGrad Norm: 2.186476\tLR: 0.030000\n",
      "Train Epoch: 15 [24576/194182 (12%)]\tLoss: 1.655602\tGrad Norm: 1.231448\tLR: 0.030000\n",
      "Train Epoch: 15 [45056/194182 (23%)]\tLoss: 1.669856\tGrad Norm: 1.561367\tLR: 0.030000\n",
      "Train Epoch: 15 [65536/194182 (33%)]\tLoss: 1.635732\tGrad Norm: 1.208335\tLR: 0.030000\n",
      "Train Epoch: 15 [86016/194182 (44%)]\tLoss: 1.657396\tGrad Norm: 1.364638\tLR: 0.030000\n",
      "Train Epoch: 15 [106496/194182 (54%)]\tLoss: 1.639172\tGrad Norm: 1.630961\tLR: 0.030000\n",
      "Train Epoch: 15 [126976/194182 (65%)]\tLoss: 1.659880\tGrad Norm: 1.420740\tLR: 0.030000\n",
      "Train Epoch: 15 [147456/194182 (75%)]\tLoss: 1.632931\tGrad Norm: 1.153278\tLR: 0.030000\n",
      "Train Epoch: 15 [167936/194182 (85%)]\tLoss: 1.634039\tGrad Norm: 1.115495\tLR: 0.030000\n",
      "Train Epoch: 15 [188416/194182 (96%)]\tLoss: 1.636206\tGrad Norm: 1.499437\tLR: 0.030000\n",
      "Train set: Average loss: 1.6522\n",
      "Test set: Average loss: 1.1567, Average MAE: 0.8543\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 15: Mean reward = 0.043 +/- 0.001\n",
      "Train Epoch: 16 [4096/194182 (2%)]\tLoss: 1.631521\tGrad Norm: 1.107094\tLR: 0.030000\n",
      "Train Epoch: 16 [24576/194182 (12%)]\tLoss: 1.612324\tGrad Norm: 1.208266\tLR: 0.030000\n",
      "Train Epoch: 16 [45056/194182 (23%)]\tLoss: 1.634111\tGrad Norm: 1.514162\tLR: 0.030000\n",
      "Train Epoch: 16 [65536/194182 (33%)]\tLoss: 1.636234\tGrad Norm: 1.175571\tLR: 0.030000\n",
      "Train Epoch: 16 [86016/194182 (44%)]\tLoss: 1.647286\tGrad Norm: 1.458836\tLR: 0.030000\n",
      "Train Epoch: 16 [106496/194182 (54%)]\tLoss: 1.636968\tGrad Norm: 1.480087\tLR: 0.030000\n",
      "Train Epoch: 16 [126976/194182 (65%)]\tLoss: 1.613727\tGrad Norm: 1.476885\tLR: 0.030000\n",
      "Train Epoch: 16 [147456/194182 (75%)]\tLoss: 1.636180\tGrad Norm: 1.153487\tLR: 0.030000\n",
      "Train Epoch: 16 [167936/194182 (85%)]\tLoss: 1.625157\tGrad Norm: 1.402475\tLR: 0.030000\n",
      "Train Epoch: 16 [188416/194182 (96%)]\tLoss: 1.642454\tGrad Norm: 1.188453\tLR: 0.030000\n",
      "Train set: Average loss: 1.6310\n",
      "Test set: Average loss: 1.1464, Average MAE: 0.8505\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 17 [4096/194182 (2%)]\tLoss: 1.642103\tGrad Norm: 1.426073\tLR: 0.030000\n",
      "Train Epoch: 17 [24576/194182 (12%)]\tLoss: 1.605649\tGrad Norm: 1.291356\tLR: 0.030000\n",
      "Train Epoch: 17 [45056/194182 (23%)]\tLoss: 1.630780\tGrad Norm: 1.544165\tLR: 0.030000\n",
      "Train Epoch: 17 [65536/194182 (33%)]\tLoss: 1.622341\tGrad Norm: 1.254853\tLR: 0.030000\n",
      "Train Epoch: 17 [86016/194182 (44%)]\tLoss: 1.595099\tGrad Norm: 1.337650\tLR: 0.030000\n",
      "Train Epoch: 17 [106496/194182 (54%)]\tLoss: 1.618076\tGrad Norm: 1.489051\tLR: 0.030000\n",
      "Train Epoch: 17 [126976/194182 (65%)]\tLoss: 1.603402\tGrad Norm: 0.783049\tLR: 0.030000\n",
      "Train Epoch: 17 [147456/194182 (75%)]\tLoss: 1.619945\tGrad Norm: 1.358461\tLR: 0.030000\n",
      "Train Epoch: 17 [167936/194182 (85%)]\tLoss: 1.612710\tGrad Norm: 1.330782\tLR: 0.030000\n",
      "Train Epoch: 17 [188416/194182 (96%)]\tLoss: 1.608571\tGrad Norm: 1.696753\tLR: 0.030000\n",
      "Train set: Average loss: 1.6109\n",
      "Test set: Average loss: 1.1184, Average MAE: 0.8414\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 18 [4096/194182 (2%)]\tLoss: 1.599106\tGrad Norm: 1.200438\tLR: 0.030000\n",
      "Train Epoch: 18 [24576/194182 (12%)]\tLoss: 1.594706\tGrad Norm: 1.413844\tLR: 0.030000\n",
      "Train Epoch: 18 [45056/194182 (23%)]\tLoss: 1.596272\tGrad Norm: 1.194833\tLR: 0.030000\n",
      "Train Epoch: 18 [65536/194182 (33%)]\tLoss: 1.592269\tGrad Norm: 1.144782\tLR: 0.030000\n",
      "Train Epoch: 18 [86016/194182 (44%)]\tLoss: 1.606456\tGrad Norm: 1.257104\tLR: 0.030000\n",
      "Train Epoch: 18 [106496/194182 (54%)]\tLoss: 1.588308\tGrad Norm: 1.073549\tLR: 0.030000\n",
      "Train Epoch: 18 [126976/194182 (65%)]\tLoss: 1.563889\tGrad Norm: 1.810737\tLR: 0.030000\n",
      "Train Epoch: 18 [147456/194182 (75%)]\tLoss: 1.576861\tGrad Norm: 0.913534\tLR: 0.030000\n",
      "Train Epoch: 18 [167936/194182 (85%)]\tLoss: 1.581798\tGrad Norm: 0.994231\tLR: 0.030000\n",
      "Train Epoch: 18 [188416/194182 (96%)]\tLoss: 1.609253\tGrad Norm: 1.067878\tLR: 0.030000\n",
      "Train set: Average loss: 1.5916\n",
      "Test set: Average loss: 1.1134, Average MAE: 0.8354\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 19 [4096/194182 (2%)]\tLoss: 1.595600\tGrad Norm: 1.624957\tLR: 0.030000\n",
      "Train Epoch: 19 [24576/194182 (12%)]\tLoss: 1.587595\tGrad Norm: 1.395742\tLR: 0.030000\n",
      "Train Epoch: 19 [45056/194182 (23%)]\tLoss: 1.576834\tGrad Norm: 1.099458\tLR: 0.030000\n",
      "Train Epoch: 19 [65536/194182 (33%)]\tLoss: 1.572612\tGrad Norm: 1.355643\tLR: 0.030000\n",
      "Train Epoch: 19 [86016/194182 (44%)]\tLoss: 1.560934\tGrad Norm: 1.564311\tLR: 0.030000\n",
      "Train Epoch: 19 [106496/194182 (54%)]\tLoss: 1.576539\tGrad Norm: 1.695529\tLR: 0.030000\n",
      "Train Epoch: 19 [126976/194182 (65%)]\tLoss: 1.571872\tGrad Norm: 1.517788\tLR: 0.030000\n",
      "Train Epoch: 19 [147456/194182 (75%)]\tLoss: 1.556152\tGrad Norm: 0.866383\tLR: 0.030000\n",
      "Train Epoch: 19 [167936/194182 (85%)]\tLoss: 1.585046\tGrad Norm: 1.721351\tLR: 0.030000\n",
      "Train Epoch: 19 [188416/194182 (96%)]\tLoss: 1.556311\tGrad Norm: 1.275600\tLR: 0.030000\n",
      "Train set: Average loss: 1.5730\n",
      "Test set: Average loss: 1.0803, Average MAE: 0.8236\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 20 [4096/194182 (2%)]\tLoss: 1.568890\tGrad Norm: 0.801603\tLR: 0.030000\n",
      "Train Epoch: 20 [24576/194182 (12%)]\tLoss: 1.566261\tGrad Norm: 1.466285\tLR: 0.030000\n",
      "Train Epoch: 20 [45056/194182 (23%)]\tLoss: 1.551604\tGrad Norm: 1.630287\tLR: 0.030000\n",
      "Train Epoch: 20 [65536/194182 (33%)]\tLoss: 1.557007\tGrad Norm: 1.118981\tLR: 0.030000\n",
      "Train Epoch: 20 [86016/194182 (44%)]\tLoss: 1.560694\tGrad Norm: 1.066295\tLR: 0.030000\n",
      "Train Epoch: 20 [106496/194182 (54%)]\tLoss: 1.550813\tGrad Norm: 1.600184\tLR: 0.030000\n",
      "Train Epoch: 20 [126976/194182 (65%)]\tLoss: 1.556067\tGrad Norm: 0.947212\tLR: 0.030000\n",
      "Train Epoch: 20 [147456/194182 (75%)]\tLoss: 1.567476\tGrad Norm: 1.597044\tLR: 0.030000\n",
      "Train Epoch: 20 [167936/194182 (85%)]\tLoss: 1.557293\tGrad Norm: 1.486585\tLR: 0.030000\n",
      "Train Epoch: 20 [188416/194182 (96%)]\tLoss: 1.558294\tGrad Norm: 1.164368\tLR: 0.030000\n",
      "Train set: Average loss: 1.5563\n",
      "Test set: Average loss: 1.0749, Average MAE: 0.8241\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 20: Mean reward = 0.043 +/- 0.001\n",
      "Train Epoch: 21 [4096/194182 (2%)]\tLoss: 1.546397\tGrad Norm: 1.328723\tLR: 0.030000\n",
      "Train Epoch: 21 [24576/194182 (12%)]\tLoss: 1.546896\tGrad Norm: 0.936624\tLR: 0.030000\n",
      "Train Epoch: 21 [45056/194182 (23%)]\tLoss: 1.547463\tGrad Norm: 0.862381\tLR: 0.030000\n",
      "Train Epoch: 21 [65536/194182 (33%)]\tLoss: 1.546284\tGrad Norm: 1.679743\tLR: 0.030000\n",
      "Train Epoch: 21 [86016/194182 (44%)]\tLoss: 1.540343\tGrad Norm: 1.008091\tLR: 0.030000\n",
      "Train Epoch: 21 [106496/194182 (54%)]\tLoss: 1.527567\tGrad Norm: 1.053728\tLR: 0.030000\n",
      "Train Epoch: 21 [126976/194182 (65%)]\tLoss: 1.535065\tGrad Norm: 1.580203\tLR: 0.030000\n",
      "Train Epoch: 21 [147456/194182 (75%)]\tLoss: 1.531302\tGrad Norm: 1.374754\tLR: 0.030000\n",
      "Train Epoch: 21 [167936/194182 (85%)]\tLoss: 1.530690\tGrad Norm: 1.059965\tLR: 0.030000\n",
      "Train Epoch: 21 [188416/194182 (96%)]\tLoss: 1.531970\tGrad Norm: 1.433944\tLR: 0.030000\n",
      "Train set: Average loss: 1.5364\n",
      "Test set: Average loss: 1.0522, Average MAE: 0.8109\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 22 [4096/194182 (2%)]\tLoss: 1.518008\tGrad Norm: 1.121257\tLR: 0.030000\n",
      "Train Epoch: 22 [24576/194182 (12%)]\tLoss: 1.520778\tGrad Norm: 1.252303\tLR: 0.030000\n",
      "Train Epoch: 22 [45056/194182 (23%)]\tLoss: 1.543401\tGrad Norm: 1.463835\tLR: 0.030000\n",
      "Train Epoch: 22 [65536/194182 (33%)]\tLoss: 1.520769\tGrad Norm: 1.232132\tLR: 0.030000\n",
      "Train Epoch: 22 [86016/194182 (44%)]\tLoss: 1.520644\tGrad Norm: 1.341631\tLR: 0.030000\n",
      "Train Epoch: 22 [106496/194182 (54%)]\tLoss: 1.517465\tGrad Norm: 1.550948\tLR: 0.030000\n",
      "Train Epoch: 22 [126976/194182 (65%)]\tLoss: 1.507159\tGrad Norm: 0.967421\tLR: 0.030000\n",
      "Train Epoch: 22 [147456/194182 (75%)]\tLoss: 1.503321\tGrad Norm: 1.176124\tLR: 0.030000\n",
      "Train Epoch: 22 [167936/194182 (85%)]\tLoss: 1.515258\tGrad Norm: 1.531869\tLR: 0.030000\n",
      "Train Epoch: 22 [188416/194182 (96%)]\tLoss: 1.511691\tGrad Norm: 1.429775\tLR: 0.030000\n",
      "Train set: Average loss: 1.5193\n",
      "Test set: Average loss: 1.0318, Average MAE: 0.8038\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 23 [4096/194182 (2%)]\tLoss: 1.507393\tGrad Norm: 1.186782\tLR: 0.030000\n",
      "Train Epoch: 23 [24576/194182 (12%)]\tLoss: 1.507977\tGrad Norm: 1.014796\tLR: 0.030000\n",
      "Train Epoch: 23 [45056/194182 (23%)]\tLoss: 1.506118\tGrad Norm: 1.154012\tLR: 0.030000\n",
      "Train Epoch: 23 [65536/194182 (33%)]\tLoss: 1.510282\tGrad Norm: 1.403670\tLR: 0.030000\n",
      "Train Epoch: 23 [86016/194182 (44%)]\tLoss: 1.472789\tGrad Norm: 1.268046\tLR: 0.030000\n",
      "Train Epoch: 23 [106496/194182 (54%)]\tLoss: 1.471882\tGrad Norm: 1.003749\tLR: 0.030000\n",
      "Train Epoch: 23 [126976/194182 (65%)]\tLoss: 1.520461\tGrad Norm: 1.532798\tLR: 0.030000\n",
      "Train Epoch: 23 [147456/194182 (75%)]\tLoss: 1.503271\tGrad Norm: 1.627101\tLR: 0.030000\n",
      "Train Epoch: 23 [167936/194182 (85%)]\tLoss: 1.487237\tGrad Norm: 1.043986\tLR: 0.030000\n",
      "Train Epoch: 23 [188416/194182 (96%)]\tLoss: 1.495439\tGrad Norm: 1.220640\tLR: 0.030000\n",
      "Train set: Average loss: 1.5006\n",
      "Test set: Average loss: 1.0256, Average MAE: 0.8072\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 24 [4096/194182 (2%)]\tLoss: 1.502762\tGrad Norm: 1.808386\tLR: 0.030000\n",
      "Train Epoch: 24 [24576/194182 (12%)]\tLoss: 1.475258\tGrad Norm: 1.009377\tLR: 0.030000\n",
      "Train Epoch: 24 [45056/194182 (23%)]\tLoss: 1.466643\tGrad Norm: 0.863948\tLR: 0.030000\n",
      "Train Epoch: 24 [65536/194182 (33%)]\tLoss: 1.467839\tGrad Norm: 1.043084\tLR: 0.030000\n",
      "Train Epoch: 24 [86016/194182 (44%)]\tLoss: 1.473158\tGrad Norm: 1.270177\tLR: 0.030000\n",
      "Train Epoch: 24 [106496/194182 (54%)]\tLoss: 1.479217\tGrad Norm: 1.564706\tLR: 0.030000\n",
      "Train Epoch: 24 [126976/194182 (65%)]\tLoss: 1.482148\tGrad Norm: 1.524027\tLR: 0.030000\n",
      "Train Epoch: 24 [147456/194182 (75%)]\tLoss: 1.476219\tGrad Norm: 1.019298\tLR: 0.030000\n",
      "Train Epoch: 24 [167936/194182 (85%)]\tLoss: 1.494596\tGrad Norm: 1.156012\tLR: 0.030000\n",
      "Train Epoch: 24 [188416/194182 (96%)]\tLoss: 1.493155\tGrad Norm: 1.321262\tLR: 0.030000\n",
      "Train set: Average loss: 1.4811\n",
      "Test set: Average loss: 1.0039, Average MAE: 0.7901\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 25 [4096/194182 (2%)]\tLoss: 1.476082\tGrad Norm: 1.329284\tLR: 0.030000\n",
      "Train Epoch: 25 [24576/194182 (12%)]\tLoss: 1.469139\tGrad Norm: 1.440451\tLR: 0.030000\n",
      "Train Epoch: 25 [45056/194182 (23%)]\tLoss: 1.481045\tGrad Norm: 1.431795\tLR: 0.030000\n",
      "Train Epoch: 25 [65536/194182 (33%)]\tLoss: 1.449175\tGrad Norm: 1.429094\tLR: 0.030000\n",
      "Train Epoch: 25 [86016/194182 (44%)]\tLoss: 1.468305\tGrad Norm: 0.940955\tLR: 0.030000\n",
      "Train Epoch: 25 [106496/194182 (54%)]\tLoss: 1.459509\tGrad Norm: 1.089631\tLR: 0.030000\n",
      "Train Epoch: 25 [126976/194182 (65%)]\tLoss: 1.491668\tGrad Norm: 1.662047\tLR: 0.030000\n",
      "Train Epoch: 25 [147456/194182 (75%)]\tLoss: 1.466751\tGrad Norm: 1.452402\tLR: 0.030000\n",
      "Train Epoch: 25 [167936/194182 (85%)]\tLoss: 1.460623\tGrad Norm: 1.140425\tLR: 0.030000\n",
      "Train Epoch: 25 [188416/194182 (96%)]\tLoss: 1.435078\tGrad Norm: 0.969307\tLR: 0.030000\n",
      "Train set: Average loss: 1.4654\n",
      "Test set: Average loss: 0.9758, Average MAE: 0.7820\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 25: Mean reward = 0.043 +/- 0.001\n",
      "Train Epoch: 26 [4096/194182 (2%)]\tLoss: 1.446514\tGrad Norm: 0.732377\tLR: 0.030000\n",
      "Train Epoch: 26 [24576/194182 (12%)]\tLoss: 1.438403\tGrad Norm: 1.147559\tLR: 0.030000\n",
      "Train Epoch: 26 [45056/194182 (23%)]\tLoss: 1.462702\tGrad Norm: 1.523795\tLR: 0.030000\n",
      "Train Epoch: 26 [65536/194182 (33%)]\tLoss: 1.429746\tGrad Norm: 1.293061\tLR: 0.030000\n",
      "Train Epoch: 26 [86016/194182 (44%)]\tLoss: 1.447977\tGrad Norm: 1.247669\tLR: 0.030000\n",
      "Train Epoch: 26 [106496/194182 (54%)]\tLoss: 1.437189\tGrad Norm: 0.874997\tLR: 0.030000\n",
      "Train Epoch: 26 [126976/194182 (65%)]\tLoss: 1.441186\tGrad Norm: 1.158535\tLR: 0.030000\n",
      "Train Epoch: 26 [147456/194182 (75%)]\tLoss: 1.440877\tGrad Norm: 1.395444\tLR: 0.030000\n",
      "Train Epoch: 26 [167936/194182 (85%)]\tLoss: 1.448992\tGrad Norm: 1.452596\tLR: 0.030000\n",
      "Train Epoch: 26 [188416/194182 (96%)]\tLoss: 1.446369\tGrad Norm: 1.090150\tLR: 0.030000\n",
      "Train set: Average loss: 1.4469\n",
      "Test set: Average loss: 0.9702, Average MAE: 0.7748\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 27 [4096/194182 (2%)]\tLoss: 1.430038\tGrad Norm: 1.204509\tLR: 0.030000\n",
      "Train Epoch: 27 [24576/194182 (12%)]\tLoss: 1.436361\tGrad Norm: 1.122005\tLR: 0.030000\n",
      "Train Epoch: 27 [45056/194182 (23%)]\tLoss: 1.447574\tGrad Norm: 1.551236\tLR: 0.030000\n",
      "Train Epoch: 27 [65536/194182 (33%)]\tLoss: 1.447916\tGrad Norm: 1.547831\tLR: 0.030000\n",
      "Train Epoch: 27 [86016/194182 (44%)]\tLoss: 1.432457\tGrad Norm: 1.609146\tLR: 0.030000\n",
      "Train Epoch: 27 [106496/194182 (54%)]\tLoss: 1.423295\tGrad Norm: 1.322955\tLR: 0.030000\n",
      "Train Epoch: 27 [126976/194182 (65%)]\tLoss: 1.434351\tGrad Norm: 1.178326\tLR: 0.030000\n",
      "Train Epoch: 27 [147456/194182 (75%)]\tLoss: 1.421330\tGrad Norm: 0.908958\tLR: 0.030000\n",
      "Train Epoch: 27 [167936/194182 (85%)]\tLoss: 1.418416\tGrad Norm: 1.132061\tLR: 0.030000\n",
      "Train Epoch: 27 [188416/194182 (96%)]\tLoss: 1.423482\tGrad Norm: 1.557429\tLR: 0.030000\n",
      "Train set: Average loss: 1.4321\n",
      "Test set: Average loss: 0.9602, Average MAE: 0.7707\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 28 [4096/194182 (2%)]\tLoss: 1.433476\tGrad Norm: 1.848323\tLR: 0.030000\n",
      "Train Epoch: 28 [24576/194182 (12%)]\tLoss: 1.428272\tGrad Norm: 1.550612\tLR: 0.030000\n",
      "Train Epoch: 28 [45056/194182 (23%)]\tLoss: 1.411419\tGrad Norm: 1.072455\tLR: 0.030000\n",
      "Train Epoch: 28 [65536/194182 (33%)]\tLoss: 1.418529\tGrad Norm: 1.259532\tLR: 0.030000\n",
      "Train Epoch: 28 [86016/194182 (44%)]\tLoss: 1.409861\tGrad Norm: 1.490780\tLR: 0.030000\n",
      "Train Epoch: 28 [106496/194182 (54%)]\tLoss: 1.417074\tGrad Norm: 1.523528\tLR: 0.030000\n",
      "Train Epoch: 28 [126976/194182 (65%)]\tLoss: 1.421170\tGrad Norm: 1.500816\tLR: 0.030000\n",
      "Train Epoch: 28 [147456/194182 (75%)]\tLoss: 1.423617\tGrad Norm: 1.072055\tLR: 0.030000\n",
      "Train Epoch: 28 [167936/194182 (85%)]\tLoss: 1.409580\tGrad Norm: 1.271783\tLR: 0.030000\n",
      "Train Epoch: 28 [188416/194182 (96%)]\tLoss: 1.421586\tGrad Norm: 1.718394\tLR: 0.030000\n",
      "Train set: Average loss: 1.4170\n",
      "Test set: Average loss: 0.9395, Average MAE: 0.7699\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 29 [4096/194182 (2%)]\tLoss: 1.390088\tGrad Norm: 1.513029\tLR: 0.030000\n",
      "Train Epoch: 29 [24576/194182 (12%)]\tLoss: 1.426637\tGrad Norm: 1.149629\tLR: 0.030000\n",
      "Train Epoch: 29 [45056/194182 (23%)]\tLoss: 1.392564\tGrad Norm: 1.222206\tLR: 0.030000\n",
      "Train Epoch: 29 [65536/194182 (33%)]\tLoss: 1.412532\tGrad Norm: 1.803755\tLR: 0.030000\n",
      "Train Epoch: 29 [86016/194182 (44%)]\tLoss: 1.396265\tGrad Norm: 1.210225\tLR: 0.030000\n",
      "Train Epoch: 29 [106496/194182 (54%)]\tLoss: 1.407093\tGrad Norm: 1.210486\tLR: 0.030000\n",
      "Train Epoch: 29 [126976/194182 (65%)]\tLoss: 1.395113\tGrad Norm: 1.106016\tLR: 0.030000\n",
      "Train Epoch: 29 [147456/194182 (75%)]\tLoss: 1.408201\tGrad Norm: 1.245052\tLR: 0.030000\n",
      "Train Epoch: 29 [167936/194182 (85%)]\tLoss: 1.394693\tGrad Norm: 1.127823\tLR: 0.030000\n",
      "Train Epoch: 29 [188416/194182 (96%)]\tLoss: 1.393353\tGrad Norm: 1.109938\tLR: 0.030000\n",
      "Train set: Average loss: 1.3991\n",
      "Test set: Average loss: 0.9183, Average MAE: 0.7566\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 30 [4096/194182 (2%)]\tLoss: 1.390696\tGrad Norm: 1.352880\tLR: 0.030000\n",
      "Train Epoch: 30 [24576/194182 (12%)]\tLoss: 1.387274\tGrad Norm: 1.317200\tLR: 0.030000\n",
      "Train Epoch: 30 [45056/194182 (23%)]\tLoss: 1.368930\tGrad Norm: 1.060606\tLR: 0.030000\n",
      "Train Epoch: 30 [65536/194182 (33%)]\tLoss: 1.382008\tGrad Norm: 1.066093\tLR: 0.030000\n",
      "Train Epoch: 30 [86016/194182 (44%)]\tLoss: 1.398018\tGrad Norm: 1.069747\tLR: 0.030000\n",
      "Train Epoch: 30 [106496/194182 (54%)]\tLoss: 1.397152\tGrad Norm: 2.013424\tLR: 0.030000\n",
      "Train Epoch: 30 [126976/194182 (65%)]\tLoss: 1.366405\tGrad Norm: 1.284811\tLR: 0.030000\n",
      "Train Epoch: 30 [147456/194182 (75%)]\tLoss: 1.379778\tGrad Norm: 1.095376\tLR: 0.030000\n",
      "Train Epoch: 30 [167936/194182 (85%)]\tLoss: 1.370681\tGrad Norm: 1.118490\tLR: 0.030000\n",
      "Train Epoch: 30 [188416/194182 (96%)]\tLoss: 1.381559\tGrad Norm: 1.712429\tLR: 0.030000\n",
      "Train set: Average loss: 1.3827\n",
      "Test set: Average loss: 0.9093, Average MAE: 0.7552\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 30: Mean reward = 0.044 +/- 0.001\n",
      "Train Epoch: 31 [4096/194182 (2%)]\tLoss: 1.364948\tGrad Norm: 1.335411\tLR: 0.030000\n",
      "Train Epoch: 31 [24576/194182 (12%)]\tLoss: 1.374818\tGrad Norm: 1.175494\tLR: 0.030000\n",
      "Train Epoch: 31 [45056/194182 (23%)]\tLoss: 1.388799\tGrad Norm: 1.295532\tLR: 0.030000\n",
      "Train Epoch: 31 [65536/194182 (33%)]\tLoss: 1.367289\tGrad Norm: 0.749495\tLR: 0.030000\n",
      "Train Epoch: 31 [86016/194182 (44%)]\tLoss: 1.360739\tGrad Norm: 0.661356\tLR: 0.030000\n",
      "Train Epoch: 31 [106496/194182 (54%)]\tLoss: 1.347510\tGrad Norm: 0.887492\tLR: 0.030000\n",
      "Train Epoch: 31 [126976/194182 (65%)]\tLoss: 1.380842\tGrad Norm: 1.700806\tLR: 0.030000\n",
      "Train Epoch: 31 [147456/194182 (75%)]\tLoss: 1.372768\tGrad Norm: 1.953976\tLR: 0.030000\n",
      "Train Epoch: 31 [167936/194182 (85%)]\tLoss: 1.362517\tGrad Norm: 1.389140\tLR: 0.030000\n",
      "Train Epoch: 31 [188416/194182 (96%)]\tLoss: 1.366880\tGrad Norm: 1.421913\tLR: 0.030000\n",
      "Train set: Average loss: 1.3684\n",
      "Test set: Average loss: 0.8966, Average MAE: 0.7474\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 32 [4096/194182 (2%)]\tLoss: 1.361191\tGrad Norm: 1.446131\tLR: 0.030000\n",
      "Train Epoch: 32 [24576/194182 (12%)]\tLoss: 1.337314\tGrad Norm: 0.944238\tLR: 0.030000\n",
      "Train Epoch: 32 [45056/194182 (23%)]\tLoss: 1.359672\tGrad Norm: 1.852389\tLR: 0.030000\n",
      "Train Epoch: 32 [65536/194182 (33%)]\tLoss: 1.349111\tGrad Norm: 1.035112\tLR: 0.030000\n",
      "Train Epoch: 32 [86016/194182 (44%)]\tLoss: 1.337215\tGrad Norm: 1.329842\tLR: 0.030000\n",
      "Train Epoch: 32 [106496/194182 (54%)]\tLoss: 1.343796\tGrad Norm: 1.031656\tLR: 0.030000\n",
      "Train Epoch: 32 [126976/194182 (65%)]\tLoss: 1.345468\tGrad Norm: 1.391475\tLR: 0.030000\n",
      "Train Epoch: 32 [147456/194182 (75%)]\tLoss: 1.343596\tGrad Norm: 1.050523\tLR: 0.030000\n",
      "Train Epoch: 32 [167936/194182 (85%)]\tLoss: 1.349170\tGrad Norm: 1.572248\tLR: 0.030000\n",
      "Train Epoch: 32 [188416/194182 (96%)]\tLoss: 1.343314\tGrad Norm: 1.548349\tLR: 0.030000\n",
      "Train set: Average loss: 1.3547\n",
      "Test set: Average loss: 0.8873, Average MAE: 0.7504\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 33 [4096/194182 (2%)]\tLoss: 1.355759\tGrad Norm: 1.800891\tLR: 0.030000\n",
      "Train Epoch: 33 [24576/194182 (12%)]\tLoss: 1.343408\tGrad Norm: 1.019860\tLR: 0.030000\n",
      "Train Epoch: 33 [45056/194182 (23%)]\tLoss: 1.349351\tGrad Norm: 0.963092\tLR: 0.030000\n",
      "Train Epoch: 33 [65536/194182 (33%)]\tLoss: 1.347152\tGrad Norm: 1.078660\tLR: 0.030000\n",
      "Train Epoch: 33 [86016/194182 (44%)]\tLoss: 1.331383\tGrad Norm: 1.191792\tLR: 0.030000\n",
      "Train Epoch: 33 [106496/194182 (54%)]\tLoss: 1.320762\tGrad Norm: 1.239203\tLR: 0.030000\n",
      "Train Epoch: 33 [126976/194182 (65%)]\tLoss: 1.334441\tGrad Norm: 1.103660\tLR: 0.030000\n",
      "Train Epoch: 33 [147456/194182 (75%)]\tLoss: 1.328265\tGrad Norm: 1.283683\tLR: 0.030000\n",
      "Train Epoch: 33 [167936/194182 (85%)]\tLoss: 1.348369\tGrad Norm: 1.640423\tLR: 0.030000\n",
      "Train Epoch: 33 [188416/194182 (96%)]\tLoss: 1.344239\tGrad Norm: 0.987986\tLR: 0.030000\n",
      "Train set: Average loss: 1.3369\n",
      "Test set: Average loss: 0.8680, Average MAE: 0.7312\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 34 [4096/194182 (2%)]\tLoss: 1.322177\tGrad Norm: 1.233944\tLR: 0.030000\n",
      "Train Epoch: 34 [24576/194182 (12%)]\tLoss: 1.330774\tGrad Norm: 1.185369\tLR: 0.030000\n",
      "Train Epoch: 34 [45056/194182 (23%)]\tLoss: 1.319257\tGrad Norm: 1.007654\tLR: 0.030000\n",
      "Train Epoch: 34 [65536/194182 (33%)]\tLoss: 1.333477\tGrad Norm: 2.206660\tLR: 0.030000\n",
      "Train Epoch: 34 [86016/194182 (44%)]\tLoss: 1.323764\tGrad Norm: 1.012909\tLR: 0.030000\n",
      "Train Epoch: 34 [106496/194182 (54%)]\tLoss: 1.328480\tGrad Norm: 1.654690\tLR: 0.030000\n",
      "Train Epoch: 34 [126976/194182 (65%)]\tLoss: 1.328035\tGrad Norm: 1.583238\tLR: 0.030000\n",
      "Train Epoch: 34 [147456/194182 (75%)]\tLoss: 1.306439\tGrad Norm: 1.214244\tLR: 0.030000\n",
      "Train Epoch: 34 [167936/194182 (85%)]\tLoss: 1.305468\tGrad Norm: 0.906074\tLR: 0.030000\n",
      "Train Epoch: 34 [188416/194182 (96%)]\tLoss: 1.311876\tGrad Norm: 1.351807\tLR: 0.030000\n",
      "Train set: Average loss: 1.3219\n",
      "Test set: Average loss: 0.8552, Average MAE: 0.7242\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 35 [4096/194182 (2%)]\tLoss: 1.312865\tGrad Norm: 1.742558\tLR: 0.030000\n",
      "Train Epoch: 35 [24576/194182 (12%)]\tLoss: 1.303590\tGrad Norm: 1.350436\tLR: 0.030000\n",
      "Train Epoch: 35 [45056/194182 (23%)]\tLoss: 1.304387\tGrad Norm: 1.040726\tLR: 0.030000\n",
      "Train Epoch: 35 [65536/194182 (33%)]\tLoss: 1.311654\tGrad Norm: 1.148566\tLR: 0.030000\n",
      "Train Epoch: 35 [86016/194182 (44%)]\tLoss: 1.286447\tGrad Norm: 0.918878\tLR: 0.030000\n",
      "Train Epoch: 35 [106496/194182 (54%)]\tLoss: 1.305425\tGrad Norm: 1.236897\tLR: 0.030000\n",
      "Train Epoch: 35 [126976/194182 (65%)]\tLoss: 1.312171\tGrad Norm: 0.996022\tLR: 0.030000\n",
      "Train Epoch: 35 [147456/194182 (75%)]\tLoss: 1.323533\tGrad Norm: 1.942285\tLR: 0.030000\n",
      "Train Epoch: 35 [167936/194182 (85%)]\tLoss: 1.308681\tGrad Norm: 1.598970\tLR: 0.030000\n",
      "Train Epoch: 35 [188416/194182 (96%)]\tLoss: 1.313369\tGrad Norm: 1.397489\tLR: 0.030000\n",
      "Train set: Average loss: 1.3113\n",
      "Test set: Average loss: 0.8382, Average MAE: 0.7292\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 35: Mean reward = 0.044 +/- 0.001\n",
      "Train Epoch: 36 [4096/194182 (2%)]\tLoss: 1.302682\tGrad Norm: 1.413703\tLR: 0.030000\n",
      "Train Epoch: 36 [24576/194182 (12%)]\tLoss: 1.291533\tGrad Norm: 0.785613\tLR: 0.030000\n",
      "Train Epoch: 36 [45056/194182 (23%)]\tLoss: 1.284360\tGrad Norm: 1.044609\tLR: 0.030000\n",
      "Train Epoch: 36 [65536/194182 (33%)]\tLoss: 1.313503\tGrad Norm: 1.656486\tLR: 0.030000\n",
      "Train Epoch: 36 [86016/194182 (44%)]\tLoss: 1.290454\tGrad Norm: 1.293113\tLR: 0.030000\n",
      "Train Epoch: 36 [106496/194182 (54%)]\tLoss: 1.283767\tGrad Norm: 0.952472\tLR: 0.030000\n",
      "Train Epoch: 36 [126976/194182 (65%)]\tLoss: 1.307513\tGrad Norm: 1.532184\tLR: 0.030000\n",
      "Train Epoch: 36 [147456/194182 (75%)]\tLoss: 1.299120\tGrad Norm: 1.058386\tLR: 0.030000\n",
      "Train Epoch: 36 [167936/194182 (85%)]\tLoss: 1.301726\tGrad Norm: 1.813635\tLR: 0.030000\n",
      "Train Epoch: 36 [188416/194182 (96%)]\tLoss: 1.282803\tGrad Norm: 1.302495\tLR: 0.030000\n",
      "Train set: Average loss: 1.2970\n",
      "Test set: Average loss: 0.8187, Average MAE: 0.7142\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 37 [4096/194182 (2%)]\tLoss: 1.269584\tGrad Norm: 0.613639\tLR: 0.030000\n",
      "Train Epoch: 37 [24576/194182 (12%)]\tLoss: 1.295418\tGrad Norm: 1.347798\tLR: 0.030000\n",
      "Train Epoch: 37 [45056/194182 (23%)]\tLoss: 1.295749\tGrad Norm: 1.531930\tLR: 0.030000\n",
      "Train Epoch: 37 [65536/194182 (33%)]\tLoss: 1.275250\tGrad Norm: 1.560358\tLR: 0.030000\n",
      "Train Epoch: 37 [86016/194182 (44%)]\tLoss: 1.291059\tGrad Norm: 1.148575\tLR: 0.030000\n",
      "Train Epoch: 37 [106496/194182 (54%)]\tLoss: 1.283654\tGrad Norm: 1.623722\tLR: 0.030000\n",
      "Train Epoch: 37 [126976/194182 (65%)]\tLoss: 1.272689\tGrad Norm: 1.357960\tLR: 0.030000\n",
      "Train Epoch: 37 [147456/194182 (75%)]\tLoss: 1.280983\tGrad Norm: 1.379155\tLR: 0.030000\n",
      "Train Epoch: 37 [167936/194182 (85%)]\tLoss: 1.274791\tGrad Norm: 1.531839\tLR: 0.030000\n",
      "Train Epoch: 37 [188416/194182 (96%)]\tLoss: 1.270173\tGrad Norm: 1.592551\tLR: 0.030000\n",
      "Train set: Average loss: 1.2823\n",
      "Test set: Average loss: 0.8082, Average MAE: 0.7090\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 38 [4096/194182 (2%)]\tLoss: 1.266975\tGrad Norm: 1.053970\tLR: 0.030000\n",
      "Train Epoch: 38 [24576/194182 (12%)]\tLoss: 1.290028\tGrad Norm: 0.873317\tLR: 0.030000\n",
      "Train Epoch: 38 [45056/194182 (23%)]\tLoss: 1.256945\tGrad Norm: 1.308703\tLR: 0.030000\n",
      "Train Epoch: 38 [65536/194182 (33%)]\tLoss: 1.262430\tGrad Norm: 1.671755\tLR: 0.030000\n",
      "Train Epoch: 38 [86016/194182 (44%)]\tLoss: 1.276814\tGrad Norm: 1.520187\tLR: 0.030000\n",
      "Train Epoch: 38 [106496/194182 (54%)]\tLoss: 1.269103\tGrad Norm: 1.399757\tLR: 0.030000\n",
      "Train Epoch: 38 [126976/194182 (65%)]\tLoss: 1.282363\tGrad Norm: 1.223340\tLR: 0.030000\n",
      "Train Epoch: 38 [147456/194182 (75%)]\tLoss: 1.248584\tGrad Norm: 0.955355\tLR: 0.030000\n",
      "Train Epoch: 38 [167936/194182 (85%)]\tLoss: 1.260775\tGrad Norm: 0.908296\tLR: 0.030000\n",
      "Train Epoch: 38 [188416/194182 (96%)]\tLoss: 1.269029\tGrad Norm: 0.937587\tLR: 0.030000\n",
      "Train set: Average loss: 1.2686\n",
      "Test set: Average loss: 0.8002, Average MAE: 0.7076\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 39 [4096/194182 (2%)]\tLoss: 1.255647\tGrad Norm: 1.229260\tLR: 0.030000\n",
      "Train Epoch: 39 [24576/194182 (12%)]\tLoss: 1.273682\tGrad Norm: 2.119580\tLR: 0.030000\n",
      "Train Epoch: 39 [45056/194182 (23%)]\tLoss: 1.269989\tGrad Norm: 1.632840\tLR: 0.030000\n",
      "Train Epoch: 39 [65536/194182 (33%)]\tLoss: 1.276472\tGrad Norm: 1.045522\tLR: 0.030000\n",
      "Train Epoch: 39 [86016/194182 (44%)]\tLoss: 1.246732\tGrad Norm: 0.959556\tLR: 0.030000\n",
      "Train Epoch: 39 [106496/194182 (54%)]\tLoss: 1.258293\tGrad Norm: 1.262578\tLR: 0.030000\n",
      "Train Epoch: 39 [126976/194182 (65%)]\tLoss: 1.241785\tGrad Norm: 1.063877\tLR: 0.030000\n",
      "Train Epoch: 39 [147456/194182 (75%)]\tLoss: 1.249342\tGrad Norm: 1.281041\tLR: 0.030000\n",
      "Train Epoch: 39 [167936/194182 (85%)]\tLoss: 1.260992\tGrad Norm: 1.898495\tLR: 0.030000\n",
      "Train Epoch: 39 [188416/194182 (96%)]\tLoss: 1.242411\tGrad Norm: 1.638352\tLR: 0.030000\n",
      "Train set: Average loss: 1.2553\n",
      "Test set: Average loss: 0.7948, Average MAE: 0.6980\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 40 [4096/194182 (2%)]\tLoss: 1.264913\tGrad Norm: 1.960111\tLR: 0.030000\n",
      "Train Epoch: 40 [24576/194182 (12%)]\tLoss: 1.241612\tGrad Norm: 0.965458\tLR: 0.030000\n",
      "Train Epoch: 40 [45056/194182 (23%)]\tLoss: 1.242162\tGrad Norm: 0.986198\tLR: 0.030000\n",
      "Train Epoch: 40 [65536/194182 (33%)]\tLoss: 1.237032\tGrad Norm: 1.289603\tLR: 0.030000\n",
      "Train Epoch: 40 [86016/194182 (44%)]\tLoss: 1.244170\tGrad Norm: 1.100185\tLR: 0.030000\n",
      "Train Epoch: 40 [106496/194182 (54%)]\tLoss: 1.238924\tGrad Norm: 1.181267\tLR: 0.030000\n",
      "Train Epoch: 40 [126976/194182 (65%)]\tLoss: 1.243789\tGrad Norm: 0.963025\tLR: 0.030000\n",
      "Train Epoch: 40 [147456/194182 (75%)]\tLoss: 1.246953\tGrad Norm: 1.581356\tLR: 0.030000\n",
      "Train Epoch: 40 [167936/194182 (85%)]\tLoss: 1.230465\tGrad Norm: 1.251529\tLR: 0.030000\n",
      "Train Epoch: 40 [188416/194182 (96%)]\tLoss: 1.221205\tGrad Norm: 0.910682\tLR: 0.030000\n",
      "Train set: Average loss: 1.2416\n",
      "Test set: Average loss: 0.7755, Average MAE: 0.6907\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 40: Mean reward = 0.044 +/- 0.001\n",
      "Train Epoch: 41 [4096/194182 (2%)]\tLoss: 1.235588\tGrad Norm: 1.415671\tLR: 0.030000\n",
      "Train Epoch: 41 [24576/194182 (12%)]\tLoss: 1.233951\tGrad Norm: 1.516509\tLR: 0.030000\n",
      "Train Epoch: 41 [45056/194182 (23%)]\tLoss: 1.244971\tGrad Norm: 1.657635\tLR: 0.030000\n",
      "Train Epoch: 41 [65536/194182 (33%)]\tLoss: 1.219234\tGrad Norm: 1.199410\tLR: 0.030000\n",
      "Train Epoch: 41 [86016/194182 (44%)]\tLoss: 1.245906\tGrad Norm: 1.647030\tLR: 0.030000\n",
      "Train Epoch: 41 [106496/194182 (54%)]\tLoss: 1.235229\tGrad Norm: 1.410575\tLR: 0.030000\n",
      "Train Epoch: 41 [126976/194182 (65%)]\tLoss: 1.231018\tGrad Norm: 1.179274\tLR: 0.030000\n",
      "Train Epoch: 41 [147456/194182 (75%)]\tLoss: 1.224552\tGrad Norm: 1.378055\tLR: 0.030000\n",
      "Train Epoch: 41 [167936/194182 (85%)]\tLoss: 1.213913\tGrad Norm: 1.035150\tLR: 0.030000\n",
      "Train Epoch: 41 [188416/194182 (96%)]\tLoss: 1.226079\tGrad Norm: 1.634051\tLR: 0.030000\n",
      "Train set: Average loss: 1.2309\n",
      "Test set: Average loss: 0.7616, Average MAE: 0.6859\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 42 [4096/194182 (2%)]\tLoss: 1.206831\tGrad Norm: 1.223857\tLR: 0.030000\n",
      "Train Epoch: 42 [24576/194182 (12%)]\tLoss: 1.233980\tGrad Norm: 1.976945\tLR: 0.030000\n",
      "Train Epoch: 42 [45056/194182 (23%)]\tLoss: 1.217168\tGrad Norm: 1.289697\tLR: 0.030000\n",
      "Train Epoch: 42 [65536/194182 (33%)]\tLoss: 1.217177\tGrad Norm: 1.535656\tLR: 0.030000\n",
      "Train Epoch: 42 [86016/194182 (44%)]\tLoss: 1.230298\tGrad Norm: 1.254556\tLR: 0.030000\n",
      "Train Epoch: 42 [106496/194182 (54%)]\tLoss: 1.199219\tGrad Norm: 1.102101\tLR: 0.030000\n",
      "Train Epoch: 42 [126976/194182 (65%)]\tLoss: 1.218462\tGrad Norm: 1.387165\tLR: 0.030000\n",
      "Train Epoch: 42 [147456/194182 (75%)]\tLoss: 1.224479\tGrad Norm: 1.450446\tLR: 0.030000\n",
      "Train Epoch: 42 [167936/194182 (85%)]\tLoss: 1.207927\tGrad Norm: 1.450664\tLR: 0.030000\n",
      "Train Epoch: 42 [188416/194182 (96%)]\tLoss: 1.208744\tGrad Norm: 1.076680\tLR: 0.030000\n",
      "Train set: Average loss: 1.2195\n",
      "Test set: Average loss: 0.7452, Average MAE: 0.6802\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 43 [4096/194182 (2%)]\tLoss: 1.207996\tGrad Norm: 0.572921\tLR: 0.030000\n",
      "Train Epoch: 43 [24576/194182 (12%)]\tLoss: 1.205618\tGrad Norm: 1.432163\tLR: 0.030000\n",
      "Train Epoch: 43 [45056/194182 (23%)]\tLoss: 1.191405\tGrad Norm: 1.175646\tLR: 0.030000\n",
      "Train Epoch: 43 [65536/194182 (33%)]\tLoss: 1.222943\tGrad Norm: 1.326816\tLR: 0.030000\n",
      "Train Epoch: 43 [86016/194182 (44%)]\tLoss: 1.206033\tGrad Norm: 1.721885\tLR: 0.030000\n",
      "Train Epoch: 43 [106496/194182 (54%)]\tLoss: 1.196536\tGrad Norm: 1.584229\tLR: 0.030000\n",
      "Train Epoch: 43 [126976/194182 (65%)]\tLoss: 1.208966\tGrad Norm: 1.523837\tLR: 0.030000\n",
      "Train Epoch: 43 [147456/194182 (75%)]\tLoss: 1.204701\tGrad Norm: 1.002490\tLR: 0.030000\n",
      "Train Epoch: 43 [167936/194182 (85%)]\tLoss: 1.190707\tGrad Norm: 1.150639\tLR: 0.030000\n",
      "Train Epoch: 43 [188416/194182 (96%)]\tLoss: 1.193323\tGrad Norm: 1.437233\tLR: 0.030000\n",
      "Train set: Average loss: 1.2073\n",
      "Test set: Average loss: 0.7393, Average MAE: 0.6759\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 44 [4096/194182 (2%)]\tLoss: 1.221909\tGrad Norm: 1.533889\tLR: 0.030000\n",
      "Train Epoch: 44 [24576/194182 (12%)]\tLoss: 1.193935\tGrad Norm: 1.823822\tLR: 0.030000\n",
      "Train Epoch: 44 [45056/194182 (23%)]\tLoss: 1.207298\tGrad Norm: 1.450372\tLR: 0.030000\n",
      "Train Epoch: 44 [65536/194182 (33%)]\tLoss: 1.193126\tGrad Norm: 1.096136\tLR: 0.030000\n",
      "Train Epoch: 44 [86016/194182 (44%)]\tLoss: 1.183882\tGrad Norm: 1.045668\tLR: 0.030000\n",
      "Train Epoch: 44 [106496/194182 (54%)]\tLoss: 1.184706\tGrad Norm: 1.052915\tLR: 0.030000\n",
      "Train Epoch: 44 [126976/194182 (65%)]\tLoss: 1.193644\tGrad Norm: 1.720548\tLR: 0.030000\n",
      "Train Epoch: 44 [147456/194182 (75%)]\tLoss: 1.194112\tGrad Norm: 1.443699\tLR: 0.030000\n",
      "Train Epoch: 44 [167936/194182 (85%)]\tLoss: 1.181784\tGrad Norm: 1.338011\tLR: 0.030000\n",
      "Train Epoch: 44 [188416/194182 (96%)]\tLoss: 1.186352\tGrad Norm: 1.198392\tLR: 0.030000\n",
      "Train set: Average loss: 1.1945\n",
      "Test set: Average loss: 0.7247, Average MAE: 0.6728\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 45 [4096/194182 (2%)]\tLoss: 1.187335\tGrad Norm: 0.622701\tLR: 0.030000\n",
      "Train Epoch: 45 [24576/194182 (12%)]\tLoss: 1.191629\tGrad Norm: 0.768877\tLR: 0.030000\n",
      "Train Epoch: 45 [45056/194182 (23%)]\tLoss: 1.204767\tGrad Norm: 1.518831\tLR: 0.030000\n",
      "Train Epoch: 45 [65536/194182 (33%)]\tLoss: 1.182286\tGrad Norm: 1.112736\tLR: 0.030000\n",
      "Train Epoch: 45 [86016/194182 (44%)]\tLoss: 1.162223\tGrad Norm: 1.001996\tLR: 0.030000\n",
      "Train Epoch: 45 [106496/194182 (54%)]\tLoss: 1.200869\tGrad Norm: 1.972734\tLR: 0.030000\n",
      "Train Epoch: 45 [126976/194182 (65%)]\tLoss: 1.171153\tGrad Norm: 0.849090\tLR: 0.030000\n",
      "Train Epoch: 45 [147456/194182 (75%)]\tLoss: 1.180578\tGrad Norm: 0.616889\tLR: 0.030000\n",
      "Train Epoch: 45 [167936/194182 (85%)]\tLoss: 1.174617\tGrad Norm: 1.431987\tLR: 0.030000\n",
      "Train Epoch: 45 [188416/194182 (96%)]\tLoss: 1.192208\tGrad Norm: 1.763148\tLR: 0.030000\n",
      "Train set: Average loss: 1.1824\n",
      "Test set: Average loss: 0.7187, Average MAE: 0.6702\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 45: Mean reward = 0.044 +/- 0.001\n",
      "Train Epoch: 46 [4096/194182 (2%)]\tLoss: 1.184654\tGrad Norm: 1.015639\tLR: 0.030000\n",
      "Train Epoch: 46 [24576/194182 (12%)]\tLoss: 1.170399\tGrad Norm: 1.030848\tLR: 0.030000\n",
      "Train Epoch: 46 [45056/194182 (23%)]\tLoss: 1.175931\tGrad Norm: 1.728739\tLR: 0.030000\n",
      "Train Epoch: 46 [65536/194182 (33%)]\tLoss: 1.171129\tGrad Norm: 0.757948\tLR: 0.030000\n",
      "Train Epoch: 46 [86016/194182 (44%)]\tLoss: 1.180492\tGrad Norm: 0.903309\tLR: 0.030000\n",
      "Train Epoch: 46 [106496/194182 (54%)]\tLoss: 1.173378\tGrad Norm: 1.486306\tLR: 0.030000\n",
      "Train Epoch: 46 [126976/194182 (65%)]\tLoss: 1.172469\tGrad Norm: 1.344209\tLR: 0.030000\n",
      "Train Epoch: 46 [147456/194182 (75%)]\tLoss: 1.165310\tGrad Norm: 1.595320\tLR: 0.030000\n",
      "Train Epoch: 46 [167936/194182 (85%)]\tLoss: 1.177973\tGrad Norm: 1.969863\tLR: 0.030000\n",
      "Train Epoch: 46 [188416/194182 (96%)]\tLoss: 1.161377\tGrad Norm: 1.226279\tLR: 0.030000\n",
      "Train set: Average loss: 1.1710\n",
      "Test set: Average loss: 0.6994, Average MAE: 0.6592\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 47 [4096/194182 (2%)]\tLoss: 1.168475\tGrad Norm: 0.624635\tLR: 0.030000\n",
      "Train Epoch: 47 [24576/194182 (12%)]\tLoss: 1.167475\tGrad Norm: 0.378304\tLR: 0.030000\n",
      "Train Epoch: 47 [45056/194182 (23%)]\tLoss: 1.169004\tGrad Norm: 1.009786\tLR: 0.030000\n",
      "Train Epoch: 47 [65536/194182 (33%)]\tLoss: 1.173717\tGrad Norm: 1.529966\tLR: 0.030000\n",
      "Train Epoch: 47 [86016/194182 (44%)]\tLoss: 1.174009\tGrad Norm: 2.035888\tLR: 0.030000\n",
      "Train Epoch: 47 [106496/194182 (54%)]\tLoss: 1.154280\tGrad Norm: 1.262317\tLR: 0.030000\n",
      "Train Epoch: 47 [126976/194182 (65%)]\tLoss: 1.159915\tGrad Norm: 1.117488\tLR: 0.030000\n",
      "Train Epoch: 47 [147456/194182 (75%)]\tLoss: 1.168003\tGrad Norm: 1.635792\tLR: 0.030000\n",
      "Train Epoch: 47 [167936/194182 (85%)]\tLoss: 1.159580\tGrad Norm: 1.385379\tLR: 0.030000\n",
      "Train Epoch: 47 [188416/194182 (96%)]\tLoss: 1.136640\tGrad Norm: 0.997102\tLR: 0.030000\n",
      "Train set: Average loss: 1.1593\n",
      "Test set: Average loss: 0.7023, Average MAE: 0.6503\n",
      "Train Epoch: 48 [4096/194182 (2%)]\tLoss: 1.155037\tGrad Norm: 1.612537\tLR: 0.030000\n",
      "Train Epoch: 48 [24576/194182 (12%)]\tLoss: 1.148630\tGrad Norm: 1.145160\tLR: 0.030000\n",
      "Train Epoch: 48 [45056/194182 (23%)]\tLoss: 1.149906\tGrad Norm: 0.970025\tLR: 0.030000\n",
      "Train Epoch: 48 [65536/194182 (33%)]\tLoss: 1.136083\tGrad Norm: 1.301203\tLR: 0.030000\n",
      "Train Epoch: 48 [86016/194182 (44%)]\tLoss: 1.135401\tGrad Norm: 1.246269\tLR: 0.030000\n",
      "Train Epoch: 48 [106496/194182 (54%)]\tLoss: 1.139467\tGrad Norm: 0.807317\tLR: 0.030000\n",
      "Train Epoch: 48 [126976/194182 (65%)]\tLoss: 1.132703\tGrad Norm: 0.927473\tLR: 0.030000\n",
      "Train Epoch: 48 [147456/194182 (75%)]\tLoss: 1.157058\tGrad Norm: 1.011178\tLR: 0.030000\n",
      "Train Epoch: 48 [167936/194182 (85%)]\tLoss: 1.153817\tGrad Norm: 1.731799\tLR: 0.030000\n",
      "Train Epoch: 48 [188416/194182 (96%)]\tLoss: 1.148593\tGrad Norm: 1.058435\tLR: 0.030000\n",
      "Train set: Average loss: 1.1486\n",
      "Test set: Average loss: 0.6893, Average MAE: 0.6533\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 49 [4096/194182 (2%)]\tLoss: 1.144113\tGrad Norm: 1.170916\tLR: 0.030000\n",
      "Train Epoch: 49 [24576/194182 (12%)]\tLoss: 1.142959\tGrad Norm: 1.429069\tLR: 0.030000\n",
      "Train Epoch: 49 [45056/194182 (23%)]\tLoss: 1.147740\tGrad Norm: 2.100975\tLR: 0.030000\n",
      "Train Epoch: 49 [65536/194182 (33%)]\tLoss: 1.147407\tGrad Norm: 1.668010\tLR: 0.030000\n",
      "Train Epoch: 49 [86016/194182 (44%)]\tLoss: 1.145193\tGrad Norm: 1.159493\tLR: 0.030000\n",
      "Train Epoch: 49 [106496/194182 (54%)]\tLoss: 1.132212\tGrad Norm: 0.740690\tLR: 0.030000\n",
      "Train Epoch: 49 [126976/194182 (65%)]\tLoss: 1.115859\tGrad Norm: 1.265477\tLR: 0.030000\n",
      "Train Epoch: 49 [147456/194182 (75%)]\tLoss: 1.143008\tGrad Norm: 1.129748\tLR: 0.030000\n",
      "Train Epoch: 49 [167936/194182 (85%)]\tLoss: 1.129519\tGrad Norm: 1.529550\tLR: 0.030000\n",
      "Train Epoch: 49 [188416/194182 (96%)]\tLoss: 1.141070\tGrad Norm: 1.413936\tLR: 0.030000\n",
      "Train set: Average loss: 1.1391\n",
      "Test set: Average loss: 0.6855, Average MAE: 0.6570\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 50 [4096/194182 (2%)]\tLoss: 1.138067\tGrad Norm: 1.501489\tLR: 0.030000\n",
      "Train Epoch: 50 [24576/194182 (12%)]\tLoss: 1.139629\tGrad Norm: 0.892839\tLR: 0.030000\n",
      "Train Epoch: 50 [45056/194182 (23%)]\tLoss: 1.121776\tGrad Norm: 1.323240\tLR: 0.030000\n",
      "Train Epoch: 50 [65536/194182 (33%)]\tLoss: 1.134903\tGrad Norm: 1.658643\tLR: 0.030000\n",
      "Train Epoch: 50 [86016/194182 (44%)]\tLoss: 1.118440\tGrad Norm: 0.686451\tLR: 0.030000\n",
      "Train Epoch: 50 [106496/194182 (54%)]\tLoss: 1.136097\tGrad Norm: 1.670195\tLR: 0.030000\n",
      "Train Epoch: 50 [126976/194182 (65%)]\tLoss: 1.134434\tGrad Norm: 1.618194\tLR: 0.030000\n",
      "Train Epoch: 50 [147456/194182 (75%)]\tLoss: 1.131993\tGrad Norm: 1.322782\tLR: 0.030000\n",
      "Train Epoch: 50 [167936/194182 (85%)]\tLoss: 1.108963\tGrad Norm: 0.946524\tLR: 0.030000\n",
      "Train Epoch: 50 [188416/194182 (96%)]\tLoss: 1.125099\tGrad Norm: 0.789814\tLR: 0.030000\n",
      "Train set: Average loss: 1.1275\n",
      "Test set: Average loss: 0.6687, Average MAE: 0.6465\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 50: Mean reward = 0.044 +/- 0.001\n",
      "Train Epoch: 51 [4096/194182 (2%)]\tLoss: 1.115616\tGrad Norm: 1.472282\tLR: 0.030000\n",
      "Train Epoch: 51 [24576/194182 (12%)]\tLoss: 1.122170\tGrad Norm: 1.626604\tLR: 0.030000\n",
      "Train Epoch: 51 [45056/194182 (23%)]\tLoss: 1.122939\tGrad Norm: 1.160791\tLR: 0.030000\n",
      "Train Epoch: 51 [65536/194182 (33%)]\tLoss: 1.111322\tGrad Norm: 0.726443\tLR: 0.030000\n",
      "Train Epoch: 51 [86016/194182 (44%)]\tLoss: 1.119967\tGrad Norm: 1.136327\tLR: 0.030000\n",
      "Train Epoch: 51 [106496/194182 (54%)]\tLoss: 1.135980\tGrad Norm: 1.303991\tLR: 0.030000\n",
      "Train Epoch: 51 [126976/194182 (65%)]\tLoss: 1.132285\tGrad Norm: 1.551794\tLR: 0.030000\n",
      "Train Epoch: 51 [147456/194182 (75%)]\tLoss: 1.128975\tGrad Norm: 1.620430\tLR: 0.030000\n",
      "Train Epoch: 51 [167936/194182 (85%)]\tLoss: 1.118710\tGrad Norm: 1.341834\tLR: 0.030000\n",
      "Train Epoch: 51 [188416/194182 (96%)]\tLoss: 1.098169\tGrad Norm: 1.011381\tLR: 0.030000\n",
      "Train set: Average loss: 1.1172\n",
      "Test set: Average loss: 0.6593, Average MAE: 0.6387\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 52 [4096/194182 (2%)]\tLoss: 1.117888\tGrad Norm: 1.295699\tLR: 0.030000\n",
      "Train Epoch: 52 [24576/194182 (12%)]\tLoss: 1.097608\tGrad Norm: 1.615001\tLR: 0.030000\n",
      "Train Epoch: 52 [45056/194182 (23%)]\tLoss: 1.102617\tGrad Norm: 0.977594\tLR: 0.030000\n",
      "Train Epoch: 52 [65536/194182 (33%)]\tLoss: 1.105060\tGrad Norm: 1.267297\tLR: 0.030000\n",
      "Train Epoch: 52 [86016/194182 (44%)]\tLoss: 1.103204\tGrad Norm: 1.046737\tLR: 0.030000\n",
      "Train Epoch: 52 [106496/194182 (54%)]\tLoss: 1.107417\tGrad Norm: 1.320507\tLR: 0.030000\n",
      "Train Epoch: 52 [126976/194182 (65%)]\tLoss: 1.099690\tGrad Norm: 1.694932\tLR: 0.030000\n",
      "Train Epoch: 52 [147456/194182 (75%)]\tLoss: 1.118294\tGrad Norm: 1.518913\tLR: 0.030000\n",
      "Train Epoch: 52 [167936/194182 (85%)]\tLoss: 1.096086\tGrad Norm: 0.960819\tLR: 0.030000\n",
      "Train Epoch: 52 [188416/194182 (96%)]\tLoss: 1.099374\tGrad Norm: 1.316597\tLR: 0.030000\n",
      "Train set: Average loss: 1.1093\n",
      "Test set: Average loss: 0.6566, Average MAE: 0.6288\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 53 [4096/194182 (2%)]\tLoss: 1.111998\tGrad Norm: 1.717737\tLR: 0.030000\n",
      "Train Epoch: 53 [24576/194182 (12%)]\tLoss: 1.098933\tGrad Norm: 0.847269\tLR: 0.030000\n",
      "Train Epoch: 53 [45056/194182 (23%)]\tLoss: 1.098557\tGrad Norm: 1.266628\tLR: 0.030000\n",
      "Train Epoch: 53 [65536/194182 (33%)]\tLoss: 1.097219\tGrad Norm: 1.101619\tLR: 0.030000\n",
      "Train Epoch: 53 [86016/194182 (44%)]\tLoss: 1.094609\tGrad Norm: 1.481565\tLR: 0.030000\n",
      "Train Epoch: 53 [106496/194182 (54%)]\tLoss: 1.114196\tGrad Norm: 1.770172\tLR: 0.030000\n",
      "Train Epoch: 53 [126976/194182 (65%)]\tLoss: 1.084755\tGrad Norm: 1.086184\tLR: 0.030000\n",
      "Train Epoch: 53 [147456/194182 (75%)]\tLoss: 1.092928\tGrad Norm: 1.073721\tLR: 0.030000\n",
      "Train Epoch: 53 [167936/194182 (85%)]\tLoss: 1.097585\tGrad Norm: 1.405787\tLR: 0.030000\n",
      "Train Epoch: 53 [188416/194182 (96%)]\tLoss: 1.087322\tGrad Norm: 1.356395\tLR: 0.030000\n",
      "Train set: Average loss: 1.0976\n",
      "Test set: Average loss: 0.6436, Average MAE: 0.6278\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 54 [4096/194182 (2%)]\tLoss: 1.097942\tGrad Norm: 1.460117\tLR: 0.030000\n",
      "Train Epoch: 54 [24576/194182 (12%)]\tLoss: 1.094067\tGrad Norm: 1.246476\tLR: 0.030000\n",
      "Train Epoch: 54 [45056/194182 (23%)]\tLoss: 1.093438\tGrad Norm: 1.536343\tLR: 0.030000\n",
      "Train Epoch: 54 [65536/194182 (33%)]\tLoss: 1.087625\tGrad Norm: 1.640120\tLR: 0.030000\n",
      "Train Epoch: 54 [86016/194182 (44%)]\tLoss: 1.076049\tGrad Norm: 1.106107\tLR: 0.030000\n",
      "Train Epoch: 54 [106496/194182 (54%)]\tLoss: 1.091782\tGrad Norm: 1.948984\tLR: 0.030000\n",
      "Train Epoch: 54 [126976/194182 (65%)]\tLoss: 1.090004\tGrad Norm: 1.574714\tLR: 0.030000\n",
      "Train Epoch: 54 [147456/194182 (75%)]\tLoss: 1.090089\tGrad Norm: 1.045012\tLR: 0.030000\n",
      "Train Epoch: 54 [167936/194182 (85%)]\tLoss: 1.083522\tGrad Norm: 0.971710\tLR: 0.030000\n",
      "Train Epoch: 54 [188416/194182 (96%)]\tLoss: 1.083594\tGrad Norm: 1.275280\tLR: 0.030000\n",
      "Train set: Average loss: 1.0893\n",
      "Test set: Average loss: 0.6386, Average MAE: 0.6318\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 55 [4096/194182 (2%)]\tLoss: 1.088564\tGrad Norm: 1.677649\tLR: 0.030000\n",
      "Train Epoch: 55 [24576/194182 (12%)]\tLoss: 1.092392\tGrad Norm: 1.942633\tLR: 0.030000\n",
      "Train Epoch: 55 [45056/194182 (23%)]\tLoss: 1.078950\tGrad Norm: 1.380287\tLR: 0.030000\n",
      "Train Epoch: 55 [65536/194182 (33%)]\tLoss: 1.087626\tGrad Norm: 1.040546\tLR: 0.030000\n",
      "Train Epoch: 55 [86016/194182 (44%)]\tLoss: 1.073515\tGrad Norm: 0.897212\tLR: 0.030000\n",
      "Train Epoch: 55 [106496/194182 (54%)]\tLoss: 1.085050\tGrad Norm: 1.243794\tLR: 0.030000\n",
      "Train Epoch: 55 [126976/194182 (65%)]\tLoss: 1.068826\tGrad Norm: 1.123621\tLR: 0.030000\n",
      "Train Epoch: 55 [147456/194182 (75%)]\tLoss: 1.075142\tGrad Norm: 0.923266\tLR: 0.030000\n",
      "Train Epoch: 55 [167936/194182 (85%)]\tLoss: 1.070826\tGrad Norm: 1.883844\tLR: 0.030000\n",
      "Train Epoch: 55 [188416/194182 (96%)]\tLoss: 1.079789\tGrad Norm: 1.292498\tLR: 0.030000\n",
      "Train set: Average loss: 1.0817\n",
      "Test set: Average loss: 0.6225, Average MAE: 0.6163\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 55: Mean reward = 0.044 +/- 0.001\n",
      "Train Epoch: 56 [4096/194182 (2%)]\tLoss: 1.071919\tGrad Norm: 1.173903\tLR: 0.030000\n",
      "Train Epoch: 56 [24576/194182 (12%)]\tLoss: 1.056782\tGrad Norm: 0.776173\tLR: 0.030000\n",
      "Train Epoch: 56 [45056/194182 (23%)]\tLoss: 1.078587\tGrad Norm: 0.888374\tLR: 0.030000\n",
      "Train Epoch: 56 [65536/194182 (33%)]\tLoss: 1.072049\tGrad Norm: 1.057779\tLR: 0.030000\n",
      "Train Epoch: 56 [86016/194182 (44%)]\tLoss: 1.067478\tGrad Norm: 1.383688\tLR: 0.030000\n",
      "Train Epoch: 56 [106496/194182 (54%)]\tLoss: 1.067735\tGrad Norm: 1.093749\tLR: 0.030000\n",
      "Train Epoch: 56 [126976/194182 (65%)]\tLoss: 1.073448\tGrad Norm: 1.692518\tLR: 0.030000\n",
      "Train Epoch: 56 [147456/194182 (75%)]\tLoss: 1.080057\tGrad Norm: 1.702324\tLR: 0.030000\n",
      "Train Epoch: 56 [167936/194182 (85%)]\tLoss: 1.057100\tGrad Norm: 1.498054\tLR: 0.030000\n",
      "Train Epoch: 56 [188416/194182 (96%)]\tLoss: 1.067971\tGrad Norm: 0.882659\tLR: 0.030000\n",
      "Train set: Average loss: 1.0696\n",
      "Test set: Average loss: 0.6107, Average MAE: 0.6162\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 57 [4096/194182 (2%)]\tLoss: 1.079306\tGrad Norm: 0.654306\tLR: 0.030000\n",
      "Train Epoch: 57 [24576/194182 (12%)]\tLoss: 1.053063\tGrad Norm: 0.818416\tLR: 0.030000\n",
      "Train Epoch: 57 [45056/194182 (23%)]\tLoss: 1.071354\tGrad Norm: 1.570684\tLR: 0.030000\n",
      "Train Epoch: 57 [65536/194182 (33%)]\tLoss: 1.073348\tGrad Norm: 1.542968\tLR: 0.030000\n",
      "Train Epoch: 57 [86016/194182 (44%)]\tLoss: 1.069489\tGrad Norm: 0.649478\tLR: 0.030000\n",
      "Train Epoch: 57 [106496/194182 (54%)]\tLoss: 1.062192\tGrad Norm: 0.707757\tLR: 0.030000\n",
      "Train Epoch: 57 [126976/194182 (65%)]\tLoss: 1.069003\tGrad Norm: 1.656769\tLR: 0.030000\n",
      "Train Epoch: 57 [147456/194182 (75%)]\tLoss: 1.080731\tGrad Norm: 1.454048\tLR: 0.030000\n",
      "Train Epoch: 57 [167936/194182 (85%)]\tLoss: 1.058517\tGrad Norm: 1.653461\tLR: 0.030000\n",
      "Train Epoch: 57 [188416/194182 (96%)]\tLoss: 1.079256\tGrad Norm: 1.907300\tLR: 0.030000\n",
      "Train set: Average loss: 1.0645\n",
      "Test set: Average loss: 0.6089, Average MAE: 0.6134\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 58 [4096/194182 (2%)]\tLoss: 1.058088\tGrad Norm: 1.371084\tLR: 0.030000\n",
      "Train Epoch: 58 [24576/194182 (12%)]\tLoss: 1.050733\tGrad Norm: 0.908143\tLR: 0.030000\n",
      "Train Epoch: 58 [45056/194182 (23%)]\tLoss: 1.059697\tGrad Norm: 1.370527\tLR: 0.030000\n",
      "Train Epoch: 58 [65536/194182 (33%)]\tLoss: 1.045549\tGrad Norm: 1.079071\tLR: 0.030000\n",
      "Train Epoch: 58 [86016/194182 (44%)]\tLoss: 1.062004\tGrad Norm: 1.255827\tLR: 0.030000\n",
      "Train Epoch: 58 [106496/194182 (54%)]\tLoss: 1.051984\tGrad Norm: 1.208875\tLR: 0.030000\n",
      "Train Epoch: 58 [126976/194182 (65%)]\tLoss: 1.042492\tGrad Norm: 0.672376\tLR: 0.030000\n",
      "Train Epoch: 58 [147456/194182 (75%)]\tLoss: 1.058088\tGrad Norm: 1.548335\tLR: 0.030000\n",
      "Train Epoch: 58 [167936/194182 (85%)]\tLoss: 1.065258\tGrad Norm: 1.964119\tLR: 0.030000\n",
      "Train Epoch: 58 [188416/194182 (96%)]\tLoss: 1.059379\tGrad Norm: 1.251519\tLR: 0.030000\n",
      "Train set: Average loss: 1.0542\n",
      "Test set: Average loss: 0.6010, Average MAE: 0.6132\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 59 [4096/194182 (2%)]\tLoss: 1.049182\tGrad Norm: 1.237854\tLR: 0.030000\n",
      "Train Epoch: 59 [24576/194182 (12%)]\tLoss: 1.053134\tGrad Norm: 1.605128\tLR: 0.030000\n",
      "Train Epoch: 59 [45056/194182 (23%)]\tLoss: 1.053559\tGrad Norm: 1.874276\tLR: 0.030000\n",
      "Train Epoch: 59 [65536/194182 (33%)]\tLoss: 1.039363\tGrad Norm: 0.969266\tLR: 0.030000\n",
      "Train Epoch: 59 [86016/194182 (44%)]\tLoss: 1.048848\tGrad Norm: 0.618786\tLR: 0.030000\n",
      "Train Epoch: 59 [106496/194182 (54%)]\tLoss: 1.029767\tGrad Norm: 1.148961\tLR: 0.030000\n",
      "Train Epoch: 59 [126976/194182 (65%)]\tLoss: 1.054469\tGrad Norm: 1.394975\tLR: 0.030000\n",
      "Train Epoch: 59 [147456/194182 (75%)]\tLoss: 1.049483\tGrad Norm: 1.612944\tLR: 0.030000\n",
      "Train Epoch: 59 [167936/194182 (85%)]\tLoss: 1.052248\tGrad Norm: 1.594584\tLR: 0.030000\n",
      "Train Epoch: 59 [188416/194182 (96%)]\tLoss: 1.037706\tGrad Norm: 1.324109\tLR: 0.030000\n",
      "Train set: Average loss: 1.0467\n",
      "Test set: Average loss: 0.5951, Average MAE: 0.5997\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 60 [4096/194182 (2%)]\tLoss: 1.041573\tGrad Norm: 1.497508\tLR: 0.030000\n",
      "Train Epoch: 60 [24576/194182 (12%)]\tLoss: 1.033736\tGrad Norm: 0.895157\tLR: 0.030000\n",
      "Train Epoch: 60 [45056/194182 (23%)]\tLoss: 1.041326\tGrad Norm: 0.992645\tLR: 0.030000\n",
      "Train Epoch: 60 [65536/194182 (33%)]\tLoss: 1.047117\tGrad Norm: 1.833892\tLR: 0.030000\n",
      "Train Epoch: 60 [86016/194182 (44%)]\tLoss: 1.035563\tGrad Norm: 0.916024\tLR: 0.030000\n",
      "Train Epoch: 60 [106496/194182 (54%)]\tLoss: 1.037839\tGrad Norm: 1.096755\tLR: 0.030000\n",
      "Train Epoch: 60 [126976/194182 (65%)]\tLoss: 1.035589\tGrad Norm: 1.345156\tLR: 0.030000\n",
      "Train Epoch: 60 [147456/194182 (75%)]\tLoss: 1.049423\tGrad Norm: 1.568403\tLR: 0.030000\n",
      "Train Epoch: 60 [167936/194182 (85%)]\tLoss: 1.038639\tGrad Norm: 1.180342\tLR: 0.030000\n",
      "Train Epoch: 60 [188416/194182 (96%)]\tLoss: 1.028706\tGrad Norm: 1.421779\tLR: 0.030000\n",
      "Train set: Average loss: 1.0374\n",
      "Test set: Average loss: 0.5839, Average MAE: 0.5980\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 60: Mean reward = 0.044 +/- 0.001\n",
      "Train Epoch: 61 [4096/194182 (2%)]\tLoss: 1.035529\tGrad Norm: 1.281730\tLR: 0.030000\n",
      "Train Epoch: 61 [24576/194182 (12%)]\tLoss: 1.016096\tGrad Norm: 0.947759\tLR: 0.030000\n",
      "Train Epoch: 61 [45056/194182 (23%)]\tLoss: 1.034041\tGrad Norm: 0.980631\tLR: 0.030000\n",
      "Train Epoch: 61 [65536/194182 (33%)]\tLoss: 1.023058\tGrad Norm: 1.386955\tLR: 0.030000\n",
      "Train Epoch: 61 [86016/194182 (44%)]\tLoss: 1.040517\tGrad Norm: 1.143623\tLR: 0.030000\n",
      "Train Epoch: 61 [106496/194182 (54%)]\tLoss: 1.018939\tGrad Norm: 1.204347\tLR: 0.030000\n",
      "Train Epoch: 61 [126976/194182 (65%)]\tLoss: 1.026568\tGrad Norm: 1.503315\tLR: 0.030000\n",
      "Train Epoch: 61 [147456/194182 (75%)]\tLoss: 1.039028\tGrad Norm: 1.707048\tLR: 0.030000\n",
      "Train Epoch: 61 [167936/194182 (85%)]\tLoss: 1.036628\tGrad Norm: 1.831028\tLR: 0.030000\n",
      "Train Epoch: 61 [188416/194182 (96%)]\tLoss: 1.038202\tGrad Norm: 1.628233\tLR: 0.030000\n",
      "Train set: Average loss: 1.0301\n",
      "Test set: Average loss: 0.5785, Average MAE: 0.5997\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 62 [4096/194182 (2%)]\tLoss: 1.026691\tGrad Norm: 1.090911\tLR: 0.030000\n",
      "Train Epoch: 62 [24576/194182 (12%)]\tLoss: 1.022158\tGrad Norm: 0.914236\tLR: 0.030000\n",
      "Train Epoch: 62 [45056/194182 (23%)]\tLoss: 1.016551\tGrad Norm: 1.114038\tLR: 0.030000\n",
      "Train Epoch: 62 [65536/194182 (33%)]\tLoss: 1.013838\tGrad Norm: 1.297917\tLR: 0.030000\n",
      "Train Epoch: 62 [86016/194182 (44%)]\tLoss: 1.026869\tGrad Norm: 1.047198\tLR: 0.030000\n",
      "Train Epoch: 62 [106496/194182 (54%)]\tLoss: 1.014204\tGrad Norm: 1.516688\tLR: 0.030000\n",
      "Train Epoch: 62 [126976/194182 (65%)]\tLoss: 1.030229\tGrad Norm: 1.878842\tLR: 0.030000\n",
      "Train Epoch: 62 [147456/194182 (75%)]\tLoss: 1.011096\tGrad Norm: 1.143219\tLR: 0.030000\n",
      "Train Epoch: 62 [167936/194182 (85%)]\tLoss: 1.010559\tGrad Norm: 0.579881\tLR: 0.030000\n",
      "Train Epoch: 62 [188416/194182 (96%)]\tLoss: 1.028798\tGrad Norm: 1.070505\tLR: 0.030000\n",
      "Train set: Average loss: 1.0199\n",
      "Test set: Average loss: 0.5693, Average MAE: 0.5941\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 63 [4096/194182 (2%)]\tLoss: 1.016321\tGrad Norm: 1.039675\tLR: 0.030000\n",
      "Train Epoch: 63 [24576/194182 (12%)]\tLoss: 1.024245\tGrad Norm: 1.213000\tLR: 0.030000\n",
      "Train Epoch: 63 [45056/194182 (23%)]\tLoss: 1.021624\tGrad Norm: 1.094287\tLR: 0.030000\n",
      "Train Epoch: 63 [65536/194182 (33%)]\tLoss: 1.010253\tGrad Norm: 1.255713\tLR: 0.030000\n",
      "Train Epoch: 63 [86016/194182 (44%)]\tLoss: 1.012069\tGrad Norm: 1.272076\tLR: 0.030000\n",
      "Train Epoch: 63 [106496/194182 (54%)]\tLoss: 1.009360\tGrad Norm: 1.681209\tLR: 0.030000\n",
      "Train Epoch: 63 [126976/194182 (65%)]\tLoss: 1.013753\tGrad Norm: 1.624299\tLR: 0.030000\n",
      "Train Epoch: 63 [147456/194182 (75%)]\tLoss: 1.015411\tGrad Norm: 1.066483\tLR: 0.030000\n",
      "Train Epoch: 63 [167936/194182 (85%)]\tLoss: 1.007290\tGrad Norm: 0.979628\tLR: 0.030000\n",
      "Train Epoch: 63 [188416/194182 (96%)]\tLoss: 1.014819\tGrad Norm: 1.959305\tLR: 0.030000\n",
      "Train set: Average loss: 1.0141\n",
      "Test set: Average loss: 0.5689, Average MAE: 0.5913\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 64 [4096/194182 (2%)]\tLoss: 1.005442\tGrad Norm: 1.582145\tLR: 0.030000\n",
      "Train Epoch: 64 [24576/194182 (12%)]\tLoss: 1.002267\tGrad Norm: 1.088771\tLR: 0.030000\n",
      "Train Epoch: 64 [45056/194182 (23%)]\tLoss: 1.019246\tGrad Norm: 1.057231\tLR: 0.030000\n",
      "Train Epoch: 64 [65536/194182 (33%)]\tLoss: 1.021027\tGrad Norm: 1.197575\tLR: 0.030000\n",
      "Train Epoch: 64 [86016/194182 (44%)]\tLoss: 0.992687\tGrad Norm: 0.687121\tLR: 0.030000\n",
      "Train Epoch: 64 [106496/194182 (54%)]\tLoss: 0.997606\tGrad Norm: 1.303496\tLR: 0.030000\n",
      "Train Epoch: 64 [126976/194182 (65%)]\tLoss: 0.997268\tGrad Norm: 1.154302\tLR: 0.030000\n",
      "Train Epoch: 64 [147456/194182 (75%)]\tLoss: 0.999547\tGrad Norm: 1.128189\tLR: 0.030000\n",
      "Train Epoch: 64 [167936/194182 (85%)]\tLoss: 1.008611\tGrad Norm: 0.937632\tLR: 0.030000\n",
      "Train Epoch: 64 [188416/194182 (96%)]\tLoss: 0.989399\tGrad Norm: 1.016919\tLR: 0.030000\n",
      "Train set: Average loss: 1.0036\n",
      "Test set: Average loss: 0.5609, Average MAE: 0.5830\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 65 [4096/194182 (2%)]\tLoss: 0.995920\tGrad Norm: 1.605450\tLR: 0.030000\n",
      "Train Epoch: 65 [24576/194182 (12%)]\tLoss: 1.005663\tGrad Norm: 1.663995\tLR: 0.030000\n",
      "Train Epoch: 65 [45056/194182 (23%)]\tLoss: 1.011687\tGrad Norm: 1.575611\tLR: 0.030000\n",
      "Train Epoch: 65 [65536/194182 (33%)]\tLoss: 1.000261\tGrad Norm: 0.959059\tLR: 0.030000\n",
      "Train Epoch: 65 [86016/194182 (44%)]\tLoss: 0.996260\tGrad Norm: 1.644618\tLR: 0.030000\n",
      "Train Epoch: 65 [106496/194182 (54%)]\tLoss: 1.003657\tGrad Norm: 1.534795\tLR: 0.030000\n",
      "Train Epoch: 65 [126976/194182 (65%)]\tLoss: 1.004260\tGrad Norm: 0.980045\tLR: 0.030000\n",
      "Train Epoch: 65 [147456/194182 (75%)]\tLoss: 1.001733\tGrad Norm: 1.476758\tLR: 0.030000\n",
      "Train Epoch: 65 [167936/194182 (85%)]\tLoss: 0.989588\tGrad Norm: 1.147072\tLR: 0.030000\n",
      "Train Epoch: 65 [188416/194182 (96%)]\tLoss: 1.002827\tGrad Norm: 1.342821\tLR: 0.030000\n",
      "Train set: Average loss: 1.0013\n",
      "Test set: Average loss: 0.5455, Average MAE: 0.5745\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 65: Mean reward = 0.044 +/- 0.001\n",
      "Train Epoch: 66 [4096/194182 (2%)]\tLoss: 0.999729\tGrad Norm: 0.931628\tLR: 0.030000\n",
      "Train Epoch: 66 [24576/194182 (12%)]\tLoss: 0.990718\tGrad Norm: 1.669179\tLR: 0.030000\n",
      "Train Epoch: 66 [45056/194182 (23%)]\tLoss: 1.018918\tGrad Norm: 2.116482\tLR: 0.030000\n",
      "Train Epoch: 66 [65536/194182 (33%)]\tLoss: 0.996823\tGrad Norm: 1.062064\tLR: 0.030000\n",
      "Train Epoch: 66 [86016/194182 (44%)]\tLoss: 0.987938\tGrad Norm: 0.965961\tLR: 0.030000\n",
      "Train Epoch: 66 [106496/194182 (54%)]\tLoss: 0.985329\tGrad Norm: 1.082375\tLR: 0.030000\n",
      "Train Epoch: 66 [126976/194182 (65%)]\tLoss: 0.983427\tGrad Norm: 0.923003\tLR: 0.030000\n",
      "Train Epoch: 66 [147456/194182 (75%)]\tLoss: 0.987199\tGrad Norm: 1.640002\tLR: 0.030000\n",
      "Train Epoch: 66 [167936/194182 (85%)]\tLoss: 0.993414\tGrad Norm: 1.389420\tLR: 0.030000\n",
      "Train Epoch: 66 [188416/194182 (96%)]\tLoss: 0.993864\tGrad Norm: 1.294175\tLR: 0.030000\n",
      "Train set: Average loss: 0.9933\n",
      "Test set: Average loss: 0.5433, Average MAE: 0.5756\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 67 [4096/194182 (2%)]\tLoss: 0.988967\tGrad Norm: 0.997275\tLR: 0.030000\n",
      "Train Epoch: 67 [24576/194182 (12%)]\tLoss: 0.996904\tGrad Norm: 1.184121\tLR: 0.030000\n",
      "Train Epoch: 67 [45056/194182 (23%)]\tLoss: 0.988694\tGrad Norm: 1.068442\tLR: 0.030000\n",
      "Train Epoch: 67 [65536/194182 (33%)]\tLoss: 0.985683\tGrad Norm: 1.331944\tLR: 0.030000\n",
      "Train Epoch: 67 [86016/194182 (44%)]\tLoss: 0.989269\tGrad Norm: 1.480662\tLR: 0.030000\n",
      "Train Epoch: 67 [106496/194182 (54%)]\tLoss: 0.985084\tGrad Norm: 1.397104\tLR: 0.030000\n",
      "Train Epoch: 67 [126976/194182 (65%)]\tLoss: 0.992837\tGrad Norm: 1.118155\tLR: 0.030000\n",
      "Train Epoch: 67 [147456/194182 (75%)]\tLoss: 0.993938\tGrad Norm: 1.106970\tLR: 0.030000\n",
      "Train Epoch: 67 [167936/194182 (85%)]\tLoss: 0.984429\tGrad Norm: 1.777983\tLR: 0.030000\n",
      "Train Epoch: 67 [188416/194182 (96%)]\tLoss: 0.987258\tGrad Norm: 1.435817\tLR: 0.030000\n",
      "Train set: Average loss: 0.9858\n",
      "Test set: Average loss: 0.5345, Average MAE: 0.5687\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 68 [4096/194182 (2%)]\tLoss: 0.967820\tGrad Norm: 0.982241\tLR: 0.030000\n",
      "Train Epoch: 68 [24576/194182 (12%)]\tLoss: 0.977881\tGrad Norm: 1.466797\tLR: 0.030000\n",
      "Train Epoch: 68 [45056/194182 (23%)]\tLoss: 0.985315\tGrad Norm: 0.994768\tLR: 0.030000\n",
      "Train Epoch: 68 [65536/194182 (33%)]\tLoss: 0.973790\tGrad Norm: 0.542169\tLR: 0.030000\n",
      "Train Epoch: 68 [86016/194182 (44%)]\tLoss: 0.972612\tGrad Norm: 1.064342\tLR: 0.030000\n",
      "Train Epoch: 68 [106496/194182 (54%)]\tLoss: 0.984982\tGrad Norm: 1.341109\tLR: 0.030000\n",
      "Train Epoch: 68 [126976/194182 (65%)]\tLoss: 0.977562\tGrad Norm: 1.473474\tLR: 0.030000\n",
      "Train Epoch: 68 [147456/194182 (75%)]\tLoss: 0.984523\tGrad Norm: 1.548017\tLR: 0.030000\n",
      "Train Epoch: 68 [167936/194182 (85%)]\tLoss: 0.973640\tGrad Norm: 1.031801\tLR: 0.030000\n",
      "Train Epoch: 68 [188416/194182 (96%)]\tLoss: 0.969598\tGrad Norm: 1.102224\tLR: 0.030000\n",
      "Train set: Average loss: 0.9769\n",
      "Test set: Average loss: 0.5280, Average MAE: 0.5690\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 69 [4096/194182 (2%)]\tLoss: 0.963403\tGrad Norm: 0.812321\tLR: 0.030000\n",
      "Train Epoch: 69 [24576/194182 (12%)]\tLoss: 0.973981\tGrad Norm: 1.499585\tLR: 0.030000\n",
      "Train Epoch: 69 [45056/194182 (23%)]\tLoss: 0.976382\tGrad Norm: 1.381512\tLR: 0.030000\n",
      "Train Epoch: 69 [65536/194182 (33%)]\tLoss: 0.969944\tGrad Norm: 0.969604\tLR: 0.030000\n",
      "Train Epoch: 69 [86016/194182 (44%)]\tLoss: 0.964186\tGrad Norm: 0.944171\tLR: 0.030000\n",
      "Train Epoch: 69 [106496/194182 (54%)]\tLoss: 0.979472\tGrad Norm: 1.539212\tLR: 0.030000\n",
      "Train Epoch: 69 [126976/194182 (65%)]\tLoss: 0.978295\tGrad Norm: 1.327980\tLR: 0.030000\n",
      "Train Epoch: 69 [147456/194182 (75%)]\tLoss: 0.969014\tGrad Norm: 1.369163\tLR: 0.030000\n",
      "Train Epoch: 69 [167936/194182 (85%)]\tLoss: 0.971132\tGrad Norm: 1.634094\tLR: 0.030000\n",
      "Train Epoch: 69 [188416/194182 (96%)]\tLoss: 0.979558\tGrad Norm: 1.833032\tLR: 0.030000\n",
      "Train set: Average loss: 0.9727\n",
      "Test set: Average loss: 0.5383, Average MAE: 0.5725\n",
      "Train Epoch: 70 [4096/194182 (2%)]\tLoss: 0.992765\tGrad Norm: 2.056656\tLR: 0.030000\n",
      "Train Epoch: 70 [24576/194182 (12%)]\tLoss: 0.971536\tGrad Norm: 1.250687\tLR: 0.030000\n",
      "Train Epoch: 70 [45056/194182 (23%)]\tLoss: 0.961814\tGrad Norm: 1.020142\tLR: 0.030000\n",
      "Train Epoch: 70 [65536/194182 (33%)]\tLoss: 0.963829\tGrad Norm: 1.106962\tLR: 0.030000\n",
      "Train Epoch: 70 [86016/194182 (44%)]\tLoss: 0.947499\tGrad Norm: 1.166508\tLR: 0.030000\n",
      "Train Epoch: 70 [106496/194182 (54%)]\tLoss: 0.956706\tGrad Norm: 0.925361\tLR: 0.030000\n",
      "Train Epoch: 70 [126976/194182 (65%)]\tLoss: 0.961799\tGrad Norm: 1.269695\tLR: 0.030000\n",
      "Train Epoch: 70 [147456/194182 (75%)]\tLoss: 0.958438\tGrad Norm: 1.682363\tLR: 0.030000\n",
      "Train Epoch: 70 [167936/194182 (85%)]\tLoss: 0.959826\tGrad Norm: 0.922848\tLR: 0.030000\n",
      "Train Epoch: 70 [188416/194182 (96%)]\tLoss: 0.956943\tGrad Norm: 1.093171\tLR: 0.030000\n",
      "Train set: Average loss: 0.9649\n",
      "Test set: Average loss: 0.5194, Average MAE: 0.5617\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 70: Mean reward = 0.044 +/- 0.001\n",
      "Train Epoch: 71 [4096/194182 (2%)]\tLoss: 0.961192\tGrad Norm: 1.141324\tLR: 0.030000\n",
      "Train Epoch: 71 [24576/194182 (12%)]\tLoss: 0.954290\tGrad Norm: 1.003511\tLR: 0.030000\n",
      "Train Epoch: 71 [45056/194182 (23%)]\tLoss: 0.951678\tGrad Norm: 0.965349\tLR: 0.030000\n",
      "Train Epoch: 71 [65536/194182 (33%)]\tLoss: 0.947905\tGrad Norm: 1.113817\tLR: 0.030000\n",
      "Train Epoch: 71 [86016/194182 (44%)]\tLoss: 0.968414\tGrad Norm: 1.660344\tLR: 0.030000\n",
      "Train Epoch: 71 [106496/194182 (54%)]\tLoss: 0.965193\tGrad Norm: 1.463220\tLR: 0.030000\n",
      "Train Epoch: 71 [126976/194182 (65%)]\tLoss: 0.954735\tGrad Norm: 1.084379\tLR: 0.030000\n",
      "Train Epoch: 71 [147456/194182 (75%)]\tLoss: 0.965622\tGrad Norm: 1.577482\tLR: 0.030000\n",
      "Train Epoch: 71 [167936/194182 (85%)]\tLoss: 0.944655\tGrad Norm: 1.117622\tLR: 0.030000\n",
      "Train Epoch: 71 [188416/194182 (96%)]\tLoss: 0.940161\tGrad Norm: 1.100444\tLR: 0.030000\n",
      "Train set: Average loss: 0.9575\n",
      "Test set: Average loss: 0.5121, Average MAE: 0.5575\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 72 [4096/194182 (2%)]\tLoss: 0.948321\tGrad Norm: 0.981142\tLR: 0.030000\n",
      "Train Epoch: 72 [24576/194182 (12%)]\tLoss: 0.951348\tGrad Norm: 1.006964\tLR: 0.030000\n",
      "Train Epoch: 72 [45056/194182 (23%)]\tLoss: 0.949377\tGrad Norm: 1.252501\tLR: 0.030000\n",
      "Train Epoch: 72 [65536/194182 (33%)]\tLoss: 0.952378\tGrad Norm: 1.555087\tLR: 0.030000\n",
      "Train Epoch: 72 [86016/194182 (44%)]\tLoss: 0.957111\tGrad Norm: 1.445838\tLR: 0.030000\n",
      "Train Epoch: 72 [106496/194182 (54%)]\tLoss: 0.952888\tGrad Norm: 1.117615\tLR: 0.030000\n",
      "Train Epoch: 72 [126976/194182 (65%)]\tLoss: 0.949216\tGrad Norm: 1.309339\tLR: 0.030000\n",
      "Train Epoch: 72 [147456/194182 (75%)]\tLoss: 0.945185\tGrad Norm: 1.317071\tLR: 0.030000\n",
      "Train Epoch: 72 [167936/194182 (85%)]\tLoss: 0.954913\tGrad Norm: 1.233569\tLR: 0.030000\n",
      "Train Epoch: 72 [188416/194182 (96%)]\tLoss: 0.942251\tGrad Norm: 0.968847\tLR: 0.030000\n",
      "Train set: Average loss: 0.9530\n",
      "Test set: Average loss: 0.5106, Average MAE: 0.5572\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 73 [4096/194182 (2%)]\tLoss: 0.946295\tGrad Norm: 1.296545\tLR: 0.030000\n",
      "Train Epoch: 73 [24576/194182 (12%)]\tLoss: 0.947659\tGrad Norm: 1.613295\tLR: 0.030000\n",
      "Train Epoch: 73 [45056/194182 (23%)]\tLoss: 0.940765\tGrad Norm: 1.402642\tLR: 0.030000\n",
      "Train Epoch: 73 [65536/194182 (33%)]\tLoss: 0.942650\tGrad Norm: 1.157688\tLR: 0.030000\n",
      "Train Epoch: 73 [86016/194182 (44%)]\tLoss: 0.939754\tGrad Norm: 1.293450\tLR: 0.030000\n",
      "Train Epoch: 73 [106496/194182 (54%)]\tLoss: 0.943234\tGrad Norm: 1.055362\tLR: 0.030000\n",
      "Train Epoch: 73 [126976/194182 (65%)]\tLoss: 0.950865\tGrad Norm: 1.203519\tLR: 0.030000\n",
      "Train Epoch: 73 [147456/194182 (75%)]\tLoss: 0.941903\tGrad Norm: 1.273015\tLR: 0.030000\n",
      "Train Epoch: 73 [167936/194182 (85%)]\tLoss: 0.933155\tGrad Norm: 0.914720\tLR: 0.030000\n",
      "Train Epoch: 73 [188416/194182 (96%)]\tLoss: 0.942388\tGrad Norm: 1.181144\tLR: 0.030000\n",
      "Train set: Average loss: 0.9461\n",
      "Test set: Average loss: 0.5014, Average MAE: 0.5487\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 74 [4096/194182 (2%)]\tLoss: 0.934051\tGrad Norm: 0.977031\tLR: 0.030000\n",
      "Train Epoch: 74 [24576/194182 (12%)]\tLoss: 0.947336\tGrad Norm: 1.508439\tLR: 0.030000\n",
      "Train Epoch: 74 [45056/194182 (23%)]\tLoss: 0.949689\tGrad Norm: 1.532760\tLR: 0.030000\n",
      "Train Epoch: 74 [65536/194182 (33%)]\tLoss: 0.939676\tGrad Norm: 1.495816\tLR: 0.030000\n",
      "Train Epoch: 74 [86016/194182 (44%)]\tLoss: 0.946956\tGrad Norm: 0.892126\tLR: 0.030000\n",
      "Train Epoch: 74 [106496/194182 (54%)]\tLoss: 0.940715\tGrad Norm: 1.206398\tLR: 0.030000\n",
      "Train Epoch: 74 [126976/194182 (65%)]\tLoss: 0.944713\tGrad Norm: 1.199053\tLR: 0.030000\n",
      "Train Epoch: 74 [147456/194182 (75%)]\tLoss: 0.948755\tGrad Norm: 1.224252\tLR: 0.030000\n",
      "Train Epoch: 74 [167936/194182 (85%)]\tLoss: 0.939620\tGrad Norm: 1.193926\tLR: 0.030000\n",
      "Train Epoch: 74 [188416/194182 (96%)]\tLoss: 0.959096\tGrad Norm: 1.641609\tLR: 0.030000\n",
      "Train set: Average loss: 0.9403\n",
      "Test set: Average loss: 0.5012, Average MAE: 0.5500\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 75 [4096/194182 (2%)]\tLoss: 0.927255\tGrad Norm: 1.225352\tLR: 0.030000\n",
      "Train Epoch: 75 [24576/194182 (12%)]\tLoss: 0.923315\tGrad Norm: 0.666209\tLR: 0.030000\n",
      "Train Epoch: 75 [45056/194182 (23%)]\tLoss: 0.933525\tGrad Norm: 0.847564\tLR: 0.030000\n",
      "Train Epoch: 75 [65536/194182 (33%)]\tLoss: 0.930863\tGrad Norm: 1.491240\tLR: 0.030000\n",
      "Train Epoch: 75 [86016/194182 (44%)]\tLoss: 0.939988\tGrad Norm: 1.364871\tLR: 0.030000\n",
      "Train Epoch: 75 [106496/194182 (54%)]\tLoss: 0.929994\tGrad Norm: 0.746020\tLR: 0.030000\n",
      "Train Epoch: 75 [126976/194182 (65%)]\tLoss: 0.936804\tGrad Norm: 1.360597\tLR: 0.030000\n",
      "Train Epoch: 75 [147456/194182 (75%)]\tLoss: 0.925952\tGrad Norm: 0.985692\tLR: 0.030000\n",
      "Train Epoch: 75 [167936/194182 (85%)]\tLoss: 0.930574\tGrad Norm: 1.287911\tLR: 0.030000\n",
      "Train Epoch: 75 [188416/194182 (96%)]\tLoss: 0.934432\tGrad Norm: 1.060859\tLR: 0.030000\n",
      "Train set: Average loss: 0.9319\n",
      "Test set: Average loss: 0.4937, Average MAE: 0.5357\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 75: Mean reward = 0.044 +/- 0.001\n",
      "Train Epoch: 76 [4096/194182 (2%)]\tLoss: 0.930960\tGrad Norm: 1.182384\tLR: 0.030000\n",
      "Train Epoch: 76 [24576/194182 (12%)]\tLoss: 0.921559\tGrad Norm: 1.620466\tLR: 0.030000\n",
      "Train Epoch: 76 [45056/194182 (23%)]\tLoss: 0.932545\tGrad Norm: 1.456130\tLR: 0.030000\n",
      "Train Epoch: 76 [65536/194182 (33%)]\tLoss: 0.937335\tGrad Norm: 1.734712\tLR: 0.030000\n",
      "Train Epoch: 76 [86016/194182 (44%)]\tLoss: 0.928092\tGrad Norm: 1.225724\tLR: 0.030000\n",
      "Train Epoch: 76 [106496/194182 (54%)]\tLoss: 0.912394\tGrad Norm: 0.725620\tLR: 0.030000\n",
      "Train Epoch: 76 [126976/194182 (65%)]\tLoss: 0.922469\tGrad Norm: 0.859643\tLR: 0.030000\n",
      "Train Epoch: 76 [147456/194182 (75%)]\tLoss: 0.922841\tGrad Norm: 1.062177\tLR: 0.030000\n",
      "Train Epoch: 76 [167936/194182 (85%)]\tLoss: 0.920751\tGrad Norm: 0.931573\tLR: 0.030000\n",
      "Train Epoch: 76 [188416/194182 (96%)]\tLoss: 0.940052\tGrad Norm: 1.034157\tLR: 0.030000\n",
      "Train set: Average loss: 0.9281\n",
      "Test set: Average loss: 0.4896, Average MAE: 0.5401\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 77 [4096/194182 (2%)]\tLoss: 0.927532\tGrad Norm: 1.400394\tLR: 0.030000\n",
      "Train Epoch: 77 [24576/194182 (12%)]\tLoss: 0.919594\tGrad Norm: 1.113891\tLR: 0.030000\n",
      "Train Epoch: 77 [45056/194182 (23%)]\tLoss: 0.921485\tGrad Norm: 1.632461\tLR: 0.030000\n",
      "Train Epoch: 77 [65536/194182 (33%)]\tLoss: 0.939549\tGrad Norm: 1.289929\tLR: 0.030000\n",
      "Train Epoch: 77 [86016/194182 (44%)]\tLoss: 0.927493\tGrad Norm: 1.240413\tLR: 0.030000\n",
      "Train Epoch: 77 [106496/194182 (54%)]\tLoss: 0.934833\tGrad Norm: 1.361115\tLR: 0.030000\n",
      "Train Epoch: 77 [126976/194182 (65%)]\tLoss: 0.922595\tGrad Norm: 0.877084\tLR: 0.030000\n",
      "Train Epoch: 77 [147456/194182 (75%)]\tLoss: 0.918927\tGrad Norm: 0.854430\tLR: 0.030000\n",
      "Train Epoch: 77 [167936/194182 (85%)]\tLoss: 0.920631\tGrad Norm: 1.535482\tLR: 0.030000\n",
      "Train Epoch: 77 [188416/194182 (96%)]\tLoss: 0.929037\tGrad Norm: 1.604601\tLR: 0.030000\n",
      "Train set: Average loss: 0.9233\n",
      "Test set: Average loss: 0.4831, Average MAE: 0.5320\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 78 [4096/194182 (2%)]\tLoss: 0.915733\tGrad Norm: 1.250022\tLR: 0.030000\n",
      "Train Epoch: 78 [24576/194182 (12%)]\tLoss: 0.917983\tGrad Norm: 0.964233\tLR: 0.030000\n",
      "Train Epoch: 78 [45056/194182 (23%)]\tLoss: 0.916849\tGrad Norm: 1.175915\tLR: 0.030000\n",
      "Train Epoch: 78 [65536/194182 (33%)]\tLoss: 0.935535\tGrad Norm: 1.427059\tLR: 0.030000\n",
      "Train Epoch: 78 [86016/194182 (44%)]\tLoss: 0.921433\tGrad Norm: 1.248595\tLR: 0.030000\n",
      "Train Epoch: 78 [106496/194182 (54%)]\tLoss: 0.914934\tGrad Norm: 0.954427\tLR: 0.030000\n",
      "Train Epoch: 78 [126976/194182 (65%)]\tLoss: 0.910359\tGrad Norm: 1.155643\tLR: 0.030000\n",
      "Train Epoch: 78 [147456/194182 (75%)]\tLoss: 0.917366\tGrad Norm: 1.334498\tLR: 0.030000\n",
      "Train Epoch: 78 [167936/194182 (85%)]\tLoss: 0.919351\tGrad Norm: 1.072773\tLR: 0.030000\n",
      "Train Epoch: 78 [188416/194182 (96%)]\tLoss: 0.925397\tGrad Norm: 1.232949\tLR: 0.030000\n",
      "Train set: Average loss: 0.9173\n",
      "Test set: Average loss: 0.4774, Average MAE: 0.5320\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 79 [4096/194182 (2%)]\tLoss: 0.904395\tGrad Norm: 1.105011\tLR: 0.030000\n",
      "Train Epoch: 79 [24576/194182 (12%)]\tLoss: 0.925903\tGrad Norm: 1.435082\tLR: 0.030000\n",
      "Train Epoch: 79 [45056/194182 (23%)]\tLoss: 0.915339\tGrad Norm: 1.488832\tLR: 0.030000\n",
      "Train Epoch: 79 [65536/194182 (33%)]\tLoss: 0.914421\tGrad Norm: 1.200705\tLR: 0.030000\n",
      "Train Epoch: 79 [86016/194182 (44%)]\tLoss: 0.906980\tGrad Norm: 0.821968\tLR: 0.030000\n",
      "Train Epoch: 79 [106496/194182 (54%)]\tLoss: 0.910846\tGrad Norm: 1.238410\tLR: 0.030000\n",
      "Train Epoch: 79 [126976/194182 (65%)]\tLoss: 0.919677\tGrad Norm: 1.500945\tLR: 0.030000\n",
      "Train Epoch: 79 [147456/194182 (75%)]\tLoss: 0.915698\tGrad Norm: 1.232007\tLR: 0.030000\n",
      "Train Epoch: 79 [167936/194182 (85%)]\tLoss: 0.908345\tGrad Norm: 0.980669\tLR: 0.030000\n",
      "Train Epoch: 79 [188416/194182 (96%)]\tLoss: 0.910134\tGrad Norm: 0.887178\tLR: 0.030000\n",
      "Train set: Average loss: 0.9117\n",
      "Test set: Average loss: 0.4696, Average MAE: 0.5327\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 80 [4096/194182 (2%)]\tLoss: 0.912711\tGrad Norm: 0.781695\tLR: 0.030000\n",
      "Train Epoch: 80 [24576/194182 (12%)]\tLoss: 0.894306\tGrad Norm: 0.766194\tLR: 0.030000\n",
      "Train Epoch: 80 [45056/194182 (23%)]\tLoss: 0.913203\tGrad Norm: 1.063221\tLR: 0.030000\n",
      "Train Epoch: 80 [65536/194182 (33%)]\tLoss: 0.914348\tGrad Norm: 1.785342\tLR: 0.030000\n",
      "Train Epoch: 80 [86016/194182 (44%)]\tLoss: 0.907688\tGrad Norm: 1.097604\tLR: 0.030000\n",
      "Train Epoch: 80 [106496/194182 (54%)]\tLoss: 0.896323\tGrad Norm: 0.971741\tLR: 0.030000\n",
      "Train Epoch: 80 [126976/194182 (65%)]\tLoss: 0.909974\tGrad Norm: 1.258674\tLR: 0.030000\n",
      "Train Epoch: 80 [147456/194182 (75%)]\tLoss: 0.902169\tGrad Norm: 1.001157\tLR: 0.030000\n",
      "Train Epoch: 80 [167936/194182 (85%)]\tLoss: 0.894076\tGrad Norm: 0.851511\tLR: 0.030000\n",
      "Train Epoch: 80 [188416/194182 (96%)]\tLoss: 0.892913\tGrad Norm: 0.899477\tLR: 0.030000\n",
      "Train set: Average loss: 0.9057\n",
      "Test set: Average loss: 0.4701, Average MAE: 0.5269\n",
      "Epoch 80: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 81 [4096/194182 (2%)]\tLoss: 0.890487\tGrad Norm: 1.469768\tLR: 0.030000\n",
      "Train Epoch: 81 [24576/194182 (12%)]\tLoss: 0.895981\tGrad Norm: 1.340276\tLR: 0.030000\n",
      "Train Epoch: 81 [45056/194182 (23%)]\tLoss: 0.907179\tGrad Norm: 1.062182\tLR: 0.030000\n",
      "Train Epoch: 81 [65536/194182 (33%)]\tLoss: 0.899077\tGrad Norm: 1.124703\tLR: 0.030000\n",
      "Train Epoch: 81 [86016/194182 (44%)]\tLoss: 0.892691\tGrad Norm: 0.977407\tLR: 0.030000\n",
      "Train Epoch: 81 [106496/194182 (54%)]\tLoss: 0.898099\tGrad Norm: 1.345934\tLR: 0.030000\n",
      "Train Epoch: 81 [126976/194182 (65%)]\tLoss: 0.892321\tGrad Norm: 0.974475\tLR: 0.030000\n",
      "Train Epoch: 81 [147456/194182 (75%)]\tLoss: 0.889417\tGrad Norm: 0.757470\tLR: 0.030000\n",
      "Train Epoch: 81 [167936/194182 (85%)]\tLoss: 0.897976\tGrad Norm: 1.059538\tLR: 0.030000\n",
      "Train Epoch: 81 [188416/194182 (96%)]\tLoss: 0.903235\tGrad Norm: 1.116521\tLR: 0.030000\n",
      "Train set: Average loss: 0.9000\n",
      "Test set: Average loss: 0.4712, Average MAE: 0.5304\n",
      "Train Epoch: 82 [4096/194182 (2%)]\tLoss: 0.896896\tGrad Norm: 1.768021\tLR: 0.030000\n",
      "Train Epoch: 82 [24576/194182 (12%)]\tLoss: 0.899194\tGrad Norm: 1.301380\tLR: 0.030000\n",
      "Train Epoch: 82 [45056/194182 (23%)]\tLoss: 0.894316\tGrad Norm: 0.919162\tLR: 0.030000\n",
      "Train Epoch: 82 [65536/194182 (33%)]\tLoss: 0.896021\tGrad Norm: 0.759356\tLR: 0.030000\n",
      "Train Epoch: 82 [86016/194182 (44%)]\tLoss: 0.892410\tGrad Norm: 0.832664\tLR: 0.030000\n",
      "Train Epoch: 82 [106496/194182 (54%)]\tLoss: 0.906256\tGrad Norm: 1.438531\tLR: 0.030000\n",
      "Train Epoch: 82 [126976/194182 (65%)]\tLoss: 0.885250\tGrad Norm: 1.358621\tLR: 0.030000\n",
      "Train Epoch: 82 [147456/194182 (75%)]\tLoss: 0.897359\tGrad Norm: 1.267250\tLR: 0.030000\n",
      "Train Epoch: 82 [167936/194182 (85%)]\tLoss: 0.901285\tGrad Norm: 1.591571\tLR: 0.030000\n",
      "Train Epoch: 82 [188416/194182 (96%)]\tLoss: 0.888055\tGrad Norm: 1.679299\tLR: 0.030000\n",
      "Train set: Average loss: 0.8963\n",
      "Test set: Average loss: 0.4597, Average MAE: 0.5232\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 83 [4096/194182 (2%)]\tLoss: 0.890108\tGrad Norm: 1.136281\tLR: 0.030000\n",
      "Train Epoch: 83 [24576/194182 (12%)]\tLoss: 0.895438\tGrad Norm: 1.019895\tLR: 0.030000\n",
      "Train Epoch: 83 [45056/194182 (23%)]\tLoss: 0.898805\tGrad Norm: 1.516047\tLR: 0.030000\n",
      "Train Epoch: 83 [65536/194182 (33%)]\tLoss: 0.897901\tGrad Norm: 1.308581\tLR: 0.030000\n",
      "Train Epoch: 83 [86016/194182 (44%)]\tLoss: 0.898499\tGrad Norm: 1.242889\tLR: 0.030000\n",
      "Train Epoch: 83 [106496/194182 (54%)]\tLoss: 0.889334\tGrad Norm: 1.231947\tLR: 0.030000\n",
      "Train Epoch: 83 [126976/194182 (65%)]\tLoss: 0.883183\tGrad Norm: 1.093259\tLR: 0.030000\n",
      "Train Epoch: 83 [147456/194182 (75%)]\tLoss: 0.893863\tGrad Norm: 1.065826\tLR: 0.030000\n",
      "Train Epoch: 83 [167936/194182 (85%)]\tLoss: 0.883482\tGrad Norm: 1.089315\tLR: 0.030000\n",
      "Train Epoch: 83 [188416/194182 (96%)]\tLoss: 0.882318\tGrad Norm: 1.058844\tLR: 0.030000\n",
      "Train set: Average loss: 0.8918\n",
      "Test set: Average loss: 0.4560, Average MAE: 0.5117\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 84 [4096/194182 (2%)]\tLoss: 0.885536\tGrad Norm: 1.069864\tLR: 0.030000\n",
      "Train Epoch: 84 [24576/194182 (12%)]\tLoss: 0.897403\tGrad Norm: 1.048266\tLR: 0.030000\n",
      "Train Epoch: 84 [45056/194182 (23%)]\tLoss: 0.891049\tGrad Norm: 1.421528\tLR: 0.030000\n",
      "Train Epoch: 84 [65536/194182 (33%)]\tLoss: 0.892583\tGrad Norm: 1.477242\tLR: 0.030000\n",
      "Train Epoch: 84 [86016/194182 (44%)]\tLoss: 0.875951\tGrad Norm: 1.087186\tLR: 0.030000\n",
      "Train Epoch: 84 [106496/194182 (54%)]\tLoss: 0.895690\tGrad Norm: 1.498939\tLR: 0.030000\n",
      "Train Epoch: 84 [126976/194182 (65%)]\tLoss: 0.885974\tGrad Norm: 1.171820\tLR: 0.030000\n",
      "Train Epoch: 84 [147456/194182 (75%)]\tLoss: 0.888651\tGrad Norm: 1.228868\tLR: 0.030000\n",
      "Train Epoch: 84 [167936/194182 (85%)]\tLoss: 0.892450\tGrad Norm: 1.195895\tLR: 0.030000\n",
      "Train Epoch: 84 [188416/194182 (96%)]\tLoss: 0.876934\tGrad Norm: 1.089439\tLR: 0.030000\n",
      "Train set: Average loss: 0.8876\n",
      "Test set: Average loss: 0.4583, Average MAE: 0.5128\n",
      "Train Epoch: 85 [4096/194182 (2%)]\tLoss: 0.887385\tGrad Norm: 1.629842\tLR: 0.030000\n",
      "Train Epoch: 85 [24576/194182 (12%)]\tLoss: 0.884175\tGrad Norm: 1.255323\tLR: 0.030000\n",
      "Train Epoch: 85 [45056/194182 (23%)]\tLoss: 0.888852\tGrad Norm: 1.337407\tLR: 0.030000\n",
      "Train Epoch: 85 [65536/194182 (33%)]\tLoss: 0.886546\tGrad Norm: 0.964626\tLR: 0.030000\n",
      "Train Epoch: 85 [86016/194182 (44%)]\tLoss: 0.885107\tGrad Norm: 0.646535\tLR: 0.030000\n",
      "Train Epoch: 85 [106496/194182 (54%)]\tLoss: 0.880755\tGrad Norm: 1.032049\tLR: 0.030000\n",
      "Train Epoch: 85 [126976/194182 (65%)]\tLoss: 0.882838\tGrad Norm: 1.189184\tLR: 0.030000\n",
      "Train Epoch: 85 [147456/194182 (75%)]\tLoss: 0.884377\tGrad Norm: 1.187293\tLR: 0.030000\n",
      "Train Epoch: 85 [167936/194182 (85%)]\tLoss: 0.888864\tGrad Norm: 1.472186\tLR: 0.030000\n",
      "Train Epoch: 85 [188416/194182 (96%)]\tLoss: 0.879305\tGrad Norm: 1.270618\tLR: 0.030000\n",
      "Train set: Average loss: 0.8813\n",
      "Test set: Average loss: 0.4507, Average MAE: 0.5138\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 85: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 86 [4096/194182 (2%)]\tLoss: 0.888113\tGrad Norm: 1.513423\tLR: 0.030000\n",
      "Train Epoch: 86 [24576/194182 (12%)]\tLoss: 0.882459\tGrad Norm: 0.711599\tLR: 0.030000\n",
      "Train Epoch: 86 [45056/194182 (23%)]\tLoss: 0.865084\tGrad Norm: 1.063125\tLR: 0.030000\n",
      "Train Epoch: 86 [65536/194182 (33%)]\tLoss: 0.880406\tGrad Norm: 1.390759\tLR: 0.030000\n",
      "Train Epoch: 86 [86016/194182 (44%)]\tLoss: 0.872482\tGrad Norm: 1.039685\tLR: 0.030000\n",
      "Train Epoch: 86 [106496/194182 (54%)]\tLoss: 0.885571\tGrad Norm: 1.260525\tLR: 0.030000\n",
      "Train Epoch: 86 [126976/194182 (65%)]\tLoss: 0.871721\tGrad Norm: 0.981470\tLR: 0.030000\n",
      "Train Epoch: 86 [147456/194182 (75%)]\tLoss: 0.871374\tGrad Norm: 1.138929\tLR: 0.030000\n",
      "Train Epoch: 86 [167936/194182 (85%)]\tLoss: 0.877280\tGrad Norm: 1.175671\tLR: 0.030000\n",
      "Train Epoch: 86 [188416/194182 (96%)]\tLoss: 0.871615\tGrad Norm: 0.969193\tLR: 0.030000\n",
      "Train set: Average loss: 0.8762\n",
      "Test set: Average loss: 0.4466, Average MAE: 0.5182\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 87 [4096/194182 (2%)]\tLoss: 0.872773\tGrad Norm: 1.183442\tLR: 0.030000\n",
      "Train Epoch: 87 [24576/194182 (12%)]\tLoss: 0.863739\tGrad Norm: 1.410860\tLR: 0.030000\n",
      "Train Epoch: 87 [45056/194182 (23%)]\tLoss: 0.873645\tGrad Norm: 1.345554\tLR: 0.030000\n",
      "Train Epoch: 87 [65536/194182 (33%)]\tLoss: 0.878762\tGrad Norm: 1.066715\tLR: 0.030000\n",
      "Train Epoch: 87 [86016/194182 (44%)]\tLoss: 0.879374\tGrad Norm: 1.295770\tLR: 0.030000\n",
      "Train Epoch: 87 [106496/194182 (54%)]\tLoss: 0.878904\tGrad Norm: 1.212045\tLR: 0.030000\n",
      "Train Epoch: 87 [126976/194182 (65%)]\tLoss: 0.881035\tGrad Norm: 1.656058\tLR: 0.030000\n",
      "Train Epoch: 87 [147456/194182 (75%)]\tLoss: 0.881635\tGrad Norm: 1.325335\tLR: 0.030000\n",
      "Train Epoch: 87 [167936/194182 (85%)]\tLoss: 0.885066\tGrad Norm: 1.339774\tLR: 0.030000\n",
      "Train Epoch: 87 [188416/194182 (96%)]\tLoss: 0.880289\tGrad Norm: 1.102704\tLR: 0.030000\n",
      "Train set: Average loss: 0.8744\n",
      "Test set: Average loss: 0.4430, Average MAE: 0.5178\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 88 [4096/194182 (2%)]\tLoss: 0.882935\tGrad Norm: 1.081804\tLR: 0.030000\n",
      "Train Epoch: 88 [24576/194182 (12%)]\tLoss: 0.864236\tGrad Norm: 0.690423\tLR: 0.030000\n",
      "Train Epoch: 88 [45056/194182 (23%)]\tLoss: 0.867492\tGrad Norm: 1.136603\tLR: 0.030000\n",
      "Train Epoch: 88 [65536/194182 (33%)]\tLoss: 0.862614\tGrad Norm: 1.180772\tLR: 0.030000\n",
      "Train Epoch: 88 [86016/194182 (44%)]\tLoss: 0.872236\tGrad Norm: 1.259408\tLR: 0.030000\n",
      "Train Epoch: 88 [106496/194182 (54%)]\tLoss: 0.867897\tGrad Norm: 1.283458\tLR: 0.030000\n",
      "Train Epoch: 88 [126976/194182 (65%)]\tLoss: 0.870693\tGrad Norm: 1.303616\tLR: 0.030000\n",
      "Train Epoch: 88 [147456/194182 (75%)]\tLoss: 0.864402\tGrad Norm: 1.110338\tLR: 0.030000\n",
      "Train Epoch: 88 [167936/194182 (85%)]\tLoss: 0.865879\tGrad Norm: 0.860397\tLR: 0.030000\n",
      "Train Epoch: 88 [188416/194182 (96%)]\tLoss: 0.872316\tGrad Norm: 1.106693\tLR: 0.030000\n",
      "Train set: Average loss: 0.8674\n",
      "Test set: Average loss: 0.4405, Average MAE: 0.5120\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 89 [4096/194182 (2%)]\tLoss: 0.870400\tGrad Norm: 1.331834\tLR: 0.030000\n",
      "Train Epoch: 89 [24576/194182 (12%)]\tLoss: 0.867172\tGrad Norm: 1.024930\tLR: 0.030000\n",
      "Train Epoch: 89 [45056/194182 (23%)]\tLoss: 0.864738\tGrad Norm: 1.305574\tLR: 0.030000\n",
      "Train Epoch: 89 [65536/194182 (33%)]\tLoss: 0.852191\tGrad Norm: 0.646203\tLR: 0.030000\n",
      "Train Epoch: 89 [86016/194182 (44%)]\tLoss: 0.860207\tGrad Norm: 1.272274\tLR: 0.030000\n",
      "Train Epoch: 89 [106496/194182 (54%)]\tLoss: 0.866844\tGrad Norm: 1.199328\tLR: 0.030000\n",
      "Train Epoch: 89 [126976/194182 (65%)]\tLoss: 0.858834\tGrad Norm: 1.152712\tLR: 0.030000\n",
      "Train Epoch: 89 [147456/194182 (75%)]\tLoss: 0.859583\tGrad Norm: 1.371402\tLR: 0.030000\n",
      "Train Epoch: 89 [167936/194182 (85%)]\tLoss: 0.887239\tGrad Norm: 1.884363\tLR: 0.030000\n",
      "Train Epoch: 89 [188416/194182 (96%)]\tLoss: 0.853219\tGrad Norm: 0.838080\tLR: 0.030000\n",
      "Train set: Average loss: 0.8648\n",
      "Test set: Average loss: 0.4274, Average MAE: 0.4987\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 90 [4096/194182 (2%)]\tLoss: 0.859166\tGrad Norm: 0.529469\tLR: 0.030000\n",
      "Train Epoch: 90 [24576/194182 (12%)]\tLoss: 0.867053\tGrad Norm: 1.189842\tLR: 0.030000\n",
      "Train Epoch: 90 [45056/194182 (23%)]\tLoss: 0.853889\tGrad Norm: 1.249454\tLR: 0.030000\n",
      "Train Epoch: 90 [65536/194182 (33%)]\tLoss: 0.865184\tGrad Norm: 0.940249\tLR: 0.030000\n",
      "Train Epoch: 90 [86016/194182 (44%)]\tLoss: 0.864427\tGrad Norm: 0.860561\tLR: 0.030000\n",
      "Train Epoch: 90 [106496/194182 (54%)]\tLoss: 0.850805\tGrad Norm: 1.522679\tLR: 0.030000\n",
      "Train Epoch: 90 [126976/194182 (65%)]\tLoss: 0.861924\tGrad Norm: 1.398946\tLR: 0.030000\n",
      "Train Epoch: 90 [147456/194182 (75%)]\tLoss: 0.859876\tGrad Norm: 1.164278\tLR: 0.030000\n",
      "Train Epoch: 90 [167936/194182 (85%)]\tLoss: 0.856401\tGrad Norm: 1.166601\tLR: 0.030000\n",
      "Train Epoch: 90 [188416/194182 (96%)]\tLoss: 0.854363\tGrad Norm: 1.098988\tLR: 0.030000\n",
      "Train set: Average loss: 0.8588\n",
      "Test set: Average loss: 0.4278, Average MAE: 0.4942\n",
      "Epoch 90: Mean reward = 0.044 +/- 0.001\n",
      "Train Epoch: 91 [4096/194182 (2%)]\tLoss: 0.847925\tGrad Norm: 0.962401\tLR: 0.030000\n",
      "Train Epoch: 91 [24576/194182 (12%)]\tLoss: 0.860594\tGrad Norm: 0.983281\tLR: 0.030000\n",
      "Train Epoch: 91 [45056/194182 (23%)]\tLoss: 0.859188\tGrad Norm: 1.029150\tLR: 0.030000\n",
      "Train Epoch: 91 [65536/194182 (33%)]\tLoss: 0.870751\tGrad Norm: 1.181231\tLR: 0.030000\n",
      "Train Epoch: 91 [86016/194182 (44%)]\tLoss: 0.851522\tGrad Norm: 0.649384\tLR: 0.030000\n",
      "Train Epoch: 91 [106496/194182 (54%)]\tLoss: 0.850110\tGrad Norm: 1.127332\tLR: 0.030000\n",
      "Train Epoch: 91 [126976/194182 (65%)]\tLoss: 0.870486\tGrad Norm: 1.291402\tLR: 0.030000\n",
      "Train Epoch: 91 [147456/194182 (75%)]\tLoss: 0.869357\tGrad Norm: 1.763425\tLR: 0.030000\n",
      "Train Epoch: 91 [167936/194182 (85%)]\tLoss: 0.848739\tGrad Norm: 0.951852\tLR: 0.030000\n",
      "Train Epoch: 91 [188416/194182 (96%)]\tLoss: 0.839944\tGrad Norm: 0.932724\tLR: 0.030000\n",
      "Train set: Average loss: 0.8550\n",
      "Test set: Average loss: 0.4289, Average MAE: 0.5017\n",
      "Train Epoch: 92 [4096/194182 (2%)]\tLoss: 0.858671\tGrad Norm: 1.321855\tLR: 0.030000\n",
      "Train Epoch: 92 [24576/194182 (12%)]\tLoss: 0.856358\tGrad Norm: 1.628811\tLR: 0.030000\n",
      "Train Epoch: 92 [45056/194182 (23%)]\tLoss: 0.852907\tGrad Norm: 1.202590\tLR: 0.030000\n",
      "Train Epoch: 92 [65536/194182 (33%)]\tLoss: 0.855283\tGrad Norm: 1.224871\tLR: 0.030000\n",
      "Train Epoch: 92 [86016/194182 (44%)]\tLoss: 0.852150\tGrad Norm: 1.123839\tLR: 0.030000\n",
      "Train Epoch: 92 [106496/194182 (54%)]\tLoss: 0.844046\tGrad Norm: 1.115634\tLR: 0.030000\n",
      "Train Epoch: 92 [126976/194182 (65%)]\tLoss: 0.845842\tGrad Norm: 0.950040\tLR: 0.030000\n",
      "Train Epoch: 92 [147456/194182 (75%)]\tLoss: 0.844132\tGrad Norm: 0.918511\tLR: 0.030000\n",
      "Train Epoch: 92 [167936/194182 (85%)]\tLoss: 0.855469\tGrad Norm: 1.159180\tLR: 0.030000\n",
      "Train Epoch: 92 [188416/194182 (96%)]\tLoss: 0.845539\tGrad Norm: 0.960014\tLR: 0.030000\n",
      "Train set: Average loss: 0.8514\n",
      "Test set: Average loss: 0.4227, Average MAE: 0.4878\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 93 [4096/194182 (2%)]\tLoss: 0.852168\tGrad Norm: 1.109275\tLR: 0.030000\n",
      "Train Epoch: 93 [24576/194182 (12%)]\tLoss: 0.846060\tGrad Norm: 1.053746\tLR: 0.030000\n",
      "Train Epoch: 93 [45056/194182 (23%)]\tLoss: 0.843698\tGrad Norm: 1.048637\tLR: 0.030000\n",
      "Train Epoch: 93 [65536/194182 (33%)]\tLoss: 0.847815\tGrad Norm: 1.244201\tLR: 0.030000\n",
      "Train Epoch: 93 [86016/194182 (44%)]\tLoss: 0.848148\tGrad Norm: 1.181701\tLR: 0.030000\n",
      "Train Epoch: 93 [106496/194182 (54%)]\tLoss: 0.853909\tGrad Norm: 1.032028\tLR: 0.030000\n",
      "Train Epoch: 93 [126976/194182 (65%)]\tLoss: 0.844864\tGrad Norm: 1.397800\tLR: 0.030000\n",
      "Train Epoch: 93 [147456/194182 (75%)]\tLoss: 0.846226\tGrad Norm: 1.163677\tLR: 0.030000\n",
      "Train Epoch: 93 [167936/194182 (85%)]\tLoss: 0.844130\tGrad Norm: 0.791861\tLR: 0.030000\n",
      "Train Epoch: 93 [188416/194182 (96%)]\tLoss: 0.837951\tGrad Norm: 1.226384\tLR: 0.030000\n",
      "Train set: Average loss: 0.8477\n",
      "Test set: Average loss: 0.4225, Average MAE: 0.4866\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 94 [4096/194182 (2%)]\tLoss: 0.847653\tGrad Norm: 1.272238\tLR: 0.030000\n",
      "Train Epoch: 94 [24576/194182 (12%)]\tLoss: 0.858071\tGrad Norm: 0.960798\tLR: 0.030000\n",
      "Train Epoch: 94 [45056/194182 (23%)]\tLoss: 0.838452\tGrad Norm: 0.743968\tLR: 0.030000\n",
      "Train Epoch: 94 [65536/194182 (33%)]\tLoss: 0.836743\tGrad Norm: 0.943402\tLR: 0.030000\n",
      "Train Epoch: 94 [86016/194182 (44%)]\tLoss: 0.837978\tGrad Norm: 1.205381\tLR: 0.030000\n",
      "Train Epoch: 94 [106496/194182 (54%)]\tLoss: 0.843757\tGrad Norm: 1.657905\tLR: 0.030000\n",
      "Train Epoch: 94 [126976/194182 (65%)]\tLoss: 0.834334\tGrad Norm: 1.157093\tLR: 0.030000\n",
      "Train Epoch: 94 [147456/194182 (75%)]\tLoss: 0.843447\tGrad Norm: 1.176271\tLR: 0.030000\n",
      "Train Epoch: 94 [167936/194182 (85%)]\tLoss: 0.842951\tGrad Norm: 0.993249\tLR: 0.030000\n",
      "Train Epoch: 94 [188416/194182 (96%)]\tLoss: 0.847558\tGrad Norm: 1.462826\tLR: 0.030000\n",
      "Train set: Average loss: 0.8431\n",
      "Test set: Average loss: 0.4179, Average MAE: 0.4969\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 95 [4096/194182 (2%)]\tLoss: 0.836479\tGrad Norm: 1.227227\tLR: 0.030000\n",
      "Train Epoch: 95 [24576/194182 (12%)]\tLoss: 0.847910\tGrad Norm: 1.052657\tLR: 0.030000\n",
      "Train Epoch: 95 [45056/194182 (23%)]\tLoss: 0.847957\tGrad Norm: 1.303711\tLR: 0.030000\n",
      "Train Epoch: 95 [65536/194182 (33%)]\tLoss: 0.833079\tGrad Norm: 1.171155\tLR: 0.030000\n",
      "Train Epoch: 95 [86016/194182 (44%)]\tLoss: 0.846989\tGrad Norm: 1.569617\tLR: 0.030000\n",
      "Train Epoch: 95 [106496/194182 (54%)]\tLoss: 0.844484\tGrad Norm: 0.582643\tLR: 0.030000\n",
      "Train Epoch: 95 [126976/194182 (65%)]\tLoss: 0.832643\tGrad Norm: 0.721546\tLR: 0.030000\n",
      "Train Epoch: 95 [147456/194182 (75%)]\tLoss: 0.835982\tGrad Norm: 1.065956\tLR: 0.030000\n",
      "Train Epoch: 95 [167936/194182 (85%)]\tLoss: 0.841631\tGrad Norm: 0.928414\tLR: 0.030000\n",
      "Train Epoch: 95 [188416/194182 (96%)]\tLoss: 0.833078\tGrad Norm: 1.078928\tLR: 0.030000\n",
      "Train set: Average loss: 0.8396\n",
      "Test set: Average loss: 0.4102, Average MAE: 0.4902\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 95: Mean reward = 0.044 +/- 0.001\n",
      "Train Epoch: 96 [4096/194182 (2%)]\tLoss: 0.836667\tGrad Norm: 0.893988\tLR: 0.030000\n",
      "Train Epoch: 96 [24576/194182 (12%)]\tLoss: 0.830778\tGrad Norm: 1.221001\tLR: 0.030000\n",
      "Train Epoch: 96 [45056/194182 (23%)]\tLoss: 0.840409\tGrad Norm: 1.209407\tLR: 0.030000\n",
      "Train Epoch: 96 [65536/194182 (33%)]\tLoss: 0.846534\tGrad Norm: 1.156285\tLR: 0.030000\n",
      "Train Epoch: 96 [86016/194182 (44%)]\tLoss: 0.829642\tGrad Norm: 1.016412\tLR: 0.030000\n",
      "Train Epoch: 96 [106496/194182 (54%)]\tLoss: 0.834346\tGrad Norm: 1.164701\tLR: 0.030000\n",
      "Train Epoch: 96 [126976/194182 (65%)]\tLoss: 0.841869\tGrad Norm: 1.236893\tLR: 0.030000\n",
      "Train Epoch: 96 [147456/194182 (75%)]\tLoss: 0.842403\tGrad Norm: 1.401343\tLR: 0.030000\n",
      "Train Epoch: 96 [167936/194182 (85%)]\tLoss: 0.830435\tGrad Norm: 0.929353\tLR: 0.030000\n",
      "Train Epoch: 96 [188416/194182 (96%)]\tLoss: 0.826997\tGrad Norm: 1.125606\tLR: 0.030000\n",
      "Train set: Average loss: 0.8358\n",
      "Test set: Average loss: 0.4143, Average MAE: 0.5045\n",
      "Train Epoch: 97 [4096/194182 (2%)]\tLoss: 0.839515\tGrad Norm: 1.425096\tLR: 0.030000\n",
      "Train Epoch: 97 [24576/194182 (12%)]\tLoss: 0.851032\tGrad Norm: 1.447631\tLR: 0.030000\n",
      "Train Epoch: 97 [45056/194182 (23%)]\tLoss: 0.827374\tGrad Norm: 1.006465\tLR: 0.030000\n",
      "Train Epoch: 97 [65536/194182 (33%)]\tLoss: 0.829829\tGrad Norm: 1.311261\tLR: 0.030000\n",
      "Train Epoch: 97 [86016/194182 (44%)]\tLoss: 0.816996\tGrad Norm: 0.939187\tLR: 0.030000\n",
      "Train Epoch: 97 [106496/194182 (54%)]\tLoss: 0.820960\tGrad Norm: 0.891769\tLR: 0.030000\n",
      "Train Epoch: 97 [126976/194182 (65%)]\tLoss: 0.839457\tGrad Norm: 1.344443\tLR: 0.030000\n",
      "Train Epoch: 97 [147456/194182 (75%)]\tLoss: 0.831703\tGrad Norm: 1.170928\tLR: 0.030000\n",
      "Train Epoch: 97 [167936/194182 (85%)]\tLoss: 0.827873\tGrad Norm: 1.240633\tLR: 0.030000\n",
      "Train Epoch: 97 [188416/194182 (96%)]\tLoss: 0.834547\tGrad Norm: 1.276470\tLR: 0.030000\n",
      "Train set: Average loss: 0.8325\n",
      "Test set: Average loss: 0.4069, Average MAE: 0.4871\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 98 [4096/194182 (2%)]\tLoss: 0.833222\tGrad Norm: 1.252699\tLR: 0.030000\n",
      "Train Epoch: 98 [24576/194182 (12%)]\tLoss: 0.824966\tGrad Norm: 0.893051\tLR: 0.030000\n",
      "Train Epoch: 98 [45056/194182 (23%)]\tLoss: 0.825420\tGrad Norm: 0.950158\tLR: 0.030000\n",
      "Train Epoch: 98 [65536/194182 (33%)]\tLoss: 0.841245\tGrad Norm: 1.537282\tLR: 0.030000\n",
      "Train Epoch: 98 [86016/194182 (44%)]\tLoss: 0.827916\tGrad Norm: 0.891462\tLR: 0.030000\n",
      "Train Epoch: 98 [106496/194182 (54%)]\tLoss: 0.838416\tGrad Norm: 0.877601\tLR: 0.030000\n",
      "Train Epoch: 98 [126976/194182 (65%)]\tLoss: 0.823463\tGrad Norm: 1.277180\tLR: 0.030000\n",
      "Train Epoch: 98 [147456/194182 (75%)]\tLoss: 0.830391\tGrad Norm: 1.384656\tLR: 0.030000\n",
      "Train Epoch: 98 [167936/194182 (85%)]\tLoss: 0.837704\tGrad Norm: 1.212782\tLR: 0.030000\n",
      "Train Epoch: 98 [188416/194182 (96%)]\tLoss: 0.836482\tGrad Norm: 1.007828\tLR: 0.030000\n",
      "Train set: Average loss: 0.8293\n",
      "Test set: Average loss: 0.4044, Average MAE: 0.4719\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 99 [4096/194182 (2%)]\tLoss: 0.812937\tGrad Norm: 1.144189\tLR: 0.030000\n",
      "Train Epoch: 99 [24576/194182 (12%)]\tLoss: 0.818892\tGrad Norm: 0.762984\tLR: 0.030000\n",
      "Train Epoch: 99 [45056/194182 (23%)]\tLoss: 0.824937\tGrad Norm: 1.083327\tLR: 0.030000\n",
      "Train Epoch: 99 [65536/194182 (33%)]\tLoss: 0.834616\tGrad Norm: 1.016387\tLR: 0.030000\n",
      "Train Epoch: 99 [86016/194182 (44%)]\tLoss: 0.825010\tGrad Norm: 1.273812\tLR: 0.030000\n",
      "Train Epoch: 99 [106496/194182 (54%)]\tLoss: 0.833627\tGrad Norm: 1.194276\tLR: 0.030000\n",
      "Train Epoch: 99 [126976/194182 (65%)]\tLoss: 0.824502\tGrad Norm: 1.066156\tLR: 0.030000\n",
      "Train Epoch: 99 [147456/194182 (75%)]\tLoss: 0.826352\tGrad Norm: 1.191907\tLR: 0.030000\n",
      "Train Epoch: 99 [167936/194182 (85%)]\tLoss: 0.819576\tGrad Norm: 1.059978\tLR: 0.030000\n",
      "Train Epoch: 99 [188416/194182 (96%)]\tLoss: 0.831897\tGrad Norm: 1.255436\tLR: 0.030000\n",
      "Train set: Average loss: 0.8247\n",
      "Test set: Average loss: 0.4026, Average MAE: 0.4907\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 100 [4096/194182 (2%)]\tLoss: 0.820462\tGrad Norm: 1.137870\tLR: 0.030000\n",
      "Train Epoch: 100 [24576/194182 (12%)]\tLoss: 0.835390\tGrad Norm: 1.431765\tLR: 0.030000\n",
      "Train Epoch: 100 [45056/194182 (23%)]\tLoss: 0.828316\tGrad Norm: 1.333521\tLR: 0.030000\n",
      "Train Epoch: 100 [65536/194182 (33%)]\tLoss: 0.823317\tGrad Norm: 1.270709\tLR: 0.030000\n",
      "Train Epoch: 100 [86016/194182 (44%)]\tLoss: 0.831710\tGrad Norm: 1.137444\tLR: 0.030000\n",
      "Train Epoch: 100 [106496/194182 (54%)]\tLoss: 0.814462\tGrad Norm: 1.417381\tLR: 0.030000\n",
      "Train Epoch: 100 [126976/194182 (65%)]\tLoss: 0.832188\tGrad Norm: 1.225858\tLR: 0.030000\n",
      "Train Epoch: 100 [147456/194182 (75%)]\tLoss: 0.824690\tGrad Norm: 1.467322\tLR: 0.030000\n",
      "Train Epoch: 100 [167936/194182 (85%)]\tLoss: 0.819406\tGrad Norm: 1.137092\tLR: 0.030000\n",
      "Train Epoch: 100 [188416/194182 (96%)]\tLoss: 0.823590\tGrad Norm: 1.170648\tLR: 0.030000\n",
      "Train set: Average loss: 0.8240\n",
      "Test set: Average loss: 0.3957, Average MAE: 0.4809\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 100: Mean reward = 0.044 +/- 0.001\n",
      "Train Epoch: 101 [4096/194182 (2%)]\tLoss: 0.814768\tGrad Norm: 0.866590\tLR: 0.030000\n",
      "Train Epoch: 101 [24576/194182 (12%)]\tLoss: 0.817361\tGrad Norm: 1.023788\tLR: 0.030000\n",
      "Train Epoch: 101 [45056/194182 (23%)]\tLoss: 0.825655\tGrad Norm: 1.049662\tLR: 0.030000\n",
      "Train Epoch: 101 [65536/194182 (33%)]\tLoss: 0.816278\tGrad Norm: 1.246278\tLR: 0.030000\n",
      "Train Epoch: 101 [86016/194182 (44%)]\tLoss: 0.809811\tGrad Norm: 0.949512\tLR: 0.030000\n",
      "Train Epoch: 101 [106496/194182 (54%)]\tLoss: 0.817532\tGrad Norm: 0.877183\tLR: 0.030000\n",
      "Train Epoch: 101 [126976/194182 (65%)]\tLoss: 0.822238\tGrad Norm: 1.371213\tLR: 0.030000\n",
      "Train Epoch: 101 [147456/194182 (75%)]\tLoss: 0.812228\tGrad Norm: 1.152349\tLR: 0.030000\n",
      "Train Epoch: 101 [167936/194182 (85%)]\tLoss: 0.811576\tGrad Norm: 1.115124\tLR: 0.030000\n",
      "Train Epoch: 101 [188416/194182 (96%)]\tLoss: 0.816981\tGrad Norm: 1.087956\tLR: 0.030000\n",
      "Train set: Average loss: 0.8176\n",
      "Test set: Average loss: 0.3932, Average MAE: 0.4755\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 102 [4096/194182 (2%)]\tLoss: 0.821442\tGrad Norm: 0.995839\tLR: 0.030000\n",
      "Train Epoch: 102 [24576/194182 (12%)]\tLoss: 0.824917\tGrad Norm: 1.235682\tLR: 0.030000\n",
      "Train Epoch: 102 [45056/194182 (23%)]\tLoss: 0.815111\tGrad Norm: 1.267329\tLR: 0.030000\n",
      "Train Epoch: 102 [65536/194182 (33%)]\tLoss: 0.821644\tGrad Norm: 1.429820\tLR: 0.030000\n",
      "Train Epoch: 102 [86016/194182 (44%)]\tLoss: 0.816899\tGrad Norm: 1.254104\tLR: 0.030000\n",
      "Train Epoch: 102 [106496/194182 (54%)]\tLoss: 0.825345\tGrad Norm: 1.189199\tLR: 0.030000\n",
      "Train Epoch: 102 [126976/194182 (65%)]\tLoss: 0.811605\tGrad Norm: 1.404682\tLR: 0.030000\n",
      "Train Epoch: 102 [147456/194182 (75%)]\tLoss: 0.819367\tGrad Norm: 1.406360\tLR: 0.030000\n",
      "Train Epoch: 102 [167936/194182 (85%)]\tLoss: 0.811633\tGrad Norm: 1.263579\tLR: 0.030000\n",
      "Train Epoch: 102 [188416/194182 (96%)]\tLoss: 0.807919\tGrad Norm: 0.808496\tLR: 0.030000\n",
      "Train set: Average loss: 0.8160\n",
      "Test set: Average loss: 0.3907, Average MAE: 0.4704\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 103 [4096/194182 (2%)]\tLoss: 0.802518\tGrad Norm: 1.101623\tLR: 0.030000\n",
      "Train Epoch: 103 [24576/194182 (12%)]\tLoss: 0.824414\tGrad Norm: 1.231186\tLR: 0.030000\n",
      "Train Epoch: 103 [45056/194182 (23%)]\tLoss: 0.798865\tGrad Norm: 1.374899\tLR: 0.030000\n",
      "Train Epoch: 103 [65536/194182 (33%)]\tLoss: 0.800181\tGrad Norm: 0.771024\tLR: 0.030000\n",
      "Train Epoch: 103 [86016/194182 (44%)]\tLoss: 0.817149\tGrad Norm: 0.925141\tLR: 0.030000\n",
      "Train Epoch: 103 [106496/194182 (54%)]\tLoss: 0.813559\tGrad Norm: 1.292299\tLR: 0.030000\n",
      "Train Epoch: 103 [126976/194182 (65%)]\tLoss: 0.817913\tGrad Norm: 1.542199\tLR: 0.030000\n",
      "Train Epoch: 103 [147456/194182 (75%)]\tLoss: 0.805276\tGrad Norm: 1.164053\tLR: 0.030000\n",
      "Train Epoch: 103 [167936/194182 (85%)]\tLoss: 0.805967\tGrad Norm: 1.428869\tLR: 0.030000\n",
      "Train Epoch: 103 [188416/194182 (96%)]\tLoss: 0.813418\tGrad Norm: 0.874747\tLR: 0.030000\n",
      "Train set: Average loss: 0.8127\n",
      "Test set: Average loss: 0.3896, Average MAE: 0.4726\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 104 [4096/194182 (2%)]\tLoss: 0.801045\tGrad Norm: 1.310269\tLR: 0.030000\n",
      "Train Epoch: 104 [24576/194182 (12%)]\tLoss: 0.802380\tGrad Norm: 0.765842\tLR: 0.030000\n",
      "Train Epoch: 104 [45056/194182 (23%)]\tLoss: 0.806692\tGrad Norm: 0.757159\tLR: 0.030000\n",
      "Train Epoch: 104 [65536/194182 (33%)]\tLoss: 0.807104\tGrad Norm: 1.136847\tLR: 0.030000\n",
      "Train Epoch: 104 [86016/194182 (44%)]\tLoss: 0.810240\tGrad Norm: 1.164371\tLR: 0.030000\n",
      "Train Epoch: 104 [106496/194182 (54%)]\tLoss: 0.818350\tGrad Norm: 1.374149\tLR: 0.030000\n",
      "Train Epoch: 104 [126976/194182 (65%)]\tLoss: 0.818142\tGrad Norm: 1.129072\tLR: 0.030000\n",
      "Train Epoch: 104 [147456/194182 (75%)]\tLoss: 0.803737\tGrad Norm: 0.818324\tLR: 0.030000\n",
      "Train Epoch: 104 [167936/194182 (85%)]\tLoss: 0.812021\tGrad Norm: 1.354267\tLR: 0.030000\n",
      "Train Epoch: 104 [188416/194182 (96%)]\tLoss: 0.801941\tGrad Norm: 0.933199\tLR: 0.030000\n",
      "Train set: Average loss: 0.8080\n",
      "Test set: Average loss: 0.3869, Average MAE: 0.4632\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 105 [4096/194182 (2%)]\tLoss: 0.812100\tGrad Norm: 1.040199\tLR: 0.030000\n",
      "Train Epoch: 105 [24576/194182 (12%)]\tLoss: 0.801793\tGrad Norm: 1.058931\tLR: 0.030000\n",
      "Train Epoch: 105 [45056/194182 (23%)]\tLoss: 0.797876\tGrad Norm: 1.025245\tLR: 0.030000\n",
      "Train Epoch: 105 [65536/194182 (33%)]\tLoss: 0.811743\tGrad Norm: 1.408867\tLR: 0.030000\n",
      "Train Epoch: 105 [86016/194182 (44%)]\tLoss: 0.813679\tGrad Norm: 1.207457\tLR: 0.030000\n",
      "Train Epoch: 105 [106496/194182 (54%)]\tLoss: 0.802641\tGrad Norm: 0.953753\tLR: 0.030000\n",
      "Train Epoch: 105 [126976/194182 (65%)]\tLoss: 0.807674\tGrad Norm: 1.511485\tLR: 0.030000\n",
      "Train Epoch: 105 [147456/194182 (75%)]\tLoss: 0.803994\tGrad Norm: 0.874549\tLR: 0.030000\n",
      "Train Epoch: 105 [167936/194182 (85%)]\tLoss: 0.799303\tGrad Norm: 0.861865\tLR: 0.030000\n",
      "Train Epoch: 105 [188416/194182 (96%)]\tLoss: 0.795724\tGrad Norm: 1.238536\tLR: 0.030000\n",
      "Train set: Average loss: 0.8058\n",
      "Test set: Average loss: 0.3840, Average MAE: 0.4684\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 105: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 106 [4096/194182 (2%)]\tLoss: 0.798562\tGrad Norm: 0.863102\tLR: 0.030000\n",
      "Train Epoch: 106 [24576/194182 (12%)]\tLoss: 0.814731\tGrad Norm: 1.383785\tLR: 0.030000\n",
      "Train Epoch: 106 [45056/194182 (23%)]\tLoss: 0.805701\tGrad Norm: 1.351166\tLR: 0.030000\n",
      "Train Epoch: 106 [65536/194182 (33%)]\tLoss: 0.812255\tGrad Norm: 1.414145\tLR: 0.030000\n",
      "Train Epoch: 106 [86016/194182 (44%)]\tLoss: 0.809242\tGrad Norm: 1.190158\tLR: 0.030000\n",
      "Train Epoch: 106 [106496/194182 (54%)]\tLoss: 0.806509\tGrad Norm: 0.792844\tLR: 0.030000\n",
      "Train Epoch: 106 [126976/194182 (65%)]\tLoss: 0.798536\tGrad Norm: 0.892344\tLR: 0.030000\n",
      "Train Epoch: 106 [147456/194182 (75%)]\tLoss: 0.806320\tGrad Norm: 1.048478\tLR: 0.030000\n",
      "Train Epoch: 106 [167936/194182 (85%)]\tLoss: 0.796415\tGrad Norm: 1.018154\tLR: 0.030000\n",
      "Train Epoch: 106 [188416/194182 (96%)]\tLoss: 0.799500\tGrad Norm: 1.307319\tLR: 0.030000\n",
      "Train set: Average loss: 0.8022\n",
      "Test set: Average loss: 0.3886, Average MAE: 0.4574\n",
      "Train Epoch: 107 [4096/194182 (2%)]\tLoss: 0.819239\tGrad Norm: 1.575810\tLR: 0.030000\n",
      "Train Epoch: 107 [24576/194182 (12%)]\tLoss: 0.791659\tGrad Norm: 1.288756\tLR: 0.030000\n",
      "Train Epoch: 107 [45056/194182 (23%)]\tLoss: 0.798578\tGrad Norm: 1.180799\tLR: 0.030000\n",
      "Train Epoch: 107 [65536/194182 (33%)]\tLoss: 0.804200\tGrad Norm: 1.025819\tLR: 0.030000\n",
      "Train Epoch: 107 [86016/194182 (44%)]\tLoss: 0.797677\tGrad Norm: 0.554462\tLR: 0.030000\n",
      "Train Epoch: 107 [106496/194182 (54%)]\tLoss: 0.794610\tGrad Norm: 0.737867\tLR: 0.030000\n",
      "Train Epoch: 107 [126976/194182 (65%)]\tLoss: 0.800935\tGrad Norm: 0.754513\tLR: 0.030000\n",
      "Train Epoch: 107 [147456/194182 (75%)]\tLoss: 0.791642\tGrad Norm: 0.890747\tLR: 0.030000\n",
      "Train Epoch: 107 [167936/194182 (85%)]\tLoss: 0.791954\tGrad Norm: 0.956330\tLR: 0.030000\n",
      "Train Epoch: 107 [188416/194182 (96%)]\tLoss: 0.804465\tGrad Norm: 0.959157\tLR: 0.030000\n",
      "Train set: Average loss: 0.7994\n",
      "Test set: Average loss: 0.3859, Average MAE: 0.4798\n",
      "Train Epoch: 108 [4096/194182 (2%)]\tLoss: 0.802352\tGrad Norm: 1.509735\tLR: 0.030000\n",
      "Train Epoch: 108 [24576/194182 (12%)]\tLoss: 0.797993\tGrad Norm: 0.983303\tLR: 0.030000\n",
      "Train Epoch: 108 [45056/194182 (23%)]\tLoss: 0.799215\tGrad Norm: 1.417137\tLR: 0.030000\n",
      "Train Epoch: 108 [65536/194182 (33%)]\tLoss: 0.810347\tGrad Norm: 1.383092\tLR: 0.030000\n",
      "Train Epoch: 108 [86016/194182 (44%)]\tLoss: 0.795259\tGrad Norm: 0.957412\tLR: 0.030000\n",
      "Train Epoch: 108 [106496/194182 (54%)]\tLoss: 0.794412\tGrad Norm: 1.014095\tLR: 0.030000\n",
      "Train Epoch: 108 [126976/194182 (65%)]\tLoss: 0.789726\tGrad Norm: 1.217359\tLR: 0.030000\n",
      "Train Epoch: 108 [147456/194182 (75%)]\tLoss: 0.789638\tGrad Norm: 1.181359\tLR: 0.030000\n",
      "Train Epoch: 108 [167936/194182 (85%)]\tLoss: 0.789922\tGrad Norm: 1.007574\tLR: 0.030000\n",
      "Train Epoch: 108 [188416/194182 (96%)]\tLoss: 0.786588\tGrad Norm: 1.125945\tLR: 0.030000\n",
      "Train set: Average loss: 0.7979\n",
      "Test set: Average loss: 0.3799, Average MAE: 0.4566\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 109 [4096/194182 (2%)]\tLoss: 0.781809\tGrad Norm: 1.191354\tLR: 0.030000\n",
      "Train Epoch: 109 [24576/194182 (12%)]\tLoss: 0.807894\tGrad Norm: 1.402031\tLR: 0.030000\n",
      "Train Epoch: 109 [45056/194182 (23%)]\tLoss: 0.798806\tGrad Norm: 1.205561\tLR: 0.030000\n",
      "Train Epoch: 109 [65536/194182 (33%)]\tLoss: 0.799737\tGrad Norm: 1.370050\tLR: 0.030000\n",
      "Train Epoch: 109 [86016/194182 (44%)]\tLoss: 0.781912\tGrad Norm: 0.816642\tLR: 0.030000\n",
      "Train Epoch: 109 [106496/194182 (54%)]\tLoss: 0.782953\tGrad Norm: 0.633025\tLR: 0.030000\n",
      "Train Epoch: 109 [126976/194182 (65%)]\tLoss: 0.791240\tGrad Norm: 1.617787\tLR: 0.030000\n",
      "Train Epoch: 109 [147456/194182 (75%)]\tLoss: 0.794627\tGrad Norm: 0.894569\tLR: 0.030000\n",
      "Train Epoch: 109 [167936/194182 (85%)]\tLoss: 0.798509\tGrad Norm: 1.223734\tLR: 0.030000\n",
      "Train Epoch: 109 [188416/194182 (96%)]\tLoss: 0.801918\tGrad Norm: 1.094712\tLR: 0.030000\n",
      "Train set: Average loss: 0.7938\n",
      "Test set: Average loss: 0.3761, Average MAE: 0.4576\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 110 [4096/194182 (2%)]\tLoss: 0.785231\tGrad Norm: 1.173724\tLR: 0.030000\n",
      "Train Epoch: 110 [24576/194182 (12%)]\tLoss: 0.792312\tGrad Norm: 1.129440\tLR: 0.030000\n",
      "Train Epoch: 110 [45056/194182 (23%)]\tLoss: 0.786317\tGrad Norm: 0.636321\tLR: 0.030000\n",
      "Train Epoch: 110 [65536/194182 (33%)]\tLoss: 0.788536\tGrad Norm: 0.820569\tLR: 0.030000\n",
      "Train Epoch: 110 [86016/194182 (44%)]\tLoss: 0.792279\tGrad Norm: 0.998655\tLR: 0.030000\n",
      "Train Epoch: 110 [106496/194182 (54%)]\tLoss: 0.781477\tGrad Norm: 1.323329\tLR: 0.030000\n",
      "Train Epoch: 110 [126976/194182 (65%)]\tLoss: 0.803902\tGrad Norm: 1.027102\tLR: 0.030000\n",
      "Train Epoch: 110 [147456/194182 (75%)]\tLoss: 0.790163\tGrad Norm: 0.587165\tLR: 0.030000\n",
      "Train Epoch: 110 [167936/194182 (85%)]\tLoss: 0.782078\tGrad Norm: 0.900390\tLR: 0.030000\n",
      "Train Epoch: 110 [188416/194182 (96%)]\tLoss: 0.787363\tGrad Norm: 1.203074\tLR: 0.030000\n",
      "Train set: Average loss: 0.7896\n",
      "Test set: Average loss: 0.3745, Average MAE: 0.4643\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 110: Mean reward = 0.044 +/- 0.001\n",
      "Train Epoch: 111 [4096/194182 (2%)]\tLoss: 0.782009\tGrad Norm: 0.898793\tLR: 0.030000\n",
      "Train Epoch: 111 [24576/194182 (12%)]\tLoss: 0.783826\tGrad Norm: 0.577240\tLR: 0.030000\n",
      "Train Epoch: 111 [45056/194182 (23%)]\tLoss: 0.785542\tGrad Norm: 0.710650\tLR: 0.030000\n",
      "Train Epoch: 111 [65536/194182 (33%)]\tLoss: 0.795858\tGrad Norm: 1.524927\tLR: 0.030000\n",
      "Train Epoch: 111 [86016/194182 (44%)]\tLoss: 0.786834\tGrad Norm: 1.508302\tLR: 0.030000\n",
      "Train Epoch: 111 [106496/194182 (54%)]\tLoss: 0.796589\tGrad Norm: 0.903825\tLR: 0.030000\n",
      "Train Epoch: 111 [126976/194182 (65%)]\tLoss: 0.779757\tGrad Norm: 1.135652\tLR: 0.030000\n",
      "Train Epoch: 111 [147456/194182 (75%)]\tLoss: 0.787415\tGrad Norm: 1.233050\tLR: 0.030000\n",
      "Train Epoch: 111 [167936/194182 (85%)]\tLoss: 0.798572\tGrad Norm: 1.188113\tLR: 0.030000\n",
      "Train Epoch: 111 [188416/194182 (96%)]\tLoss: 0.795143\tGrad Norm: 1.425071\tLR: 0.030000\n",
      "Train set: Average loss: 0.7886\n",
      "Test set: Average loss: 0.3772, Average MAE: 0.4775\n",
      "Train Epoch: 112 [4096/194182 (2%)]\tLoss: 0.804633\tGrad Norm: 1.217810\tLR: 0.030000\n",
      "Train Epoch: 112 [24576/194182 (12%)]\tLoss: 0.778835\tGrad Norm: 1.190043\tLR: 0.030000\n",
      "Train Epoch: 112 [45056/194182 (23%)]\tLoss: 0.797248\tGrad Norm: 1.523444\tLR: 0.030000\n",
      "Train Epoch: 112 [65536/194182 (33%)]\tLoss: 0.789228\tGrad Norm: 0.664262\tLR: 0.030000\n",
      "Train Epoch: 112 [86016/194182 (44%)]\tLoss: 0.785848\tGrad Norm: 0.764149\tLR: 0.030000\n",
      "Train Epoch: 112 [106496/194182 (54%)]\tLoss: 0.781842\tGrad Norm: 1.389271\tLR: 0.030000\n",
      "Train Epoch: 112 [126976/194182 (65%)]\tLoss: 0.785396\tGrad Norm: 1.082024\tLR: 0.030000\n",
      "Train Epoch: 112 [147456/194182 (75%)]\tLoss: 0.783102\tGrad Norm: 1.139305\tLR: 0.030000\n",
      "Train Epoch: 112 [167936/194182 (85%)]\tLoss: 0.781624\tGrad Norm: 0.676997\tLR: 0.030000\n",
      "Train Epoch: 112 [188416/194182 (96%)]\tLoss: 0.793332\tGrad Norm: 1.526962\tLR: 0.030000\n",
      "Train set: Average loss: 0.7855\n",
      "Test set: Average loss: 0.3709, Average MAE: 0.4613\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 113 [4096/194182 (2%)]\tLoss: 0.781921\tGrad Norm: 0.946312\tLR: 0.030000\n",
      "Train Epoch: 113 [24576/194182 (12%)]\tLoss: 0.774064\tGrad Norm: 1.109622\tLR: 0.030000\n",
      "Train Epoch: 113 [45056/194182 (23%)]\tLoss: 0.784355\tGrad Norm: 1.250085\tLR: 0.030000\n",
      "Train Epoch: 113 [65536/194182 (33%)]\tLoss: 0.791353\tGrad Norm: 1.124375\tLR: 0.030000\n",
      "Train Epoch: 113 [86016/194182 (44%)]\tLoss: 0.786332\tGrad Norm: 1.348421\tLR: 0.030000\n",
      "Train Epoch: 113 [106496/194182 (54%)]\tLoss: 0.789765\tGrad Norm: 1.323416\tLR: 0.030000\n",
      "Train Epoch: 113 [126976/194182 (65%)]\tLoss: 0.776068\tGrad Norm: 1.156862\tLR: 0.030000\n",
      "Train Epoch: 113 [147456/194182 (75%)]\tLoss: 0.786514\tGrad Norm: 1.071666\tLR: 0.030000\n",
      "Train Epoch: 113 [167936/194182 (85%)]\tLoss: 0.785974\tGrad Norm: 1.072890\tLR: 0.030000\n",
      "Train Epoch: 113 [188416/194182 (96%)]\tLoss: 0.790673\tGrad Norm: 1.103290\tLR: 0.030000\n",
      "Train set: Average loss: 0.7841\n",
      "Test set: Average loss: 0.3712, Average MAE: 0.4661\n",
      "Train Epoch: 114 [4096/194182 (2%)]\tLoss: 0.782303\tGrad Norm: 1.208684\tLR: 0.030000\n",
      "Train Epoch: 114 [24576/194182 (12%)]\tLoss: 0.775586\tGrad Norm: 1.160615\tLR: 0.030000\n",
      "Train Epoch: 114 [45056/194182 (23%)]\tLoss: 0.780949\tGrad Norm: 1.004132\tLR: 0.030000\n",
      "Train Epoch: 114 [65536/194182 (33%)]\tLoss: 0.781955\tGrad Norm: 1.501925\tLR: 0.030000\n",
      "Train Epoch: 114 [86016/194182 (44%)]\tLoss: 0.778497\tGrad Norm: 1.340606\tLR: 0.030000\n",
      "Train Epoch: 114 [106496/194182 (54%)]\tLoss: 0.770753\tGrad Norm: 1.131188\tLR: 0.030000\n",
      "Train Epoch: 114 [126976/194182 (65%)]\tLoss: 0.777539\tGrad Norm: 0.903218\tLR: 0.030000\n",
      "Train Epoch: 114 [147456/194182 (75%)]\tLoss: 0.777807\tGrad Norm: 1.047132\tLR: 0.030000\n",
      "Train Epoch: 114 [167936/194182 (85%)]\tLoss: 0.781268\tGrad Norm: 1.615071\tLR: 0.030000\n",
      "Train Epoch: 114 [188416/194182 (96%)]\tLoss: 0.781147\tGrad Norm: 1.435072\tLR: 0.030000\n",
      "Train set: Average loss: 0.7815\n",
      "Test set: Average loss: 0.3646, Average MAE: 0.4504\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 115 [4096/194182 (2%)]\tLoss: 0.772217\tGrad Norm: 1.010679\tLR: 0.030000\n",
      "Train Epoch: 115 [24576/194182 (12%)]\tLoss: 0.786089\tGrad Norm: 1.123712\tLR: 0.030000\n",
      "Train Epoch: 115 [45056/194182 (23%)]\tLoss: 0.768855\tGrad Norm: 0.740546\tLR: 0.030000\n",
      "Train Epoch: 115 [65536/194182 (33%)]\tLoss: 0.785159\tGrad Norm: 1.225516\tLR: 0.030000\n",
      "Train Epoch: 115 [86016/194182 (44%)]\tLoss: 0.783521\tGrad Norm: 1.221610\tLR: 0.030000\n",
      "Train Epoch: 115 [106496/194182 (54%)]\tLoss: 0.791866\tGrad Norm: 1.191818\tLR: 0.030000\n",
      "Train Epoch: 115 [126976/194182 (65%)]\tLoss: 0.783188\tGrad Norm: 1.305068\tLR: 0.030000\n",
      "Train Epoch: 115 [147456/194182 (75%)]\tLoss: 0.773155\tGrad Norm: 1.160366\tLR: 0.030000\n",
      "Train Epoch: 115 [167936/194182 (85%)]\tLoss: 0.763743\tGrad Norm: 0.802462\tLR: 0.030000\n",
      "Train Epoch: 115 [188416/194182 (96%)]\tLoss: 0.775763\tGrad Norm: 1.463068\tLR: 0.030000\n",
      "Train set: Average loss: 0.7787\n",
      "Test set: Average loss: 0.3685, Average MAE: 0.4657\n",
      "Epoch 115: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 116 [4096/194182 (2%)]\tLoss: 0.780737\tGrad Norm: 1.684313\tLR: 0.030000\n",
      "Train Epoch: 116 [24576/194182 (12%)]\tLoss: 0.783420\tGrad Norm: 0.811093\tLR: 0.030000\n",
      "Train Epoch: 116 [45056/194182 (23%)]\tLoss: 0.777168\tGrad Norm: 0.870851\tLR: 0.030000\n",
      "Train Epoch: 116 [65536/194182 (33%)]\tLoss: 0.784164\tGrad Norm: 0.950203\tLR: 0.030000\n",
      "Train Epoch: 116 [86016/194182 (44%)]\tLoss: 0.772175\tGrad Norm: 1.286955\tLR: 0.030000\n",
      "Train Epoch: 116 [106496/194182 (54%)]\tLoss: 0.777241\tGrad Norm: 1.440216\tLR: 0.030000\n",
      "Train Epoch: 116 [126976/194182 (65%)]\tLoss: 0.779370\tGrad Norm: 1.307086\tLR: 0.030000\n",
      "Train Epoch: 116 [147456/194182 (75%)]\tLoss: 0.766075\tGrad Norm: 0.879946\tLR: 0.030000\n",
      "Train Epoch: 116 [167936/194182 (85%)]\tLoss: 0.766012\tGrad Norm: 1.000184\tLR: 0.030000\n",
      "Train Epoch: 116 [188416/194182 (96%)]\tLoss: 0.772851\tGrad Norm: 1.181065\tLR: 0.030000\n",
      "Train set: Average loss: 0.7763\n",
      "Test set: Average loss: 0.3664, Average MAE: 0.4621\n",
      "Train Epoch: 117 [4096/194182 (2%)]\tLoss: 0.776881\tGrad Norm: 1.224879\tLR: 0.030000\n",
      "Train Epoch: 117 [24576/194182 (12%)]\tLoss: 0.779835\tGrad Norm: 0.881311\tLR: 0.030000\n",
      "Train Epoch: 117 [45056/194182 (23%)]\tLoss: 0.775924\tGrad Norm: 1.077174\tLR: 0.030000\n",
      "Train Epoch: 117 [65536/194182 (33%)]\tLoss: 0.775609\tGrad Norm: 0.764141\tLR: 0.030000\n",
      "Train Epoch: 117 [86016/194182 (44%)]\tLoss: 0.775587\tGrad Norm: 0.694263\tLR: 0.030000\n",
      "Train Epoch: 117 [106496/194182 (54%)]\tLoss: 0.768969\tGrad Norm: 1.171632\tLR: 0.030000\n",
      "Train Epoch: 117 [126976/194182 (65%)]\tLoss: 0.771983\tGrad Norm: 0.986343\tLR: 0.030000\n",
      "Train Epoch: 117 [147456/194182 (75%)]\tLoss: 0.772057\tGrad Norm: 1.179160\tLR: 0.030000\n",
      "Train Epoch: 117 [167936/194182 (85%)]\tLoss: 0.773840\tGrad Norm: 1.220351\tLR: 0.030000\n",
      "Train Epoch: 117 [188416/194182 (96%)]\tLoss: 0.780981\tGrad Norm: 0.934688\tLR: 0.030000\n",
      "Train set: Average loss: 0.7721\n",
      "Test set: Average loss: 0.3624, Average MAE: 0.4578\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 118 [4096/194182 (2%)]\tLoss: 0.770149\tGrad Norm: 1.024800\tLR: 0.030000\n",
      "Train Epoch: 118 [24576/194182 (12%)]\tLoss: 0.781868\tGrad Norm: 1.200334\tLR: 0.030000\n",
      "Train Epoch: 118 [45056/194182 (23%)]\tLoss: 0.784758\tGrad Norm: 1.382066\tLR: 0.030000\n",
      "Train Epoch: 118 [65536/194182 (33%)]\tLoss: 0.772094\tGrad Norm: 1.337167\tLR: 0.030000\n",
      "Train Epoch: 118 [86016/194182 (44%)]\tLoss: 0.768051\tGrad Norm: 0.838961\tLR: 0.030000\n",
      "Train Epoch: 118 [106496/194182 (54%)]\tLoss: 0.767148\tGrad Norm: 0.758024\tLR: 0.030000\n",
      "Train Epoch: 118 [126976/194182 (65%)]\tLoss: 0.759474\tGrad Norm: 0.491843\tLR: 0.030000\n",
      "Train Epoch: 118 [147456/194182 (75%)]\tLoss: 0.772039\tGrad Norm: 1.324399\tLR: 0.030000\n",
      "Train Epoch: 118 [167936/194182 (85%)]\tLoss: 0.775588\tGrad Norm: 1.602322\tLR: 0.030000\n",
      "Train Epoch: 118 [188416/194182 (96%)]\tLoss: 0.772853\tGrad Norm: 1.199485\tLR: 0.030000\n",
      "Train set: Average loss: 0.7722\n",
      "Test set: Average loss: 0.3581, Average MAE: 0.4507\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 119 [4096/194182 (2%)]\tLoss: 0.761058\tGrad Norm: 0.879405\tLR: 0.030000\n",
      "Train Epoch: 119 [24576/194182 (12%)]\tLoss: 0.773625\tGrad Norm: 0.625580\tLR: 0.030000\n",
      "Train Epoch: 119 [45056/194182 (23%)]\tLoss: 0.765019\tGrad Norm: 0.956687\tLR: 0.030000\n",
      "Train Epoch: 119 [65536/194182 (33%)]\tLoss: 0.765374\tGrad Norm: 1.084773\tLR: 0.030000\n",
      "Train Epoch: 119 [86016/194182 (44%)]\tLoss: 0.771976\tGrad Norm: 1.301518\tLR: 0.030000\n",
      "Train Epoch: 119 [106496/194182 (54%)]\tLoss: 0.770233\tGrad Norm: 1.165288\tLR: 0.030000\n",
      "Train Epoch: 119 [126976/194182 (65%)]\tLoss: 0.756884\tGrad Norm: 1.428399\tLR: 0.030000\n",
      "Train Epoch: 119 [147456/194182 (75%)]\tLoss: 0.775376\tGrad Norm: 1.020368\tLR: 0.030000\n",
      "Train Epoch: 119 [167936/194182 (85%)]\tLoss: 0.768978\tGrad Norm: 1.547643\tLR: 0.030000\n",
      "Train Epoch: 119 [188416/194182 (96%)]\tLoss: 0.774945\tGrad Norm: 1.148032\tLR: 0.030000\n",
      "Train set: Average loss: 0.7677\n",
      "Test set: Average loss: 0.3597, Average MAE: 0.4569\n",
      "Train Epoch: 120 [4096/194182 (2%)]\tLoss: 0.768180\tGrad Norm: 1.203918\tLR: 0.030000\n",
      "Train Epoch: 120 [24576/194182 (12%)]\tLoss: 0.773244\tGrad Norm: 1.227235\tLR: 0.030000\n",
      "Train Epoch: 120 [45056/194182 (23%)]\tLoss: 0.766178\tGrad Norm: 1.291797\tLR: 0.030000\n",
      "Train Epoch: 120 [65536/194182 (33%)]\tLoss: 0.764206\tGrad Norm: 0.834101\tLR: 0.030000\n",
      "Train Epoch: 120 [86016/194182 (44%)]\tLoss: 0.773844\tGrad Norm: 0.931939\tLR: 0.030000\n",
      "Train Epoch: 120 [106496/194182 (54%)]\tLoss: 0.769471\tGrad Norm: 0.989449\tLR: 0.030000\n",
      "Train Epoch: 120 [126976/194182 (65%)]\tLoss: 0.755188\tGrad Norm: 1.161273\tLR: 0.030000\n",
      "Train Epoch: 120 [147456/194182 (75%)]\tLoss: 0.776668\tGrad Norm: 1.341885\tLR: 0.030000\n",
      "Train Epoch: 120 [167936/194182 (85%)]\tLoss: 0.770330\tGrad Norm: 1.373763\tLR: 0.030000\n",
      "Train Epoch: 120 [188416/194182 (96%)]\tLoss: 0.762459\tGrad Norm: 0.944595\tLR: 0.030000\n",
      "Train set: Average loss: 0.7676\n",
      "Test set: Average loss: 0.3583, Average MAE: 0.4466\n",
      "Epoch 120: Mean reward = 0.044 +/- 0.001\n",
      "Train Epoch: 121 [4096/194182 (2%)]\tLoss: 0.763284\tGrad Norm: 1.273125\tLR: 0.030000\n",
      "Train Epoch: 121 [24576/194182 (12%)]\tLoss: 0.763917\tGrad Norm: 1.188770\tLR: 0.030000\n",
      "Train Epoch: 121 [45056/194182 (23%)]\tLoss: 0.763862\tGrad Norm: 1.098624\tLR: 0.030000\n",
      "Train Epoch: 121 [65536/194182 (33%)]\tLoss: 0.769187\tGrad Norm: 1.480181\tLR: 0.030000\n",
      "Train Epoch: 121 [86016/194182 (44%)]\tLoss: 0.764172\tGrad Norm: 1.206749\tLR: 0.030000\n",
      "Train Epoch: 121 [106496/194182 (54%)]\tLoss: 0.757891\tGrad Norm: 1.203237\tLR: 0.030000\n",
      "Train Epoch: 121 [126976/194182 (65%)]\tLoss: 0.769406\tGrad Norm: 0.895940\tLR: 0.030000\n",
      "Train Epoch: 121 [147456/194182 (75%)]\tLoss: 0.759020\tGrad Norm: 0.989215\tLR: 0.030000\n",
      "Train Epoch: 121 [167936/194182 (85%)]\tLoss: 0.763426\tGrad Norm: 1.227227\tLR: 0.030000\n",
      "Train Epoch: 121 [188416/194182 (96%)]\tLoss: 0.754879\tGrad Norm: 1.206936\tLR: 0.030000\n",
      "Train set: Average loss: 0.7653\n",
      "Test set: Average loss: 0.3571, Average MAE: 0.4589\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 122 [4096/194182 (2%)]\tLoss: 0.758805\tGrad Norm: 1.176430\tLR: 0.030000\n",
      "Train Epoch: 122 [24576/194182 (12%)]\tLoss: 0.766045\tGrad Norm: 1.119404\tLR: 0.030000\n",
      "Train Epoch: 122 [45056/194182 (23%)]\tLoss: 0.769585\tGrad Norm: 1.643866\tLR: 0.030000\n",
      "Train Epoch: 122 [65536/194182 (33%)]\tLoss: 0.756220\tGrad Norm: 0.945203\tLR: 0.030000\n",
      "Train Epoch: 122 [86016/194182 (44%)]\tLoss: 0.757613\tGrad Norm: 0.892911\tLR: 0.030000\n",
      "Train Epoch: 122 [106496/194182 (54%)]\tLoss: 0.759342\tGrad Norm: 0.989371\tLR: 0.030000\n",
      "Train Epoch: 122 [126976/194182 (65%)]\tLoss: 0.754264\tGrad Norm: 0.955756\tLR: 0.030000\n",
      "Train Epoch: 122 [147456/194182 (75%)]\tLoss: 0.752324\tGrad Norm: 0.971116\tLR: 0.030000\n",
      "Train Epoch: 122 [167936/194182 (85%)]\tLoss: 0.757227\tGrad Norm: 1.092636\tLR: 0.030000\n",
      "Train Epoch: 122 [188416/194182 (96%)]\tLoss: 0.753343\tGrad Norm: 0.779949\tLR: 0.030000\n",
      "Train set: Average loss: 0.7616\n",
      "Test set: Average loss: 0.3535, Average MAE: 0.4462\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 123 [4096/194182 (2%)]\tLoss: 0.752482\tGrad Norm: 1.180717\tLR: 0.030000\n",
      "Train Epoch: 123 [24576/194182 (12%)]\tLoss: 0.760210\tGrad Norm: 1.131871\tLR: 0.030000\n",
      "Train Epoch: 123 [45056/194182 (23%)]\tLoss: 0.757042\tGrad Norm: 0.927878\tLR: 0.030000\n",
      "Train Epoch: 123 [65536/194182 (33%)]\tLoss: 0.761579\tGrad Norm: 0.765793\tLR: 0.030000\n",
      "Train Epoch: 123 [86016/194182 (44%)]\tLoss: 0.752267\tGrad Norm: 1.259946\tLR: 0.030000\n",
      "Train Epoch: 123 [106496/194182 (54%)]\tLoss: 0.756293\tGrad Norm: 1.304205\tLR: 0.030000\n",
      "Train Epoch: 123 [126976/194182 (65%)]\tLoss: 0.759422\tGrad Norm: 0.824474\tLR: 0.030000\n",
      "Train Epoch: 123 [147456/194182 (75%)]\tLoss: 0.749487\tGrad Norm: 0.606389\tLR: 0.030000\n",
      "Train Epoch: 123 [167936/194182 (85%)]\tLoss: 0.743636\tGrad Norm: 1.556112\tLR: 0.030000\n",
      "Train Epoch: 123 [188416/194182 (96%)]\tLoss: 0.761374\tGrad Norm: 1.053001\tLR: 0.030000\n",
      "Train set: Average loss: 0.7595\n",
      "Test set: Average loss: 0.3504, Average MAE: 0.4521\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 124 [4096/194182 (2%)]\tLoss: 0.750075\tGrad Norm: 1.126574\tLR: 0.030000\n",
      "Train Epoch: 124 [24576/194182 (12%)]\tLoss: 0.767021\tGrad Norm: 1.353905\tLR: 0.030000\n",
      "Train Epoch: 124 [45056/194182 (23%)]\tLoss: 0.748599\tGrad Norm: 1.189805\tLR: 0.030000\n",
      "Train Epoch: 124 [65536/194182 (33%)]\tLoss: 0.755428\tGrad Norm: 0.942715\tLR: 0.030000\n",
      "Train Epoch: 124 [86016/194182 (44%)]\tLoss: 0.750445\tGrad Norm: 0.798217\tLR: 0.030000\n",
      "Train Epoch: 124 [106496/194182 (54%)]\tLoss: 0.749954\tGrad Norm: 0.834497\tLR: 0.030000\n",
      "Train Epoch: 124 [126976/194182 (65%)]\tLoss: 0.755606\tGrad Norm: 0.976158\tLR: 0.030000\n",
      "Train Epoch: 124 [147456/194182 (75%)]\tLoss: 0.767419\tGrad Norm: 1.257509\tLR: 0.030000\n",
      "Train Epoch: 124 [167936/194182 (85%)]\tLoss: 0.766289\tGrad Norm: 1.200615\tLR: 0.030000\n",
      "Train Epoch: 124 [188416/194182 (96%)]\tLoss: 0.762839\tGrad Norm: 0.912712\tLR: 0.030000\n",
      "Train set: Average loss: 0.7577\n",
      "Test set: Average loss: 0.3520, Average MAE: 0.4386\n",
      "Train Epoch: 125 [4096/194182 (2%)]\tLoss: 0.744187\tGrad Norm: 1.258410\tLR: 0.030000\n",
      "Train Epoch: 125 [24576/194182 (12%)]\tLoss: 0.755990\tGrad Norm: 0.721426\tLR: 0.030000\n",
      "Train Epoch: 125 [45056/194182 (23%)]\tLoss: 0.762273\tGrad Norm: 1.235656\tLR: 0.030000\n",
      "Train Epoch: 125 [65536/194182 (33%)]\tLoss: 0.761667\tGrad Norm: 1.322233\tLR: 0.030000\n",
      "Train Epoch: 125 [86016/194182 (44%)]\tLoss: 0.756438\tGrad Norm: 1.047024\tLR: 0.030000\n",
      "Train Epoch: 125 [106496/194182 (54%)]\tLoss: 0.754855\tGrad Norm: 1.156904\tLR: 0.030000\n",
      "Train Epoch: 125 [126976/194182 (65%)]\tLoss: 0.749654\tGrad Norm: 1.053627\tLR: 0.030000\n",
      "Train Epoch: 125 [147456/194182 (75%)]\tLoss: 0.752996\tGrad Norm: 1.111879\tLR: 0.030000\n",
      "Train Epoch: 125 [167936/194182 (85%)]\tLoss: 0.756570\tGrad Norm: 0.838625\tLR: 0.030000\n",
      "Train Epoch: 125 [188416/194182 (96%)]\tLoss: 0.757403\tGrad Norm: 1.066731\tLR: 0.030000\n",
      "Train set: Average loss: 0.7551\n",
      "Test set: Average loss: 0.3556, Average MAE: 0.4553\n",
      "Epoch 125: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 126 [4096/194182 (2%)]\tLoss: 0.753842\tGrad Norm: 1.528366\tLR: 0.030000\n",
      "Train Epoch: 126 [24576/194182 (12%)]\tLoss: 0.756721\tGrad Norm: 1.164056\tLR: 0.030000\n",
      "Train Epoch: 126 [45056/194182 (23%)]\tLoss: 0.752620\tGrad Norm: 0.904998\tLR: 0.030000\n",
      "Train Epoch: 126 [65536/194182 (33%)]\tLoss: 0.748783\tGrad Norm: 0.885180\tLR: 0.030000\n",
      "Train Epoch: 126 [86016/194182 (44%)]\tLoss: 0.755973\tGrad Norm: 0.968844\tLR: 0.030000\n",
      "Train Epoch: 126 [106496/194182 (54%)]\tLoss: 0.752634\tGrad Norm: 0.774155\tLR: 0.030000\n",
      "Train Epoch: 126 [126976/194182 (65%)]\tLoss: 0.749368\tGrad Norm: 0.708721\tLR: 0.030000\n",
      "Train Epoch: 126 [147456/194182 (75%)]\tLoss: 0.760461\tGrad Norm: 1.057029\tLR: 0.030000\n",
      "Train Epoch: 126 [167936/194182 (85%)]\tLoss: 0.761033\tGrad Norm: 1.517130\tLR: 0.030000\n",
      "Train Epoch: 126 [188416/194182 (96%)]\tLoss: 0.751742\tGrad Norm: 0.825151\tLR: 0.030000\n",
      "Train set: Average loss: 0.7532\n",
      "Test set: Average loss: 0.3496, Average MAE: 0.4319\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 127 [4096/194182 (2%)]\tLoss: 0.747534\tGrad Norm: 1.293131\tLR: 0.030000\n",
      "Train Epoch: 127 [24576/194182 (12%)]\tLoss: 0.740239\tGrad Norm: 1.156102\tLR: 0.030000\n",
      "Train Epoch: 127 [45056/194182 (23%)]\tLoss: 0.747783\tGrad Norm: 0.994708\tLR: 0.030000\n",
      "Train Epoch: 127 [65536/194182 (33%)]\tLoss: 0.753508\tGrad Norm: 1.060940\tLR: 0.030000\n",
      "Train Epoch: 127 [86016/194182 (44%)]\tLoss: 0.749962\tGrad Norm: 1.012773\tLR: 0.030000\n",
      "Train Epoch: 127 [106496/194182 (54%)]\tLoss: 0.743278\tGrad Norm: 0.887566\tLR: 0.030000\n",
      "Train Epoch: 127 [126976/194182 (65%)]\tLoss: 0.763904\tGrad Norm: 1.095619\tLR: 0.030000\n",
      "Train Epoch: 127 [147456/194182 (75%)]\tLoss: 0.751712\tGrad Norm: 0.916505\tLR: 0.030000\n",
      "Train Epoch: 127 [167936/194182 (85%)]\tLoss: 0.759433\tGrad Norm: 1.340334\tLR: 0.030000\n",
      "Train Epoch: 127 [188416/194182 (96%)]\tLoss: 0.752874\tGrad Norm: 1.428385\tLR: 0.030000\n",
      "Train set: Average loss: 0.7518\n",
      "Test set: Average loss: 0.3475, Average MAE: 0.4415\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 128 [4096/194182 (2%)]\tLoss: 0.743182\tGrad Norm: 1.173796\tLR: 0.030000\n",
      "Train Epoch: 128 [24576/194182 (12%)]\tLoss: 0.739875\tGrad Norm: 0.749316\tLR: 0.030000\n",
      "Train Epoch: 128 [45056/194182 (23%)]\tLoss: 0.748384\tGrad Norm: 0.856958\tLR: 0.030000\n",
      "Train Epoch: 128 [65536/194182 (33%)]\tLoss: 0.741239\tGrad Norm: 0.803189\tLR: 0.030000\n",
      "Train Epoch: 128 [86016/194182 (44%)]\tLoss: 0.749151\tGrad Norm: 1.025448\tLR: 0.030000\n",
      "Train Epoch: 128 [106496/194182 (54%)]\tLoss: 0.753100\tGrad Norm: 1.291446\tLR: 0.030000\n",
      "Train Epoch: 128 [126976/194182 (65%)]\tLoss: 0.746669\tGrad Norm: 0.932789\tLR: 0.030000\n",
      "Train Epoch: 128 [147456/194182 (75%)]\tLoss: 0.735742\tGrad Norm: 1.393357\tLR: 0.030000\n",
      "Train Epoch: 128 [167936/194182 (85%)]\tLoss: 0.741593\tGrad Norm: 1.453434\tLR: 0.030000\n",
      "Train Epoch: 128 [188416/194182 (96%)]\tLoss: 0.760631\tGrad Norm: 0.753282\tLR: 0.030000\n",
      "Train set: Average loss: 0.7484\n",
      "Test set: Average loss: 0.3417, Average MAE: 0.4344\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 129 [4096/194182 (2%)]\tLoss: 0.744930\tGrad Norm: 0.921805\tLR: 0.030000\n",
      "Train Epoch: 129 [24576/194182 (12%)]\tLoss: 0.746312\tGrad Norm: 1.251758\tLR: 0.030000\n",
      "Train Epoch: 129 [45056/194182 (23%)]\tLoss: 0.746950\tGrad Norm: 1.255903\tLR: 0.030000\n",
      "Train Epoch: 129 [65536/194182 (33%)]\tLoss: 0.743508\tGrad Norm: 1.014136\tLR: 0.030000\n",
      "Train Epoch: 129 [86016/194182 (44%)]\tLoss: 0.752641\tGrad Norm: 1.390376\tLR: 0.030000\n",
      "Train Epoch: 129 [106496/194182 (54%)]\tLoss: 0.745466\tGrad Norm: 1.052581\tLR: 0.030000\n",
      "Train Epoch: 129 [126976/194182 (65%)]\tLoss: 0.742285\tGrad Norm: 1.018357\tLR: 0.030000\n",
      "Train Epoch: 129 [147456/194182 (75%)]\tLoss: 0.741884\tGrad Norm: 0.983314\tLR: 0.030000\n",
      "Train Epoch: 129 [167936/194182 (85%)]\tLoss: 0.751228\tGrad Norm: 1.611524\tLR: 0.030000\n",
      "Train Epoch: 129 [188416/194182 (96%)]\tLoss: 0.748118\tGrad Norm: 0.716829\tLR: 0.030000\n",
      "Train set: Average loss: 0.7487\n",
      "Test set: Average loss: 0.3396, Average MAE: 0.4340\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 130 [4096/194182 (2%)]\tLoss: 0.747730\tGrad Norm: 0.741479\tLR: 0.030000\n",
      "Train Epoch: 130 [24576/194182 (12%)]\tLoss: 0.752768\tGrad Norm: 1.097200\tLR: 0.030000\n",
      "Train Epoch: 130 [45056/194182 (23%)]\tLoss: 0.744615\tGrad Norm: 1.042837\tLR: 0.030000\n",
      "Train Epoch: 130 [65536/194182 (33%)]\tLoss: 0.736327\tGrad Norm: 0.616429\tLR: 0.030000\n",
      "Train Epoch: 130 [86016/194182 (44%)]\tLoss: 0.746370\tGrad Norm: 0.950865\tLR: 0.030000\n",
      "Train Epoch: 130 [106496/194182 (54%)]\tLoss: 0.754822\tGrad Norm: 0.990971\tLR: 0.030000\n",
      "Train Epoch: 130 [126976/194182 (65%)]\tLoss: 0.745170\tGrad Norm: 1.424255\tLR: 0.030000\n",
      "Train Epoch: 130 [147456/194182 (75%)]\tLoss: 0.749057\tGrad Norm: 1.095198\tLR: 0.030000\n",
      "Train Epoch: 130 [167936/194182 (85%)]\tLoss: 0.743410\tGrad Norm: 1.262963\tLR: 0.030000\n",
      "Train Epoch: 130 [188416/194182 (96%)]\tLoss: 0.741244\tGrad Norm: 0.779685\tLR: 0.030000\n",
      "Train set: Average loss: 0.7448\n",
      "Test set: Average loss: 0.3378, Average MAE: 0.4233\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 130: Mean reward = 0.057 +/- 0.053\n",
      "Train Epoch: 131 [4096/194182 (2%)]\tLoss: 0.743927\tGrad Norm: 0.713622\tLR: 0.030000\n",
      "Train Epoch: 131 [24576/194182 (12%)]\tLoss: 0.748273\tGrad Norm: 1.795172\tLR: 0.030000\n",
      "Train Epoch: 131 [45056/194182 (23%)]\tLoss: 0.759437\tGrad Norm: 1.299110\tLR: 0.030000\n",
      "Train Epoch: 131 [65536/194182 (33%)]\tLoss: 0.740985\tGrad Norm: 1.493109\tLR: 0.030000\n",
      "Train Epoch: 131 [86016/194182 (44%)]\tLoss: 0.736188\tGrad Norm: 1.338534\tLR: 0.030000\n",
      "Train Epoch: 131 [106496/194182 (54%)]\tLoss: 0.739207\tGrad Norm: 1.033341\tLR: 0.030000\n",
      "Train Epoch: 131 [126976/194182 (65%)]\tLoss: 0.749273\tGrad Norm: 0.865593\tLR: 0.030000\n",
      "Train Epoch: 131 [147456/194182 (75%)]\tLoss: 0.739293\tGrad Norm: 1.077078\tLR: 0.030000\n",
      "Train Epoch: 131 [167936/194182 (85%)]\tLoss: 0.740563\tGrad Norm: 0.878203\tLR: 0.030000\n",
      "Train Epoch: 131 [188416/194182 (96%)]\tLoss: 0.742034\tGrad Norm: 1.450095\tLR: 0.030000\n",
      "Train set: Average loss: 0.7449\n",
      "Test set: Average loss: 0.3397, Average MAE: 0.4414\n",
      "Train Epoch: 132 [4096/194182 (2%)]\tLoss: 0.743378\tGrad Norm: 0.816458\tLR: 0.030000\n",
      "Train Epoch: 132 [24576/194182 (12%)]\tLoss: 0.742011\tGrad Norm: 1.128014\tLR: 0.030000\n",
      "Train Epoch: 132 [45056/194182 (23%)]\tLoss: 0.754395\tGrad Norm: 1.414110\tLR: 0.030000\n",
      "Train Epoch: 132 [65536/194182 (33%)]\tLoss: 0.736278\tGrad Norm: 0.941216\tLR: 0.030000\n",
      "Train Epoch: 132 [86016/194182 (44%)]\tLoss: 0.738119\tGrad Norm: 1.023508\tLR: 0.030000\n",
      "Train Epoch: 132 [106496/194182 (54%)]\tLoss: 0.741649\tGrad Norm: 1.238686\tLR: 0.030000\n",
      "Train Epoch: 132 [126976/194182 (65%)]\tLoss: 0.750558\tGrad Norm: 1.288373\tLR: 0.030000\n",
      "Train Epoch: 132 [147456/194182 (75%)]\tLoss: 0.745582\tGrad Norm: 0.908333\tLR: 0.030000\n",
      "Train Epoch: 132 [167936/194182 (85%)]\tLoss: 0.734499\tGrad Norm: 1.105002\tLR: 0.030000\n",
      "Train Epoch: 132 [188416/194182 (96%)]\tLoss: 0.737765\tGrad Norm: 0.826617\tLR: 0.030000\n",
      "Train set: Average loss: 0.7418\n",
      "Test set: Average loss: 0.3380, Average MAE: 0.4355\n",
      "Train Epoch: 133 [4096/194182 (2%)]\tLoss: 0.747170\tGrad Norm: 0.868069\tLR: 0.030000\n",
      "Train Epoch: 133 [24576/194182 (12%)]\tLoss: 0.743135\tGrad Norm: 1.177198\tLR: 0.030000\n",
      "Train Epoch: 133 [45056/194182 (23%)]\tLoss: 0.742648\tGrad Norm: 0.872905\tLR: 0.030000\n",
      "Train Epoch: 133 [65536/194182 (33%)]\tLoss: 0.741404\tGrad Norm: 1.233608\tLR: 0.030000\n",
      "Train Epoch: 133 [86016/194182 (44%)]\tLoss: 0.732784\tGrad Norm: 1.094068\tLR: 0.030000\n",
      "Train Epoch: 133 [106496/194182 (54%)]\tLoss: 0.737771\tGrad Norm: 1.281008\tLR: 0.030000\n",
      "Train Epoch: 133 [126976/194182 (65%)]\tLoss: 0.744217\tGrad Norm: 1.484354\tLR: 0.030000\n",
      "Train Epoch: 133 [147456/194182 (75%)]\tLoss: 0.738420\tGrad Norm: 0.965726\tLR: 0.030000\n",
      "Train Epoch: 133 [167936/194182 (85%)]\tLoss: 0.735362\tGrad Norm: 0.578148\tLR: 0.030000\n",
      "Train Epoch: 133 [188416/194182 (96%)]\tLoss: 0.740909\tGrad Norm: 0.814709\tLR: 0.030000\n",
      "Train set: Average loss: 0.7398\n",
      "Test set: Average loss: 0.3370, Average MAE: 0.4284\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 134 [4096/194182 (2%)]\tLoss: 0.737296\tGrad Norm: 1.161495\tLR: 0.030000\n",
      "Train Epoch: 134 [24576/194182 (12%)]\tLoss: 0.743357\tGrad Norm: 1.095454\tLR: 0.030000\n",
      "Train Epoch: 134 [45056/194182 (23%)]\tLoss: 0.737392\tGrad Norm: 1.105272\tLR: 0.030000\n",
      "Train Epoch: 134 [65536/194182 (33%)]\tLoss: 0.749152\tGrad Norm: 1.226677\tLR: 0.030000\n",
      "Train Epoch: 134 [86016/194182 (44%)]\tLoss: 0.733544\tGrad Norm: 0.824244\tLR: 0.030000\n",
      "Train Epoch: 134 [106496/194182 (54%)]\tLoss: 0.746985\tGrad Norm: 0.944338\tLR: 0.030000\n",
      "Train Epoch: 134 [126976/194182 (65%)]\tLoss: 0.744378\tGrad Norm: 1.367219\tLR: 0.030000\n",
      "Train Epoch: 134 [147456/194182 (75%)]\tLoss: 0.745360\tGrad Norm: 1.416978\tLR: 0.030000\n",
      "Train Epoch: 134 [167936/194182 (85%)]\tLoss: 0.744413\tGrad Norm: 1.379154\tLR: 0.030000\n",
      "Train Epoch: 134 [188416/194182 (96%)]\tLoss: 0.741055\tGrad Norm: 1.028995\tLR: 0.030000\n",
      "Train set: Average loss: 0.7402\n",
      "Test set: Average loss: 0.3319, Average MAE: 0.4259\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 135 [4096/194182 (2%)]\tLoss: 0.731690\tGrad Norm: 0.470257\tLR: 0.030000\n",
      "Train Epoch: 135 [24576/194182 (12%)]\tLoss: 0.739491\tGrad Norm: 1.167625\tLR: 0.030000\n",
      "Train Epoch: 135 [45056/194182 (23%)]\tLoss: 0.728106\tGrad Norm: 0.892661\tLR: 0.030000\n",
      "Train Epoch: 135 [65536/194182 (33%)]\tLoss: 0.730444\tGrad Norm: 0.752179\tLR: 0.030000\n",
      "Train Epoch: 135 [86016/194182 (44%)]\tLoss: 0.729048\tGrad Norm: 0.858889\tLR: 0.030000\n",
      "Train Epoch: 135 [106496/194182 (54%)]\tLoss: 0.730719\tGrad Norm: 1.138757\tLR: 0.030000\n",
      "Train Epoch: 135 [126976/194182 (65%)]\tLoss: 0.730475\tGrad Norm: 1.330988\tLR: 0.030000\n",
      "Train Epoch: 135 [147456/194182 (75%)]\tLoss: 0.736926\tGrad Norm: 1.210861\tLR: 0.030000\n",
      "Train Epoch: 135 [167936/194182 (85%)]\tLoss: 0.733622\tGrad Norm: 0.582158\tLR: 0.030000\n",
      "Train Epoch: 135 [188416/194182 (96%)]\tLoss: 0.736352\tGrad Norm: 1.035129\tLR: 0.030000\n",
      "Train set: Average loss: 0.7343\n",
      "Test set: Average loss: 0.3330, Average MAE: 0.4275\n",
      "Epoch 135: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 136 [4096/194182 (2%)]\tLoss: 0.727468\tGrad Norm: 0.916129\tLR: 0.030000\n",
      "Train Epoch: 136 [24576/194182 (12%)]\tLoss: 0.725537\tGrad Norm: 0.846939\tLR: 0.030000\n",
      "Train Epoch: 136 [45056/194182 (23%)]\tLoss: 0.728715\tGrad Norm: 0.858423\tLR: 0.030000\n",
      "Train Epoch: 136 [65536/194182 (33%)]\tLoss: 0.733175\tGrad Norm: 1.147645\tLR: 0.030000\n",
      "Train Epoch: 136 [86016/194182 (44%)]\tLoss: 0.732928\tGrad Norm: 1.267355\tLR: 0.030000\n",
      "Train Epoch: 136 [106496/194182 (54%)]\tLoss: 0.738965\tGrad Norm: 1.164877\tLR: 0.030000\n",
      "Train Epoch: 136 [126976/194182 (65%)]\tLoss: 0.733491\tGrad Norm: 1.197234\tLR: 0.030000\n",
      "Train Epoch: 136 [147456/194182 (75%)]\tLoss: 0.723241\tGrad Norm: 0.906992\tLR: 0.030000\n",
      "Train Epoch: 136 [167936/194182 (85%)]\tLoss: 0.740902\tGrad Norm: 1.108769\tLR: 0.030000\n",
      "Train Epoch: 136 [188416/194182 (96%)]\tLoss: 0.733379\tGrad Norm: 1.305943\tLR: 0.030000\n",
      "Train set: Average loss: 0.7344\n",
      "Test set: Average loss: 0.3346, Average MAE: 0.4354\n",
      "Train Epoch: 137 [4096/194182 (2%)]\tLoss: 0.731976\tGrad Norm: 0.891149\tLR: 0.030000\n",
      "Train Epoch: 137 [24576/194182 (12%)]\tLoss: 0.747425\tGrad Norm: 1.989316\tLR: 0.030000\n",
      "Train Epoch: 137 [45056/194182 (23%)]\tLoss: 0.739417\tGrad Norm: 1.296493\tLR: 0.030000\n",
      "Train Epoch: 137 [65536/194182 (33%)]\tLoss: 0.735100\tGrad Norm: 1.571621\tLR: 0.030000\n",
      "Train Epoch: 137 [86016/194182 (44%)]\tLoss: 0.734205\tGrad Norm: 1.277758\tLR: 0.030000\n",
      "Train Epoch: 137 [106496/194182 (54%)]\tLoss: 0.731986\tGrad Norm: 1.011926\tLR: 0.030000\n",
      "Train Epoch: 137 [126976/194182 (65%)]\tLoss: 0.724803\tGrad Norm: 0.942030\tLR: 0.030000\n",
      "Train Epoch: 137 [147456/194182 (75%)]\tLoss: 0.733717\tGrad Norm: 1.318488\tLR: 0.030000\n",
      "Train Epoch: 137 [167936/194182 (85%)]\tLoss: 0.729735\tGrad Norm: 0.607559\tLR: 0.030000\n",
      "Train Epoch: 137 [188416/194182 (96%)]\tLoss: 0.734850\tGrad Norm: 1.664621\tLR: 0.030000\n",
      "Train set: Average loss: 0.7355\n",
      "Test set: Average loss: 0.3348, Average MAE: 0.4219\n",
      "Train Epoch: 138 [4096/194182 (2%)]\tLoss: 0.727991\tGrad Norm: 0.949949\tLR: 0.030000\n",
      "Train Epoch: 138 [24576/194182 (12%)]\tLoss: 0.733308\tGrad Norm: 1.259641\tLR: 0.030000\n",
      "Train Epoch: 138 [45056/194182 (23%)]\tLoss: 0.731723\tGrad Norm: 0.789860\tLR: 0.030000\n",
      "Train Epoch: 138 [65536/194182 (33%)]\tLoss: 0.721290\tGrad Norm: 0.766300\tLR: 0.030000\n",
      "Train Epoch: 138 [86016/194182 (44%)]\tLoss: 0.726383\tGrad Norm: 1.094939\tLR: 0.030000\n",
      "Train Epoch: 138 [106496/194182 (54%)]\tLoss: 0.724556\tGrad Norm: 1.001977\tLR: 0.030000\n",
      "Train Epoch: 138 [126976/194182 (65%)]\tLoss: 0.724138\tGrad Norm: 1.077760\tLR: 0.030000\n",
      "Train Epoch: 138 [147456/194182 (75%)]\tLoss: 0.737905\tGrad Norm: 1.123499\tLR: 0.030000\n",
      "Train Epoch: 138 [167936/194182 (85%)]\tLoss: 0.735951\tGrad Norm: 1.447059\tLR: 0.030000\n",
      "Train Epoch: 138 [188416/194182 (96%)]\tLoss: 0.719107\tGrad Norm: 1.096032\tLR: 0.030000\n",
      "Train set: Average loss: 0.7310\n",
      "Test set: Average loss: 0.3308, Average MAE: 0.4237\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 139 [4096/194182 (2%)]\tLoss: 0.729864\tGrad Norm: 0.754889\tLR: 0.030000\n",
      "Train Epoch: 139 [24576/194182 (12%)]\tLoss: 0.732241\tGrad Norm: 1.247088\tLR: 0.030000\n",
      "Train Epoch: 139 [45056/194182 (23%)]\tLoss: 0.726769\tGrad Norm: 0.791849\tLR: 0.030000\n",
      "Train Epoch: 139 [65536/194182 (33%)]\tLoss: 0.729221\tGrad Norm: 0.886287\tLR: 0.030000\n",
      "Train Epoch: 139 [86016/194182 (44%)]\tLoss: 0.719249\tGrad Norm: 1.202814\tLR: 0.030000\n",
      "Train Epoch: 139 [106496/194182 (54%)]\tLoss: 0.723245\tGrad Norm: 0.678316\tLR: 0.030000\n",
      "Train Epoch: 139 [126976/194182 (65%)]\tLoss: 0.732341\tGrad Norm: 1.016838\tLR: 0.030000\n",
      "Train Epoch: 139 [147456/194182 (75%)]\tLoss: 0.731005\tGrad Norm: 0.839298\tLR: 0.030000\n",
      "Train Epoch: 139 [167936/194182 (85%)]\tLoss: 0.727305\tGrad Norm: 1.098929\tLR: 0.030000\n",
      "Train Epoch: 139 [188416/194182 (96%)]\tLoss: 0.723676\tGrad Norm: 1.207977\tLR: 0.030000\n",
      "Train set: Average loss: 0.7284\n",
      "Test set: Average loss: 0.3302, Average MAE: 0.4225\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 140 [4096/194182 (2%)]\tLoss: 0.717910\tGrad Norm: 0.965197\tLR: 0.030000\n",
      "Train Epoch: 140 [24576/194182 (12%)]\tLoss: 0.725057\tGrad Norm: 1.128739\tLR: 0.030000\n",
      "Train Epoch: 140 [45056/194182 (23%)]\tLoss: 0.730201\tGrad Norm: 1.669609\tLR: 0.030000\n",
      "Train Epoch: 140 [65536/194182 (33%)]\tLoss: 0.729034\tGrad Norm: 1.280314\tLR: 0.030000\n",
      "Train Epoch: 140 [86016/194182 (44%)]\tLoss: 0.732751\tGrad Norm: 1.325139\tLR: 0.030000\n",
      "Train Epoch: 140 [106496/194182 (54%)]\tLoss: 0.743441\tGrad Norm: 1.340187\tLR: 0.030000\n",
      "Train Epoch: 140 [126976/194182 (65%)]\tLoss: 0.734772\tGrad Norm: 1.241873\tLR: 0.030000\n",
      "Train Epoch: 140 [147456/194182 (75%)]\tLoss: 0.728526\tGrad Norm: 1.163571\tLR: 0.030000\n",
      "Train Epoch: 140 [167936/194182 (85%)]\tLoss: 0.726109\tGrad Norm: 1.128730\tLR: 0.030000\n",
      "Train Epoch: 140 [188416/194182 (96%)]\tLoss: 0.723383\tGrad Norm: 0.881092\tLR: 0.030000\n",
      "Train set: Average loss: 0.7292\n",
      "Test set: Average loss: 0.3271, Average MAE: 0.4235\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 140: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 141 [4096/194182 (2%)]\tLoss: 0.721226\tGrad Norm: 0.477043\tLR: 0.030000\n",
      "Train Epoch: 141 [24576/194182 (12%)]\tLoss: 0.725454\tGrad Norm: 0.633348\tLR: 0.030000\n",
      "Train Epoch: 141 [45056/194182 (23%)]\tLoss: 0.735438\tGrad Norm: 1.346673\tLR: 0.030000\n",
      "Train Epoch: 141 [65536/194182 (33%)]\tLoss: 0.722810\tGrad Norm: 0.791815\tLR: 0.030000\n",
      "Train Epoch: 141 [86016/194182 (44%)]\tLoss: 0.727974\tGrad Norm: 0.898494\tLR: 0.030000\n",
      "Train Epoch: 141 [106496/194182 (54%)]\tLoss: 0.718742\tGrad Norm: 0.981710\tLR: 0.030000\n",
      "Train Epoch: 141 [126976/194182 (65%)]\tLoss: 0.730151\tGrad Norm: 1.086878\tLR: 0.030000\n",
      "Train Epoch: 141 [147456/194182 (75%)]\tLoss: 0.725901\tGrad Norm: 0.953426\tLR: 0.030000\n",
      "Train Epoch: 141 [167936/194182 (85%)]\tLoss: 0.724743\tGrad Norm: 0.866550\tLR: 0.030000\n",
      "Train Epoch: 141 [188416/194182 (96%)]\tLoss: 0.722150\tGrad Norm: 1.060522\tLR: 0.030000\n",
      "Train set: Average loss: 0.7258\n",
      "Test set: Average loss: 0.3326, Average MAE: 0.4179\n",
      "Train Epoch: 142 [4096/194182 (2%)]\tLoss: 0.717759\tGrad Norm: 1.170536\tLR: 0.030000\n",
      "Train Epoch: 142 [24576/194182 (12%)]\tLoss: 0.724156\tGrad Norm: 1.023436\tLR: 0.030000\n",
      "Train Epoch: 142 [45056/194182 (23%)]\tLoss: 0.715780\tGrad Norm: 0.932489\tLR: 0.030000\n",
      "Train Epoch: 142 [65536/194182 (33%)]\tLoss: 0.732229\tGrad Norm: 1.046352\tLR: 0.030000\n",
      "Train Epoch: 142 [86016/194182 (44%)]\tLoss: 0.729454\tGrad Norm: 1.161571\tLR: 0.030000\n",
      "Train Epoch: 142 [106496/194182 (54%)]\tLoss: 0.726439\tGrad Norm: 1.566313\tLR: 0.030000\n",
      "Train Epoch: 142 [126976/194182 (65%)]\tLoss: 0.727773\tGrad Norm: 1.600734\tLR: 0.030000\n",
      "Train Epoch: 142 [147456/194182 (75%)]\tLoss: 0.726788\tGrad Norm: 1.231239\tLR: 0.030000\n",
      "Train Epoch: 142 [167936/194182 (85%)]\tLoss: 0.716533\tGrad Norm: 1.086142\tLR: 0.030000\n",
      "Train Epoch: 142 [188416/194182 (96%)]\tLoss: 0.725377\tGrad Norm: 1.034556\tLR: 0.030000\n",
      "Train set: Average loss: 0.7258\n",
      "Test set: Average loss: 0.3252, Average MAE: 0.4236\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 143 [4096/194182 (2%)]\tLoss: 0.722559\tGrad Norm: 0.513198\tLR: 0.030000\n",
      "Train Epoch: 143 [24576/194182 (12%)]\tLoss: 0.731409\tGrad Norm: 0.900223\tLR: 0.030000\n",
      "Train Epoch: 143 [45056/194182 (23%)]\tLoss: 0.733575\tGrad Norm: 1.612123\tLR: 0.030000\n",
      "Train Epoch: 143 [65536/194182 (33%)]\tLoss: 0.717107\tGrad Norm: 0.759547\tLR: 0.030000\n",
      "Train Epoch: 143 [86016/194182 (44%)]\tLoss: 0.720022\tGrad Norm: 0.748518\tLR: 0.030000\n",
      "Train Epoch: 143 [106496/194182 (54%)]\tLoss: 0.723345\tGrad Norm: 0.978770\tLR: 0.030000\n",
      "Train Epoch: 143 [126976/194182 (65%)]\tLoss: 0.722439\tGrad Norm: 1.142929\tLR: 0.030000\n",
      "Train Epoch: 143 [147456/194182 (75%)]\tLoss: 0.733947\tGrad Norm: 1.251967\tLR: 0.030000\n",
      "Train Epoch: 143 [167936/194182 (85%)]\tLoss: 0.719218\tGrad Norm: 1.182657\tLR: 0.030000\n",
      "Train Epoch: 143 [188416/194182 (96%)]\tLoss: 0.718461\tGrad Norm: 0.938450\tLR: 0.030000\n",
      "Train set: Average loss: 0.7231\n",
      "Test set: Average loss: 0.3346, Average MAE: 0.4381\n",
      "Train Epoch: 144 [4096/194182 (2%)]\tLoss: 0.727906\tGrad Norm: 1.439166\tLR: 0.030000\n",
      "Train Epoch: 144 [24576/194182 (12%)]\tLoss: 0.733934\tGrad Norm: 1.320632\tLR: 0.030000\n",
      "Train Epoch: 144 [45056/194182 (23%)]\tLoss: 0.716777\tGrad Norm: 0.924933\tLR: 0.030000\n",
      "Train Epoch: 144 [65536/194182 (33%)]\tLoss: 0.716574\tGrad Norm: 0.851348\tLR: 0.030000\n",
      "Train Epoch: 144 [86016/194182 (44%)]\tLoss: 0.730789\tGrad Norm: 0.996783\tLR: 0.030000\n",
      "Train Epoch: 144 [106496/194182 (54%)]\tLoss: 0.727704\tGrad Norm: 0.949226\tLR: 0.030000\n",
      "Train Epoch: 144 [126976/194182 (65%)]\tLoss: 0.713304\tGrad Norm: 0.907082\tLR: 0.030000\n",
      "Train Epoch: 144 [147456/194182 (75%)]\tLoss: 0.721341\tGrad Norm: 0.750810\tLR: 0.030000\n",
      "Train Epoch: 144 [167936/194182 (85%)]\tLoss: 0.710397\tGrad Norm: 1.730908\tLR: 0.030000\n",
      "Train Epoch: 144 [188416/194182 (96%)]\tLoss: 0.723012\tGrad Norm: 1.301433\tLR: 0.030000\n",
      "Train set: Average loss: 0.7216\n",
      "Test set: Average loss: 0.3267, Average MAE: 0.4167\n",
      "Train Epoch: 145 [4096/194182 (2%)]\tLoss: 0.720036\tGrad Norm: 1.056357\tLR: 0.030000\n",
      "Train Epoch: 145 [24576/194182 (12%)]\tLoss: 0.727970\tGrad Norm: 1.156857\tLR: 0.030000\n",
      "Train Epoch: 145 [45056/194182 (23%)]\tLoss: 0.719353\tGrad Norm: 1.237238\tLR: 0.030000\n",
      "Train Epoch: 145 [65536/194182 (33%)]\tLoss: 0.717444\tGrad Norm: 1.353342\tLR: 0.030000\n",
      "Train Epoch: 145 [86016/194182 (44%)]\tLoss: 0.711057\tGrad Norm: 0.948803\tLR: 0.030000\n",
      "Train Epoch: 145 [106496/194182 (54%)]\tLoss: 0.732199\tGrad Norm: 0.924605\tLR: 0.030000\n",
      "Train Epoch: 145 [126976/194182 (65%)]\tLoss: 0.717972\tGrad Norm: 1.198360\tLR: 0.030000\n",
      "Train Epoch: 145 [147456/194182 (75%)]\tLoss: 0.719412\tGrad Norm: 0.747679\tLR: 0.030000\n",
      "Train Epoch: 145 [167936/194182 (85%)]\tLoss: 0.715224\tGrad Norm: 1.186182\tLR: 0.030000\n",
      "Train Epoch: 145 [188416/194182 (96%)]\tLoss: 0.719635\tGrad Norm: 1.205106\tLR: 0.030000\n",
      "Train set: Average loss: 0.7211\n",
      "Test set: Average loss: 0.3303, Average MAE: 0.4379\n",
      "Epoch 145: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 146 [4096/194182 (2%)]\tLoss: 0.726998\tGrad Norm: 1.382959\tLR: 0.030000\n",
      "Train Epoch: 146 [24576/194182 (12%)]\tLoss: 0.707272\tGrad Norm: 0.778678\tLR: 0.030000\n",
      "Train Epoch: 146 [45056/194182 (23%)]\tLoss: 0.731541\tGrad Norm: 1.237682\tLR: 0.030000\n",
      "Train Epoch: 146 [65536/194182 (33%)]\tLoss: 0.727032\tGrad Norm: 1.289029\tLR: 0.030000\n",
      "Train Epoch: 146 [86016/194182 (44%)]\tLoss: 0.716318\tGrad Norm: 0.863013\tLR: 0.030000\n",
      "Train Epoch: 146 [106496/194182 (54%)]\tLoss: 0.723587\tGrad Norm: 1.120424\tLR: 0.030000\n",
      "Train Epoch: 146 [126976/194182 (65%)]\tLoss: 0.726045\tGrad Norm: 1.216965\tLR: 0.030000\n",
      "Train Epoch: 146 [147456/194182 (75%)]\tLoss: 0.715655\tGrad Norm: 0.780002\tLR: 0.030000\n",
      "Train Epoch: 146 [167936/194182 (85%)]\tLoss: 0.725829\tGrad Norm: 0.891177\tLR: 0.030000\n",
      "Train Epoch: 146 [188416/194182 (96%)]\tLoss: 0.713367\tGrad Norm: 1.430301\tLR: 0.030000\n",
      "Train set: Average loss: 0.7185\n",
      "Test set: Average loss: 0.3281, Average MAE: 0.4139\n",
      "Train Epoch: 147 [4096/194182 (2%)]\tLoss: 0.721848\tGrad Norm: 1.259037\tLR: 0.030000\n",
      "Train Epoch: 147 [24576/194182 (12%)]\tLoss: 0.718554\tGrad Norm: 0.732936\tLR: 0.030000\n",
      "Train Epoch: 147 [45056/194182 (23%)]\tLoss: 0.713335\tGrad Norm: 1.234840\tLR: 0.030000\n",
      "Train Epoch: 147 [65536/194182 (33%)]\tLoss: 0.720919\tGrad Norm: 1.296513\tLR: 0.030000\n",
      "Train Epoch: 147 [86016/194182 (44%)]\tLoss: 0.726405\tGrad Norm: 1.095177\tLR: 0.030000\n",
      "Train Epoch: 147 [106496/194182 (54%)]\tLoss: 0.713915\tGrad Norm: 1.118485\tLR: 0.030000\n",
      "Train Epoch: 147 [126976/194182 (65%)]\tLoss: 0.724746\tGrad Norm: 1.212889\tLR: 0.030000\n",
      "Train Epoch: 147 [147456/194182 (75%)]\tLoss: 0.716567\tGrad Norm: 1.098659\tLR: 0.030000\n",
      "Train Epoch: 147 [167936/194182 (85%)]\tLoss: 0.715887\tGrad Norm: 0.753666\tLR: 0.030000\n",
      "Train Epoch: 147 [188416/194182 (96%)]\tLoss: 0.708402\tGrad Norm: 0.807165\tLR: 0.030000\n",
      "Train set: Average loss: 0.7174\n",
      "Test set: Average loss: 0.3206, Average MAE: 0.4142\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 148 [4096/194182 (2%)]\tLoss: 0.711878\tGrad Norm: 0.654266\tLR: 0.030000\n",
      "Train Epoch: 148 [24576/194182 (12%)]\tLoss: 0.709742\tGrad Norm: 1.065052\tLR: 0.030000\n",
      "Train Epoch: 148 [45056/194182 (23%)]\tLoss: 0.715360\tGrad Norm: 1.231188\tLR: 0.030000\n",
      "Train Epoch: 148 [65536/194182 (33%)]\tLoss: 0.715544\tGrad Norm: 1.331127\tLR: 0.030000\n",
      "Train Epoch: 148 [86016/194182 (44%)]\tLoss: 0.725582\tGrad Norm: 1.342425\tLR: 0.030000\n",
      "Train Epoch: 148 [106496/194182 (54%)]\tLoss: 0.718498\tGrad Norm: 0.808227\tLR: 0.030000\n",
      "Train Epoch: 148 [126976/194182 (65%)]\tLoss: 0.709118\tGrad Norm: 0.731748\tLR: 0.030000\n",
      "Train Epoch: 148 [147456/194182 (75%)]\tLoss: 0.721720\tGrad Norm: 1.363619\tLR: 0.030000\n",
      "Train Epoch: 148 [167936/194182 (85%)]\tLoss: 0.704505\tGrad Norm: 0.351034\tLR: 0.030000\n",
      "Train Epoch: 148 [188416/194182 (96%)]\tLoss: 0.722393\tGrad Norm: 1.248241\tLR: 0.030000\n",
      "Train set: Average loss: 0.7154\n",
      "Test set: Average loss: 0.3293, Average MAE: 0.4357\n",
      "Train Epoch: 149 [4096/194182 (2%)]\tLoss: 0.719027\tGrad Norm: 1.378822\tLR: 0.030000\n",
      "Train Epoch: 149 [24576/194182 (12%)]\tLoss: 0.706325\tGrad Norm: 0.846365\tLR: 0.030000\n",
      "Train Epoch: 149 [45056/194182 (23%)]\tLoss: 0.717018\tGrad Norm: 1.127264\tLR: 0.030000\n",
      "Train Epoch: 149 [65536/194182 (33%)]\tLoss: 0.707714\tGrad Norm: 1.262232\tLR: 0.030000\n",
      "Train Epoch: 149 [86016/194182 (44%)]\tLoss: 0.710965\tGrad Norm: 0.771977\tLR: 0.030000\n",
      "Train Epoch: 149 [106496/194182 (54%)]\tLoss: 0.712205\tGrad Norm: 1.281764\tLR: 0.030000\n",
      "Train Epoch: 149 [126976/194182 (65%)]\tLoss: 0.718605\tGrad Norm: 1.300284\tLR: 0.030000\n",
      "Train Epoch: 149 [147456/194182 (75%)]\tLoss: 0.708837\tGrad Norm: 0.839271\tLR: 0.030000\n",
      "Train Epoch: 149 [167936/194182 (85%)]\tLoss: 0.722035\tGrad Norm: 1.174162\tLR: 0.030000\n",
      "Train Epoch: 149 [188416/194182 (96%)]\tLoss: 0.713442\tGrad Norm: 1.348129\tLR: 0.030000\n",
      "Train set: Average loss: 0.7147\n",
      "Test set: Average loss: 0.3230, Average MAE: 0.4313\n",
      "Train Epoch: 150 [4096/194182 (2%)]\tLoss: 0.722890\tGrad Norm: 0.832806\tLR: 0.030000\n",
      "Train Epoch: 150 [24576/194182 (12%)]\tLoss: 0.709940\tGrad Norm: 0.719996\tLR: 0.030000\n",
      "Train Epoch: 150 [45056/194182 (23%)]\tLoss: 0.723402\tGrad Norm: 1.154667\tLR: 0.030000\n",
      "Train Epoch: 150 [65536/194182 (33%)]\tLoss: 0.707688\tGrad Norm: 1.155757\tLR: 0.030000\n",
      "Train Epoch: 150 [86016/194182 (44%)]\tLoss: 0.716671\tGrad Norm: 1.140930\tLR: 0.030000\n",
      "Train Epoch: 150 [106496/194182 (54%)]\tLoss: 0.718577\tGrad Norm: 1.509176\tLR: 0.030000\n",
      "Train Epoch: 150 [126976/194182 (65%)]\tLoss: 0.730064\tGrad Norm: 1.367082\tLR: 0.030000\n",
      "Train Epoch: 150 [147456/194182 (75%)]\tLoss: 0.715761\tGrad Norm: 1.194510\tLR: 0.030000\n",
      "Train Epoch: 150 [167936/194182 (85%)]\tLoss: 0.703798\tGrad Norm: 0.982428\tLR: 0.030000\n",
      "Train Epoch: 150 [188416/194182 (96%)]\tLoss: 0.716028\tGrad Norm: 0.736225\tLR: 0.030000\n",
      "Train set: Average loss: 0.7138\n",
      "Test set: Average loss: 0.3210, Average MAE: 0.4237\n",
      "Epoch 150: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 151 [4096/194182 (2%)]\tLoss: 0.707788\tGrad Norm: 0.852888\tLR: 0.030000\n",
      "Train Epoch: 151 [24576/194182 (12%)]\tLoss: 0.720983\tGrad Norm: 1.687130\tLR: 0.030000\n",
      "Train Epoch: 151 [45056/194182 (23%)]\tLoss: 0.723187\tGrad Norm: 1.251798\tLR: 0.030000\n",
      "Train Epoch: 151 [65536/194182 (33%)]\tLoss: 0.708969\tGrad Norm: 1.136155\tLR: 0.030000\n",
      "Train Epoch: 151 [86016/194182 (44%)]\tLoss: 0.716576\tGrad Norm: 0.945880\tLR: 0.030000\n",
      "Train Epoch: 151 [106496/194182 (54%)]\tLoss: 0.706686\tGrad Norm: 0.965692\tLR: 0.030000\n",
      "Train Epoch: 151 [126976/194182 (65%)]\tLoss: 0.724794\tGrad Norm: 1.529917\tLR: 0.030000\n",
      "Train Epoch: 151 [147456/194182 (75%)]\tLoss: 0.717058\tGrad Norm: 1.374904\tLR: 0.030000\n",
      "Train Epoch: 151 [167936/194182 (85%)]\tLoss: 0.717333\tGrad Norm: 1.093418\tLR: 0.030000\n",
      "Train Epoch: 151 [188416/194182 (96%)]\tLoss: 0.711948\tGrad Norm: 1.211565\tLR: 0.030000\n",
      "Train set: Average loss: 0.7134\n",
      "Test set: Average loss: 0.3209, Average MAE: 0.4120\n",
      "Train Epoch: 152 [4096/194182 (2%)]\tLoss: 0.713207\tGrad Norm: 1.179519\tLR: 0.030000\n",
      "Train Epoch: 152 [24576/194182 (12%)]\tLoss: 0.702654\tGrad Norm: 0.666224\tLR: 0.030000\n",
      "Train Epoch: 152 [45056/194182 (23%)]\tLoss: 0.709208\tGrad Norm: 1.035485\tLR: 0.030000\n",
      "Train Epoch: 152 [65536/194182 (33%)]\tLoss: 0.704019\tGrad Norm: 0.710617\tLR: 0.030000\n",
      "Train Epoch: 152 [86016/194182 (44%)]\tLoss: 0.714182\tGrad Norm: 1.300448\tLR: 0.030000\n",
      "Train Epoch: 152 [106496/194182 (54%)]\tLoss: 0.714814\tGrad Norm: 0.901627\tLR: 0.030000\n",
      "Train Epoch: 152 [126976/194182 (65%)]\tLoss: 0.710138\tGrad Norm: 1.354075\tLR: 0.030000\n",
      "Train Epoch: 152 [147456/194182 (75%)]\tLoss: 0.713902\tGrad Norm: 1.331596\tLR: 0.030000\n",
      "Train Epoch: 152 [167936/194182 (85%)]\tLoss: 0.703543\tGrad Norm: 0.969141\tLR: 0.030000\n",
      "Train Epoch: 152 [188416/194182 (96%)]\tLoss: 0.714806\tGrad Norm: 1.236395\tLR: 0.030000\n",
      "Train set: Average loss: 0.7107\n",
      "Test set: Average loss: 0.3277, Average MAE: 0.4001\n",
      "Train Epoch: 153 [4096/194182 (2%)]\tLoss: 0.714974\tGrad Norm: 1.554631\tLR: 0.030000\n",
      "Train Epoch: 153 [24576/194182 (12%)]\tLoss: 0.704034\tGrad Norm: 1.234361\tLR: 0.030000\n",
      "Train Epoch: 153 [45056/194182 (23%)]\tLoss: 0.716087\tGrad Norm: 1.187215\tLR: 0.030000\n",
      "Train Epoch: 153 [65536/194182 (33%)]\tLoss: 0.717003\tGrad Norm: 1.424238\tLR: 0.030000\n",
      "Train Epoch: 153 [86016/194182 (44%)]\tLoss: 0.702189\tGrad Norm: 0.847470\tLR: 0.030000\n",
      "Train Epoch: 153 [106496/194182 (54%)]\tLoss: 0.707191\tGrad Norm: 0.743372\tLR: 0.030000\n",
      "Train Epoch: 153 [126976/194182 (65%)]\tLoss: 0.704738\tGrad Norm: 0.948960\tLR: 0.030000\n",
      "Train Epoch: 153 [147456/194182 (75%)]\tLoss: 0.707481\tGrad Norm: 0.845837\tLR: 0.030000\n",
      "Train Epoch: 153 [167936/194182 (85%)]\tLoss: 0.704787\tGrad Norm: 0.916151\tLR: 0.030000\n",
      "Train Epoch: 153 [188416/194182 (96%)]\tLoss: 0.698214\tGrad Norm: 0.599435\tLR: 0.030000\n",
      "Train set: Average loss: 0.7094\n",
      "Test set: Average loss: 0.3219, Average MAE: 0.4303\n",
      "Train Epoch: 154 [4096/194182 (2%)]\tLoss: 0.714176\tGrad Norm: 0.937126\tLR: 0.030000\n",
      "Train Epoch: 154 [24576/194182 (12%)]\tLoss: 0.704059\tGrad Norm: 1.209565\tLR: 0.030000\n",
      "Train Epoch: 154 [45056/194182 (23%)]\tLoss: 0.718643\tGrad Norm: 1.102772\tLR: 0.030000\n",
      "Train Epoch: 154 [65536/194182 (33%)]\tLoss: 0.711009\tGrad Norm: 1.118479\tLR: 0.030000\n",
      "Train Epoch: 154 [86016/194182 (44%)]\tLoss: 0.697443\tGrad Norm: 0.988800\tLR: 0.030000\n",
      "Train Epoch: 154 [106496/194182 (54%)]\tLoss: 0.706848\tGrad Norm: 1.323533\tLR: 0.030000\n",
      "Train Epoch: 154 [126976/194182 (65%)]\tLoss: 0.702652\tGrad Norm: 0.929624\tLR: 0.030000\n",
      "Train Epoch: 154 [147456/194182 (75%)]\tLoss: 0.709287\tGrad Norm: 1.297181\tLR: 0.030000\n",
      "Train Epoch: 154 [167936/194182 (85%)]\tLoss: 0.700846\tGrad Norm: 0.968329\tLR: 0.030000\n",
      "Train Epoch: 154 [188416/194182 (96%)]\tLoss: 0.703195\tGrad Norm: 1.179086\tLR: 0.030000\n",
      "Train set: Average loss: 0.7089\n",
      "Test set: Average loss: 0.3243, Average MAE: 0.4063\n",
      "Train Epoch: 155 [4096/194182 (2%)]\tLoss: 0.714119\tGrad Norm: 1.527137\tLR: 0.030000\n",
      "Train Epoch: 155 [24576/194182 (12%)]\tLoss: 0.707554\tGrad Norm: 1.029767\tLR: 0.030000\n",
      "Train Epoch: 155 [45056/194182 (23%)]\tLoss: 0.708136\tGrad Norm: 0.777934\tLR: 0.030000\n",
      "Train Epoch: 155 [65536/194182 (33%)]\tLoss: 0.707990\tGrad Norm: 1.073560\tLR: 0.030000\n",
      "Train Epoch: 155 [86016/194182 (44%)]\tLoss: 0.701138\tGrad Norm: 1.058952\tLR: 0.030000\n",
      "Train Epoch: 155 [106496/194182 (54%)]\tLoss: 0.699820\tGrad Norm: 1.179340\tLR: 0.030000\n",
      "Train Epoch: 155 [126976/194182 (65%)]\tLoss: 0.729437\tGrad Norm: 1.321653\tLR: 0.030000\n",
      "Train Epoch: 155 [147456/194182 (75%)]\tLoss: 0.697575\tGrad Norm: 0.919083\tLR: 0.030000\n",
      "Train Epoch: 155 [167936/194182 (85%)]\tLoss: 0.696307\tGrad Norm: 0.960894\tLR: 0.030000\n",
      "Train Epoch: 155 [188416/194182 (96%)]\tLoss: 0.700538\tGrad Norm: 0.978537\tLR: 0.030000\n",
      "Train set: Average loss: 0.7066\n",
      "Test set: Average loss: 0.3165, Average MAE: 0.4109\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 155: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 156 [4096/194182 (2%)]\tLoss: 0.701478\tGrad Norm: 0.864512\tLR: 0.030000\n",
      "Train Epoch: 156 [24576/194182 (12%)]\tLoss: 0.701184\tGrad Norm: 1.052415\tLR: 0.030000\n",
      "Train Epoch: 156 [45056/194182 (23%)]\tLoss: 0.721663\tGrad Norm: 1.524772\tLR: 0.030000\n",
      "Train Epoch: 156 [65536/194182 (33%)]\tLoss: 0.712040\tGrad Norm: 1.239359\tLR: 0.030000\n",
      "Train Epoch: 156 [86016/194182 (44%)]\tLoss: 0.706197\tGrad Norm: 0.922180\tLR: 0.030000\n",
      "Train Epoch: 156 [106496/194182 (54%)]\tLoss: 0.697394\tGrad Norm: 0.708628\tLR: 0.030000\n",
      "Train Epoch: 156 [126976/194182 (65%)]\tLoss: 0.712174\tGrad Norm: 1.217704\tLR: 0.030000\n",
      "Train Epoch: 156 [147456/194182 (75%)]\tLoss: 0.699876\tGrad Norm: 0.947350\tLR: 0.030000\n",
      "Train Epoch: 156 [167936/194182 (85%)]\tLoss: 0.693866\tGrad Norm: 0.740958\tLR: 0.030000\n",
      "Train Epoch: 156 [188416/194182 (96%)]\tLoss: 0.702855\tGrad Norm: 1.139788\tLR: 0.030000\n",
      "Train set: Average loss: 0.7056\n",
      "Test set: Average loss: 0.3180, Average MAE: 0.4173\n",
      "Train Epoch: 157 [4096/194182 (2%)]\tLoss: 0.709480\tGrad Norm: 1.130547\tLR: 0.030000\n",
      "Train Epoch: 157 [24576/194182 (12%)]\tLoss: 0.711907\tGrad Norm: 1.214501\tLR: 0.030000\n",
      "Train Epoch: 157 [45056/194182 (23%)]\tLoss: 0.700466\tGrad Norm: 0.892042\tLR: 0.030000\n",
      "Train Epoch: 157 [65536/194182 (33%)]\tLoss: 0.696450\tGrad Norm: 0.983558\tLR: 0.030000\n",
      "Train Epoch: 157 [86016/194182 (44%)]\tLoss: 0.702275\tGrad Norm: 1.078957\tLR: 0.030000\n",
      "Train Epoch: 157 [106496/194182 (54%)]\tLoss: 0.708898\tGrad Norm: 0.988211\tLR: 0.030000\n",
      "Train Epoch: 157 [126976/194182 (65%)]\tLoss: 0.698366\tGrad Norm: 0.833241\tLR: 0.030000\n",
      "Train Epoch: 157 [147456/194182 (75%)]\tLoss: 0.702103\tGrad Norm: 1.295678\tLR: 0.030000\n",
      "Train Epoch: 157 [167936/194182 (85%)]\tLoss: 0.702260\tGrad Norm: 0.756398\tLR: 0.030000\n",
      "Train Epoch: 157 [188416/194182 (96%)]\tLoss: 0.708203\tGrad Norm: 1.218517\tLR: 0.030000\n",
      "Train set: Average loss: 0.7038\n",
      "Test set: Average loss: 0.3154, Average MAE: 0.4208\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 158 [4096/194182 (2%)]\tLoss: 0.700109\tGrad Norm: 0.807928\tLR: 0.030000\n",
      "Train Epoch: 158 [24576/194182 (12%)]\tLoss: 0.703245\tGrad Norm: 0.876670\tLR: 0.030000\n",
      "Train Epoch: 158 [45056/194182 (23%)]\tLoss: 0.702632\tGrad Norm: 0.775681\tLR: 0.030000\n",
      "Train Epoch: 158 [65536/194182 (33%)]\tLoss: 0.699371\tGrad Norm: 0.965895\tLR: 0.030000\n",
      "Train Epoch: 158 [86016/194182 (44%)]\tLoss: 0.712478\tGrad Norm: 1.270347\tLR: 0.030000\n",
      "Train Epoch: 158 [106496/194182 (54%)]\tLoss: 0.708989\tGrad Norm: 1.434556\tLR: 0.030000\n",
      "Train Epoch: 158 [126976/194182 (65%)]\tLoss: 0.702765\tGrad Norm: 0.787949\tLR: 0.030000\n",
      "Train Epoch: 158 [147456/194182 (75%)]\tLoss: 0.697963\tGrad Norm: 0.912388\tLR: 0.030000\n",
      "Train Epoch: 158 [167936/194182 (85%)]\tLoss: 0.706273\tGrad Norm: 1.283931\tLR: 0.030000\n",
      "Train Epoch: 158 [188416/194182 (96%)]\tLoss: 0.706558\tGrad Norm: 1.414224\tLR: 0.030000\n",
      "Train set: Average loss: 0.7034\n",
      "Test set: Average loss: 0.3224, Average MAE: 0.4294\n",
      "Train Epoch: 159 [4096/194182 (2%)]\tLoss: 0.703328\tGrad Norm: 1.531562\tLR: 0.030000\n",
      "Train Epoch: 159 [24576/194182 (12%)]\tLoss: 0.709984\tGrad Norm: 1.027056\tLR: 0.030000\n",
      "Train Epoch: 159 [45056/194182 (23%)]\tLoss: 0.700132\tGrad Norm: 0.786585\tLR: 0.030000\n",
      "Train Epoch: 159 [65536/194182 (33%)]\tLoss: 0.694810\tGrad Norm: 1.066881\tLR: 0.030000\n",
      "Train Epoch: 159 [86016/194182 (44%)]\tLoss: 0.704259\tGrad Norm: 0.816350\tLR: 0.030000\n",
      "Train Epoch: 159 [106496/194182 (54%)]\tLoss: 0.700154\tGrad Norm: 0.951620\tLR: 0.030000\n",
      "Train Epoch: 159 [126976/194182 (65%)]\tLoss: 0.713159\tGrad Norm: 1.295483\tLR: 0.030000\n",
      "Train Epoch: 159 [147456/194182 (75%)]\tLoss: 0.699420\tGrad Norm: 1.046960\tLR: 0.030000\n",
      "Train Epoch: 159 [167936/194182 (85%)]\tLoss: 0.696578\tGrad Norm: 0.995785\tLR: 0.030000\n",
      "Train Epoch: 159 [188416/194182 (96%)]\tLoss: 0.699642\tGrad Norm: 0.866433\tLR: 0.030000\n",
      "Train set: Average loss: 0.7016\n",
      "Test set: Average loss: 0.3162, Average MAE: 0.4040\n",
      "Train Epoch: 160 [4096/194182 (2%)]\tLoss: 0.705165\tGrad Norm: 1.047034\tLR: 0.030000\n",
      "Train Epoch: 160 [24576/194182 (12%)]\tLoss: 0.697456\tGrad Norm: 1.031370\tLR: 0.030000\n",
      "Train Epoch: 160 [45056/194182 (23%)]\tLoss: 0.699773\tGrad Norm: 0.974533\tLR: 0.030000\n",
      "Train Epoch: 160 [65536/194182 (33%)]\tLoss: 0.696711\tGrad Norm: 0.611561\tLR: 0.030000\n",
      "Train Epoch: 160 [86016/194182 (44%)]\tLoss: 0.694167\tGrad Norm: 0.932652\tLR: 0.030000\n",
      "Train Epoch: 160 [106496/194182 (54%)]\tLoss: 0.698984\tGrad Norm: 0.835113\tLR: 0.030000\n",
      "Train Epoch: 160 [126976/194182 (65%)]\tLoss: 0.698187\tGrad Norm: 1.227666\tLR: 0.030000\n",
      "Train Epoch: 160 [147456/194182 (75%)]\tLoss: 0.704547\tGrad Norm: 1.216748\tLR: 0.030000\n",
      "Train Epoch: 160 [167936/194182 (85%)]\tLoss: 0.702549\tGrad Norm: 1.620463\tLR: 0.030000\n",
      "Train Epoch: 160 [188416/194182 (96%)]\tLoss: 0.710125\tGrad Norm: 1.148221\tLR: 0.030000\n",
      "Train set: Average loss: 0.7012\n",
      "Test set: Average loss: 0.3146, Average MAE: 0.4177\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 160: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 161 [4096/194182 (2%)]\tLoss: 0.705254\tGrad Norm: 0.832241\tLR: 0.030000\n",
      "Train Epoch: 161 [24576/194182 (12%)]\tLoss: 0.702436\tGrad Norm: 1.457588\tLR: 0.030000\n",
      "Train Epoch: 161 [45056/194182 (23%)]\tLoss: 0.694397\tGrad Norm: 0.959342\tLR: 0.030000\n",
      "Train Epoch: 161 [65536/194182 (33%)]\tLoss: 0.702886\tGrad Norm: 0.814367\tLR: 0.030000\n",
      "Train Epoch: 161 [86016/194182 (44%)]\tLoss: 0.700573\tGrad Norm: 1.529238\tLR: 0.030000\n",
      "Train Epoch: 161 [106496/194182 (54%)]\tLoss: 0.686423\tGrad Norm: 1.198863\tLR: 0.030000\n",
      "Train Epoch: 161 [126976/194182 (65%)]\tLoss: 0.702343\tGrad Norm: 0.896446\tLR: 0.030000\n",
      "Train Epoch: 161 [147456/194182 (75%)]\tLoss: 0.699478\tGrad Norm: 1.014795\tLR: 0.030000\n",
      "Train Epoch: 161 [167936/194182 (85%)]\tLoss: 0.703611\tGrad Norm: 1.158580\tLR: 0.030000\n",
      "Train Epoch: 161 [188416/194182 (96%)]\tLoss: 0.691295\tGrad Norm: 0.905596\tLR: 0.030000\n",
      "Train set: Average loss: 0.6998\n",
      "Test set: Average loss: 0.3114, Average MAE: 0.4083\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 162 [4096/194182 (2%)]\tLoss: 0.692124\tGrad Norm: 0.597019\tLR: 0.030000\n",
      "Train Epoch: 162 [24576/194182 (12%)]\tLoss: 0.705140\tGrad Norm: 1.323094\tLR: 0.030000\n",
      "Train Epoch: 162 [45056/194182 (23%)]\tLoss: 0.699165\tGrad Norm: 1.485956\tLR: 0.030000\n",
      "Train Epoch: 162 [65536/194182 (33%)]\tLoss: 0.705200\tGrad Norm: 1.261943\tLR: 0.030000\n",
      "Train Epoch: 162 [86016/194182 (44%)]\tLoss: 0.700665\tGrad Norm: 1.016847\tLR: 0.030000\n",
      "Train Epoch: 162 [106496/194182 (54%)]\tLoss: 0.685313\tGrad Norm: 0.874759\tLR: 0.030000\n",
      "Train Epoch: 162 [126976/194182 (65%)]\tLoss: 0.696951\tGrad Norm: 0.746622\tLR: 0.030000\n",
      "Train Epoch: 162 [147456/194182 (75%)]\tLoss: 0.699610\tGrad Norm: 1.288728\tLR: 0.030000\n",
      "Train Epoch: 162 [167936/194182 (85%)]\tLoss: 0.705286\tGrad Norm: 1.115779\tLR: 0.030000\n",
      "Train Epoch: 162 [188416/194182 (96%)]\tLoss: 0.708088\tGrad Norm: 1.195412\tLR: 0.030000\n",
      "Train set: Average loss: 0.6982\n",
      "Test set: Average loss: 0.3106, Average MAE: 0.4046\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 163 [4096/194182 (2%)]\tLoss: 0.692838\tGrad Norm: 0.471366\tLR: 0.030000\n",
      "Train Epoch: 163 [24576/194182 (12%)]\tLoss: 0.693968\tGrad Norm: 0.527939\tLR: 0.030000\n",
      "Train Epoch: 163 [45056/194182 (23%)]\tLoss: 0.690179\tGrad Norm: 1.012868\tLR: 0.030000\n",
      "Train Epoch: 163 [65536/194182 (33%)]\tLoss: 0.702838\tGrad Norm: 0.835780\tLR: 0.030000\n",
      "Train Epoch: 163 [86016/194182 (44%)]\tLoss: 0.692725\tGrad Norm: 0.750038\tLR: 0.030000\n",
      "Train Epoch: 163 [106496/194182 (54%)]\tLoss: 0.696423\tGrad Norm: 1.006275\tLR: 0.030000\n",
      "Train Epoch: 163 [126976/194182 (65%)]\tLoss: 0.694879\tGrad Norm: 1.013914\tLR: 0.030000\n",
      "Train Epoch: 163 [147456/194182 (75%)]\tLoss: 0.702026\tGrad Norm: 1.233831\tLR: 0.030000\n",
      "Train Epoch: 163 [167936/194182 (85%)]\tLoss: 0.695222\tGrad Norm: 0.793161\tLR: 0.030000\n",
      "Train Epoch: 163 [188416/194182 (96%)]\tLoss: 0.690475\tGrad Norm: 1.055036\tLR: 0.030000\n",
      "Train set: Average loss: 0.6955\n",
      "Test set: Average loss: 0.3101, Average MAE: 0.4089\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 164 [4096/194182 (2%)]\tLoss: 0.688507\tGrad Norm: 0.722433\tLR: 0.030000\n",
      "Train Epoch: 164 [24576/194182 (12%)]\tLoss: 0.691086\tGrad Norm: 1.135141\tLR: 0.030000\n",
      "Train Epoch: 164 [45056/194182 (23%)]\tLoss: 0.691287\tGrad Norm: 1.094899\tLR: 0.030000\n",
      "Train Epoch: 164 [65536/194182 (33%)]\tLoss: 0.699921\tGrad Norm: 1.331987\tLR: 0.030000\n",
      "Train Epoch: 164 [86016/194182 (44%)]\tLoss: 0.698924\tGrad Norm: 0.981071\tLR: 0.030000\n",
      "Train Epoch: 164 [106496/194182 (54%)]\tLoss: 0.704023\tGrad Norm: 1.354747\tLR: 0.030000\n",
      "Train Epoch: 164 [126976/194182 (65%)]\tLoss: 0.690750\tGrad Norm: 0.540902\tLR: 0.030000\n",
      "Train Epoch: 164 [147456/194182 (75%)]\tLoss: 0.697165\tGrad Norm: 0.889448\tLR: 0.030000\n",
      "Train Epoch: 164 [167936/194182 (85%)]\tLoss: 0.703675\tGrad Norm: 1.394932\tLR: 0.030000\n",
      "Train Epoch: 164 [188416/194182 (96%)]\tLoss: 0.711234\tGrad Norm: 1.849314\tLR: 0.030000\n",
      "Train set: Average loss: 0.6972\n",
      "Test set: Average loss: 0.3163, Average MAE: 0.4078\n",
      "Train Epoch: 165 [4096/194182 (2%)]\tLoss: 0.689849\tGrad Norm: 2.114109\tLR: 0.030000\n",
      "Train Epoch: 165 [24576/194182 (12%)]\tLoss: 0.697598\tGrad Norm: 0.835315\tLR: 0.030000\n",
      "Train Epoch: 165 [45056/194182 (23%)]\tLoss: 0.690256\tGrad Norm: 0.933403\tLR: 0.030000\n",
      "Train Epoch: 165 [65536/194182 (33%)]\tLoss: 0.696246\tGrad Norm: 0.994189\tLR: 0.030000\n",
      "Train Epoch: 165 [86016/194182 (44%)]\tLoss: 0.697264\tGrad Norm: 0.972872\tLR: 0.030000\n",
      "Train Epoch: 165 [106496/194182 (54%)]\tLoss: 0.698140\tGrad Norm: 1.133783\tLR: 0.030000\n",
      "Train Epoch: 165 [126976/194182 (65%)]\tLoss: 0.695143\tGrad Norm: 0.820008\tLR: 0.030000\n",
      "Train Epoch: 165 [147456/194182 (75%)]\tLoss: 0.697574\tGrad Norm: 1.241436\tLR: 0.030000\n",
      "Train Epoch: 165 [167936/194182 (85%)]\tLoss: 0.691165\tGrad Norm: 0.814804\tLR: 0.030000\n",
      "Train Epoch: 165 [188416/194182 (96%)]\tLoss: 0.689927\tGrad Norm: 0.583082\tLR: 0.030000\n",
      "Train set: Average loss: 0.6940\n",
      "Test set: Average loss: 0.3109, Average MAE: 0.3999\n",
      "Epoch 165: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 166 [4096/194182 (2%)]\tLoss: 0.683995\tGrad Norm: 0.903838\tLR: 0.030000\n",
      "Train Epoch: 166 [24576/194182 (12%)]\tLoss: 0.697707\tGrad Norm: 1.313844\tLR: 0.030000\n",
      "Train Epoch: 166 [45056/194182 (23%)]\tLoss: 0.701906\tGrad Norm: 1.074648\tLR: 0.030000\n",
      "Train Epoch: 166 [65536/194182 (33%)]\tLoss: 0.698098\tGrad Norm: 1.418312\tLR: 0.030000\n",
      "Train Epoch: 166 [86016/194182 (44%)]\tLoss: 0.694816\tGrad Norm: 1.076150\tLR: 0.030000\n",
      "Train Epoch: 166 [106496/194182 (54%)]\tLoss: 0.679625\tGrad Norm: 0.721014\tLR: 0.030000\n",
      "Train Epoch: 166 [126976/194182 (65%)]\tLoss: 0.682785\tGrad Norm: 0.551972\tLR: 0.030000\n",
      "Train Epoch: 166 [147456/194182 (75%)]\tLoss: 0.693619\tGrad Norm: 1.607984\tLR: 0.030000\n",
      "Train Epoch: 166 [167936/194182 (85%)]\tLoss: 0.683243\tGrad Norm: 0.648612\tLR: 0.030000\n",
      "Train Epoch: 166 [188416/194182 (96%)]\tLoss: 0.691011\tGrad Norm: 0.865681\tLR: 0.030000\n",
      "Train set: Average loss: 0.6927\n",
      "Test set: Average loss: 0.3096, Average MAE: 0.4004\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 167 [4096/194182 (2%)]\tLoss: 0.675351\tGrad Norm: 0.837652\tLR: 0.030000\n",
      "Train Epoch: 167 [24576/194182 (12%)]\tLoss: 0.690983\tGrad Norm: 0.725505\tLR: 0.030000\n",
      "Train Epoch: 167 [45056/194182 (23%)]\tLoss: 0.683496\tGrad Norm: 0.845866\tLR: 0.030000\n",
      "Train Epoch: 167 [65536/194182 (33%)]\tLoss: 0.693083\tGrad Norm: 1.103824\tLR: 0.030000\n",
      "Train Epoch: 167 [86016/194182 (44%)]\tLoss: 0.684450\tGrad Norm: 0.869467\tLR: 0.030000\n",
      "Train Epoch: 167 [106496/194182 (54%)]\tLoss: 0.701822\tGrad Norm: 1.330250\tLR: 0.030000\n",
      "Train Epoch: 167 [126976/194182 (65%)]\tLoss: 0.697975\tGrad Norm: 1.574538\tLR: 0.030000\n",
      "Train Epoch: 167 [147456/194182 (75%)]\tLoss: 0.701017\tGrad Norm: 1.376429\tLR: 0.030000\n",
      "Train Epoch: 167 [167936/194182 (85%)]\tLoss: 0.699008\tGrad Norm: 1.210941\tLR: 0.030000\n",
      "Train Epoch: 167 [188416/194182 (96%)]\tLoss: 0.697361\tGrad Norm: 0.863377\tLR: 0.030000\n",
      "Train set: Average loss: 0.6933\n",
      "Test set: Average loss: 0.3080, Average MAE: 0.4098\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 168 [4096/194182 (2%)]\tLoss: 0.683357\tGrad Norm: 0.675132\tLR: 0.030000\n",
      "Train Epoch: 168 [24576/194182 (12%)]\tLoss: 0.688363\tGrad Norm: 1.197063\tLR: 0.030000\n",
      "Train Epoch: 168 [45056/194182 (23%)]\tLoss: 0.697796\tGrad Norm: 1.020008\tLR: 0.030000\n",
      "Train Epoch: 168 [65536/194182 (33%)]\tLoss: 0.685120\tGrad Norm: 1.002736\tLR: 0.030000\n",
      "Train Epoch: 168 [86016/194182 (44%)]\tLoss: 0.697454\tGrad Norm: 1.026249\tLR: 0.030000\n",
      "Train Epoch: 168 [106496/194182 (54%)]\tLoss: 0.689447\tGrad Norm: 0.971652\tLR: 0.030000\n",
      "Train Epoch: 168 [126976/194182 (65%)]\tLoss: 0.691663\tGrad Norm: 0.841275\tLR: 0.030000\n",
      "Train Epoch: 168 [147456/194182 (75%)]\tLoss: 0.683226\tGrad Norm: 0.931384\tLR: 0.030000\n",
      "Train Epoch: 168 [167936/194182 (85%)]\tLoss: 0.701959\tGrad Norm: 1.383617\tLR: 0.030000\n",
      "Train Epoch: 168 [188416/194182 (96%)]\tLoss: 0.691048\tGrad Norm: 1.141815\tLR: 0.030000\n",
      "Train set: Average loss: 0.6904\n",
      "Test set: Average loss: 0.3124, Average MAE: 0.4007\n",
      "Train Epoch: 169 [4096/194182 (2%)]\tLoss: 0.691375\tGrad Norm: 1.224712\tLR: 0.030000\n",
      "Train Epoch: 169 [24576/194182 (12%)]\tLoss: 0.691432\tGrad Norm: 1.031117\tLR: 0.030000\n",
      "Train Epoch: 169 [45056/194182 (23%)]\tLoss: 0.691290\tGrad Norm: 1.570249\tLR: 0.030000\n",
      "Train Epoch: 169 [65536/194182 (33%)]\tLoss: 0.680794\tGrad Norm: 0.849554\tLR: 0.030000\n",
      "Train Epoch: 169 [86016/194182 (44%)]\tLoss: 0.685343\tGrad Norm: 1.125686\tLR: 0.030000\n",
      "Train Epoch: 169 [106496/194182 (54%)]\tLoss: 0.706277\tGrad Norm: 1.501978\tLR: 0.030000\n",
      "Train Epoch: 169 [126976/194182 (65%)]\tLoss: 0.688560\tGrad Norm: 1.100545\tLR: 0.030000\n",
      "Train Epoch: 169 [147456/194182 (75%)]\tLoss: 0.677667\tGrad Norm: 1.032380\tLR: 0.030000\n",
      "Train Epoch: 169 [167936/194182 (85%)]\tLoss: 0.688154\tGrad Norm: 1.158004\tLR: 0.030000\n",
      "Train Epoch: 169 [188416/194182 (96%)]\tLoss: 0.689863\tGrad Norm: 1.236008\tLR: 0.030000\n",
      "Train set: Average loss: 0.6909\n",
      "Test set: Average loss: 0.3110, Average MAE: 0.4082\n",
      "Train Epoch: 170 [4096/194182 (2%)]\tLoss: 0.689549\tGrad Norm: 1.067148\tLR: 0.030000\n",
      "Train Epoch: 170 [24576/194182 (12%)]\tLoss: 0.684223\tGrad Norm: 1.165675\tLR: 0.030000\n",
      "Train Epoch: 170 [45056/194182 (23%)]\tLoss: 0.695028\tGrad Norm: 1.228569\tLR: 0.030000\n",
      "Train Epoch: 170 [65536/194182 (33%)]\tLoss: 0.693690\tGrad Norm: 0.968027\tLR: 0.030000\n",
      "Train Epoch: 170 [86016/194182 (44%)]\tLoss: 0.695850\tGrad Norm: 0.988783\tLR: 0.030000\n",
      "Train Epoch: 170 [106496/194182 (54%)]\tLoss: 0.684554\tGrad Norm: 0.916352\tLR: 0.030000\n",
      "Train Epoch: 170 [126976/194182 (65%)]\tLoss: 0.690705\tGrad Norm: 1.042297\tLR: 0.030000\n",
      "Train Epoch: 170 [147456/194182 (75%)]\tLoss: 0.686024\tGrad Norm: 0.770377\tLR: 0.030000\n",
      "Train Epoch: 170 [167936/194182 (85%)]\tLoss: 0.694630\tGrad Norm: 1.208456\tLR: 0.030000\n",
      "Train Epoch: 170 [188416/194182 (96%)]\tLoss: 0.693812\tGrad Norm: 1.061535\tLR: 0.030000\n",
      "Train set: Average loss: 0.6882\n",
      "Test set: Average loss: 0.3148, Average MAE: 0.4243\n",
      "Epoch 170: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 171 [4096/194182 (2%)]\tLoss: 0.699578\tGrad Norm: 1.257496\tLR: 0.030000\n",
      "Train Epoch: 171 [24576/194182 (12%)]\tLoss: 0.681853\tGrad Norm: 1.405471\tLR: 0.030000\n",
      "Train Epoch: 171 [45056/194182 (23%)]\tLoss: 0.688988\tGrad Norm: 1.105541\tLR: 0.030000\n",
      "Train Epoch: 171 [65536/194182 (33%)]\tLoss: 0.695034\tGrad Norm: 1.065330\tLR: 0.030000\n",
      "Train Epoch: 171 [86016/194182 (44%)]\tLoss: 0.697174\tGrad Norm: 0.950516\tLR: 0.030000\n",
      "Train Epoch: 171 [106496/194182 (54%)]\tLoss: 0.689560\tGrad Norm: 1.164699\tLR: 0.030000\n",
      "Train Epoch: 171 [126976/194182 (65%)]\tLoss: 0.693335\tGrad Norm: 1.190280\tLR: 0.030000\n",
      "Train Epoch: 171 [147456/194182 (75%)]\tLoss: 0.688692\tGrad Norm: 0.864116\tLR: 0.030000\n",
      "Train Epoch: 171 [167936/194182 (85%)]\tLoss: 0.686747\tGrad Norm: 1.026970\tLR: 0.030000\n",
      "Train Epoch: 171 [188416/194182 (96%)]\tLoss: 0.682475\tGrad Norm: 1.152564\tLR: 0.030000\n",
      "Train set: Average loss: 0.6885\n",
      "Test set: Average loss: 0.3100, Average MAE: 0.4038\n",
      "Train Epoch: 172 [4096/194182 (2%)]\tLoss: 0.688486\tGrad Norm: 0.986709\tLR: 0.030000\n",
      "Train Epoch: 172 [24576/194182 (12%)]\tLoss: 0.689657\tGrad Norm: 1.415169\tLR: 0.030000\n",
      "Train Epoch: 172 [45056/194182 (23%)]\tLoss: 0.680855\tGrad Norm: 1.025929\tLR: 0.030000\n",
      "Train Epoch: 172 [65536/194182 (33%)]\tLoss: 0.692835\tGrad Norm: 1.044941\tLR: 0.030000\n",
      "Train Epoch: 172 [86016/194182 (44%)]\tLoss: 0.691228\tGrad Norm: 1.348478\tLR: 0.030000\n",
      "Train Epoch: 172 [106496/194182 (54%)]\tLoss: 0.686714\tGrad Norm: 1.054161\tLR: 0.030000\n",
      "Train Epoch: 172 [126976/194182 (65%)]\tLoss: 0.676880\tGrad Norm: 0.738792\tLR: 0.030000\n",
      "Train Epoch: 172 [147456/194182 (75%)]\tLoss: 0.686711\tGrad Norm: 1.376211\tLR: 0.030000\n",
      "Train Epoch: 172 [167936/194182 (85%)]\tLoss: 0.683707\tGrad Norm: 0.865664\tLR: 0.030000\n",
      "Train Epoch: 172 [188416/194182 (96%)]\tLoss: 0.670499\tGrad Norm: 0.874862\tLR: 0.030000\n",
      "Train set: Average loss: 0.6875\n",
      "Test set: Average loss: 0.3124, Average MAE: 0.4208\n",
      "Train Epoch: 173 [4096/194182 (2%)]\tLoss: 0.690945\tGrad Norm: 1.232844\tLR: 0.030000\n",
      "Train Epoch: 173 [24576/194182 (12%)]\tLoss: 0.684417\tGrad Norm: 0.918659\tLR: 0.030000\n",
      "Train Epoch: 173 [45056/194182 (23%)]\tLoss: 0.686637\tGrad Norm: 0.941879\tLR: 0.030000\n",
      "Train Epoch: 173 [65536/194182 (33%)]\tLoss: 0.676876\tGrad Norm: 0.644645\tLR: 0.030000\n",
      "Train Epoch: 173 [86016/194182 (44%)]\tLoss: 0.688124\tGrad Norm: 1.143269\tLR: 0.030000\n",
      "Train Epoch: 173 [106496/194182 (54%)]\tLoss: 0.695075\tGrad Norm: 1.126009\tLR: 0.030000\n",
      "Train Epoch: 173 [126976/194182 (65%)]\tLoss: 0.687276\tGrad Norm: 1.300799\tLR: 0.030000\n",
      "Train Epoch: 173 [147456/194182 (75%)]\tLoss: 0.688251\tGrad Norm: 1.311900\tLR: 0.030000\n",
      "Train Epoch: 173 [167936/194182 (85%)]\tLoss: 0.689261\tGrad Norm: 1.404174\tLR: 0.030000\n",
      "Train Epoch: 173 [188416/194182 (96%)]\tLoss: 0.678438\tGrad Norm: 0.973964\tLR: 0.030000\n",
      "Train set: Average loss: 0.6863\n",
      "Test set: Average loss: 0.3064, Average MAE: 0.3953\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 174 [4096/194182 (2%)]\tLoss: 0.695303\tGrad Norm: 0.984300\tLR: 0.030000\n",
      "Train Epoch: 174 [24576/194182 (12%)]\tLoss: 0.688372\tGrad Norm: 1.016126\tLR: 0.030000\n",
      "Train Epoch: 174 [45056/194182 (23%)]\tLoss: 0.681880\tGrad Norm: 0.840295\tLR: 0.030000\n",
      "Train Epoch: 174 [65536/194182 (33%)]\tLoss: 0.681904\tGrad Norm: 0.968003\tLR: 0.030000\n",
      "Train Epoch: 174 [86016/194182 (44%)]\tLoss: 0.690001\tGrad Norm: 1.140957\tLR: 0.030000\n",
      "Train Epoch: 174 [106496/194182 (54%)]\tLoss: 0.685683\tGrad Norm: 0.758777\tLR: 0.030000\n",
      "Train Epoch: 174 [126976/194182 (65%)]\tLoss: 0.681429\tGrad Norm: 1.059725\tLR: 0.030000\n",
      "Train Epoch: 174 [147456/194182 (75%)]\tLoss: 0.686375\tGrad Norm: 1.121478\tLR: 0.030000\n",
      "Train Epoch: 174 [167936/194182 (85%)]\tLoss: 0.684051\tGrad Norm: 1.245441\tLR: 0.030000\n",
      "Train Epoch: 174 [188416/194182 (96%)]\tLoss: 0.681871\tGrad Norm: 1.206765\tLR: 0.030000\n",
      "Train set: Average loss: 0.6850\n",
      "Test set: Average loss: 0.3120, Average MAE: 0.4170\n",
      "Train Epoch: 175 [4096/194182 (2%)]\tLoss: 0.687364\tGrad Norm: 1.326565\tLR: 0.030000\n",
      "Train Epoch: 175 [24576/194182 (12%)]\tLoss: 0.675760\tGrad Norm: 0.730371\tLR: 0.030000\n",
      "Train Epoch: 175 [45056/194182 (23%)]\tLoss: 0.676756\tGrad Norm: 0.825165\tLR: 0.030000\n",
      "Train Epoch: 175 [65536/194182 (33%)]\tLoss: 0.692197\tGrad Norm: 1.215588\tLR: 0.030000\n",
      "Train Epoch: 175 [86016/194182 (44%)]\tLoss: 0.690036\tGrad Norm: 1.310146\tLR: 0.030000\n",
      "Train Epoch: 175 [106496/194182 (54%)]\tLoss: 0.688156\tGrad Norm: 1.097253\tLR: 0.030000\n",
      "Train Epoch: 175 [126976/194182 (65%)]\tLoss: 0.676508\tGrad Norm: 0.641999\tLR: 0.030000\n",
      "Train Epoch: 175 [147456/194182 (75%)]\tLoss: 0.671029\tGrad Norm: 0.780335\tLR: 0.030000\n",
      "Train Epoch: 175 [167936/194182 (85%)]\tLoss: 0.681241\tGrad Norm: 1.147898\tLR: 0.030000\n",
      "Train Epoch: 175 [188416/194182 (96%)]\tLoss: 0.683562\tGrad Norm: 0.835338\tLR: 0.030000\n",
      "Train set: Average loss: 0.6835\n",
      "Test set: Average loss: 0.3042, Average MAE: 0.4052\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 175: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 176 [4096/194182 (2%)]\tLoss: 0.679491\tGrad Norm: 0.683383\tLR: 0.030000\n",
      "Train Epoch: 176 [24576/194182 (12%)]\tLoss: 0.684124\tGrad Norm: 0.629941\tLR: 0.030000\n",
      "Train Epoch: 176 [45056/194182 (23%)]\tLoss: 0.685904\tGrad Norm: 0.917216\tLR: 0.030000\n",
      "Train Epoch: 176 [65536/194182 (33%)]\tLoss: 0.679835\tGrad Norm: 0.660267\tLR: 0.030000\n",
      "Train Epoch: 176 [86016/194182 (44%)]\tLoss: 0.683181\tGrad Norm: 1.088993\tLR: 0.030000\n",
      "Train Epoch: 176 [106496/194182 (54%)]\tLoss: 0.688502\tGrad Norm: 1.334652\tLR: 0.030000\n",
      "Train Epoch: 176 [126976/194182 (65%)]\tLoss: 0.672570\tGrad Norm: 1.221147\tLR: 0.030000\n",
      "Train Epoch: 176 [147456/194182 (75%)]\tLoss: 0.679928\tGrad Norm: 0.984956\tLR: 0.030000\n",
      "Train Epoch: 176 [167936/194182 (85%)]\tLoss: 0.675091\tGrad Norm: 0.664731\tLR: 0.030000\n",
      "Train Epoch: 176 [188416/194182 (96%)]\tLoss: 0.677901\tGrad Norm: 0.899809\tLR: 0.030000\n",
      "Train set: Average loss: 0.6810\n",
      "Test set: Average loss: 0.3083, Average MAE: 0.3943\n",
      "Train Epoch: 177 [4096/194182 (2%)]\tLoss: 0.682099\tGrad Norm: 1.137661\tLR: 0.030000\n",
      "Train Epoch: 177 [24576/194182 (12%)]\tLoss: 0.689840\tGrad Norm: 1.271921\tLR: 0.030000\n",
      "Train Epoch: 177 [45056/194182 (23%)]\tLoss: 0.677287\tGrad Norm: 0.704863\tLR: 0.030000\n",
      "Train Epoch: 177 [65536/194182 (33%)]\tLoss: 0.683843\tGrad Norm: 1.033235\tLR: 0.030000\n",
      "Train Epoch: 177 [86016/194182 (44%)]\tLoss: 0.682434\tGrad Norm: 1.187498\tLR: 0.030000\n",
      "Train Epoch: 177 [106496/194182 (54%)]\tLoss: 0.694877\tGrad Norm: 1.302826\tLR: 0.030000\n",
      "Train Epoch: 177 [126976/194182 (65%)]\tLoss: 0.684625\tGrad Norm: 1.056982\tLR: 0.030000\n",
      "Train Epoch: 177 [147456/194182 (75%)]\tLoss: 0.679616\tGrad Norm: 0.942952\tLR: 0.030000\n",
      "Train Epoch: 177 [167936/194182 (85%)]\tLoss: 0.677851\tGrad Norm: 0.624645\tLR: 0.030000\n",
      "Train Epoch: 177 [188416/194182 (96%)]\tLoss: 0.678409\tGrad Norm: 1.003770\tLR: 0.030000\n",
      "Train set: Average loss: 0.6807\n",
      "Test set: Average loss: 0.3072, Average MAE: 0.3928\n",
      "Train Epoch: 178 [4096/194182 (2%)]\tLoss: 0.687244\tGrad Norm: 1.026313\tLR: 0.030000\n",
      "Train Epoch: 178 [24576/194182 (12%)]\tLoss: 0.679625\tGrad Norm: 1.413416\tLR: 0.030000\n",
      "Train Epoch: 178 [45056/194182 (23%)]\tLoss: 0.677831\tGrad Norm: 1.066646\tLR: 0.030000\n",
      "Train Epoch: 178 [65536/194182 (33%)]\tLoss: 0.691081\tGrad Norm: 1.710215\tLR: 0.030000\n",
      "Train Epoch: 178 [86016/194182 (44%)]\tLoss: 0.674381\tGrad Norm: 1.203306\tLR: 0.030000\n",
      "Train Epoch: 178 [106496/194182 (54%)]\tLoss: 0.688391\tGrad Norm: 1.294502\tLR: 0.030000\n",
      "Train Epoch: 178 [126976/194182 (65%)]\tLoss: 0.683024\tGrad Norm: 0.917648\tLR: 0.030000\n",
      "Train Epoch: 178 [147456/194182 (75%)]\tLoss: 0.670940\tGrad Norm: 0.497080\tLR: 0.030000\n",
      "Train Epoch: 178 [167936/194182 (85%)]\tLoss: 0.675711\tGrad Norm: 0.781357\tLR: 0.030000\n",
      "Train Epoch: 178 [188416/194182 (96%)]\tLoss: 0.676006\tGrad Norm: 0.964872\tLR: 0.030000\n",
      "Train set: Average loss: 0.6809\n",
      "Test set: Average loss: 0.3055, Average MAE: 0.4038\n",
      "Train Epoch: 179 [4096/194182 (2%)]\tLoss: 0.676694\tGrad Norm: 1.142580\tLR: 0.030000\n",
      "Train Epoch: 179 [24576/194182 (12%)]\tLoss: 0.680323\tGrad Norm: 1.368189\tLR: 0.030000\n",
      "Train Epoch: 179 [45056/194182 (23%)]\tLoss: 0.685379\tGrad Norm: 1.090801\tLR: 0.030000\n",
      "Train Epoch: 179 [65536/194182 (33%)]\tLoss: 0.689220\tGrad Norm: 1.187412\tLR: 0.030000\n",
      "Train Epoch: 179 [86016/194182 (44%)]\tLoss: 0.676999\tGrad Norm: 1.194132\tLR: 0.030000\n",
      "Train Epoch: 179 [106496/194182 (54%)]\tLoss: 0.681456\tGrad Norm: 1.117419\tLR: 0.030000\n",
      "Train Epoch: 179 [126976/194182 (65%)]\tLoss: 0.676551\tGrad Norm: 1.049835\tLR: 0.030000\n",
      "Train Epoch: 179 [147456/194182 (75%)]\tLoss: 0.677832\tGrad Norm: 0.929308\tLR: 0.030000\n",
      "Train Epoch: 179 [167936/194182 (85%)]\tLoss: 0.684845\tGrad Norm: 0.979429\tLR: 0.030000\n",
      "Train Epoch: 179 [188416/194182 (96%)]\tLoss: 0.674463\tGrad Norm: 0.830181\tLR: 0.030000\n",
      "Train set: Average loss: 0.6796\n",
      "Test set: Average loss: 0.3037, Average MAE: 0.3978\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 180 [4096/194182 (2%)]\tLoss: 0.684772\tGrad Norm: 1.032661\tLR: 0.030000\n",
      "Train Epoch: 180 [24576/194182 (12%)]\tLoss: 0.684241\tGrad Norm: 1.175364\tLR: 0.030000\n",
      "Train Epoch: 180 [45056/194182 (23%)]\tLoss: 0.673981\tGrad Norm: 0.804002\tLR: 0.030000\n",
      "Train Epoch: 180 [65536/194182 (33%)]\tLoss: 0.669515\tGrad Norm: 0.830972\tLR: 0.030000\n",
      "Train Epoch: 180 [86016/194182 (44%)]\tLoss: 0.669789\tGrad Norm: 0.752550\tLR: 0.030000\n",
      "Train Epoch: 180 [106496/194182 (54%)]\tLoss: 0.672773\tGrad Norm: 0.961982\tLR: 0.030000\n",
      "Train Epoch: 180 [126976/194182 (65%)]\tLoss: 0.680135\tGrad Norm: 0.836803\tLR: 0.030000\n",
      "Train Epoch: 180 [147456/194182 (75%)]\tLoss: 0.692130\tGrad Norm: 1.294580\tLR: 0.030000\n",
      "Train Epoch: 180 [167936/194182 (85%)]\tLoss: 0.670867\tGrad Norm: 1.445598\tLR: 0.030000\n",
      "Train Epoch: 180 [188416/194182 (96%)]\tLoss: 0.690737\tGrad Norm: 1.810533\tLR: 0.030000\n",
      "Train set: Average loss: 0.6795\n",
      "Test set: Average loss: 0.3114, Average MAE: 0.4028\n",
      "Epoch 180: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 181 [4096/194182 (2%)]\tLoss: 0.683460\tGrad Norm: 1.261213\tLR: 0.030000\n",
      "Train Epoch: 181 [24576/194182 (12%)]\tLoss: 0.685799\tGrad Norm: 1.192555\tLR: 0.030000\n",
      "Train Epoch: 181 [45056/194182 (23%)]\tLoss: 0.675513\tGrad Norm: 0.954718\tLR: 0.030000\n",
      "Train Epoch: 181 [65536/194182 (33%)]\tLoss: 0.676031\tGrad Norm: 1.091224\tLR: 0.030000\n",
      "Train Epoch: 181 [86016/194182 (44%)]\tLoss: 0.668723\tGrad Norm: 0.745063\tLR: 0.030000\n",
      "Train Epoch: 181 [106496/194182 (54%)]\tLoss: 0.673813\tGrad Norm: 0.921278\tLR: 0.030000\n",
      "Train Epoch: 181 [126976/194182 (65%)]\tLoss: 0.687518\tGrad Norm: 1.317990\tLR: 0.030000\n",
      "Train Epoch: 181 [147456/194182 (75%)]\tLoss: 0.677953\tGrad Norm: 1.109036\tLR: 0.030000\n",
      "Train Epoch: 181 [167936/194182 (85%)]\tLoss: 0.672940\tGrad Norm: 1.116259\tLR: 0.030000\n",
      "Train Epoch: 181 [188416/194182 (96%)]\tLoss: 0.671813\tGrad Norm: 0.854658\tLR: 0.030000\n",
      "Train set: Average loss: 0.6780\n",
      "Test set: Average loss: 0.3043, Average MAE: 0.4091\n",
      "Train Epoch: 182 [4096/194182 (2%)]\tLoss: 0.667189\tGrad Norm: 1.400701\tLR: 0.030000\n",
      "Train Epoch: 182 [24576/194182 (12%)]\tLoss: 0.683146\tGrad Norm: 1.006397\tLR: 0.030000\n",
      "Train Epoch: 182 [45056/194182 (23%)]\tLoss: 0.681336\tGrad Norm: 1.126416\tLR: 0.030000\n",
      "Train Epoch: 182 [65536/194182 (33%)]\tLoss: 0.672089\tGrad Norm: 0.986360\tLR: 0.030000\n",
      "Train Epoch: 182 [86016/194182 (44%)]\tLoss: 0.676320\tGrad Norm: 1.213340\tLR: 0.030000\n",
      "Train Epoch: 182 [106496/194182 (54%)]\tLoss: 0.690353\tGrad Norm: 1.400791\tLR: 0.030000\n",
      "Train Epoch: 182 [126976/194182 (65%)]\tLoss: 0.682094\tGrad Norm: 1.259954\tLR: 0.030000\n",
      "Train Epoch: 182 [147456/194182 (75%)]\tLoss: 0.684991\tGrad Norm: 0.729582\tLR: 0.030000\n",
      "Train Epoch: 182 [167936/194182 (85%)]\tLoss: 0.674021\tGrad Norm: 1.206257\tLR: 0.030000\n",
      "Train Epoch: 182 [188416/194182 (96%)]\tLoss: 0.681169\tGrad Norm: 1.154070\tLR: 0.030000\n",
      "Train set: Average loss: 0.6773\n",
      "Test set: Average loss: 0.3055, Average MAE: 0.4094\n",
      "Train Epoch: 183 [4096/194182 (2%)]\tLoss: 0.667884\tGrad Norm: 0.932080\tLR: 0.030000\n",
      "Train Epoch: 183 [24576/194182 (12%)]\tLoss: 0.671288\tGrad Norm: 1.390208\tLR: 0.030000\n",
      "Train Epoch: 183 [45056/194182 (23%)]\tLoss: 0.669465\tGrad Norm: 0.721243\tLR: 0.030000\n",
      "Train Epoch: 183 [65536/194182 (33%)]\tLoss: 0.676363\tGrad Norm: 1.134627\tLR: 0.030000\n",
      "Train Epoch: 183 [86016/194182 (44%)]\tLoss: 0.684466\tGrad Norm: 0.949619\tLR: 0.030000\n",
      "Train Epoch: 183 [106496/194182 (54%)]\tLoss: 0.665366\tGrad Norm: 0.901247\tLR: 0.030000\n",
      "Train Epoch: 183 [126976/194182 (65%)]\tLoss: 0.682746\tGrad Norm: 0.686463\tLR: 0.030000\n",
      "Train Epoch: 183 [147456/194182 (75%)]\tLoss: 0.672199\tGrad Norm: 1.156117\tLR: 0.030000\n",
      "Train Epoch: 183 [167936/194182 (85%)]\tLoss: 0.685828\tGrad Norm: 1.423889\tLR: 0.030000\n",
      "Train Epoch: 183 [188416/194182 (96%)]\tLoss: 0.670614\tGrad Norm: 0.923048\tLR: 0.030000\n",
      "Train set: Average loss: 0.6760\n",
      "Test set: Average loss: 0.3018, Average MAE: 0.3982\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 184 [4096/194182 (2%)]\tLoss: 0.672736\tGrad Norm: 0.771224\tLR: 0.030000\n",
      "Train Epoch: 184 [24576/194182 (12%)]\tLoss: 0.677242\tGrad Norm: 0.914416\tLR: 0.030000\n",
      "Train Epoch: 184 [45056/194182 (23%)]\tLoss: 0.675350\tGrad Norm: 1.174006\tLR: 0.030000\n",
      "Train Epoch: 184 [65536/194182 (33%)]\tLoss: 0.682917\tGrad Norm: 1.044696\tLR: 0.030000\n",
      "Train Epoch: 184 [86016/194182 (44%)]\tLoss: 0.671959\tGrad Norm: 1.297575\tLR: 0.030000\n",
      "Train Epoch: 184 [106496/194182 (54%)]\tLoss: 0.684546\tGrad Norm: 1.201205\tLR: 0.030000\n",
      "Train Epoch: 184 [126976/194182 (65%)]\tLoss: 0.671620\tGrad Norm: 1.307276\tLR: 0.030000\n",
      "Train Epoch: 184 [147456/194182 (75%)]\tLoss: 0.686500\tGrad Norm: 1.309492\tLR: 0.030000\n",
      "Train Epoch: 184 [167936/194182 (85%)]\tLoss: 0.682527\tGrad Norm: 1.091553\tLR: 0.030000\n",
      "Train Epoch: 184 [188416/194182 (96%)]\tLoss: 0.672171\tGrad Norm: 1.357206\tLR: 0.030000\n",
      "Train set: Average loss: 0.6754\n",
      "Test set: Average loss: 0.3042, Average MAE: 0.4069\n",
      "Train Epoch: 185 [4096/194182 (2%)]\tLoss: 0.678501\tGrad Norm: 1.255036\tLR: 0.030000\n",
      "Train Epoch: 185 [24576/194182 (12%)]\tLoss: 0.682639\tGrad Norm: 1.021631\tLR: 0.030000\n",
      "Train Epoch: 185 [45056/194182 (23%)]\tLoss: 0.674228\tGrad Norm: 1.308659\tLR: 0.030000\n",
      "Train Epoch: 185 [65536/194182 (33%)]\tLoss: 0.676244\tGrad Norm: 1.240918\tLR: 0.030000\n",
      "Train Epoch: 185 [86016/194182 (44%)]\tLoss: 0.664151\tGrad Norm: 0.884430\tLR: 0.030000\n",
      "Train Epoch: 185 [106496/194182 (54%)]\tLoss: 0.676211\tGrad Norm: 1.124335\tLR: 0.030000\n",
      "Train Epoch: 185 [126976/194182 (65%)]\tLoss: 0.668583\tGrad Norm: 0.985676\tLR: 0.030000\n",
      "Train Epoch: 185 [147456/194182 (75%)]\tLoss: 0.671244\tGrad Norm: 0.965955\tLR: 0.030000\n",
      "Train Epoch: 185 [167936/194182 (85%)]\tLoss: 0.667562\tGrad Norm: 1.250808\tLR: 0.030000\n",
      "Train Epoch: 185 [188416/194182 (96%)]\tLoss: 0.658919\tGrad Norm: 0.583517\tLR: 0.030000\n",
      "Train set: Average loss: 0.6738\n",
      "Test set: Average loss: 0.3007, Average MAE: 0.3969\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 185: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 186 [4096/194182 (2%)]\tLoss: 0.667485\tGrad Norm: 0.774850\tLR: 0.030000\n",
      "Train Epoch: 186 [24576/194182 (12%)]\tLoss: 0.682083\tGrad Norm: 1.036229\tLR: 0.030000\n",
      "Train Epoch: 186 [45056/194182 (23%)]\tLoss: 0.678655\tGrad Norm: 1.058528\tLR: 0.030000\n",
      "Train Epoch: 186 [65536/194182 (33%)]\tLoss: 0.678238\tGrad Norm: 1.003384\tLR: 0.030000\n",
      "Train Epoch: 186 [86016/194182 (44%)]\tLoss: 0.675429\tGrad Norm: 1.247756\tLR: 0.030000\n",
      "Train Epoch: 186 [106496/194182 (54%)]\tLoss: 0.666191\tGrad Norm: 0.841797\tLR: 0.030000\n",
      "Train Epoch: 186 [126976/194182 (65%)]\tLoss: 0.677170\tGrad Norm: 1.556294\tLR: 0.030000\n",
      "Train Epoch: 186 [147456/194182 (75%)]\tLoss: 0.672970\tGrad Norm: 0.890646\tLR: 0.030000\n",
      "Train Epoch: 186 [167936/194182 (85%)]\tLoss: 0.666994\tGrad Norm: 0.472521\tLR: 0.030000\n",
      "Train Epoch: 186 [188416/194182 (96%)]\tLoss: 0.668350\tGrad Norm: 0.734156\tLR: 0.030000\n",
      "Train set: Average loss: 0.6714\n",
      "Test set: Average loss: 0.3012, Average MAE: 0.4045\n",
      "Train Epoch: 187 [4096/194182 (2%)]\tLoss: 0.674395\tGrad Norm: 0.975423\tLR: 0.030000\n",
      "Train Epoch: 187 [24576/194182 (12%)]\tLoss: 0.671100\tGrad Norm: 0.812499\tLR: 0.030000\n",
      "Train Epoch: 187 [45056/194182 (23%)]\tLoss: 0.662798\tGrad Norm: 0.761740\tLR: 0.030000\n",
      "Train Epoch: 187 [65536/194182 (33%)]\tLoss: 0.679353\tGrad Norm: 1.753513\tLR: 0.030000\n",
      "Train Epoch: 187 [86016/194182 (44%)]\tLoss: 0.673293\tGrad Norm: 1.130917\tLR: 0.030000\n",
      "Train Epoch: 187 [106496/194182 (54%)]\tLoss: 0.668180\tGrad Norm: 1.234658\tLR: 0.030000\n",
      "Train Epoch: 187 [126976/194182 (65%)]\tLoss: 0.664493\tGrad Norm: 1.197088\tLR: 0.030000\n",
      "Train Epoch: 187 [147456/194182 (75%)]\tLoss: 0.676072\tGrad Norm: 1.056929\tLR: 0.030000\n",
      "Train Epoch: 187 [167936/194182 (85%)]\tLoss: 0.667048\tGrad Norm: 0.775019\tLR: 0.030000\n",
      "Train Epoch: 187 [188416/194182 (96%)]\tLoss: 0.660971\tGrad Norm: 0.888817\tLR: 0.030000\n",
      "Train set: Average loss: 0.6719\n",
      "Test set: Average loss: 0.3027, Average MAE: 0.4049\n",
      "Train Epoch: 188 [4096/194182 (2%)]\tLoss: 0.674980\tGrad Norm: 1.153287\tLR: 0.030000\n",
      "Train Epoch: 188 [24576/194182 (12%)]\tLoss: 0.653412\tGrad Norm: 1.024860\tLR: 0.030000\n",
      "Train Epoch: 188 [45056/194182 (23%)]\tLoss: 0.675930\tGrad Norm: 0.859410\tLR: 0.030000\n",
      "Train Epoch: 188 [65536/194182 (33%)]\tLoss: 0.676495\tGrad Norm: 1.226626\tLR: 0.030000\n",
      "Train Epoch: 188 [86016/194182 (44%)]\tLoss: 0.680236\tGrad Norm: 1.113237\tLR: 0.030000\n",
      "Train Epoch: 188 [106496/194182 (54%)]\tLoss: 0.662266\tGrad Norm: 0.643819\tLR: 0.030000\n",
      "Train Epoch: 188 [126976/194182 (65%)]\tLoss: 0.667701\tGrad Norm: 0.959172\tLR: 0.030000\n",
      "Train Epoch: 188 [147456/194182 (75%)]\tLoss: 0.671508\tGrad Norm: 1.404656\tLR: 0.030000\n",
      "Train Epoch: 188 [167936/194182 (85%)]\tLoss: 0.672660\tGrad Norm: 0.863386\tLR: 0.030000\n",
      "Train Epoch: 188 [188416/194182 (96%)]\tLoss: 0.666634\tGrad Norm: 1.031197\tLR: 0.030000\n",
      "Train set: Average loss: 0.6697\n",
      "Test set: Average loss: 0.3031, Average MAE: 0.4110\n",
      "Train Epoch: 189 [4096/194182 (2%)]\tLoss: 0.666249\tGrad Norm: 1.012784\tLR: 0.030000\n",
      "Train Epoch: 189 [24576/194182 (12%)]\tLoss: 0.680350\tGrad Norm: 1.332106\tLR: 0.030000\n",
      "Train Epoch: 189 [45056/194182 (23%)]\tLoss: 0.671459\tGrad Norm: 1.208508\tLR: 0.030000\n",
      "Train Epoch: 189 [65536/194182 (33%)]\tLoss: 0.669778\tGrad Norm: 0.979384\tLR: 0.030000\n",
      "Train Epoch: 189 [86016/194182 (44%)]\tLoss: 0.683125\tGrad Norm: 1.634648\tLR: 0.030000\n",
      "Train Epoch: 189 [106496/194182 (54%)]\tLoss: 0.670637\tGrad Norm: 1.255487\tLR: 0.030000\n",
      "Train Epoch: 189 [126976/194182 (65%)]\tLoss: 0.671904\tGrad Norm: 1.530230\tLR: 0.030000\n",
      "Train Epoch: 189 [147456/194182 (75%)]\tLoss: 0.662119\tGrad Norm: 0.827827\tLR: 0.030000\n",
      "Train Epoch: 189 [167936/194182 (85%)]\tLoss: 0.677641\tGrad Norm: 0.903913\tLR: 0.030000\n",
      "Train Epoch: 189 [188416/194182 (96%)]\tLoss: 0.672867\tGrad Norm: 1.269029\tLR: 0.030000\n",
      "Train set: Average loss: 0.6718\n",
      "Test set: Average loss: 0.3011, Average MAE: 0.3860\n",
      "Train Epoch: 190 [4096/194182 (2%)]\tLoss: 0.668988\tGrad Norm: 0.957462\tLR: 0.030000\n",
      "Train Epoch: 190 [24576/194182 (12%)]\tLoss: 0.674590\tGrad Norm: 1.159802\tLR: 0.030000\n",
      "Train Epoch: 190 [45056/194182 (23%)]\tLoss: 0.659297\tGrad Norm: 0.724875\tLR: 0.030000\n",
      "Train Epoch: 190 [65536/194182 (33%)]\tLoss: 0.674620\tGrad Norm: 1.360619\tLR: 0.030000\n",
      "Train Epoch: 190 [86016/194182 (44%)]\tLoss: 0.676373\tGrad Norm: 1.233429\tLR: 0.030000\n",
      "Train Epoch: 190 [106496/194182 (54%)]\tLoss: 0.668955\tGrad Norm: 1.412817\tLR: 0.030000\n",
      "Train Epoch: 190 [126976/194182 (65%)]\tLoss: 0.668013\tGrad Norm: 1.239905\tLR: 0.030000\n",
      "Train Epoch: 190 [147456/194182 (75%)]\tLoss: 0.667098\tGrad Norm: 1.218596\tLR: 0.030000\n",
      "Train Epoch: 190 [167936/194182 (85%)]\tLoss: 0.676842\tGrad Norm: 1.023228\tLR: 0.030000\n",
      "Train Epoch: 190 [188416/194182 (96%)]\tLoss: 0.665921\tGrad Norm: 0.715288\tLR: 0.030000\n",
      "Train set: Average loss: 0.6698\n",
      "Test set: Average loss: 0.3008, Average MAE: 0.3998\n",
      "Epoch 190: Mean reward = 0.044 +/- 0.001\n",
      "Train Epoch: 191 [4096/194182 (2%)]\tLoss: 0.681290\tGrad Norm: 0.918179\tLR: 0.030000\n",
      "Train Epoch: 191 [24576/194182 (12%)]\tLoss: 0.660958\tGrad Norm: 0.733971\tLR: 0.030000\n",
      "Train Epoch: 191 [45056/194182 (23%)]\tLoss: 0.665325\tGrad Norm: 0.996307\tLR: 0.030000\n",
      "Train Epoch: 191 [65536/194182 (33%)]\tLoss: 0.664873\tGrad Norm: 0.845195\tLR: 0.030000\n",
      "Train Epoch: 191 [86016/194182 (44%)]\tLoss: 0.678572\tGrad Norm: 1.266086\tLR: 0.030000\n",
      "Train Epoch: 191 [106496/194182 (54%)]\tLoss: 0.667724\tGrad Norm: 1.075851\tLR: 0.030000\n",
      "Train Epoch: 191 [126976/194182 (65%)]\tLoss: 0.666697\tGrad Norm: 1.164751\tLR: 0.030000\n",
      "Train Epoch: 191 [147456/194182 (75%)]\tLoss: 0.671079\tGrad Norm: 1.203749\tLR: 0.030000\n",
      "Train Epoch: 191 [167936/194182 (85%)]\tLoss: 0.668113\tGrad Norm: 1.335488\tLR: 0.030000\n",
      "Train Epoch: 191 [188416/194182 (96%)]\tLoss: 0.671807\tGrad Norm: 1.334764\tLR: 0.030000\n",
      "Train set: Average loss: 0.6686\n",
      "Test set: Average loss: 0.3063, Average MAE: 0.4056\n",
      "Train Epoch: 192 [4096/194182 (2%)]\tLoss: 0.672019\tGrad Norm: 1.259140\tLR: 0.030000\n",
      "Train Epoch: 192 [24576/194182 (12%)]\tLoss: 0.669310\tGrad Norm: 1.057747\tLR: 0.030000\n",
      "Train Epoch: 192 [45056/194182 (23%)]\tLoss: 0.656259\tGrad Norm: 0.930299\tLR: 0.030000\n",
      "Train Epoch: 192 [65536/194182 (33%)]\tLoss: 0.668332\tGrad Norm: 0.969432\tLR: 0.030000\n",
      "Train Epoch: 192 [86016/194182 (44%)]\tLoss: 0.673444\tGrad Norm: 0.755498\tLR: 0.030000\n",
      "Train Epoch: 192 [106496/194182 (54%)]\tLoss: 0.670151\tGrad Norm: 0.657832\tLR: 0.030000\n",
      "Train Epoch: 192 [126976/194182 (65%)]\tLoss: 0.663771\tGrad Norm: 0.903774\tLR: 0.030000\n",
      "Train Epoch: 192 [147456/194182 (75%)]\tLoss: 0.675089\tGrad Norm: 1.350392\tLR: 0.030000\n",
      "Train Epoch: 192 [167936/194182 (85%)]\tLoss: 0.669211\tGrad Norm: 0.938479\tLR: 0.030000\n",
      "Train Epoch: 192 [188416/194182 (96%)]\tLoss: 0.678379\tGrad Norm: 1.385420\tLR: 0.030000\n",
      "Train set: Average loss: 0.6674\n",
      "Test set: Average loss: 0.3099, Average MAE: 0.4203\n",
      "Train Epoch: 193 [4096/194182 (2%)]\tLoss: 0.671832\tGrad Norm: 1.550218\tLR: 0.030000\n",
      "Train Epoch: 193 [24576/194182 (12%)]\tLoss: 0.677452\tGrad Norm: 1.472782\tLR: 0.030000\n",
      "Train Epoch: 193 [45056/194182 (23%)]\tLoss: 0.672081\tGrad Norm: 1.248202\tLR: 0.030000\n",
      "Train Epoch: 193 [65536/194182 (33%)]\tLoss: 0.666100\tGrad Norm: 0.899041\tLR: 0.030000\n",
      "Train Epoch: 193 [86016/194182 (44%)]\tLoss: 0.670281\tGrad Norm: 0.939147\tLR: 0.030000\n",
      "Train Epoch: 193 [106496/194182 (54%)]\tLoss: 0.665744\tGrad Norm: 1.101579\tLR: 0.030000\n",
      "Train Epoch: 193 [126976/194182 (65%)]\tLoss: 0.658912\tGrad Norm: 0.542468\tLR: 0.030000\n",
      "Train Epoch: 193 [147456/194182 (75%)]\tLoss: 0.653337\tGrad Norm: 1.023195\tLR: 0.030000\n",
      "Train Epoch: 193 [167936/194182 (85%)]\tLoss: 0.659984\tGrad Norm: 0.983505\tLR: 0.030000\n",
      "Train Epoch: 193 [188416/194182 (96%)]\tLoss: 0.665436\tGrad Norm: 1.172314\tLR: 0.030000\n",
      "Train set: Average loss: 0.6665\n",
      "Test set: Average loss: 0.3054, Average MAE: 0.4086\n",
      "Train Epoch: 194 [4096/194182 (2%)]\tLoss: 0.669228\tGrad Norm: 1.263800\tLR: 0.030000\n",
      "Train Epoch: 194 [24576/194182 (12%)]\tLoss: 0.663882\tGrad Norm: 1.084613\tLR: 0.030000\n",
      "Train Epoch: 194 [45056/194182 (23%)]\tLoss: 0.661311\tGrad Norm: 1.206820\tLR: 0.030000\n",
      "Train Epoch: 194 [65536/194182 (33%)]\tLoss: 0.666914\tGrad Norm: 0.796501\tLR: 0.030000\n",
      "Train Epoch: 194 [86016/194182 (44%)]\tLoss: 0.662302\tGrad Norm: 1.069892\tLR: 0.030000\n",
      "Train Epoch: 194 [106496/194182 (54%)]\tLoss: 0.670496\tGrad Norm: 1.007933\tLR: 0.030000\n",
      "Train Epoch: 194 [126976/194182 (65%)]\tLoss: 0.666023\tGrad Norm: 1.341518\tLR: 0.030000\n",
      "Train Epoch: 194 [147456/194182 (75%)]\tLoss: 0.672088\tGrad Norm: 0.862719\tLR: 0.030000\n",
      "Train Epoch: 194 [167936/194182 (85%)]\tLoss: 0.661788\tGrad Norm: 0.917072\tLR: 0.030000\n",
      "Train Epoch: 194 [188416/194182 (96%)]\tLoss: 0.667002\tGrad Norm: 1.028449\tLR: 0.030000\n",
      "Train set: Average loss: 0.6653\n",
      "Test set: Average loss: 0.3058, Average MAE: 0.4041\n",
      "Train Epoch: 195 [4096/194182 (2%)]\tLoss: 0.669301\tGrad Norm: 1.322172\tLR: 0.030000\n",
      "Train Epoch: 195 [24576/194182 (12%)]\tLoss: 0.661338\tGrad Norm: 1.062925\tLR: 0.030000\n",
      "Train Epoch: 195 [45056/194182 (23%)]\tLoss: 0.673998\tGrad Norm: 1.024985\tLR: 0.030000\n",
      "Train Epoch: 195 [65536/194182 (33%)]\tLoss: 0.657854\tGrad Norm: 0.911108\tLR: 0.030000\n",
      "Train Epoch: 195 [86016/194182 (44%)]\tLoss: 0.657323\tGrad Norm: 0.759395\tLR: 0.030000\n",
      "Train Epoch: 195 [106496/194182 (54%)]\tLoss: 0.676436\tGrad Norm: 1.361462\tLR: 0.030000\n",
      "Train Epoch: 195 [126976/194182 (65%)]\tLoss: 0.655214\tGrad Norm: 0.873762\tLR: 0.030000\n",
      "Train Epoch: 195 [147456/194182 (75%)]\tLoss: 0.664844\tGrad Norm: 1.187001\tLR: 0.030000\n",
      "Train Epoch: 195 [167936/194182 (85%)]\tLoss: 0.661729\tGrad Norm: 1.064115\tLR: 0.030000\n",
      "Train Epoch: 195 [188416/194182 (96%)]\tLoss: 0.665318\tGrad Norm: 1.401160\tLR: 0.030000\n",
      "Train set: Average loss: 0.6655\n",
      "Test set: Average loss: 0.3016, Average MAE: 0.4042\n",
      "Epoch 195: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 196 [4096/194182 (2%)]\tLoss: 0.669214\tGrad Norm: 1.052012\tLR: 0.030000\n",
      "Train Epoch: 196 [24576/194182 (12%)]\tLoss: 0.667698\tGrad Norm: 1.137553\tLR: 0.030000\n",
      "Train Epoch: 196 [45056/194182 (23%)]\tLoss: 0.667752\tGrad Norm: 1.183013\tLR: 0.030000\n",
      "Train Epoch: 196 [65536/194182 (33%)]\tLoss: 0.655410\tGrad Norm: 0.962020\tLR: 0.030000\n",
      "Train Epoch: 196 [86016/194182 (44%)]\tLoss: 0.659936\tGrad Norm: 1.004450\tLR: 0.030000\n",
      "Train Epoch: 196 [106496/194182 (54%)]\tLoss: 0.666667\tGrad Norm: 0.873886\tLR: 0.030000\n",
      "Train Epoch: 196 [126976/194182 (65%)]\tLoss: 0.662733\tGrad Norm: 1.083239\tLR: 0.030000\n",
      "Train Epoch: 196 [147456/194182 (75%)]\tLoss: 0.655674\tGrad Norm: 0.601897\tLR: 0.030000\n",
      "Train Epoch: 196 [167936/194182 (85%)]\tLoss: 0.656922\tGrad Norm: 1.143505\tLR: 0.030000\n",
      "Train Epoch: 196 [188416/194182 (96%)]\tLoss: 0.658450\tGrad Norm: 0.771018\tLR: 0.030000\n",
      "Train set: Average loss: 0.6631\n",
      "Test set: Average loss: 0.3005, Average MAE: 0.4002\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 197 [4096/194182 (2%)]\tLoss: 0.671280\tGrad Norm: 1.157916\tLR: 0.030000\n",
      "Train Epoch: 197 [24576/194182 (12%)]\tLoss: 0.668519\tGrad Norm: 1.085178\tLR: 0.030000\n",
      "Train Epoch: 197 [45056/194182 (23%)]\tLoss: 0.665512\tGrad Norm: 0.719323\tLR: 0.030000\n",
      "Train Epoch: 197 [65536/194182 (33%)]\tLoss: 0.672086\tGrad Norm: 1.010907\tLR: 0.030000\n",
      "Train Epoch: 197 [86016/194182 (44%)]\tLoss: 0.663158\tGrad Norm: 1.099548\tLR: 0.030000\n",
      "Train Epoch: 197 [106496/194182 (54%)]\tLoss: 0.667926\tGrad Norm: 0.907408\tLR: 0.030000\n",
      "Train Epoch: 197 [126976/194182 (65%)]\tLoss: 0.666575\tGrad Norm: 0.839200\tLR: 0.030000\n",
      "Train Epoch: 197 [147456/194182 (75%)]\tLoss: 0.664593\tGrad Norm: 1.133498\tLR: 0.030000\n",
      "Train Epoch: 197 [167936/194182 (85%)]\tLoss: 0.659262\tGrad Norm: 0.900559\tLR: 0.030000\n",
      "Train Epoch: 197 [188416/194182 (96%)]\tLoss: 0.662521\tGrad Norm: 1.500225\tLR: 0.030000\n",
      "Train set: Average loss: 0.6622\n",
      "Test set: Average loss: 0.3045, Average MAE: 0.3881\n",
      "Train Epoch: 198 [4096/194182 (2%)]\tLoss: 0.664287\tGrad Norm: 1.293917\tLR: 0.030000\n",
      "Train Epoch: 198 [24576/194182 (12%)]\tLoss: 0.663435\tGrad Norm: 0.881994\tLR: 0.030000\n",
      "Train Epoch: 198 [45056/194182 (23%)]\tLoss: 0.657696\tGrad Norm: 0.955602\tLR: 0.030000\n",
      "Train Epoch: 198 [65536/194182 (33%)]\tLoss: 0.669366\tGrad Norm: 0.953413\tLR: 0.030000\n",
      "Train Epoch: 198 [86016/194182 (44%)]\tLoss: 0.655889\tGrad Norm: 0.876874\tLR: 0.030000\n",
      "Train Epoch: 198 [106496/194182 (54%)]\tLoss: 0.658107\tGrad Norm: 1.162222\tLR: 0.030000\n",
      "Train Epoch: 198 [126976/194182 (65%)]\tLoss: 0.661659\tGrad Norm: 1.202094\tLR: 0.030000\n",
      "Train Epoch: 198 [147456/194182 (75%)]\tLoss: 0.664676\tGrad Norm: 1.359722\tLR: 0.030000\n",
      "Train Epoch: 198 [167936/194182 (85%)]\tLoss: 0.659543\tGrad Norm: 1.302889\tLR: 0.030000\n",
      "Train Epoch: 198 [188416/194182 (96%)]\tLoss: 0.658450\tGrad Norm: 0.976474\tLR: 0.030000\n",
      "Train set: Average loss: 0.6624\n",
      "Test set: Average loss: 0.3002, Average MAE: 0.3847\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 199 [4096/194182 (2%)]\tLoss: 0.673686\tGrad Norm: 1.241610\tLR: 0.030000\n",
      "Train Epoch: 199 [24576/194182 (12%)]\tLoss: 0.659525\tGrad Norm: 1.233083\tLR: 0.030000\n",
      "Train Epoch: 199 [45056/194182 (23%)]\tLoss: 0.663886\tGrad Norm: 0.897505\tLR: 0.030000\n",
      "Train Epoch: 199 [65536/194182 (33%)]\tLoss: 0.659726\tGrad Norm: 1.058069\tLR: 0.030000\n",
      "Train Epoch: 199 [86016/194182 (44%)]\tLoss: 0.669935\tGrad Norm: 1.314945\tLR: 0.030000\n",
      "Train Epoch: 199 [106496/194182 (54%)]\tLoss: 0.661776\tGrad Norm: 0.954266\tLR: 0.030000\n",
      "Train Epoch: 199 [126976/194182 (65%)]\tLoss: 0.654453\tGrad Norm: 0.870593\tLR: 0.030000\n",
      "Train Epoch: 199 [147456/194182 (75%)]\tLoss: 0.657500\tGrad Norm: 0.884283\tLR: 0.030000\n",
      "Train Epoch: 199 [167936/194182 (85%)]\tLoss: 0.660942\tGrad Norm: 1.013329\tLR: 0.030000\n",
      "Train Epoch: 199 [188416/194182 (96%)]\tLoss: 0.652617\tGrad Norm: 0.770203\tLR: 0.030000\n",
      "Train set: Average loss: 0.6609\n",
      "Test set: Average loss: 0.3001, Average MAE: 0.4026\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 200 [4096/194182 (2%)]\tLoss: 0.658130\tGrad Norm: 1.246424\tLR: 0.030000\n",
      "Train Epoch: 200 [24576/194182 (12%)]\tLoss: 0.669335\tGrad Norm: 1.082028\tLR: 0.030000\n",
      "Train Epoch: 200 [45056/194182 (23%)]\tLoss: 0.660018\tGrad Norm: 0.973934\tLR: 0.030000\n",
      "Train Epoch: 200 [65536/194182 (33%)]\tLoss: 0.661352\tGrad Norm: 0.881024\tLR: 0.030000\n",
      "Train Epoch: 200 [86016/194182 (44%)]\tLoss: 0.665306\tGrad Norm: 1.155655\tLR: 0.030000\n",
      "Train Epoch: 200 [106496/194182 (54%)]\tLoss: 0.654686\tGrad Norm: 0.917446\tLR: 0.030000\n",
      "Train Epoch: 200 [126976/194182 (65%)]\tLoss: 0.661115\tGrad Norm: 1.284766\tLR: 0.030000\n",
      "Train Epoch: 200 [147456/194182 (75%)]\tLoss: 0.655134\tGrad Norm: 1.093694\tLR: 0.030000\n",
      "Train Epoch: 200 [167936/194182 (85%)]\tLoss: 0.658580\tGrad Norm: 1.065446\tLR: 0.030000\n",
      "Train Epoch: 200 [188416/194182 (96%)]\tLoss: 0.657046\tGrad Norm: 0.911568\tLR: 0.030000\n",
      "Train set: Average loss: 0.6597\n",
      "Test set: Average loss: 0.3005, Average MAE: 0.4050\n",
      "Epoch 200: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 201 [4096/194182 (2%)]\tLoss: 0.665379\tGrad Norm: 1.176193\tLR: 0.030000\n",
      "Train Epoch: 201 [24576/194182 (12%)]\tLoss: 0.672473\tGrad Norm: 1.281667\tLR: 0.030000\n",
      "Train Epoch: 201 [45056/194182 (23%)]\tLoss: 0.660897\tGrad Norm: 0.848394\tLR: 0.030000\n",
      "Train Epoch: 201 [65536/194182 (33%)]\tLoss: 0.658526\tGrad Norm: 1.131086\tLR: 0.030000\n",
      "Train Epoch: 201 [86016/194182 (44%)]\tLoss: 0.658389\tGrad Norm: 0.673890\tLR: 0.030000\n",
      "Train Epoch: 201 [106496/194182 (54%)]\tLoss: 0.656732\tGrad Norm: 0.983644\tLR: 0.030000\n",
      "Train Epoch: 201 [126976/194182 (65%)]\tLoss: 0.656788\tGrad Norm: 1.034842\tLR: 0.030000\n",
      "Train Epoch: 201 [147456/194182 (75%)]\tLoss: 0.661019\tGrad Norm: 1.336476\tLR: 0.030000\n",
      "Train Epoch: 201 [167936/194182 (85%)]\tLoss: 0.660175\tGrad Norm: 1.004016\tLR: 0.030000\n",
      "Train Epoch: 201 [188416/194182 (96%)]\tLoss: 0.652891\tGrad Norm: 0.943674\tLR: 0.030000\n",
      "Train set: Average loss: 0.6602\n",
      "Test set: Average loss: 0.2972, Average MAE: 0.4044\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 202 [4096/194182 (2%)]\tLoss: 0.662260\tGrad Norm: 0.777090\tLR: 0.030000\n",
      "Train Epoch: 202 [24576/194182 (12%)]\tLoss: 0.655565\tGrad Norm: 0.633352\tLR: 0.030000\n",
      "Train Epoch: 202 [45056/194182 (23%)]\tLoss: 0.659168\tGrad Norm: 1.078141\tLR: 0.030000\n",
      "Train Epoch: 202 [65536/194182 (33%)]\tLoss: 0.657052\tGrad Norm: 0.400775\tLR: 0.030000\n",
      "Train Epoch: 202 [86016/194182 (44%)]\tLoss: 0.659278\tGrad Norm: 0.855166\tLR: 0.030000\n",
      "Train Epoch: 202 [106496/194182 (54%)]\tLoss: 0.652122\tGrad Norm: 0.647318\tLR: 0.030000\n",
      "Train Epoch: 202 [126976/194182 (65%)]\tLoss: 0.663588\tGrad Norm: 0.980416\tLR: 0.030000\n",
      "Train Epoch: 202 [147456/194182 (75%)]\tLoss: 0.653232\tGrad Norm: 1.071363\tLR: 0.030000\n",
      "Train Epoch: 202 [167936/194182 (85%)]\tLoss: 0.644378\tGrad Norm: 1.373673\tLR: 0.030000\n",
      "Train Epoch: 202 [188416/194182 (96%)]\tLoss: 0.665605\tGrad Norm: 1.403406\tLR: 0.030000\n",
      "Train set: Average loss: 0.6573\n",
      "Test set: Average loss: 0.3025, Average MAE: 0.3889\n",
      "Train Epoch: 203 [4096/194182 (2%)]\tLoss: 0.670305\tGrad Norm: 2.070640\tLR: 0.030000\n",
      "Train Epoch: 203 [24576/194182 (12%)]\tLoss: 0.661645\tGrad Norm: 1.293120\tLR: 0.030000\n",
      "Train Epoch: 203 [45056/194182 (23%)]\tLoss: 0.659529\tGrad Norm: 0.755569\tLR: 0.030000\n",
      "Train Epoch: 203 [65536/194182 (33%)]\tLoss: 0.650408\tGrad Norm: 0.810621\tLR: 0.030000\n",
      "Train Epoch: 203 [86016/194182 (44%)]\tLoss: 0.657746\tGrad Norm: 1.037154\tLR: 0.030000\n",
      "Train Epoch: 203 [106496/194182 (54%)]\tLoss: 0.655659\tGrad Norm: 0.693320\tLR: 0.030000\n",
      "Train Epoch: 203 [126976/194182 (65%)]\tLoss: 0.653699\tGrad Norm: 0.770856\tLR: 0.030000\n",
      "Train Epoch: 203 [147456/194182 (75%)]\tLoss: 0.650199\tGrad Norm: 1.184341\tLR: 0.030000\n",
      "Train Epoch: 203 [167936/194182 (85%)]\tLoss: 0.660667\tGrad Norm: 1.215530\tLR: 0.030000\n",
      "Train Epoch: 203 [188416/194182 (96%)]\tLoss: 0.651983\tGrad Norm: 1.129218\tLR: 0.030000\n",
      "Train set: Average loss: 0.6571\n",
      "Test set: Average loss: 0.3017, Average MAE: 0.4012\n",
      "Train Epoch: 204 [4096/194182 (2%)]\tLoss: 0.661242\tGrad Norm: 1.259947\tLR: 0.030000\n",
      "Train Epoch: 204 [24576/194182 (12%)]\tLoss: 0.663436\tGrad Norm: 1.051347\tLR: 0.030000\n",
      "Train Epoch: 204 [45056/194182 (23%)]\tLoss: 0.660009\tGrad Norm: 1.184281\tLR: 0.030000\n",
      "Train Epoch: 204 [65536/194182 (33%)]\tLoss: 0.655705\tGrad Norm: 0.767500\tLR: 0.030000\n",
      "Train Epoch: 204 [86016/194182 (44%)]\tLoss: 0.655053\tGrad Norm: 0.927248\tLR: 0.030000\n",
      "Train Epoch: 204 [106496/194182 (54%)]\tLoss: 0.644711\tGrad Norm: 0.857385\tLR: 0.030000\n",
      "Train Epoch: 204 [126976/194182 (65%)]\tLoss: 0.653315\tGrad Norm: 1.364656\tLR: 0.030000\n",
      "Train Epoch: 204 [147456/194182 (75%)]\tLoss: 0.663327\tGrad Norm: 1.114054\tLR: 0.030000\n",
      "Train Epoch: 204 [167936/194182 (85%)]\tLoss: 0.666532\tGrad Norm: 1.172504\tLR: 0.030000\n",
      "Train Epoch: 204 [188416/194182 (96%)]\tLoss: 0.646646\tGrad Norm: 0.977468\tLR: 0.030000\n",
      "Train set: Average loss: 0.6561\n",
      "Test set: Average loss: 0.2954, Average MAE: 0.3860\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 205 [4096/194182 (2%)]\tLoss: 0.655293\tGrad Norm: 0.880342\tLR: 0.030000\n",
      "Train Epoch: 205 [24576/194182 (12%)]\tLoss: 0.650731\tGrad Norm: 0.527285\tLR: 0.030000\n",
      "Train Epoch: 205 [45056/194182 (23%)]\tLoss: 0.652020\tGrad Norm: 0.837988\tLR: 0.030000\n",
      "Train Epoch: 205 [65536/194182 (33%)]\tLoss: 0.650852\tGrad Norm: 1.275504\tLR: 0.030000\n",
      "Train Epoch: 205 [86016/194182 (44%)]\tLoss: 0.656274\tGrad Norm: 1.103366\tLR: 0.030000\n",
      "Train Epoch: 205 [106496/194182 (54%)]\tLoss: 0.644950\tGrad Norm: 0.980873\tLR: 0.030000\n",
      "Train Epoch: 205 [126976/194182 (65%)]\tLoss: 0.655740\tGrad Norm: 0.939117\tLR: 0.030000\n",
      "Train Epoch: 205 [147456/194182 (75%)]\tLoss: 0.665020\tGrad Norm: 1.333752\tLR: 0.030000\n",
      "Train Epoch: 205 [167936/194182 (85%)]\tLoss: 0.654750\tGrad Norm: 1.272370\tLR: 0.030000\n",
      "Train Epoch: 205 [188416/194182 (96%)]\tLoss: 0.654917\tGrad Norm: 0.943080\tLR: 0.030000\n",
      "Train set: Average loss: 0.6553\n",
      "Test set: Average loss: 0.2980, Average MAE: 0.4033\n",
      "Epoch 205: Mean reward = 0.040 +/- 0.013\n",
      "Train Epoch: 206 [4096/194182 (2%)]\tLoss: 0.659627\tGrad Norm: 1.349737\tLR: 0.030000\n",
      "Train Epoch: 206 [24576/194182 (12%)]\tLoss: 0.649600\tGrad Norm: 0.769828\tLR: 0.030000\n",
      "Train Epoch: 206 [45056/194182 (23%)]\tLoss: 0.651038\tGrad Norm: 0.803252\tLR: 0.030000\n",
      "Train Epoch: 206 [65536/194182 (33%)]\tLoss: 0.666334\tGrad Norm: 1.013799\tLR: 0.030000\n",
      "Train Epoch: 206 [86016/194182 (44%)]\tLoss: 0.654404\tGrad Norm: 0.921401\tLR: 0.030000\n",
      "Train Epoch: 206 [106496/194182 (54%)]\tLoss: 0.655380\tGrad Norm: 1.119717\tLR: 0.030000\n",
      "Train Epoch: 206 [126976/194182 (65%)]\tLoss: 0.666992\tGrad Norm: 1.409472\tLR: 0.030000\n",
      "Train Epoch: 206 [147456/194182 (75%)]\tLoss: 0.649633\tGrad Norm: 1.308838\tLR: 0.030000\n",
      "Train Epoch: 206 [167936/194182 (85%)]\tLoss: 0.637543\tGrad Norm: 1.084971\tLR: 0.030000\n",
      "Train Epoch: 206 [188416/194182 (96%)]\tLoss: 0.641874\tGrad Norm: 0.689144\tLR: 0.030000\n",
      "Train set: Average loss: 0.6549\n",
      "Test set: Average loss: 0.3010, Average MAE: 0.4119\n",
      "Train Epoch: 207 [4096/194182 (2%)]\tLoss: 0.665129\tGrad Norm: 1.666304\tLR: 0.030000\n",
      "Train Epoch: 207 [24576/194182 (12%)]\tLoss: 0.652828\tGrad Norm: 0.909524\tLR: 0.030000\n",
      "Train Epoch: 207 [45056/194182 (23%)]\tLoss: 0.658002\tGrad Norm: 1.539783\tLR: 0.030000\n",
      "Train Epoch: 207 [65536/194182 (33%)]\tLoss: 0.657817\tGrad Norm: 1.304415\tLR: 0.030000\n",
      "Train Epoch: 207 [86016/194182 (44%)]\tLoss: 0.651417\tGrad Norm: 0.831571\tLR: 0.030000\n",
      "Train Epoch: 207 [106496/194182 (54%)]\tLoss: 0.646333\tGrad Norm: 0.841619\tLR: 0.030000\n",
      "Train Epoch: 207 [126976/194182 (65%)]\tLoss: 0.652797\tGrad Norm: 0.880802\tLR: 0.030000\n",
      "Train Epoch: 207 [147456/194182 (75%)]\tLoss: 0.658000\tGrad Norm: 1.178634\tLR: 0.030000\n",
      "Train Epoch: 207 [167936/194182 (85%)]\tLoss: 0.648824\tGrad Norm: 0.923592\tLR: 0.030000\n",
      "Train Epoch: 207 [188416/194182 (96%)]\tLoss: 0.644510\tGrad Norm: 0.553889\tLR: 0.030000\n",
      "Train set: Average loss: 0.6537\n",
      "Test set: Average loss: 0.2965, Average MAE: 0.3889\n",
      "Train Epoch: 208 [4096/194182 (2%)]\tLoss: 0.660328\tGrad Norm: 1.061279\tLR: 0.030000\n",
      "Train Epoch: 208 [24576/194182 (12%)]\tLoss: 0.646488\tGrad Norm: 1.218395\tLR: 0.030000\n",
      "Train Epoch: 208 [45056/194182 (23%)]\tLoss: 0.665526\tGrad Norm: 1.307928\tLR: 0.030000\n",
      "Train Epoch: 208 [65536/194182 (33%)]\tLoss: 0.654119\tGrad Norm: 1.064323\tLR: 0.030000\n",
      "Train Epoch: 208 [86016/194182 (44%)]\tLoss: 0.658830\tGrad Norm: 1.198895\tLR: 0.030000\n",
      "Train Epoch: 208 [106496/194182 (54%)]\tLoss: 0.656513\tGrad Norm: 1.238119\tLR: 0.030000\n",
      "Train Epoch: 208 [126976/194182 (65%)]\tLoss: 0.656011\tGrad Norm: 1.359872\tLR: 0.030000\n",
      "Train Epoch: 208 [147456/194182 (75%)]\tLoss: 0.650815\tGrad Norm: 0.639112\tLR: 0.030000\n",
      "Train Epoch: 208 [167936/194182 (85%)]\tLoss: 0.648030\tGrad Norm: 0.824419\tLR: 0.030000\n",
      "Train Epoch: 208 [188416/194182 (96%)]\tLoss: 0.654729\tGrad Norm: 0.916442\tLR: 0.030000\n",
      "Train set: Average loss: 0.6537\n",
      "Test set: Average loss: 0.2957, Average MAE: 0.3847\n",
      "Train Epoch: 209 [4096/194182 (2%)]\tLoss: 0.655161\tGrad Norm: 1.082891\tLR: 0.030000\n",
      "Train Epoch: 209 [24576/194182 (12%)]\tLoss: 0.653863\tGrad Norm: 0.888919\tLR: 0.030000\n",
      "Train Epoch: 209 [45056/194182 (23%)]\tLoss: 0.645862\tGrad Norm: 0.950033\tLR: 0.030000\n",
      "Train Epoch: 209 [65536/194182 (33%)]\tLoss: 0.653421\tGrad Norm: 0.930170\tLR: 0.030000\n",
      "Train Epoch: 209 [86016/194182 (44%)]\tLoss: 0.654985\tGrad Norm: 0.886374\tLR: 0.030000\n",
      "Train Epoch: 209 [106496/194182 (54%)]\tLoss: 0.651897\tGrad Norm: 0.798163\tLR: 0.030000\n",
      "Train Epoch: 209 [126976/194182 (65%)]\tLoss: 0.646029\tGrad Norm: 0.890071\tLR: 0.030000\n",
      "Train Epoch: 209 [147456/194182 (75%)]\tLoss: 0.646882\tGrad Norm: 1.106497\tLR: 0.030000\n",
      "Train Epoch: 209 [167936/194182 (85%)]\tLoss: 0.652410\tGrad Norm: 1.695944\tLR: 0.030000\n",
      "Train Epoch: 209 [188416/194182 (96%)]\tLoss: 0.653470\tGrad Norm: 1.566455\tLR: 0.030000\n",
      "Train set: Average loss: 0.6533\n",
      "Test set: Average loss: 0.3038, Average MAE: 0.4112\n",
      "Train Epoch: 210 [4096/194182 (2%)]\tLoss: 0.661522\tGrad Norm: 1.409408\tLR: 0.030000\n",
      "Train Epoch: 210 [24576/194182 (12%)]\tLoss: 0.646104\tGrad Norm: 1.007103\tLR: 0.030000\n",
      "Train Epoch: 210 [45056/194182 (23%)]\tLoss: 0.642123\tGrad Norm: 0.847238\tLR: 0.030000\n",
      "Train Epoch: 210 [65536/194182 (33%)]\tLoss: 0.655691\tGrad Norm: 0.854424\tLR: 0.030000\n",
      "Train Epoch: 210 [86016/194182 (44%)]\tLoss: 0.646942\tGrad Norm: 0.952841\tLR: 0.030000\n",
      "Train Epoch: 210 [106496/194182 (54%)]\tLoss: 0.659592\tGrad Norm: 1.032118\tLR: 0.030000\n",
      "Train Epoch: 210 [126976/194182 (65%)]\tLoss: 0.659370\tGrad Norm: 0.941548\tLR: 0.030000\n",
      "Train Epoch: 210 [147456/194182 (75%)]\tLoss: 0.655372\tGrad Norm: 1.042476\tLR: 0.030000\n",
      "Train Epoch: 210 [167936/194182 (85%)]\tLoss: 0.662862\tGrad Norm: 1.324715\tLR: 0.030000\n",
      "Train Epoch: 210 [188416/194182 (96%)]\tLoss: 0.649206\tGrad Norm: 1.196540\tLR: 0.030000\n",
      "Train set: Average loss: 0.6513\n",
      "Test set: Average loss: 0.2945, Average MAE: 0.3813\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 210: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 211 [4096/194182 (2%)]\tLoss: 0.639229\tGrad Norm: 0.881830\tLR: 0.030000\n",
      "Train Epoch: 211 [24576/194182 (12%)]\tLoss: 0.639242\tGrad Norm: 0.841559\tLR: 0.030000\n",
      "Train Epoch: 211 [45056/194182 (23%)]\tLoss: 0.658388\tGrad Norm: 0.943297\tLR: 0.030000\n",
      "Train Epoch: 211 [65536/194182 (33%)]\tLoss: 0.647585\tGrad Norm: 0.982827\tLR: 0.030000\n",
      "Train Epoch: 211 [86016/194182 (44%)]\tLoss: 0.652960\tGrad Norm: 1.049629\tLR: 0.030000\n",
      "Train Epoch: 211 [106496/194182 (54%)]\tLoss: 0.642056\tGrad Norm: 0.857268\tLR: 0.030000\n",
      "Train Epoch: 211 [126976/194182 (65%)]\tLoss: 0.647304\tGrad Norm: 0.563876\tLR: 0.030000\n",
      "Train Epoch: 211 [147456/194182 (75%)]\tLoss: 0.650067\tGrad Norm: 1.078203\tLR: 0.030000\n",
      "Train Epoch: 211 [167936/194182 (85%)]\tLoss: 0.651715\tGrad Norm: 1.265434\tLR: 0.030000\n",
      "Train Epoch: 211 [188416/194182 (96%)]\tLoss: 0.649847\tGrad Norm: 1.104969\tLR: 0.030000\n",
      "Train set: Average loss: 0.6501\n",
      "Test set: Average loss: 0.2937, Average MAE: 0.3927\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 212 [4096/194182 (2%)]\tLoss: 0.651201\tGrad Norm: 0.869762\tLR: 0.030000\n",
      "Train Epoch: 212 [24576/194182 (12%)]\tLoss: 0.656548\tGrad Norm: 1.154871\tLR: 0.030000\n",
      "Train Epoch: 212 [45056/194182 (23%)]\tLoss: 0.657152\tGrad Norm: 0.995719\tLR: 0.030000\n",
      "Train Epoch: 212 [65536/194182 (33%)]\tLoss: 0.648796\tGrad Norm: 1.091015\tLR: 0.030000\n",
      "Train Epoch: 212 [86016/194182 (44%)]\tLoss: 0.660688\tGrad Norm: 1.005501\tLR: 0.030000\n",
      "Train Epoch: 212 [106496/194182 (54%)]\tLoss: 0.641907\tGrad Norm: 0.588250\tLR: 0.030000\n",
      "Train Epoch: 212 [126976/194182 (65%)]\tLoss: 0.643017\tGrad Norm: 0.812582\tLR: 0.030000\n",
      "Train Epoch: 212 [147456/194182 (75%)]\tLoss: 0.657510\tGrad Norm: 1.080352\tLR: 0.030000\n",
      "Train Epoch: 212 [167936/194182 (85%)]\tLoss: 0.648539\tGrad Norm: 1.203677\tLR: 0.030000\n",
      "Train Epoch: 212 [188416/194182 (96%)]\tLoss: 0.642224\tGrad Norm: 1.096414\tLR: 0.030000\n",
      "Train set: Average loss: 0.6497\n",
      "Test set: Average loss: 0.3012, Average MAE: 0.4127\n",
      "Train Epoch: 213 [4096/194182 (2%)]\tLoss: 0.650220\tGrad Norm: 1.281495\tLR: 0.030000\n",
      "Train Epoch: 213 [24576/194182 (12%)]\tLoss: 0.645520\tGrad Norm: 1.327816\tLR: 0.030000\n",
      "Train Epoch: 213 [45056/194182 (23%)]\tLoss: 0.645891\tGrad Norm: 0.984093\tLR: 0.030000\n",
      "Train Epoch: 213 [65536/194182 (33%)]\tLoss: 0.650481\tGrad Norm: 2.397138\tLR: 0.030000\n",
      "Train Epoch: 213 [86016/194182 (44%)]\tLoss: 0.660188\tGrad Norm: 1.175344\tLR: 0.030000\n",
      "Train Epoch: 213 [106496/194182 (54%)]\tLoss: 0.647699\tGrad Norm: 0.890300\tLR: 0.030000\n",
      "Train Epoch: 213 [126976/194182 (65%)]\tLoss: 0.651640\tGrad Norm: 0.802806\tLR: 0.030000\n",
      "Train Epoch: 213 [147456/194182 (75%)]\tLoss: 0.643418\tGrad Norm: 1.014544\tLR: 0.030000\n",
      "Train Epoch: 213 [167936/194182 (85%)]\tLoss: 0.641833\tGrad Norm: 1.034101\tLR: 0.030000\n",
      "Train Epoch: 213 [188416/194182 (96%)]\tLoss: 0.645761\tGrad Norm: 0.654102\tLR: 0.030000\n",
      "Train set: Average loss: 0.6493\n",
      "Test set: Average loss: 0.2944, Average MAE: 0.3822\n",
      "Train Epoch: 214 [4096/194182 (2%)]\tLoss: 0.645167\tGrad Norm: 1.274421\tLR: 0.030000\n",
      "Train Epoch: 214 [24576/194182 (12%)]\tLoss: 0.635283\tGrad Norm: 0.810888\tLR: 0.030000\n",
      "Train Epoch: 214 [45056/194182 (23%)]\tLoss: 0.646147\tGrad Norm: 1.090351\tLR: 0.030000\n",
      "Train Epoch: 214 [65536/194182 (33%)]\tLoss: 0.655562\tGrad Norm: 1.026595\tLR: 0.030000\n",
      "Train Epoch: 214 [86016/194182 (44%)]\tLoss: 0.636858\tGrad Norm: 0.724651\tLR: 0.030000\n",
      "Train Epoch: 214 [106496/194182 (54%)]\tLoss: 0.641956\tGrad Norm: 0.812631\tLR: 0.030000\n",
      "Train Epoch: 214 [126976/194182 (65%)]\tLoss: 0.651405\tGrad Norm: 0.692718\tLR: 0.030000\n",
      "Train Epoch: 214 [147456/194182 (75%)]\tLoss: 0.644923\tGrad Norm: 1.170634\tLR: 0.030000\n",
      "Train Epoch: 214 [167936/194182 (85%)]\tLoss: 0.652034\tGrad Norm: 1.350292\tLR: 0.030000\n",
      "Train Epoch: 214 [188416/194182 (96%)]\tLoss: 0.654024\tGrad Norm: 1.180634\tLR: 0.030000\n",
      "Train set: Average loss: 0.6481\n",
      "Test set: Average loss: 0.2978, Average MAE: 0.4019\n",
      "Train Epoch: 215 [4096/194182 (2%)]\tLoss: 0.653302\tGrad Norm: 1.301222\tLR: 0.030000\n",
      "Train Epoch: 215 [24576/194182 (12%)]\tLoss: 0.641418\tGrad Norm: 0.930598\tLR: 0.030000\n",
      "Train Epoch: 215 [45056/194182 (23%)]\tLoss: 0.654803\tGrad Norm: 0.868624\tLR: 0.030000\n",
      "Train Epoch: 215 [65536/194182 (33%)]\tLoss: 0.641341\tGrad Norm: 0.819127\tLR: 0.030000\n",
      "Train Epoch: 215 [86016/194182 (44%)]\tLoss: 0.647725\tGrad Norm: 0.675078\tLR: 0.030000\n",
      "Train Epoch: 215 [106496/194182 (54%)]\tLoss: 0.656277\tGrad Norm: 1.074562\tLR: 0.030000\n",
      "Train Epoch: 215 [126976/194182 (65%)]\tLoss: 0.649864\tGrad Norm: 1.308822\tLR: 0.030000\n",
      "Train Epoch: 215 [147456/194182 (75%)]\tLoss: 0.637889\tGrad Norm: 0.799729\tLR: 0.030000\n",
      "Train Epoch: 215 [167936/194182 (85%)]\tLoss: 0.656992\tGrad Norm: 0.985438\tLR: 0.030000\n",
      "Train Epoch: 215 [188416/194182 (96%)]\tLoss: 0.645027\tGrad Norm: 1.054182\tLR: 0.030000\n",
      "Train set: Average loss: 0.6470\n",
      "Test set: Average loss: 0.2924, Average MAE: 0.3959\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 215: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 216 [4096/194182 (2%)]\tLoss: 0.633267\tGrad Norm: 0.725348\tLR: 0.030000\n",
      "Train Epoch: 216 [24576/194182 (12%)]\tLoss: 0.642980\tGrad Norm: 1.032779\tLR: 0.030000\n",
      "Train Epoch: 216 [45056/194182 (23%)]\tLoss: 0.656024\tGrad Norm: 1.576679\tLR: 0.030000\n",
      "Train Epoch: 216 [65536/194182 (33%)]\tLoss: 0.642466\tGrad Norm: 0.630296\tLR: 0.030000\n",
      "Train Epoch: 216 [86016/194182 (44%)]\tLoss: 0.651435\tGrad Norm: 1.155821\tLR: 0.030000\n",
      "Train Epoch: 216 [106496/194182 (54%)]\tLoss: 0.641895\tGrad Norm: 1.055415\tLR: 0.030000\n",
      "Train Epoch: 216 [126976/194182 (65%)]\tLoss: 0.662856\tGrad Norm: 1.125841\tLR: 0.030000\n",
      "Train Epoch: 216 [147456/194182 (75%)]\tLoss: 0.642425\tGrad Norm: 1.259015\tLR: 0.030000\n",
      "Train Epoch: 216 [167936/194182 (85%)]\tLoss: 0.647502\tGrad Norm: 1.330512\tLR: 0.030000\n",
      "Train Epoch: 216 [188416/194182 (96%)]\tLoss: 0.636166\tGrad Norm: 0.871483\tLR: 0.030000\n",
      "Train set: Average loss: 0.6476\n",
      "Test set: Average loss: 0.2907, Average MAE: 0.3883\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 217 [4096/194182 (2%)]\tLoss: 0.648213\tGrad Norm: 0.661123\tLR: 0.030000\n",
      "Train Epoch: 217 [24576/194182 (12%)]\tLoss: 0.631017\tGrad Norm: 0.682085\tLR: 0.030000\n",
      "Train Epoch: 217 [45056/194182 (23%)]\tLoss: 0.641979\tGrad Norm: 0.924528\tLR: 0.030000\n",
      "Train Epoch: 217 [65536/194182 (33%)]\tLoss: 0.650783\tGrad Norm: 0.936785\tLR: 0.030000\n",
      "Train Epoch: 217 [86016/194182 (44%)]\tLoss: 0.643540\tGrad Norm: 1.331038\tLR: 0.030000\n",
      "Train Epoch: 217 [106496/194182 (54%)]\tLoss: 0.645620\tGrad Norm: 0.770978\tLR: 0.030000\n",
      "Train Epoch: 217 [126976/194182 (65%)]\tLoss: 0.641817\tGrad Norm: 0.679018\tLR: 0.030000\n",
      "Train Epoch: 217 [147456/194182 (75%)]\tLoss: 0.640645\tGrad Norm: 0.881711\tLR: 0.030000\n",
      "Train Epoch: 217 [167936/194182 (85%)]\tLoss: 0.648871\tGrad Norm: 1.125021\tLR: 0.030000\n",
      "Train Epoch: 217 [188416/194182 (96%)]\tLoss: 0.648940\tGrad Norm: 1.515321\tLR: 0.030000\n",
      "Train set: Average loss: 0.6443\n",
      "Test set: Average loss: 0.3007, Average MAE: 0.3857\n",
      "Train Epoch: 218 [4096/194182 (2%)]\tLoss: 0.648299\tGrad Norm: 1.405501\tLR: 0.030000\n",
      "Train Epoch: 218 [24576/194182 (12%)]\tLoss: 0.634446\tGrad Norm: 0.812925\tLR: 0.030000\n",
      "Train Epoch: 218 [45056/194182 (23%)]\tLoss: 0.641066\tGrad Norm: 1.195933\tLR: 0.030000\n",
      "Train Epoch: 218 [65536/194182 (33%)]\tLoss: 0.640665\tGrad Norm: 1.197856\tLR: 0.030000\n",
      "Train Epoch: 218 [86016/194182 (44%)]\tLoss: 0.633903\tGrad Norm: 0.680207\tLR: 0.030000\n",
      "Train Epoch: 218 [106496/194182 (54%)]\tLoss: 0.657418\tGrad Norm: 0.983754\tLR: 0.030000\n",
      "Train Epoch: 218 [126976/194182 (65%)]\tLoss: 0.641604\tGrad Norm: 1.468551\tLR: 0.030000\n",
      "Train Epoch: 218 [147456/194182 (75%)]\tLoss: 0.650183\tGrad Norm: 1.203388\tLR: 0.030000\n",
      "Train Epoch: 218 [167936/194182 (85%)]\tLoss: 0.657402\tGrad Norm: 1.655019\tLR: 0.030000\n",
      "Train Epoch: 218 [188416/194182 (96%)]\tLoss: 0.635271\tGrad Norm: 0.838848\tLR: 0.030000\n",
      "Train set: Average loss: 0.6458\n",
      "Test set: Average loss: 0.2909, Average MAE: 0.3945\n",
      "Train Epoch: 219 [4096/194182 (2%)]\tLoss: 0.644061\tGrad Norm: 0.923398\tLR: 0.030000\n",
      "Train Epoch: 219 [24576/194182 (12%)]\tLoss: 0.643706\tGrad Norm: 0.797950\tLR: 0.030000\n",
      "Train Epoch: 219 [45056/194182 (23%)]\tLoss: 0.646697\tGrad Norm: 1.256385\tLR: 0.030000\n",
      "Train Epoch: 219 [65536/194182 (33%)]\tLoss: 0.645469\tGrad Norm: 1.035629\tLR: 0.030000\n",
      "Train Epoch: 219 [86016/194182 (44%)]\tLoss: 0.647317\tGrad Norm: 1.104637\tLR: 0.030000\n",
      "Train Epoch: 219 [106496/194182 (54%)]\tLoss: 0.646158\tGrad Norm: 0.964868\tLR: 0.030000\n",
      "Train Epoch: 219 [126976/194182 (65%)]\tLoss: 0.641996\tGrad Norm: 1.179478\tLR: 0.030000\n",
      "Train Epoch: 219 [147456/194182 (75%)]\tLoss: 0.645233\tGrad Norm: 0.847798\tLR: 0.030000\n",
      "Train Epoch: 219 [167936/194182 (85%)]\tLoss: 0.644601\tGrad Norm: 1.166026\tLR: 0.030000\n",
      "Train Epoch: 219 [188416/194182 (96%)]\tLoss: 0.642893\tGrad Norm: 0.968301\tLR: 0.030000\n",
      "Train set: Average loss: 0.6434\n",
      "Test set: Average loss: 0.2976, Average MAE: 0.3974\n",
      "Train Epoch: 220 [4096/194182 (2%)]\tLoss: 0.649386\tGrad Norm: 1.386560\tLR: 0.030000\n",
      "Train Epoch: 220 [24576/194182 (12%)]\tLoss: 0.642185\tGrad Norm: 1.191355\tLR: 0.030000\n",
      "Train Epoch: 220 [45056/194182 (23%)]\tLoss: 0.644924\tGrad Norm: 1.138843\tLR: 0.030000\n",
      "Train Epoch: 220 [65536/194182 (33%)]\tLoss: 0.638985\tGrad Norm: 0.784970\tLR: 0.030000\n",
      "Train Epoch: 220 [86016/194182 (44%)]\tLoss: 0.645628\tGrad Norm: 0.864359\tLR: 0.030000\n",
      "Train Epoch: 220 [106496/194182 (54%)]\tLoss: 0.630476\tGrad Norm: 0.992391\tLR: 0.030000\n",
      "Train Epoch: 220 [126976/194182 (65%)]\tLoss: 0.639680\tGrad Norm: 0.965862\tLR: 0.030000\n",
      "Train Epoch: 220 [147456/194182 (75%)]\tLoss: 0.638631\tGrad Norm: 1.205155\tLR: 0.030000\n",
      "Train Epoch: 220 [167936/194182 (85%)]\tLoss: 0.639782\tGrad Norm: 0.886766\tLR: 0.030000\n",
      "Train Epoch: 220 [188416/194182 (96%)]\tLoss: 0.643546\tGrad Norm: 0.886778\tLR: 0.030000\n",
      "Train set: Average loss: 0.6430\n",
      "Test set: Average loss: 0.2995, Average MAE: 0.4108\n",
      "Epoch 220: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 221 [4096/194182 (2%)]\tLoss: 0.654241\tGrad Norm: 1.190497\tLR: 0.030000\n",
      "Train Epoch: 221 [24576/194182 (12%)]\tLoss: 0.645733\tGrad Norm: 1.513071\tLR: 0.030000\n",
      "Train Epoch: 221 [45056/194182 (23%)]\tLoss: 0.648794\tGrad Norm: 1.046181\tLR: 0.030000\n",
      "Train Epoch: 221 [65536/194182 (33%)]\tLoss: 0.648588\tGrad Norm: 1.189343\tLR: 0.030000\n",
      "Train Epoch: 221 [86016/194182 (44%)]\tLoss: 0.646429\tGrad Norm: 0.831895\tLR: 0.030000\n",
      "Train Epoch: 221 [106496/194182 (54%)]\tLoss: 0.647570\tGrad Norm: 0.799925\tLR: 0.030000\n",
      "Train Epoch: 221 [126976/194182 (65%)]\tLoss: 0.637310\tGrad Norm: 0.982055\tLR: 0.030000\n",
      "Train Epoch: 221 [147456/194182 (75%)]\tLoss: 0.640230\tGrad Norm: 0.876395\tLR: 0.030000\n",
      "Train Epoch: 221 [167936/194182 (85%)]\tLoss: 0.648719\tGrad Norm: 0.934806\tLR: 0.030000\n",
      "Train Epoch: 221 [188416/194182 (96%)]\tLoss: 0.640769\tGrad Norm: 1.126175\tLR: 0.030000\n",
      "Train set: Average loss: 0.6428\n",
      "Test set: Average loss: 0.2910, Average MAE: 0.3934\n",
      "Train Epoch: 222 [4096/194182 (2%)]\tLoss: 0.647869\tGrad Norm: 0.757541\tLR: 0.030000\n",
      "Train Epoch: 222 [24576/194182 (12%)]\tLoss: 0.643340\tGrad Norm: 0.958673\tLR: 0.030000\n",
      "Train Epoch: 222 [45056/194182 (23%)]\tLoss: 0.643922\tGrad Norm: 0.960125\tLR: 0.030000\n",
      "Train Epoch: 222 [65536/194182 (33%)]\tLoss: 0.643010\tGrad Norm: 0.913714\tLR: 0.030000\n",
      "Train Epoch: 222 [86016/194182 (44%)]\tLoss: 0.641688\tGrad Norm: 1.232279\tLR: 0.030000\n",
      "Train Epoch: 222 [106496/194182 (54%)]\tLoss: 0.652407\tGrad Norm: 1.176990\tLR: 0.030000\n",
      "Train Epoch: 222 [126976/194182 (65%)]\tLoss: 0.638684\tGrad Norm: 0.951581\tLR: 0.030000\n",
      "Train Epoch: 222 [147456/194182 (75%)]\tLoss: 0.636439\tGrad Norm: 0.921371\tLR: 0.030000\n",
      "Train Epoch: 222 [167936/194182 (85%)]\tLoss: 0.641487\tGrad Norm: 0.859074\tLR: 0.030000\n",
      "Train Epoch: 222 [188416/194182 (96%)]\tLoss: 0.643848\tGrad Norm: 1.266488\tLR: 0.030000\n",
      "Train set: Average loss: 0.6416\n",
      "Test set: Average loss: 0.2930, Average MAE: 0.3913\n",
      "Train Epoch: 223 [4096/194182 (2%)]\tLoss: 0.646250\tGrad Norm: 0.977027\tLR: 0.030000\n",
      "Train Epoch: 223 [24576/194182 (12%)]\tLoss: 0.639614\tGrad Norm: 0.890010\tLR: 0.030000\n",
      "Train Epoch: 223 [45056/194182 (23%)]\tLoss: 0.650944\tGrad Norm: 1.548825\tLR: 0.030000\n",
      "Train Epoch: 223 [65536/194182 (33%)]\tLoss: 0.637576\tGrad Norm: 0.845319\tLR: 0.030000\n",
      "Train Epoch: 223 [86016/194182 (44%)]\tLoss: 0.647264\tGrad Norm: 1.224418\tLR: 0.030000\n",
      "Train Epoch: 223 [106496/194182 (54%)]\tLoss: 0.646266\tGrad Norm: 1.218385\tLR: 0.030000\n",
      "Train Epoch: 223 [126976/194182 (65%)]\tLoss: 0.642670\tGrad Norm: 1.083534\tLR: 0.030000\n",
      "Train Epoch: 223 [147456/194182 (75%)]\tLoss: 0.636834\tGrad Norm: 1.045296\tLR: 0.030000\n",
      "Train Epoch: 223 [167936/194182 (85%)]\tLoss: 0.631405\tGrad Norm: 0.523705\tLR: 0.030000\n",
      "Train Epoch: 223 [188416/194182 (96%)]\tLoss: 0.634439\tGrad Norm: 0.628653\tLR: 0.030000\n",
      "Train set: Average loss: 0.6403\n",
      "Test set: Average loss: 0.2885, Average MAE: 0.3827\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 224 [4096/194182 (2%)]\tLoss: 0.635006\tGrad Norm: 0.848626\tLR: 0.030000\n",
      "Train Epoch: 224 [24576/194182 (12%)]\tLoss: 0.635795\tGrad Norm: 0.750274\tLR: 0.030000\n",
      "Train Epoch: 224 [45056/194182 (23%)]\tLoss: 0.629147\tGrad Norm: 0.969872\tLR: 0.030000\n",
      "Train Epoch: 224 [65536/194182 (33%)]\tLoss: 0.638973\tGrad Norm: 0.967691\tLR: 0.030000\n",
      "Train Epoch: 224 [86016/194182 (44%)]\tLoss: 0.638761\tGrad Norm: 1.018904\tLR: 0.030000\n",
      "Train Epoch: 224 [106496/194182 (54%)]\tLoss: 0.654071\tGrad Norm: 1.407300\tLR: 0.030000\n",
      "Train Epoch: 224 [126976/194182 (65%)]\tLoss: 0.655077\tGrad Norm: 1.379548\tLR: 0.030000\n",
      "Train Epoch: 224 [147456/194182 (75%)]\tLoss: 0.641567\tGrad Norm: 1.118947\tLR: 0.030000\n",
      "Train Epoch: 224 [167936/194182 (85%)]\tLoss: 0.640348\tGrad Norm: 1.381825\tLR: 0.030000\n",
      "Train Epoch: 224 [188416/194182 (96%)]\tLoss: 0.647405\tGrad Norm: 1.094815\tLR: 0.030000\n",
      "Train set: Average loss: 0.6412\n",
      "Test set: Average loss: 0.2945, Average MAE: 0.4043\n",
      "Train Epoch: 225 [4096/194182 (2%)]\tLoss: 0.645457\tGrad Norm: 1.110927\tLR: 0.030000\n",
      "Train Epoch: 225 [24576/194182 (12%)]\tLoss: 0.632912\tGrad Norm: 0.718190\tLR: 0.030000\n",
      "Train Epoch: 225 [45056/194182 (23%)]\tLoss: 0.639642\tGrad Norm: 0.552418\tLR: 0.030000\n",
      "Train Epoch: 225 [65536/194182 (33%)]\tLoss: 0.641785\tGrad Norm: 1.082857\tLR: 0.030000\n",
      "Train Epoch: 225 [86016/194182 (44%)]\tLoss: 0.643643\tGrad Norm: 1.210907\tLR: 0.030000\n",
      "Train Epoch: 225 [106496/194182 (54%)]\tLoss: 0.636261\tGrad Norm: 1.098782\tLR: 0.030000\n",
      "Train Epoch: 225 [126976/194182 (65%)]\tLoss: 0.650464\tGrad Norm: 1.448179\tLR: 0.030000\n",
      "Train Epoch: 225 [147456/194182 (75%)]\tLoss: 0.639419\tGrad Norm: 1.235585\tLR: 0.030000\n",
      "Train Epoch: 225 [167936/194182 (85%)]\tLoss: 0.636056\tGrad Norm: 0.718350\tLR: 0.030000\n",
      "Train Epoch: 225 [188416/194182 (96%)]\tLoss: 0.641996\tGrad Norm: 0.960815\tLR: 0.030000\n",
      "Train set: Average loss: 0.6392\n",
      "Test set: Average loss: 0.2940, Average MAE: 0.3885\n",
      "Epoch 225: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 226 [4096/194182 (2%)]\tLoss: 0.642120\tGrad Norm: 1.163021\tLR: 0.030000\n",
      "Train Epoch: 226 [24576/194182 (12%)]\tLoss: 0.643978\tGrad Norm: 1.312676\tLR: 0.030000\n",
      "Train Epoch: 226 [45056/194182 (23%)]\tLoss: 0.637347\tGrad Norm: 1.309595\tLR: 0.030000\n",
      "Train Epoch: 226 [65536/194182 (33%)]\tLoss: 0.642442\tGrad Norm: 0.940146\tLR: 0.030000\n",
      "Train Epoch: 226 [86016/194182 (44%)]\tLoss: 0.632773\tGrad Norm: 0.729476\tLR: 0.030000\n",
      "Train Epoch: 226 [106496/194182 (54%)]\tLoss: 0.628598\tGrad Norm: 0.897721\tLR: 0.030000\n",
      "Train Epoch: 226 [126976/194182 (65%)]\tLoss: 0.651284\tGrad Norm: 1.383191\tLR: 0.030000\n",
      "Train Epoch: 226 [147456/194182 (75%)]\tLoss: 0.642111\tGrad Norm: 1.066557\tLR: 0.030000\n",
      "Train Epoch: 226 [167936/194182 (85%)]\tLoss: 0.629064\tGrad Norm: 0.722312\tLR: 0.030000\n",
      "Train Epoch: 226 [188416/194182 (96%)]\tLoss: 0.635683\tGrad Norm: 0.895973\tLR: 0.030000\n",
      "Train set: Average loss: 0.6382\n",
      "Test set: Average loss: 0.2947, Average MAE: 0.3793\n",
      "Train Epoch: 227 [4096/194182 (2%)]\tLoss: 0.645278\tGrad Norm: 1.302935\tLR: 0.030000\n",
      "Train Epoch: 227 [24576/194182 (12%)]\tLoss: 0.632761\tGrad Norm: 0.763200\tLR: 0.030000\n",
      "Train Epoch: 227 [45056/194182 (23%)]\tLoss: 0.638045\tGrad Norm: 0.852697\tLR: 0.030000\n",
      "Train Epoch: 227 [65536/194182 (33%)]\tLoss: 0.635535\tGrad Norm: 1.440485\tLR: 0.030000\n",
      "Train Epoch: 227 [86016/194182 (44%)]\tLoss: 0.636980\tGrad Norm: 1.114582\tLR: 0.030000\n",
      "Train Epoch: 227 [106496/194182 (54%)]\tLoss: 0.643413\tGrad Norm: 1.513566\tLR: 0.030000\n",
      "Train Epoch: 227 [126976/194182 (65%)]\tLoss: 0.638061\tGrad Norm: 0.892051\tLR: 0.030000\n",
      "Train Epoch: 227 [147456/194182 (75%)]\tLoss: 0.635421\tGrad Norm: 1.008290\tLR: 0.030000\n",
      "Train Epoch: 227 [167936/194182 (85%)]\tLoss: 0.646130\tGrad Norm: 1.391148\tLR: 0.030000\n",
      "Train Epoch: 227 [188416/194182 (96%)]\tLoss: 0.647250\tGrad Norm: 1.038077\tLR: 0.030000\n",
      "Train set: Average loss: 0.6386\n",
      "Test set: Average loss: 0.2907, Average MAE: 0.3854\n",
      "Train Epoch: 228 [4096/194182 (2%)]\tLoss: 0.639313\tGrad Norm: 0.936887\tLR: 0.030000\n",
      "Train Epoch: 228 [24576/194182 (12%)]\tLoss: 0.633715\tGrad Norm: 0.938967\tLR: 0.030000\n",
      "Train Epoch: 228 [45056/194182 (23%)]\tLoss: 0.644237\tGrad Norm: 1.252976\tLR: 0.030000\n",
      "Train Epoch: 228 [65536/194182 (33%)]\tLoss: 0.638780\tGrad Norm: 1.408693\tLR: 0.030000\n",
      "Train Epoch: 228 [86016/194182 (44%)]\tLoss: 0.636240\tGrad Norm: 1.206038\tLR: 0.030000\n",
      "Train Epoch: 228 [106496/194182 (54%)]\tLoss: 0.642963\tGrad Norm: 1.328045\tLR: 0.030000\n",
      "Train Epoch: 228 [126976/194182 (65%)]\tLoss: 0.635269\tGrad Norm: 0.843053\tLR: 0.030000\n",
      "Train Epoch: 228 [147456/194182 (75%)]\tLoss: 0.640359\tGrad Norm: 1.078740\tLR: 0.030000\n",
      "Train Epoch: 228 [167936/194182 (85%)]\tLoss: 0.641303\tGrad Norm: 0.956109\tLR: 0.030000\n",
      "Train Epoch: 228 [188416/194182 (96%)]\tLoss: 0.637800\tGrad Norm: 1.071624\tLR: 0.030000\n",
      "Train set: Average loss: 0.6376\n",
      "Test set: Average loss: 0.2924, Average MAE: 0.4017\n",
      "Train Epoch: 229 [4096/194182 (2%)]\tLoss: 0.635171\tGrad Norm: 1.004819\tLR: 0.030000\n",
      "Train Epoch: 229 [24576/194182 (12%)]\tLoss: 0.647194\tGrad Norm: 1.314999\tLR: 0.030000\n",
      "Train Epoch: 229 [45056/194182 (23%)]\tLoss: 0.645742\tGrad Norm: 0.725617\tLR: 0.030000\n",
      "Train Epoch: 229 [65536/194182 (33%)]\tLoss: 0.634075\tGrad Norm: 1.259720\tLR: 0.030000\n",
      "Train Epoch: 229 [86016/194182 (44%)]\tLoss: 0.630159\tGrad Norm: 0.924699\tLR: 0.030000\n",
      "Train Epoch: 229 [106496/194182 (54%)]\tLoss: 0.626801\tGrad Norm: 0.735356\tLR: 0.030000\n",
      "Train Epoch: 229 [126976/194182 (65%)]\tLoss: 0.632777\tGrad Norm: 1.165942\tLR: 0.030000\n",
      "Train Epoch: 229 [147456/194182 (75%)]\tLoss: 0.631984\tGrad Norm: 0.888616\tLR: 0.030000\n",
      "Train Epoch: 229 [167936/194182 (85%)]\tLoss: 0.633090\tGrad Norm: 1.135462\tLR: 0.030000\n",
      "Train Epoch: 229 [188416/194182 (96%)]\tLoss: 0.638204\tGrad Norm: 1.048551\tLR: 0.030000\n",
      "Train set: Average loss: 0.6353\n",
      "Test set: Average loss: 0.2975, Average MAE: 0.3938\n",
      "Train Epoch: 230 [4096/194182 (2%)]\tLoss: 0.634851\tGrad Norm: 1.423115\tLR: 0.030000\n",
      "Train Epoch: 230 [24576/194182 (12%)]\tLoss: 0.628101\tGrad Norm: 1.167520\tLR: 0.030000\n",
      "Train Epoch: 230 [45056/194182 (23%)]\tLoss: 0.636221\tGrad Norm: 1.001040\tLR: 0.030000\n",
      "Train Epoch: 230 [65536/194182 (33%)]\tLoss: 0.635702\tGrad Norm: 0.882962\tLR: 0.030000\n",
      "Train Epoch: 230 [86016/194182 (44%)]\tLoss: 0.639407\tGrad Norm: 0.982526\tLR: 0.030000\n",
      "Train Epoch: 230 [106496/194182 (54%)]\tLoss: 0.646739\tGrad Norm: 1.398410\tLR: 0.030000\n",
      "Train Epoch: 230 [126976/194182 (65%)]\tLoss: 0.644042\tGrad Norm: 1.319712\tLR: 0.030000\n",
      "Train Epoch: 230 [147456/194182 (75%)]\tLoss: 0.634484\tGrad Norm: 1.237937\tLR: 0.030000\n",
      "Train Epoch: 230 [167936/194182 (85%)]\tLoss: 0.639707\tGrad Norm: 1.037664\tLR: 0.030000\n",
      "Train Epoch: 230 [188416/194182 (96%)]\tLoss: 0.626497\tGrad Norm: 0.497788\tLR: 0.030000\n",
      "Train set: Average loss: 0.6361\n",
      "Test set: Average loss: 0.2872, Average MAE: 0.3872\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 230: Mean reward = 0.040 +/- 0.013\n",
      "Train Epoch: 231 [4096/194182 (2%)]\tLoss: 0.633483\tGrad Norm: 0.716249\tLR: 0.030000\n",
      "Train Epoch: 231 [24576/194182 (12%)]\tLoss: 0.630691\tGrad Norm: 1.243948\tLR: 0.030000\n",
      "Train Epoch: 231 [45056/194182 (23%)]\tLoss: 0.638904\tGrad Norm: 1.124630\tLR: 0.030000\n",
      "Train Epoch: 231 [65536/194182 (33%)]\tLoss: 0.640343\tGrad Norm: 1.108445\tLR: 0.030000\n",
      "Train Epoch: 231 [86016/194182 (44%)]\tLoss: 0.630787\tGrad Norm: 1.224269\tLR: 0.030000\n",
      "Train Epoch: 231 [106496/194182 (54%)]\tLoss: 0.627859\tGrad Norm: 0.801070\tLR: 0.030000\n",
      "Train Epoch: 231 [126976/194182 (65%)]\tLoss: 0.622250\tGrad Norm: 0.458265\tLR: 0.030000\n",
      "Train Epoch: 231 [147456/194182 (75%)]\tLoss: 0.631780\tGrad Norm: 1.254786\tLR: 0.030000\n",
      "Train Epoch: 231 [167936/194182 (85%)]\tLoss: 0.628934\tGrad Norm: 0.859345\tLR: 0.030000\n",
      "Train Epoch: 231 [188416/194182 (96%)]\tLoss: 0.641156\tGrad Norm: 1.214433\tLR: 0.030000\n",
      "Train set: Average loss: 0.6338\n",
      "Test set: Average loss: 0.2903, Average MAE: 0.3791\n",
      "Train Epoch: 232 [4096/194182 (2%)]\tLoss: 0.640244\tGrad Norm: 1.030726\tLR: 0.030000\n",
      "Train Epoch: 232 [24576/194182 (12%)]\tLoss: 0.640111\tGrad Norm: 1.472577\tLR: 0.030000\n",
      "Train Epoch: 232 [45056/194182 (23%)]\tLoss: 0.620209\tGrad Norm: 0.775548\tLR: 0.030000\n",
      "Train Epoch: 232 [65536/194182 (33%)]\tLoss: 0.643317\tGrad Norm: 0.954413\tLR: 0.030000\n",
      "Train Epoch: 232 [86016/194182 (44%)]\tLoss: 0.634652\tGrad Norm: 0.739887\tLR: 0.030000\n",
      "Train Epoch: 232 [106496/194182 (54%)]\tLoss: 0.635357\tGrad Norm: 1.102055\tLR: 0.030000\n",
      "Train Epoch: 232 [126976/194182 (65%)]\tLoss: 0.637335\tGrad Norm: 1.109925\tLR: 0.030000\n",
      "Train Epoch: 232 [147456/194182 (75%)]\tLoss: 0.638060\tGrad Norm: 1.195682\tLR: 0.030000\n",
      "Train Epoch: 232 [167936/194182 (85%)]\tLoss: 0.638308\tGrad Norm: 1.175279\tLR: 0.030000\n",
      "Train Epoch: 232 [188416/194182 (96%)]\tLoss: 0.635036\tGrad Norm: 1.235928\tLR: 0.030000\n",
      "Train set: Average loss: 0.6345\n",
      "Test set: Average loss: 0.2949, Average MAE: 0.4041\n",
      "Train Epoch: 233 [4096/194182 (2%)]\tLoss: 0.633404\tGrad Norm: 1.283398\tLR: 0.030000\n",
      "Train Epoch: 233 [24576/194182 (12%)]\tLoss: 0.623894\tGrad Norm: 0.743946\tLR: 0.030000\n",
      "Train Epoch: 233 [45056/194182 (23%)]\tLoss: 0.637025\tGrad Norm: 0.946894\tLR: 0.030000\n",
      "Train Epoch: 233 [65536/194182 (33%)]\tLoss: 0.630821\tGrad Norm: 1.011328\tLR: 0.030000\n",
      "Train Epoch: 233 [86016/194182 (44%)]\tLoss: 0.625776\tGrad Norm: 0.498073\tLR: 0.030000\n",
      "Train Epoch: 233 [106496/194182 (54%)]\tLoss: 0.643632\tGrad Norm: 1.167303\tLR: 0.030000\n",
      "Train Epoch: 233 [126976/194182 (65%)]\tLoss: 0.625175\tGrad Norm: 0.638748\tLR: 0.030000\n",
      "Train Epoch: 233 [147456/194182 (75%)]\tLoss: 0.637327\tGrad Norm: 1.026775\tLR: 0.030000\n",
      "Train Epoch: 233 [167936/194182 (85%)]\tLoss: 0.625250\tGrad Norm: 0.766713\tLR: 0.030000\n",
      "Train Epoch: 233 [188416/194182 (96%)]\tLoss: 0.642683\tGrad Norm: 1.164936\tLR: 0.030000\n",
      "Train set: Average loss: 0.6314\n",
      "Test set: Average loss: 0.2959, Average MAE: 0.4053\n",
      "Train Epoch: 234 [4096/194182 (2%)]\tLoss: 0.635603\tGrad Norm: 1.288393\tLR: 0.030000\n",
      "Train Epoch: 234 [24576/194182 (12%)]\tLoss: 0.629896\tGrad Norm: 1.290055\tLR: 0.030000\n",
      "Train Epoch: 234 [45056/194182 (23%)]\tLoss: 0.639588\tGrad Norm: 1.370700\tLR: 0.030000\n",
      "Train Epoch: 234 [65536/194182 (33%)]\tLoss: 0.636909\tGrad Norm: 1.054385\tLR: 0.030000\n",
      "Train Epoch: 234 [86016/194182 (44%)]\tLoss: 0.630987\tGrad Norm: 1.184233\tLR: 0.030000\n",
      "Train Epoch: 234 [106496/194182 (54%)]\tLoss: 0.639234\tGrad Norm: 1.092697\tLR: 0.030000\n",
      "Train Epoch: 234 [126976/194182 (65%)]\tLoss: 0.623478\tGrad Norm: 0.575335\tLR: 0.030000\n",
      "Train Epoch: 234 [147456/194182 (75%)]\tLoss: 0.632889\tGrad Norm: 0.926535\tLR: 0.030000\n",
      "Train Epoch: 234 [167936/194182 (85%)]\tLoss: 0.627134\tGrad Norm: 1.175053\tLR: 0.030000\n",
      "Train Epoch: 234 [188416/194182 (96%)]\tLoss: 0.634058\tGrad Norm: 1.119748\tLR: 0.030000\n",
      "Train set: Average loss: 0.6330\n",
      "Test set: Average loss: 0.2927, Average MAE: 0.3974\n",
      "Train Epoch: 235 [4096/194182 (2%)]\tLoss: 0.632133\tGrad Norm: 1.229823\tLR: 0.030000\n",
      "Train Epoch: 235 [24576/194182 (12%)]\tLoss: 0.641437\tGrad Norm: 1.285098\tLR: 0.030000\n",
      "Train Epoch: 235 [45056/194182 (23%)]\tLoss: 0.636464\tGrad Norm: 1.060599\tLR: 0.030000\n",
      "Train Epoch: 235 [65536/194182 (33%)]\tLoss: 0.632475\tGrad Norm: 1.195315\tLR: 0.030000\n",
      "Train Epoch: 235 [86016/194182 (44%)]\tLoss: 0.632153\tGrad Norm: 1.421913\tLR: 0.030000\n",
      "Train Epoch: 235 [106496/194182 (54%)]\tLoss: 0.631169\tGrad Norm: 0.980979\tLR: 0.030000\n",
      "Train Epoch: 235 [126976/194182 (65%)]\tLoss: 0.634551\tGrad Norm: 0.662895\tLR: 0.030000\n",
      "Train Epoch: 235 [147456/194182 (75%)]\tLoss: 0.627914\tGrad Norm: 0.484472\tLR: 0.030000\n",
      "Train Epoch: 235 [167936/194182 (85%)]\tLoss: 0.626025\tGrad Norm: 0.922598\tLR: 0.030000\n",
      "Train Epoch: 235 [188416/194182 (96%)]\tLoss: 0.639097\tGrad Norm: 1.345636\tLR: 0.030000\n",
      "Train set: Average loss: 0.6314\n",
      "Test set: Average loss: 0.2948, Average MAE: 0.3832\n",
      "Epoch 235: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 236 [4096/194182 (2%)]\tLoss: 0.632796\tGrad Norm: 1.454353\tLR: 0.030000\n",
      "Train Epoch: 236 [24576/194182 (12%)]\tLoss: 0.638530\tGrad Norm: 1.298958\tLR: 0.030000\n",
      "Train Epoch: 236 [45056/194182 (23%)]\tLoss: 0.632818\tGrad Norm: 1.073585\tLR: 0.030000\n",
      "Train Epoch: 236 [65536/194182 (33%)]\tLoss: 0.633033\tGrad Norm: 0.920432\tLR: 0.030000\n",
      "Train Epoch: 236 [86016/194182 (44%)]\tLoss: 0.621088\tGrad Norm: 0.695939\tLR: 0.030000\n",
      "Train Epoch: 236 [106496/194182 (54%)]\tLoss: 0.637766\tGrad Norm: 0.753124\tLR: 0.030000\n",
      "Train Epoch: 236 [126976/194182 (65%)]\tLoss: 0.632504\tGrad Norm: 0.899399\tLR: 0.030000\n",
      "Train Epoch: 236 [147456/194182 (75%)]\tLoss: 0.630839\tGrad Norm: 0.888581\tLR: 0.030000\n",
      "Train Epoch: 236 [167936/194182 (85%)]\tLoss: 0.640820\tGrad Norm: 1.161641\tLR: 0.030000\n",
      "Train Epoch: 236 [188416/194182 (96%)]\tLoss: 0.633871\tGrad Norm: 1.081544\tLR: 0.030000\n",
      "Train set: Average loss: 0.6308\n",
      "Test set: Average loss: 0.2895, Average MAE: 0.3932\n",
      "Train Epoch: 237 [4096/194182 (2%)]\tLoss: 0.629311\tGrad Norm: 0.995636\tLR: 0.030000\n",
      "Train Epoch: 237 [24576/194182 (12%)]\tLoss: 0.630156\tGrad Norm: 1.389108\tLR: 0.030000\n",
      "Train Epoch: 237 [45056/194182 (23%)]\tLoss: 0.634570\tGrad Norm: 0.842218\tLR: 0.030000\n",
      "Train Epoch: 237 [65536/194182 (33%)]\tLoss: 0.631725\tGrad Norm: 1.100438\tLR: 0.030000\n",
      "Train Epoch: 237 [86016/194182 (44%)]\tLoss: 0.635471\tGrad Norm: 0.929469\tLR: 0.030000\n",
      "Train Epoch: 237 [106496/194182 (54%)]\tLoss: 0.630543\tGrad Norm: 1.442246\tLR: 0.030000\n",
      "Train Epoch: 237 [126976/194182 (65%)]\tLoss: 0.635662\tGrad Norm: 1.038862\tLR: 0.030000\n",
      "Train Epoch: 237 [147456/194182 (75%)]\tLoss: 0.621506\tGrad Norm: 0.939648\tLR: 0.030000\n",
      "Train Epoch: 237 [167936/194182 (85%)]\tLoss: 0.621551\tGrad Norm: 0.755546\tLR: 0.030000\n",
      "Train Epoch: 237 [188416/194182 (96%)]\tLoss: 0.630610\tGrad Norm: 1.127424\tLR: 0.030000\n",
      "Train set: Average loss: 0.6305\n",
      "Test set: Average loss: 0.2914, Average MAE: 0.3936\n",
      "Train Epoch: 238 [4096/194182 (2%)]\tLoss: 0.629345\tGrad Norm: 1.108604\tLR: 0.030000\n",
      "Train Epoch: 238 [24576/194182 (12%)]\tLoss: 0.626896\tGrad Norm: 1.149260\tLR: 0.030000\n",
      "Train Epoch: 238 [45056/194182 (23%)]\tLoss: 0.632171\tGrad Norm: 1.425875\tLR: 0.030000\n",
      "Train Epoch: 238 [65536/194182 (33%)]\tLoss: 0.630814\tGrad Norm: 1.496433\tLR: 0.030000\n",
      "Train Epoch: 238 [86016/194182 (44%)]\tLoss: 0.622852\tGrad Norm: 0.848722\tLR: 0.030000\n",
      "Train Epoch: 238 [106496/194182 (54%)]\tLoss: 0.630605\tGrad Norm: 0.983422\tLR: 0.030000\n",
      "Train Epoch: 238 [126976/194182 (65%)]\tLoss: 0.619192\tGrad Norm: 0.691398\tLR: 0.030000\n",
      "Train Epoch: 238 [147456/194182 (75%)]\tLoss: 0.619770\tGrad Norm: 0.909013\tLR: 0.030000\n",
      "Train Epoch: 238 [167936/194182 (85%)]\tLoss: 0.618884\tGrad Norm: 0.706187\tLR: 0.030000\n",
      "Train Epoch: 238 [188416/194182 (96%)]\tLoss: 0.627598\tGrad Norm: 1.056362\tLR: 0.030000\n",
      "Train set: Average loss: 0.6288\n",
      "Test set: Average loss: 0.2881, Average MAE: 0.3879\n",
      "Train Epoch: 239 [4096/194182 (2%)]\tLoss: 0.630616\tGrad Norm: 1.045218\tLR: 0.030000\n",
      "Train Epoch: 239 [24576/194182 (12%)]\tLoss: 0.625843\tGrad Norm: 0.574745\tLR: 0.030000\n",
      "Train Epoch: 239 [45056/194182 (23%)]\tLoss: 0.631540\tGrad Norm: 0.821656\tLR: 0.030000\n",
      "Train Epoch: 239 [65536/194182 (33%)]\tLoss: 0.632645\tGrad Norm: 1.071926\tLR: 0.030000\n",
      "Train Epoch: 239 [86016/194182 (44%)]\tLoss: 0.633991\tGrad Norm: 1.121392\tLR: 0.030000\n",
      "Train Epoch: 239 [106496/194182 (54%)]\tLoss: 0.637102\tGrad Norm: 1.449405\tLR: 0.030000\n",
      "Train Epoch: 239 [126976/194182 (65%)]\tLoss: 0.632279\tGrad Norm: 1.144616\tLR: 0.030000\n",
      "Train Epoch: 239 [147456/194182 (75%)]\tLoss: 0.617648\tGrad Norm: 0.578552\tLR: 0.030000\n",
      "Train Epoch: 239 [167936/194182 (85%)]\tLoss: 0.623449\tGrad Norm: 1.069694\tLR: 0.030000\n",
      "Train Epoch: 239 [188416/194182 (96%)]\tLoss: 0.635879\tGrad Norm: 1.305533\tLR: 0.030000\n",
      "Train set: Average loss: 0.6281\n",
      "Test set: Average loss: 0.2934, Average MAE: 0.3826\n",
      "Train Epoch: 240 [4096/194182 (2%)]\tLoss: 0.626111\tGrad Norm: 1.359632\tLR: 0.030000\n",
      "Train Epoch: 240 [24576/194182 (12%)]\tLoss: 0.637968\tGrad Norm: 1.309950\tLR: 0.030000\n",
      "Train Epoch: 240 [45056/194182 (23%)]\tLoss: 0.629925\tGrad Norm: 1.416229\tLR: 0.030000\n",
      "Train Epoch: 240 [65536/194182 (33%)]\tLoss: 0.619116\tGrad Norm: 1.068166\tLR: 0.030000\n",
      "Train Epoch: 240 [86016/194182 (44%)]\tLoss: 0.627753\tGrad Norm: 0.898570\tLR: 0.030000\n",
      "Train Epoch: 240 [106496/194182 (54%)]\tLoss: 0.630409\tGrad Norm: 1.017026\tLR: 0.030000\n",
      "Train Epoch: 240 [126976/194182 (65%)]\tLoss: 0.622238\tGrad Norm: 0.962653\tLR: 0.030000\n",
      "Train Epoch: 240 [147456/194182 (75%)]\tLoss: 0.629161\tGrad Norm: 0.969403\tLR: 0.030000\n",
      "Train Epoch: 240 [167936/194182 (85%)]\tLoss: 0.632345\tGrad Norm: 1.045252\tLR: 0.030000\n",
      "Train Epoch: 240 [188416/194182 (96%)]\tLoss: 0.628721\tGrad Norm: 0.792457\tLR: 0.030000\n",
      "Train set: Average loss: 0.6287\n",
      "Test set: Average loss: 0.2894, Average MAE: 0.3739\n",
      "Epoch 240: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 241 [4096/194182 (2%)]\tLoss: 0.614485\tGrad Norm: 1.014283\tLR: 0.030000\n",
      "Train Epoch: 241 [24576/194182 (12%)]\tLoss: 0.632630\tGrad Norm: 1.068753\tLR: 0.030000\n",
      "Train Epoch: 241 [45056/194182 (23%)]\tLoss: 0.635312\tGrad Norm: 1.429492\tLR: 0.030000\n",
      "Train Epoch: 241 [65536/194182 (33%)]\tLoss: 0.629825\tGrad Norm: 1.149773\tLR: 0.030000\n",
      "Train Epoch: 241 [86016/194182 (44%)]\tLoss: 0.625030\tGrad Norm: 0.987942\tLR: 0.030000\n",
      "Train Epoch: 241 [106496/194182 (54%)]\tLoss: 0.623041\tGrad Norm: 0.975878\tLR: 0.030000\n",
      "Train Epoch: 241 [126976/194182 (65%)]\tLoss: 0.625891\tGrad Norm: 1.118730\tLR: 0.030000\n",
      "Train Epoch: 241 [147456/194182 (75%)]\tLoss: 0.626427\tGrad Norm: 0.947473\tLR: 0.030000\n",
      "Train Epoch: 241 [167936/194182 (85%)]\tLoss: 0.625123\tGrad Norm: 1.254699\tLR: 0.030000\n",
      "Train Epoch: 241 [188416/194182 (96%)]\tLoss: 0.625967\tGrad Norm: 0.639494\tLR: 0.030000\n",
      "Train set: Average loss: 0.6283\n",
      "Test set: Average loss: 0.2866, Average MAE: 0.3929\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 242 [4096/194182 (2%)]\tLoss: 0.628483\tGrad Norm: 0.766492\tLR: 0.030000\n",
      "Train Epoch: 242 [24576/194182 (12%)]\tLoss: 0.629997\tGrad Norm: 1.124485\tLR: 0.030000\n",
      "Train Epoch: 242 [45056/194182 (23%)]\tLoss: 0.621724\tGrad Norm: 0.680959\tLR: 0.030000\n",
      "Train Epoch: 242 [65536/194182 (33%)]\tLoss: 0.625803\tGrad Norm: 1.199617\tLR: 0.030000\n",
      "Train Epoch: 242 [86016/194182 (44%)]\tLoss: 0.634170\tGrad Norm: 0.872667\tLR: 0.030000\n",
      "Train Epoch: 242 [106496/194182 (54%)]\tLoss: 0.627539\tGrad Norm: 0.964054\tLR: 0.030000\n",
      "Train Epoch: 242 [126976/194182 (65%)]\tLoss: 0.626774\tGrad Norm: 1.046347\tLR: 0.030000\n",
      "Train Epoch: 242 [147456/194182 (75%)]\tLoss: 0.626464\tGrad Norm: 1.166317\tLR: 0.030000\n",
      "Train Epoch: 242 [167936/194182 (85%)]\tLoss: 0.634423\tGrad Norm: 1.235427\tLR: 0.030000\n",
      "Train Epoch: 242 [188416/194182 (96%)]\tLoss: 0.627484\tGrad Norm: 1.368356\tLR: 0.030000\n",
      "Train set: Average loss: 0.6259\n",
      "Test set: Average loss: 0.2925, Average MAE: 0.3917\n",
      "Train Epoch: 243 [4096/194182 (2%)]\tLoss: 0.627615\tGrad Norm: 1.166356\tLR: 0.030000\n",
      "Train Epoch: 243 [24576/194182 (12%)]\tLoss: 0.633256\tGrad Norm: 1.175553\tLR: 0.030000\n",
      "Train Epoch: 243 [45056/194182 (23%)]\tLoss: 0.633136\tGrad Norm: 1.073971\tLR: 0.030000\n",
      "Train Epoch: 243 [65536/194182 (33%)]\tLoss: 0.618478\tGrad Norm: 1.242064\tLR: 0.030000\n",
      "Train Epoch: 243 [86016/194182 (44%)]\tLoss: 0.627273\tGrad Norm: 1.064922\tLR: 0.030000\n",
      "Train Epoch: 243 [106496/194182 (54%)]\tLoss: 0.622519\tGrad Norm: 0.899484\tLR: 0.030000\n",
      "Train Epoch: 243 [126976/194182 (65%)]\tLoss: 0.632178\tGrad Norm: 1.106052\tLR: 0.030000\n",
      "Train Epoch: 243 [147456/194182 (75%)]\tLoss: 0.627417\tGrad Norm: 1.072960\tLR: 0.030000\n",
      "Train Epoch: 243 [167936/194182 (85%)]\tLoss: 0.620178\tGrad Norm: 0.969706\tLR: 0.030000\n",
      "Train Epoch: 243 [188416/194182 (96%)]\tLoss: 0.625335\tGrad Norm: 1.175825\tLR: 0.030000\n",
      "Train set: Average loss: 0.6259\n",
      "Test set: Average loss: 0.2892, Average MAE: 0.3755\n",
      "Train Epoch: 244 [4096/194182 (2%)]\tLoss: 0.622436\tGrad Norm: 1.078003\tLR: 0.030000\n",
      "Train Epoch: 244 [24576/194182 (12%)]\tLoss: 0.624246\tGrad Norm: 1.219860\tLR: 0.030000\n",
      "Train Epoch: 244 [45056/194182 (23%)]\tLoss: 0.625582\tGrad Norm: 1.028288\tLR: 0.030000\n",
      "Train Epoch: 244 [65536/194182 (33%)]\tLoss: 0.628054\tGrad Norm: 1.201444\tLR: 0.030000\n",
      "Train Epoch: 244 [86016/194182 (44%)]\tLoss: 0.636729\tGrad Norm: 1.352226\tLR: 0.030000\n",
      "Train Epoch: 244 [106496/194182 (54%)]\tLoss: 0.625774\tGrad Norm: 1.099299\tLR: 0.030000\n",
      "Train Epoch: 244 [126976/194182 (65%)]\tLoss: 0.628375\tGrad Norm: 1.160837\tLR: 0.030000\n",
      "Train Epoch: 244 [147456/194182 (75%)]\tLoss: 0.617623\tGrad Norm: 0.810022\tLR: 0.030000\n",
      "Train Epoch: 244 [167936/194182 (85%)]\tLoss: 0.619153\tGrad Norm: 0.735723\tLR: 0.030000\n",
      "Train Epoch: 244 [188416/194182 (96%)]\tLoss: 0.623242\tGrad Norm: 1.152198\tLR: 0.030000\n",
      "Train set: Average loss: 0.6255\n",
      "Test set: Average loss: 0.2868, Average MAE: 0.3900\n",
      "Train Epoch: 245 [4096/194182 (2%)]\tLoss: 0.617245\tGrad Norm: 0.997432\tLR: 0.030000\n",
      "Train Epoch: 245 [24576/194182 (12%)]\tLoss: 0.625362\tGrad Norm: 1.171967\tLR: 0.030000\n",
      "Train Epoch: 245 [45056/194182 (23%)]\tLoss: 0.624981\tGrad Norm: 1.126045\tLR: 0.030000\n",
      "Train Epoch: 245 [65536/194182 (33%)]\tLoss: 0.617203\tGrad Norm: 1.245818\tLR: 0.030000\n",
      "Train Epoch: 245 [86016/194182 (44%)]\tLoss: 0.622670\tGrad Norm: 0.567007\tLR: 0.030000\n",
      "Train Epoch: 245 [106496/194182 (54%)]\tLoss: 0.621404\tGrad Norm: 0.542258\tLR: 0.030000\n",
      "Train Epoch: 245 [126976/194182 (65%)]\tLoss: 0.609970\tGrad Norm: 0.777513\tLR: 0.030000\n",
      "Train Epoch: 245 [147456/194182 (75%)]\tLoss: 0.630002\tGrad Norm: 1.376196\tLR: 0.030000\n",
      "Train Epoch: 245 [167936/194182 (85%)]\tLoss: 0.628157\tGrad Norm: 1.248160\tLR: 0.030000\n",
      "Train Epoch: 245 [188416/194182 (96%)]\tLoss: 0.630421\tGrad Norm: 1.316209\tLR: 0.030000\n",
      "Train set: Average loss: 0.6243\n",
      "Test set: Average loss: 0.2917, Average MAE: 0.4024\n",
      "Epoch 245: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 246 [4096/194182 (2%)]\tLoss: 0.627120\tGrad Norm: 1.117161\tLR: 0.030000\n",
      "Train Epoch: 246 [24576/194182 (12%)]\tLoss: 0.628790\tGrad Norm: 1.232924\tLR: 0.030000\n",
      "Train Epoch: 246 [45056/194182 (23%)]\tLoss: 0.624171\tGrad Norm: 0.834172\tLR: 0.030000\n",
      "Train Epoch: 246 [65536/194182 (33%)]\tLoss: 0.623412\tGrad Norm: 0.811138\tLR: 0.030000\n",
      "Train Epoch: 246 [86016/194182 (44%)]\tLoss: 0.622552\tGrad Norm: 1.111109\tLR: 0.030000\n",
      "Train Epoch: 246 [106496/194182 (54%)]\tLoss: 0.620343\tGrad Norm: 0.750721\tLR: 0.030000\n",
      "Train Epoch: 246 [126976/194182 (65%)]\tLoss: 0.619470\tGrad Norm: 0.797919\tLR: 0.030000\n",
      "Train Epoch: 246 [147456/194182 (75%)]\tLoss: 0.623400\tGrad Norm: 0.768489\tLR: 0.030000\n",
      "Train Epoch: 246 [167936/194182 (85%)]\tLoss: 0.625246\tGrad Norm: 1.201352\tLR: 0.030000\n",
      "Train Epoch: 246 [188416/194182 (96%)]\tLoss: 0.627935\tGrad Norm: 1.088594\tLR: 0.030000\n",
      "Train set: Average loss: 0.6224\n",
      "Test set: Average loss: 0.2882, Average MAE: 0.3727\n",
      "Train Epoch: 247 [4096/194182 (2%)]\tLoss: 0.621654\tGrad Norm: 1.071125\tLR: 0.030000\n",
      "Train Epoch: 247 [24576/194182 (12%)]\tLoss: 0.619279\tGrad Norm: 0.986576\tLR: 0.030000\n",
      "Train Epoch: 247 [45056/194182 (23%)]\tLoss: 0.620274\tGrad Norm: 0.863445\tLR: 0.030000\n",
      "Train Epoch: 247 [65536/194182 (33%)]\tLoss: 0.628844\tGrad Norm: 1.090311\tLR: 0.030000\n",
      "Train Epoch: 247 [86016/194182 (44%)]\tLoss: 0.632714\tGrad Norm: 1.485859\tLR: 0.030000\n",
      "Train Epoch: 247 [106496/194182 (54%)]\tLoss: 0.626572\tGrad Norm: 1.467787\tLR: 0.030000\n",
      "Train Epoch: 247 [126976/194182 (65%)]\tLoss: 0.625859\tGrad Norm: 1.392066\tLR: 0.030000\n",
      "Train Epoch: 247 [147456/194182 (75%)]\tLoss: 0.632041\tGrad Norm: 1.063281\tLR: 0.030000\n",
      "Train Epoch: 247 [167936/194182 (85%)]\tLoss: 0.626163\tGrad Norm: 0.846355\tLR: 0.030000\n",
      "Train Epoch: 247 [188416/194182 (96%)]\tLoss: 0.626118\tGrad Norm: 0.839111\tLR: 0.030000\n",
      "Train set: Average loss: 0.6235\n",
      "Test set: Average loss: 0.2907, Average MAE: 0.3950\n",
      "Train Epoch: 248 [4096/194182 (2%)]\tLoss: 0.617834\tGrad Norm: 1.156530\tLR: 0.030000\n",
      "Train Epoch: 248 [24576/194182 (12%)]\tLoss: 0.625464\tGrad Norm: 1.030049\tLR: 0.030000\n",
      "Train Epoch: 248 [45056/194182 (23%)]\tLoss: 0.619017\tGrad Norm: 1.028103\tLR: 0.030000\n",
      "Train Epoch: 248 [65536/194182 (33%)]\tLoss: 0.632973\tGrad Norm: 1.307577\tLR: 0.030000\n",
      "Train Epoch: 248 [86016/194182 (44%)]\tLoss: 0.617186\tGrad Norm: 0.991729\tLR: 0.030000\n",
      "Train Epoch: 248 [106496/194182 (54%)]\tLoss: 0.617966\tGrad Norm: 0.971113\tLR: 0.030000\n",
      "Train Epoch: 248 [126976/194182 (65%)]\tLoss: 0.625889\tGrad Norm: 1.012092\tLR: 0.030000\n",
      "Train Epoch: 248 [147456/194182 (75%)]\tLoss: 0.620388\tGrad Norm: 0.598618\tLR: 0.030000\n",
      "Train Epoch: 248 [167936/194182 (85%)]\tLoss: 0.615921\tGrad Norm: 0.677509\tLR: 0.030000\n",
      "Train Epoch: 248 [188416/194182 (96%)]\tLoss: 0.620531\tGrad Norm: 0.585571\tLR: 0.030000\n",
      "Train set: Average loss: 0.6202\n",
      "Test set: Average loss: 0.2842, Average MAE: 0.3811\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 249 [4096/194182 (2%)]\tLoss: 0.604387\tGrad Norm: 0.673652\tLR: 0.030000\n",
      "Train Epoch: 249 [24576/194182 (12%)]\tLoss: 0.619996\tGrad Norm: 0.935186\tLR: 0.030000\n",
      "Train Epoch: 249 [45056/194182 (23%)]\tLoss: 0.624957\tGrad Norm: 1.045627\tLR: 0.030000\n",
      "Train Epoch: 249 [65536/194182 (33%)]\tLoss: 0.619459\tGrad Norm: 0.782175\tLR: 0.030000\n",
      "Train Epoch: 249 [86016/194182 (44%)]\tLoss: 0.625806\tGrad Norm: 0.835487\tLR: 0.030000\n",
      "Train Epoch: 249 [106496/194182 (54%)]\tLoss: 0.618119\tGrad Norm: 1.057094\tLR: 0.030000\n",
      "Train Epoch: 249 [126976/194182 (65%)]\tLoss: 0.625667\tGrad Norm: 0.824293\tLR: 0.030000\n",
      "Train Epoch: 249 [147456/194182 (75%)]\tLoss: 0.610458\tGrad Norm: 0.610281\tLR: 0.030000\n",
      "Train Epoch: 249 [167936/194182 (85%)]\tLoss: 0.615168\tGrad Norm: 0.871316\tLR: 0.030000\n",
      "Train Epoch: 249 [188416/194182 (96%)]\tLoss: 0.626514\tGrad Norm: 0.869300\tLR: 0.030000\n",
      "Train set: Average loss: 0.6190\n",
      "Test set: Average loss: 0.2875, Average MAE: 0.3766\n",
      "Train Epoch: 250 [4096/194182 (2%)]\tLoss: 0.635470\tGrad Norm: 1.126728\tLR: 0.030000\n",
      "Train Epoch: 250 [24576/194182 (12%)]\tLoss: 0.631822\tGrad Norm: 1.799196\tLR: 0.030000\n",
      "Train Epoch: 250 [45056/194182 (23%)]\tLoss: 0.628672\tGrad Norm: 1.674302\tLR: 0.030000\n",
      "Train Epoch: 250 [65536/194182 (33%)]\tLoss: 0.619592\tGrad Norm: 1.234359\tLR: 0.030000\n",
      "Train Epoch: 250 [86016/194182 (44%)]\tLoss: 0.628017\tGrad Norm: 1.406477\tLR: 0.030000\n",
      "Train Epoch: 250 [106496/194182 (54%)]\tLoss: 0.622434\tGrad Norm: 0.809816\tLR: 0.030000\n",
      "Train Epoch: 250 [126976/194182 (65%)]\tLoss: 0.609158\tGrad Norm: 1.159398\tLR: 0.030000\n",
      "Train Epoch: 250 [147456/194182 (75%)]\tLoss: 0.617222\tGrad Norm: 0.705515\tLR: 0.030000\n",
      "Train Epoch: 250 [167936/194182 (85%)]\tLoss: 0.614446\tGrad Norm: 0.693930\tLR: 0.030000\n",
      "Train Epoch: 250 [188416/194182 (96%)]\tLoss: 0.624166\tGrad Norm: 0.870504\tLR: 0.030000\n",
      "Train set: Average loss: 0.6217\n",
      "Test set: Average loss: 0.2840, Average MAE: 0.3805\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 250: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 251 [4096/194182 (2%)]\tLoss: 0.615260\tGrad Norm: 0.788673\tLR: 0.030000\n",
      "Train Epoch: 251 [24576/194182 (12%)]\tLoss: 0.623426\tGrad Norm: 1.019612\tLR: 0.030000\n",
      "Train Epoch: 251 [45056/194182 (23%)]\tLoss: 0.622074\tGrad Norm: 1.098459\tLR: 0.030000\n",
      "Train Epoch: 251 [65536/194182 (33%)]\tLoss: 0.621803\tGrad Norm: 1.072610\tLR: 0.030000\n",
      "Train Epoch: 251 [86016/194182 (44%)]\tLoss: 0.615705\tGrad Norm: 0.961140\tLR: 0.030000\n",
      "Train Epoch: 251 [106496/194182 (54%)]\tLoss: 0.624788\tGrad Norm: 1.299664\tLR: 0.030000\n",
      "Train Epoch: 251 [126976/194182 (65%)]\tLoss: 0.614867\tGrad Norm: 1.298745\tLR: 0.030000\n",
      "Train Epoch: 251 [147456/194182 (75%)]\tLoss: 0.634648\tGrad Norm: 1.337600\tLR: 0.030000\n",
      "Train Epoch: 251 [167936/194182 (85%)]\tLoss: 0.616551\tGrad Norm: 1.235666\tLR: 0.030000\n",
      "Train Epoch: 251 [188416/194182 (96%)]\tLoss: 0.626816\tGrad Norm: 1.062375\tLR: 0.030000\n",
      "Train set: Average loss: 0.6207\n",
      "Test set: Average loss: 0.2842, Average MAE: 0.3749\n",
      "Train Epoch: 252 [4096/194182 (2%)]\tLoss: 0.615756\tGrad Norm: 0.744247\tLR: 0.030000\n",
      "Train Epoch: 252 [24576/194182 (12%)]\tLoss: 0.603408\tGrad Norm: 0.673840\tLR: 0.030000\n",
      "Train Epoch: 252 [45056/194182 (23%)]\tLoss: 0.620330\tGrad Norm: 1.020951\tLR: 0.030000\n",
      "Train Epoch: 252 [65536/194182 (33%)]\tLoss: 0.608544\tGrad Norm: 1.013511\tLR: 0.030000\n",
      "Train Epoch: 252 [86016/194182 (44%)]\tLoss: 0.621154\tGrad Norm: 1.336395\tLR: 0.030000\n",
      "Train Epoch: 252 [106496/194182 (54%)]\tLoss: 0.626166\tGrad Norm: 1.078771\tLR: 0.030000\n",
      "Train Epoch: 252 [126976/194182 (65%)]\tLoss: 0.617295\tGrad Norm: 0.884130\tLR: 0.030000\n",
      "Train Epoch: 252 [147456/194182 (75%)]\tLoss: 0.622031\tGrad Norm: 0.929850\tLR: 0.030000\n",
      "Train Epoch: 252 [167936/194182 (85%)]\tLoss: 0.612574\tGrad Norm: 0.770349\tLR: 0.030000\n",
      "Train Epoch: 252 [188416/194182 (96%)]\tLoss: 0.623617\tGrad Norm: 0.717902\tLR: 0.030000\n",
      "Train set: Average loss: 0.6169\n",
      "Test set: Average loss: 0.2830, Average MAE: 0.3771\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 253 [4096/194182 (2%)]\tLoss: 0.617872\tGrad Norm: 0.829824\tLR: 0.030000\n",
      "Train Epoch: 253 [24576/194182 (12%)]\tLoss: 0.621788\tGrad Norm: 1.214767\tLR: 0.030000\n",
      "Train Epoch: 253 [45056/194182 (23%)]\tLoss: 0.620209\tGrad Norm: 1.171854\tLR: 0.030000\n",
      "Train Epoch: 253 [65536/194182 (33%)]\tLoss: 0.613195\tGrad Norm: 0.774673\tLR: 0.030000\n",
      "Train Epoch: 253 [86016/194182 (44%)]\tLoss: 0.610560\tGrad Norm: 0.861301\tLR: 0.030000\n",
      "Train Epoch: 253 [106496/194182 (54%)]\tLoss: 0.616463\tGrad Norm: 1.417051\tLR: 0.030000\n",
      "Train Epoch: 253 [126976/194182 (65%)]\tLoss: 0.629555\tGrad Norm: 1.589988\tLR: 0.030000\n",
      "Train Epoch: 253 [147456/194182 (75%)]\tLoss: 0.611339\tGrad Norm: 1.496470\tLR: 0.030000\n",
      "Train Epoch: 253 [167936/194182 (85%)]\tLoss: 0.618657\tGrad Norm: 1.097934\tLR: 0.030000\n",
      "Train Epoch: 253 [188416/194182 (96%)]\tLoss: 0.619220\tGrad Norm: 1.027392\tLR: 0.030000\n",
      "Train set: Average loss: 0.6200\n",
      "Test set: Average loss: 0.2908, Average MAE: 0.4028\n",
      "Train Epoch: 254 [4096/194182 (2%)]\tLoss: 0.616328\tGrad Norm: 1.253664\tLR: 0.030000\n",
      "Train Epoch: 254 [24576/194182 (12%)]\tLoss: 0.613057\tGrad Norm: 1.144558\tLR: 0.030000\n",
      "Train Epoch: 254 [45056/194182 (23%)]\tLoss: 0.610377\tGrad Norm: 0.662782\tLR: 0.030000\n",
      "Train Epoch: 254 [65536/194182 (33%)]\tLoss: 0.632205\tGrad Norm: 1.697269\tLR: 0.030000\n",
      "Train Epoch: 254 [86016/194182 (44%)]\tLoss: 0.622663\tGrad Norm: 0.594454\tLR: 0.030000\n",
      "Train Epoch: 254 [106496/194182 (54%)]\tLoss: 0.602193\tGrad Norm: 0.539400\tLR: 0.030000\n",
      "Train Epoch: 254 [126976/194182 (65%)]\tLoss: 0.616754\tGrad Norm: 0.827911\tLR: 0.030000\n",
      "Train Epoch: 254 [147456/194182 (75%)]\tLoss: 0.626067\tGrad Norm: 0.849638\tLR: 0.030000\n",
      "Train Epoch: 254 [167936/194182 (85%)]\tLoss: 0.612632\tGrad Norm: 0.760572\tLR: 0.030000\n",
      "Train Epoch: 254 [188416/194182 (96%)]\tLoss: 0.613432\tGrad Norm: 1.189628\tLR: 0.030000\n",
      "Train set: Average loss: 0.6158\n",
      "Test set: Average loss: 0.2963, Average MAE: 0.3985\n",
      "Train Epoch: 255 [4096/194182 (2%)]\tLoss: 0.615287\tGrad Norm: 1.603708\tLR: 0.030000\n",
      "Train Epoch: 255 [24576/194182 (12%)]\tLoss: 0.626670\tGrad Norm: 1.119946\tLR: 0.030000\n",
      "Train Epoch: 255 [45056/194182 (23%)]\tLoss: 0.616822\tGrad Norm: 0.873436\tLR: 0.030000\n",
      "Train Epoch: 255 [65536/194182 (33%)]\tLoss: 0.610322\tGrad Norm: 0.735346\tLR: 0.030000\n",
      "Train Epoch: 255 [86016/194182 (44%)]\tLoss: 0.612450\tGrad Norm: 0.927875\tLR: 0.030000\n",
      "Train Epoch: 255 [106496/194182 (54%)]\tLoss: 0.621536\tGrad Norm: 1.066689\tLR: 0.030000\n",
      "Train Epoch: 255 [126976/194182 (65%)]\tLoss: 0.616038\tGrad Norm: 1.283167\tLR: 0.030000\n",
      "Train Epoch: 255 [147456/194182 (75%)]\tLoss: 0.613956\tGrad Norm: 1.258610\tLR: 0.030000\n",
      "Train Epoch: 255 [167936/194182 (85%)]\tLoss: 0.613933\tGrad Norm: 1.098223\tLR: 0.030000\n",
      "Train Epoch: 255 [188416/194182 (96%)]\tLoss: 0.614920\tGrad Norm: 1.191913\tLR: 0.030000\n",
      "Train set: Average loss: 0.6177\n",
      "Test set: Average loss: 0.2879, Average MAE: 0.3773\n",
      "Epoch 255: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 256 [4096/194182 (2%)]\tLoss: 0.609412\tGrad Norm: 1.437886\tLR: 0.030000\n",
      "Train Epoch: 256 [24576/194182 (12%)]\tLoss: 0.628301\tGrad Norm: 0.861341\tLR: 0.030000\n",
      "Train Epoch: 256 [45056/194182 (23%)]\tLoss: 0.621709\tGrad Norm: 0.977451\tLR: 0.030000\n",
      "Train Epoch: 256 [65536/194182 (33%)]\tLoss: 0.611098\tGrad Norm: 0.786669\tLR: 0.030000\n",
      "Train Epoch: 256 [86016/194182 (44%)]\tLoss: 0.607457\tGrad Norm: 0.791525\tLR: 0.030000\n",
      "Train Epoch: 256 [106496/194182 (54%)]\tLoss: 0.617652\tGrad Norm: 1.416079\tLR: 0.030000\n",
      "Train Epoch: 256 [126976/194182 (65%)]\tLoss: 0.609414\tGrad Norm: 1.079829\tLR: 0.030000\n",
      "Train Epoch: 256 [147456/194182 (75%)]\tLoss: 0.623921\tGrad Norm: 1.233672\tLR: 0.030000\n",
      "Train Epoch: 256 [167936/194182 (85%)]\tLoss: 0.615082\tGrad Norm: 1.090654\tLR: 0.030000\n",
      "Train Epoch: 256 [188416/194182 (96%)]\tLoss: 0.621478\tGrad Norm: 0.969099\tLR: 0.030000\n",
      "Train set: Average loss: 0.6161\n",
      "Test set: Average loss: 0.2867, Average MAE: 0.3916\n",
      "Train Epoch: 257 [4096/194182 (2%)]\tLoss: 0.615904\tGrad Norm: 1.019668\tLR: 0.030000\n",
      "Train Epoch: 257 [24576/194182 (12%)]\tLoss: 0.605178\tGrad Norm: 1.057415\tLR: 0.030000\n",
      "Train Epoch: 257 [45056/194182 (23%)]\tLoss: 0.616475\tGrad Norm: 0.993485\tLR: 0.030000\n",
      "Train Epoch: 257 [65536/194182 (33%)]\tLoss: 0.612733\tGrad Norm: 0.989485\tLR: 0.030000\n",
      "Train Epoch: 257 [86016/194182 (44%)]\tLoss: 0.613835\tGrad Norm: 0.752597\tLR: 0.030000\n",
      "Train Epoch: 257 [106496/194182 (54%)]\tLoss: 0.614219\tGrad Norm: 0.447413\tLR: 0.030000\n",
      "Train Epoch: 257 [126976/194182 (65%)]\tLoss: 0.607632\tGrad Norm: 0.860713\tLR: 0.030000\n",
      "Train Epoch: 257 [147456/194182 (75%)]\tLoss: 0.614906\tGrad Norm: 0.911678\tLR: 0.030000\n",
      "Train Epoch: 257 [167936/194182 (85%)]\tLoss: 0.611637\tGrad Norm: 0.699051\tLR: 0.030000\n",
      "Train Epoch: 257 [188416/194182 (96%)]\tLoss: 0.610339\tGrad Norm: 1.025679\tLR: 0.030000\n",
      "Train set: Average loss: 0.6137\n",
      "Test set: Average loss: 0.2874, Average MAE: 0.3833\n",
      "Train Epoch: 258 [4096/194182 (2%)]\tLoss: 0.615758\tGrad Norm: 1.074549\tLR: 0.030000\n",
      "Train Epoch: 258 [24576/194182 (12%)]\tLoss: 0.611373\tGrad Norm: 1.297952\tLR: 0.030000\n",
      "Train Epoch: 258 [45056/194182 (23%)]\tLoss: 0.620349\tGrad Norm: 1.091750\tLR: 0.030000\n",
      "Train Epoch: 258 [65536/194182 (33%)]\tLoss: 0.606233\tGrad Norm: 1.026347\tLR: 0.030000\n",
      "Train Epoch: 258 [86016/194182 (44%)]\tLoss: 0.621339\tGrad Norm: 1.038685\tLR: 0.030000\n",
      "Train Epoch: 258 [106496/194182 (54%)]\tLoss: 0.612156\tGrad Norm: 1.033957\tLR: 0.030000\n",
      "Train Epoch: 258 [126976/194182 (65%)]\tLoss: 0.625796\tGrad Norm: 1.255928\tLR: 0.030000\n",
      "Train Epoch: 258 [147456/194182 (75%)]\tLoss: 0.622072\tGrad Norm: 1.046568\tLR: 0.030000\n",
      "Train Epoch: 258 [167936/194182 (85%)]\tLoss: 0.616704\tGrad Norm: 1.177077\tLR: 0.030000\n",
      "Train Epoch: 258 [188416/194182 (96%)]\tLoss: 0.608302\tGrad Norm: 1.011389\tLR: 0.030000\n",
      "Train set: Average loss: 0.6156\n",
      "Test set: Average loss: 0.2886, Average MAE: 0.3984\n",
      "Train Epoch: 259 [4096/194182 (2%)]\tLoss: 0.626034\tGrad Norm: 1.548722\tLR: 0.030000\n",
      "Train Epoch: 259 [24576/194182 (12%)]\tLoss: 0.613092\tGrad Norm: 1.211418\tLR: 0.030000\n",
      "Train Epoch: 259 [45056/194182 (23%)]\tLoss: 0.612540\tGrad Norm: 0.915665\tLR: 0.030000\n",
      "Train Epoch: 259 [65536/194182 (33%)]\tLoss: 0.609092\tGrad Norm: 0.981167\tLR: 0.030000\n",
      "Train Epoch: 259 [86016/194182 (44%)]\tLoss: 0.608252\tGrad Norm: 1.060669\tLR: 0.030000\n",
      "Train Epoch: 259 [106496/194182 (54%)]\tLoss: 0.613211\tGrad Norm: 1.110489\tLR: 0.030000\n",
      "Train Epoch: 259 [126976/194182 (65%)]\tLoss: 0.611176\tGrad Norm: 1.169756\tLR: 0.030000\n",
      "Train Epoch: 259 [147456/194182 (75%)]\tLoss: 0.612262\tGrad Norm: 0.793812\tLR: 0.030000\n",
      "Train Epoch: 259 [167936/194182 (85%)]\tLoss: 0.621476\tGrad Norm: 0.654153\tLR: 0.030000\n",
      "Train Epoch: 259 [188416/194182 (96%)]\tLoss: 0.606043\tGrad Norm: 0.947190\tLR: 0.030000\n",
      "Train set: Average loss: 0.6133\n",
      "Test set: Average loss: 0.2832, Average MAE: 0.3818\n",
      "Train Epoch: 260 [4096/194182 (2%)]\tLoss: 0.611477\tGrad Norm: 0.789022\tLR: 0.030000\n",
      "Train Epoch: 260 [24576/194182 (12%)]\tLoss: 0.606482\tGrad Norm: 0.880111\tLR: 0.030000\n",
      "Train Epoch: 260 [45056/194182 (23%)]\tLoss: 0.619434\tGrad Norm: 1.118190\tLR: 0.030000\n",
      "Train Epoch: 260 [65536/194182 (33%)]\tLoss: 0.621967\tGrad Norm: 1.156870\tLR: 0.030000\n",
      "Train Epoch: 260 [86016/194182 (44%)]\tLoss: 0.615548\tGrad Norm: 0.909455\tLR: 0.030000\n",
      "Train Epoch: 260 [106496/194182 (54%)]\tLoss: 0.617063\tGrad Norm: 1.108062\tLR: 0.030000\n",
      "Train Epoch: 260 [126976/194182 (65%)]\tLoss: 0.619475\tGrad Norm: 1.482514\tLR: 0.030000\n",
      "Train Epoch: 260 [147456/194182 (75%)]\tLoss: 0.619636\tGrad Norm: 0.895320\tLR: 0.030000\n",
      "Train Epoch: 260 [167936/194182 (85%)]\tLoss: 0.607159\tGrad Norm: 1.011421\tLR: 0.030000\n",
      "Train Epoch: 260 [188416/194182 (96%)]\tLoss: 0.608257\tGrad Norm: 0.872325\tLR: 0.030000\n",
      "Train set: Average loss: 0.6127\n",
      "Test set: Average loss: 0.2866, Average MAE: 0.3838\n",
      "Epoch 260: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 261 [4096/194182 (2%)]\tLoss: 0.611635\tGrad Norm: 1.311718\tLR: 0.030000\n",
      "Train Epoch: 261 [24576/194182 (12%)]\tLoss: 0.614515\tGrad Norm: 1.076055\tLR: 0.030000\n",
      "Train Epoch: 261 [45056/194182 (23%)]\tLoss: 0.624069\tGrad Norm: 1.208177\tLR: 0.030000\n",
      "Train Epoch: 261 [65536/194182 (33%)]\tLoss: 0.607272\tGrad Norm: 1.085505\tLR: 0.030000\n",
      "Train Epoch: 261 [86016/194182 (44%)]\tLoss: 0.614530\tGrad Norm: 1.132107\tLR: 0.030000\n",
      "Train Epoch: 261 [106496/194182 (54%)]\tLoss: 0.607134\tGrad Norm: 0.971131\tLR: 0.030000\n",
      "Train Epoch: 261 [126976/194182 (65%)]\tLoss: 0.627276\tGrad Norm: 1.220487\tLR: 0.030000\n",
      "Train Epoch: 261 [147456/194182 (75%)]\tLoss: 0.607223\tGrad Norm: 0.881424\tLR: 0.030000\n",
      "Train Epoch: 261 [167936/194182 (85%)]\tLoss: 0.609551\tGrad Norm: 1.061268\tLR: 0.030000\n",
      "Train Epoch: 261 [188416/194182 (96%)]\tLoss: 0.611476\tGrad Norm: 1.155173\tLR: 0.030000\n",
      "Train set: Average loss: 0.6127\n",
      "Test set: Average loss: 0.2895, Average MAE: 0.4048\n",
      "Train Epoch: 262 [4096/194182 (2%)]\tLoss: 0.613053\tGrad Norm: 1.350866\tLR: 0.030000\n",
      "Train Epoch: 262 [24576/194182 (12%)]\tLoss: 0.611179\tGrad Norm: 1.035164\tLR: 0.030000\n",
      "Train Epoch: 262 [45056/194182 (23%)]\tLoss: 0.617099\tGrad Norm: 1.315195\tLR: 0.030000\n",
      "Train Epoch: 262 [65536/194182 (33%)]\tLoss: 0.624817\tGrad Norm: 1.365957\tLR: 0.030000\n",
      "Train Epoch: 262 [86016/194182 (44%)]\tLoss: 0.608998\tGrad Norm: 1.374663\tLR: 0.030000\n",
      "Train Epoch: 262 [106496/194182 (54%)]\tLoss: 0.610288\tGrad Norm: 0.848411\tLR: 0.030000\n",
      "Train Epoch: 262 [126976/194182 (65%)]\tLoss: 0.612932\tGrad Norm: 1.228509\tLR: 0.030000\n",
      "Train Epoch: 262 [147456/194182 (75%)]\tLoss: 0.610319\tGrad Norm: 0.853789\tLR: 0.030000\n",
      "Train Epoch: 262 [167936/194182 (85%)]\tLoss: 0.614496\tGrad Norm: 1.194424\tLR: 0.030000\n",
      "Train Epoch: 262 [188416/194182 (96%)]\tLoss: 0.610606\tGrad Norm: 1.084172\tLR: 0.030000\n",
      "Train set: Average loss: 0.6127\n",
      "Test set: Average loss: 0.2858, Average MAE: 0.3889\n",
      "Train Epoch: 263 [4096/194182 (2%)]\tLoss: 0.604273\tGrad Norm: 0.995520\tLR: 0.030000\n",
      "Train Epoch: 263 [24576/194182 (12%)]\tLoss: 0.612645\tGrad Norm: 1.136960\tLR: 0.030000\n",
      "Train Epoch: 263 [45056/194182 (23%)]\tLoss: 0.609353\tGrad Norm: 0.941354\tLR: 0.030000\n",
      "Train Epoch: 263 [65536/194182 (33%)]\tLoss: 0.598087\tGrad Norm: 0.804036\tLR: 0.030000\n",
      "Train Epoch: 263 [86016/194182 (44%)]\tLoss: 0.600644\tGrad Norm: 0.766786\tLR: 0.030000\n",
      "Train Epoch: 263 [106496/194182 (54%)]\tLoss: 0.611188\tGrad Norm: 1.102890\tLR: 0.030000\n",
      "Train Epoch: 263 [126976/194182 (65%)]\tLoss: 0.612849\tGrad Norm: 0.986914\tLR: 0.030000\n",
      "Train Epoch: 263 [147456/194182 (75%)]\tLoss: 0.619714\tGrad Norm: 1.056007\tLR: 0.030000\n",
      "Train Epoch: 263 [167936/194182 (85%)]\tLoss: 0.602919\tGrad Norm: 1.070903\tLR: 0.030000\n",
      "Train Epoch: 263 [188416/194182 (96%)]\tLoss: 0.616920\tGrad Norm: 0.949931\tLR: 0.030000\n",
      "Train set: Average loss: 0.6096\n",
      "Test set: Average loss: 0.2870, Average MAE: 0.3944\n",
      "Train Epoch: 264 [4096/194182 (2%)]\tLoss: 0.608871\tGrad Norm: 1.225366\tLR: 0.030000\n",
      "Train Epoch: 264 [24576/194182 (12%)]\tLoss: 0.602656\tGrad Norm: 0.929076\tLR: 0.030000\n",
      "Train Epoch: 264 [45056/194182 (23%)]\tLoss: 0.610986\tGrad Norm: 1.146452\tLR: 0.030000\n",
      "Train Epoch: 264 [65536/194182 (33%)]\tLoss: 0.611103\tGrad Norm: 0.923755\tLR: 0.030000\n",
      "Train Epoch: 264 [86016/194182 (44%)]\tLoss: 0.614769\tGrad Norm: 1.174891\tLR: 0.030000\n",
      "Train Epoch: 264 [106496/194182 (54%)]\tLoss: 0.612203\tGrad Norm: 1.257170\tLR: 0.030000\n",
      "Train Epoch: 264 [126976/194182 (65%)]\tLoss: 0.605113\tGrad Norm: 1.037861\tLR: 0.030000\n",
      "Train Epoch: 264 [147456/194182 (75%)]\tLoss: 0.610752\tGrad Norm: 1.300259\tLR: 0.030000\n",
      "Train Epoch: 264 [167936/194182 (85%)]\tLoss: 0.612088\tGrad Norm: 0.933562\tLR: 0.030000\n",
      "Train Epoch: 264 [188416/194182 (96%)]\tLoss: 0.603544\tGrad Norm: 1.098065\tLR: 0.030000\n",
      "Train set: Average loss: 0.6117\n",
      "Test set: Average loss: 0.2903, Average MAE: 0.3770\n",
      "Train Epoch: 265 [4096/194182 (2%)]\tLoss: 0.610485\tGrad Norm: 1.557106\tLR: 0.030000\n",
      "Train Epoch: 265 [24576/194182 (12%)]\tLoss: 0.599420\tGrad Norm: 1.012262\tLR: 0.030000\n",
      "Train Epoch: 265 [45056/194182 (23%)]\tLoss: 0.610759\tGrad Norm: 0.997557\tLR: 0.030000\n",
      "Train Epoch: 265 [65536/194182 (33%)]\tLoss: 0.620482\tGrad Norm: 1.277773\tLR: 0.030000\n",
      "Train Epoch: 265 [86016/194182 (44%)]\tLoss: 0.604909\tGrad Norm: 0.900854\tLR: 0.030000\n",
      "Train Epoch: 265 [106496/194182 (54%)]\tLoss: 0.603907\tGrad Norm: 1.019756\tLR: 0.030000\n",
      "Train Epoch: 265 [126976/194182 (65%)]\tLoss: 0.611700\tGrad Norm: 0.787256\tLR: 0.030000\n",
      "Train Epoch: 265 [147456/194182 (75%)]\tLoss: 0.599968\tGrad Norm: 0.626509\tLR: 0.030000\n",
      "Train Epoch: 265 [167936/194182 (85%)]\tLoss: 0.615685\tGrad Norm: 1.243228\tLR: 0.030000\n",
      "Train Epoch: 265 [188416/194182 (96%)]\tLoss: 0.614550\tGrad Norm: 1.036904\tLR: 0.030000\n",
      "Train set: Average loss: 0.6087\n",
      "Test set: Average loss: 0.2832, Average MAE: 0.3727\n",
      "Epoch 265: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 266 [4096/194182 (2%)]\tLoss: 0.606721\tGrad Norm: 0.994887\tLR: 0.030000\n",
      "Train Epoch: 266 [24576/194182 (12%)]\tLoss: 0.610903\tGrad Norm: 1.097341\tLR: 0.030000\n",
      "Train Epoch: 266 [45056/194182 (23%)]\tLoss: 0.621727\tGrad Norm: 1.071365\tLR: 0.030000\n",
      "Train Epoch: 266 [65536/194182 (33%)]\tLoss: 0.605605\tGrad Norm: 0.934205\tLR: 0.030000\n",
      "Train Epoch: 266 [86016/194182 (44%)]\tLoss: 0.601000\tGrad Norm: 1.083916\tLR: 0.030000\n",
      "Train Epoch: 266 [106496/194182 (54%)]\tLoss: 0.619934\tGrad Norm: 1.473857\tLR: 0.030000\n",
      "Train Epoch: 266 [126976/194182 (65%)]\tLoss: 0.618279\tGrad Norm: 1.241082\tLR: 0.030000\n",
      "Train Epoch: 266 [147456/194182 (75%)]\tLoss: 0.612184\tGrad Norm: 1.165987\tLR: 0.030000\n",
      "Train Epoch: 266 [167936/194182 (85%)]\tLoss: 0.609645\tGrad Norm: 0.910323\tLR: 0.030000\n",
      "Train Epoch: 266 [188416/194182 (96%)]\tLoss: 0.597286\tGrad Norm: 0.683613\tLR: 0.030000\n",
      "Train set: Average loss: 0.6092\n",
      "Test set: Average loss: 0.2814, Average MAE: 0.3736\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 267 [4096/194182 (2%)]\tLoss: 0.604072\tGrad Norm: 0.851321\tLR: 0.030000\n",
      "Train Epoch: 267 [24576/194182 (12%)]\tLoss: 0.605961\tGrad Norm: 0.813854\tLR: 0.030000\n",
      "Train Epoch: 267 [45056/194182 (23%)]\tLoss: 0.603846\tGrad Norm: 0.904124\tLR: 0.030000\n",
      "Train Epoch: 267 [65536/194182 (33%)]\tLoss: 0.603255\tGrad Norm: 1.066618\tLR: 0.030000\n",
      "Train Epoch: 267 [86016/194182 (44%)]\tLoss: 0.605548\tGrad Norm: 1.063105\tLR: 0.030000\n",
      "Train Epoch: 267 [106496/194182 (54%)]\tLoss: 0.602747\tGrad Norm: 0.698549\tLR: 0.030000\n",
      "Train Epoch: 267 [126976/194182 (65%)]\tLoss: 0.611471\tGrad Norm: 0.786095\tLR: 0.030000\n",
      "Train Epoch: 267 [147456/194182 (75%)]\tLoss: 0.617499\tGrad Norm: 1.341145\tLR: 0.030000\n",
      "Train Epoch: 267 [167936/194182 (85%)]\tLoss: 0.609167\tGrad Norm: 1.325126\tLR: 0.030000\n",
      "Train Epoch: 267 [188416/194182 (96%)]\tLoss: 0.603579\tGrad Norm: 1.095560\tLR: 0.030000\n",
      "Train set: Average loss: 0.6079\n",
      "Test set: Average loss: 0.2831, Average MAE: 0.3750\n",
      "Train Epoch: 268 [4096/194182 (2%)]\tLoss: 0.602678\tGrad Norm: 1.025169\tLR: 0.030000\n",
      "Train Epoch: 268 [24576/194182 (12%)]\tLoss: 0.606268\tGrad Norm: 1.043782\tLR: 0.030000\n",
      "Train Epoch: 268 [45056/194182 (23%)]\tLoss: 0.605224\tGrad Norm: 0.360478\tLR: 0.030000\n",
      "Train Epoch: 268 [65536/194182 (33%)]\tLoss: 0.609199\tGrad Norm: 0.859673\tLR: 0.030000\n",
      "Train Epoch: 268 [86016/194182 (44%)]\tLoss: 0.595561\tGrad Norm: 0.413538\tLR: 0.030000\n",
      "Train Epoch: 268 [106496/194182 (54%)]\tLoss: 0.598928\tGrad Norm: 0.759166\tLR: 0.030000\n",
      "Train Epoch: 268 [126976/194182 (65%)]\tLoss: 0.605934\tGrad Norm: 1.260807\tLR: 0.030000\n",
      "Train Epoch: 268 [147456/194182 (75%)]\tLoss: 0.617299\tGrad Norm: 1.533549\tLR: 0.030000\n",
      "Train Epoch: 268 [167936/194182 (85%)]\tLoss: 0.612397\tGrad Norm: 1.228587\tLR: 0.030000\n",
      "Train Epoch: 268 [188416/194182 (96%)]\tLoss: 0.611338\tGrad Norm: 1.181955\tLR: 0.030000\n",
      "Train set: Average loss: 0.6058\n",
      "Test set: Average loss: 0.2834, Average MAE: 0.3696\n",
      "Train Epoch: 269 [4096/194182 (2%)]\tLoss: 0.613928\tGrad Norm: 1.219161\tLR: 0.030000\n",
      "Train Epoch: 269 [24576/194182 (12%)]\tLoss: 0.609723\tGrad Norm: 0.927431\tLR: 0.030000\n",
      "Train Epoch: 269 [45056/194182 (23%)]\tLoss: 0.604425\tGrad Norm: 0.706702\tLR: 0.030000\n",
      "Train Epoch: 269 [65536/194182 (33%)]\tLoss: 0.598565\tGrad Norm: 1.031217\tLR: 0.030000\n",
      "Train Epoch: 269 [86016/194182 (44%)]\tLoss: 0.593890\tGrad Norm: 0.941597\tLR: 0.030000\n",
      "Train Epoch: 269 [106496/194182 (54%)]\tLoss: 0.609391\tGrad Norm: 1.133135\tLR: 0.030000\n",
      "Train Epoch: 269 [126976/194182 (65%)]\tLoss: 0.608329\tGrad Norm: 1.161978\tLR: 0.030000\n",
      "Train Epoch: 269 [147456/194182 (75%)]\tLoss: 0.608884\tGrad Norm: 0.895513\tLR: 0.030000\n",
      "Train Epoch: 269 [167936/194182 (85%)]\tLoss: 0.604164\tGrad Norm: 1.179445\tLR: 0.030000\n",
      "Train Epoch: 269 [188416/194182 (96%)]\tLoss: 0.605306\tGrad Norm: 0.778188\tLR: 0.030000\n",
      "Train set: Average loss: 0.6063\n",
      "Test set: Average loss: 0.2819, Average MAE: 0.3878\n",
      "Train Epoch: 270 [4096/194182 (2%)]\tLoss: 0.605261\tGrad Norm: 0.839725\tLR: 0.030000\n",
      "Train Epoch: 270 [24576/194182 (12%)]\tLoss: 0.598456\tGrad Norm: 1.026152\tLR: 0.030000\n",
      "Train Epoch: 270 [45056/194182 (23%)]\tLoss: 0.604429\tGrad Norm: 1.293430\tLR: 0.030000\n",
      "Train Epoch: 270 [65536/194182 (33%)]\tLoss: 0.611417\tGrad Norm: 1.455798\tLR: 0.030000\n",
      "Train Epoch: 270 [86016/194182 (44%)]\tLoss: 0.609377\tGrad Norm: 1.415269\tLR: 0.030000\n",
      "Train Epoch: 270 [106496/194182 (54%)]\tLoss: 0.609107\tGrad Norm: 1.251254\tLR: 0.030000\n",
      "Train Epoch: 270 [126976/194182 (65%)]\tLoss: 0.602345\tGrad Norm: 0.911930\tLR: 0.030000\n",
      "Train Epoch: 270 [147456/194182 (75%)]\tLoss: 0.613131\tGrad Norm: 1.160653\tLR: 0.030000\n",
      "Train Epoch: 270 [167936/194182 (85%)]\tLoss: 0.607542\tGrad Norm: 0.768385\tLR: 0.030000\n",
      "Train Epoch: 270 [188416/194182 (96%)]\tLoss: 0.597058\tGrad Norm: 0.875663\tLR: 0.030000\n",
      "Train set: Average loss: 0.6069\n",
      "Test set: Average loss: 0.2818, Average MAE: 0.3701\n",
      "Epoch 270: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 271 [4096/194182 (2%)]\tLoss: 0.608402\tGrad Norm: 1.089986\tLR: 0.030000\n",
      "Train Epoch: 271 [24576/194182 (12%)]\tLoss: 0.603795\tGrad Norm: 0.957046\tLR: 0.030000\n",
      "Train Epoch: 271 [45056/194182 (23%)]\tLoss: 0.609388\tGrad Norm: 1.313808\tLR: 0.030000\n",
      "Train Epoch: 271 [65536/194182 (33%)]\tLoss: 0.604100\tGrad Norm: 1.263865\tLR: 0.030000\n",
      "Train Epoch: 271 [86016/194182 (44%)]\tLoss: 0.601216\tGrad Norm: 1.070736\tLR: 0.030000\n",
      "Train Epoch: 271 [106496/194182 (54%)]\tLoss: 0.602949\tGrad Norm: 0.994406\tLR: 0.030000\n",
      "Train Epoch: 271 [126976/194182 (65%)]\tLoss: 0.610028\tGrad Norm: 1.157208\tLR: 0.030000\n",
      "Train Epoch: 271 [147456/194182 (75%)]\tLoss: 0.604645\tGrad Norm: 1.255904\tLR: 0.030000\n",
      "Train Epoch: 271 [167936/194182 (85%)]\tLoss: 0.607180\tGrad Norm: 1.067688\tLR: 0.030000\n",
      "Train Epoch: 271 [188416/194182 (96%)]\tLoss: 0.605711\tGrad Norm: 1.344027\tLR: 0.030000\n",
      "Train set: Average loss: 0.6067\n",
      "Test set: Average loss: 0.2829, Average MAE: 0.3832\n",
      "Train Epoch: 272 [4096/194182 (2%)]\tLoss: 0.610790\tGrad Norm: 0.988205\tLR: 0.030000\n",
      "Train Epoch: 272 [24576/194182 (12%)]\tLoss: 0.603417\tGrad Norm: 0.853956\tLR: 0.030000\n",
      "Train Epoch: 272 [45056/194182 (23%)]\tLoss: 0.600307\tGrad Norm: 1.075056\tLR: 0.030000\n",
      "Train Epoch: 272 [65536/194182 (33%)]\tLoss: 0.605173\tGrad Norm: 1.044941\tLR: 0.030000\n",
      "Train Epoch: 272 [86016/194182 (44%)]\tLoss: 0.600277\tGrad Norm: 0.633323\tLR: 0.030000\n",
      "Train Epoch: 272 [106496/194182 (54%)]\tLoss: 0.591126\tGrad Norm: 0.766334\tLR: 0.030000\n",
      "Train Epoch: 272 [126976/194182 (65%)]\tLoss: 0.599255\tGrad Norm: 0.502856\tLR: 0.030000\n",
      "Train Epoch: 272 [147456/194182 (75%)]\tLoss: 0.602261\tGrad Norm: 0.730564\tLR: 0.030000\n",
      "Train Epoch: 272 [167936/194182 (85%)]\tLoss: 0.603807\tGrad Norm: 0.810945\tLR: 0.030000\n",
      "Train Epoch: 272 [188416/194182 (96%)]\tLoss: 0.606811\tGrad Norm: 0.957437\tLR: 0.030000\n",
      "Train set: Average loss: 0.6018\n",
      "Test set: Average loss: 0.2838, Average MAE: 0.3954\n",
      "Train Epoch: 273 [4096/194182 (2%)]\tLoss: 0.596601\tGrad Norm: 1.194706\tLR: 0.030000\n",
      "Train Epoch: 273 [24576/194182 (12%)]\tLoss: 0.607514\tGrad Norm: 1.070151\tLR: 0.030000\n",
      "Train Epoch: 273 [45056/194182 (23%)]\tLoss: 0.610281\tGrad Norm: 1.258732\tLR: 0.030000\n",
      "Train Epoch: 273 [65536/194182 (33%)]\tLoss: 0.613398\tGrad Norm: 1.694159\tLR: 0.030000\n",
      "Train Epoch: 273 [86016/194182 (44%)]\tLoss: 0.608124\tGrad Norm: 1.298897\tLR: 0.030000\n",
      "Train Epoch: 273 [106496/194182 (54%)]\tLoss: 0.594986\tGrad Norm: 0.953198\tLR: 0.030000\n",
      "Train Epoch: 273 [126976/194182 (65%)]\tLoss: 0.598282\tGrad Norm: 1.007000\tLR: 0.030000\n",
      "Train Epoch: 273 [147456/194182 (75%)]\tLoss: 0.601756\tGrad Norm: 0.956904\tLR: 0.030000\n",
      "Train Epoch: 273 [167936/194182 (85%)]\tLoss: 0.601631\tGrad Norm: 1.227052\tLR: 0.030000\n",
      "Train Epoch: 273 [188416/194182 (96%)]\tLoss: 0.606631\tGrad Norm: 1.220930\tLR: 0.030000\n",
      "Train set: Average loss: 0.6052\n",
      "Test set: Average loss: 0.2873, Average MAE: 0.3796\n",
      "Train Epoch: 274 [4096/194182 (2%)]\tLoss: 0.608053\tGrad Norm: 1.295991\tLR: 0.030000\n",
      "Train Epoch: 274 [24576/194182 (12%)]\tLoss: 0.598840\tGrad Norm: 1.170966\tLR: 0.030000\n",
      "Train Epoch: 274 [45056/194182 (23%)]\tLoss: 0.603085\tGrad Norm: 1.006843\tLR: 0.030000\n",
      "Train Epoch: 274 [65536/194182 (33%)]\tLoss: 0.598214\tGrad Norm: 0.722237\tLR: 0.030000\n",
      "Train Epoch: 274 [86016/194182 (44%)]\tLoss: 0.598227\tGrad Norm: 0.524868\tLR: 0.030000\n",
      "Train Epoch: 274 [106496/194182 (54%)]\tLoss: 0.590502\tGrad Norm: 0.983071\tLR: 0.030000\n",
      "Train Epoch: 274 [126976/194182 (65%)]\tLoss: 0.605858\tGrad Norm: 0.684394\tLR: 0.030000\n",
      "Train Epoch: 274 [147456/194182 (75%)]\tLoss: 0.600974\tGrad Norm: 0.905731\tLR: 0.030000\n",
      "Train Epoch: 274 [167936/194182 (85%)]\tLoss: 0.595337\tGrad Norm: 1.108079\tLR: 0.030000\n",
      "Train Epoch: 274 [188416/194182 (96%)]\tLoss: 0.601354\tGrad Norm: 1.098058\tLR: 0.030000\n",
      "Train set: Average loss: 0.6014\n",
      "Test set: Average loss: 0.2819, Average MAE: 0.3887\n",
      "Train Epoch: 275 [4096/194182 (2%)]\tLoss: 0.597021\tGrad Norm: 0.960791\tLR: 0.030000\n",
      "Train Epoch: 275 [24576/194182 (12%)]\tLoss: 0.602837\tGrad Norm: 1.203011\tLR: 0.030000\n",
      "Train Epoch: 275 [45056/194182 (23%)]\tLoss: 0.603580\tGrad Norm: 1.190921\tLR: 0.030000\n",
      "Train Epoch: 275 [65536/194182 (33%)]\tLoss: 0.610977\tGrad Norm: 1.209706\tLR: 0.030000\n",
      "Train Epoch: 275 [86016/194182 (44%)]\tLoss: 0.613888\tGrad Norm: 1.458857\tLR: 0.030000\n",
      "Train Epoch: 275 [106496/194182 (54%)]\tLoss: 0.614482\tGrad Norm: 1.537341\tLR: 0.030000\n",
      "Train Epoch: 275 [126976/194182 (65%)]\tLoss: 0.607060\tGrad Norm: 1.281794\tLR: 0.030000\n",
      "Train Epoch: 275 [147456/194182 (75%)]\tLoss: 0.599114\tGrad Norm: 0.898656\tLR: 0.030000\n",
      "Train Epoch: 275 [167936/194182 (85%)]\tLoss: 0.607052\tGrad Norm: 1.267764\tLR: 0.030000\n",
      "Train Epoch: 275 [188416/194182 (96%)]\tLoss: 0.609563\tGrad Norm: 1.283047\tLR: 0.030000\n",
      "Train set: Average loss: 0.6049\n",
      "Test set: Average loss: 0.2852, Average MAE: 0.3931\n",
      "Epoch 275: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 276 [4096/194182 (2%)]\tLoss: 0.600859\tGrad Norm: 1.218832\tLR: 0.030000\n",
      "Train Epoch: 276 [24576/194182 (12%)]\tLoss: 0.601326\tGrad Norm: 1.285585\tLR: 0.030000\n",
      "Train Epoch: 276 [45056/194182 (23%)]\tLoss: 0.602786\tGrad Norm: 1.181449\tLR: 0.030000\n",
      "Train Epoch: 276 [65536/194182 (33%)]\tLoss: 0.598438\tGrad Norm: 0.961426\tLR: 0.030000\n",
      "Train Epoch: 276 [86016/194182 (44%)]\tLoss: 0.601743\tGrad Norm: 0.690482\tLR: 0.030000\n",
      "Train Epoch: 276 [106496/194182 (54%)]\tLoss: 0.598103\tGrad Norm: 0.771669\tLR: 0.030000\n",
      "Train Epoch: 276 [126976/194182 (65%)]\tLoss: 0.594197\tGrad Norm: 0.658479\tLR: 0.030000\n",
      "Train Epoch: 276 [147456/194182 (75%)]\tLoss: 0.603262\tGrad Norm: 0.768979\tLR: 0.030000\n",
      "Train Epoch: 276 [167936/194182 (85%)]\tLoss: 0.595533\tGrad Norm: 0.925559\tLR: 0.030000\n",
      "Train Epoch: 276 [188416/194182 (96%)]\tLoss: 0.607312\tGrad Norm: 1.341871\tLR: 0.030000\n",
      "Train set: Average loss: 0.6008\n",
      "Test set: Average loss: 0.2810, Average MAE: 0.3698\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 277 [4096/194182 (2%)]\tLoss: 0.602086\tGrad Norm: 0.934247\tLR: 0.030000\n",
      "Train Epoch: 277 [24576/194182 (12%)]\tLoss: 0.598087\tGrad Norm: 0.816332\tLR: 0.030000\n",
      "Train Epoch: 277 [45056/194182 (23%)]\tLoss: 0.595059\tGrad Norm: 0.748289\tLR: 0.030000\n",
      "Train Epoch: 277 [65536/194182 (33%)]\tLoss: 0.610310\tGrad Norm: 0.935045\tLR: 0.030000\n",
      "Train Epoch: 277 [86016/194182 (44%)]\tLoss: 0.598764\tGrad Norm: 1.055777\tLR: 0.030000\n",
      "Train Epoch: 277 [106496/194182 (54%)]\tLoss: 0.594830\tGrad Norm: 1.173550\tLR: 0.030000\n",
      "Train Epoch: 277 [126976/194182 (65%)]\tLoss: 0.598891\tGrad Norm: 1.217490\tLR: 0.030000\n",
      "Train Epoch: 277 [147456/194182 (75%)]\tLoss: 0.606744\tGrad Norm: 1.080714\tLR: 0.030000\n",
      "Train Epoch: 277 [167936/194182 (85%)]\tLoss: 0.605270\tGrad Norm: 1.045967\tLR: 0.030000\n",
      "Train Epoch: 277 [188416/194182 (96%)]\tLoss: 0.593394\tGrad Norm: 0.930228\tLR: 0.030000\n",
      "Train set: Average loss: 0.6005\n",
      "Test set: Average loss: 0.2839, Average MAE: 0.3945\n",
      "Train Epoch: 278 [4096/194182 (2%)]\tLoss: 0.600513\tGrad Norm: 1.097004\tLR: 0.030000\n",
      "Train Epoch: 278 [24576/194182 (12%)]\tLoss: 0.598795\tGrad Norm: 0.788941\tLR: 0.030000\n",
      "Train Epoch: 278 [45056/194182 (23%)]\tLoss: 0.599531\tGrad Norm: 1.381239\tLR: 0.030000\n",
      "Train Epoch: 278 [65536/194182 (33%)]\tLoss: 0.601615\tGrad Norm: 1.294210\tLR: 0.030000\n",
      "Train Epoch: 278 [86016/194182 (44%)]\tLoss: 0.607275\tGrad Norm: 1.257445\tLR: 0.030000\n",
      "Train Epoch: 278 [106496/194182 (54%)]\tLoss: 0.606346\tGrad Norm: 1.204244\tLR: 0.030000\n",
      "Train Epoch: 278 [126976/194182 (65%)]\tLoss: 0.608886\tGrad Norm: 0.910538\tLR: 0.030000\n",
      "Train Epoch: 278 [147456/194182 (75%)]\tLoss: 0.601746\tGrad Norm: 0.911792\tLR: 0.030000\n",
      "Train Epoch: 278 [167936/194182 (85%)]\tLoss: 0.604965\tGrad Norm: 0.953011\tLR: 0.030000\n",
      "Train Epoch: 278 [188416/194182 (96%)]\tLoss: 0.596403\tGrad Norm: 0.650995\tLR: 0.030000\n",
      "Train set: Average loss: 0.6002\n",
      "Test set: Average loss: 0.2789, Average MAE: 0.3745\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 279 [4096/194182 (2%)]\tLoss: 0.598508\tGrad Norm: 0.845534\tLR: 0.030000\n",
      "Train Epoch: 279 [24576/194182 (12%)]\tLoss: 0.596543\tGrad Norm: 0.943128\tLR: 0.030000\n",
      "Train Epoch: 279 [45056/194182 (23%)]\tLoss: 0.596079\tGrad Norm: 0.990788\tLR: 0.030000\n",
      "Train Epoch: 279 [65536/194182 (33%)]\tLoss: 0.597512\tGrad Norm: 0.924221\tLR: 0.030000\n",
      "Train Epoch: 279 [86016/194182 (44%)]\tLoss: 0.610512\tGrad Norm: 1.936654\tLR: 0.030000\n",
      "Train Epoch: 279 [106496/194182 (54%)]\tLoss: 0.607895\tGrad Norm: 1.439255\tLR: 0.030000\n",
      "Train Epoch: 279 [126976/194182 (65%)]\tLoss: 0.608134\tGrad Norm: 1.145292\tLR: 0.030000\n",
      "Train Epoch: 279 [147456/194182 (75%)]\tLoss: 0.599616\tGrad Norm: 1.203592\tLR: 0.030000\n",
      "Train Epoch: 279 [167936/194182 (85%)]\tLoss: 0.591210\tGrad Norm: 0.312108\tLR: 0.030000\n",
      "Train Epoch: 279 [188416/194182 (96%)]\tLoss: 0.599512\tGrad Norm: 0.695759\tLR: 0.030000\n",
      "Train set: Average loss: 0.5991\n",
      "Test set: Average loss: 0.2780, Average MAE: 0.3828\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 280 [4096/194182 (2%)]\tLoss: 0.592829\tGrad Norm: 0.893439\tLR: 0.030000\n",
      "Train Epoch: 280 [24576/194182 (12%)]\tLoss: 0.596693\tGrad Norm: 1.274424\tLR: 0.030000\n",
      "Train Epoch: 280 [45056/194182 (23%)]\tLoss: 0.607561\tGrad Norm: 1.056632\tLR: 0.030000\n",
      "Train Epoch: 280 [65536/194182 (33%)]\tLoss: 0.596936\tGrad Norm: 0.917821\tLR: 0.030000\n",
      "Train Epoch: 280 [86016/194182 (44%)]\tLoss: 0.604224\tGrad Norm: 1.219891\tLR: 0.030000\n",
      "Train Epoch: 280 [106496/194182 (54%)]\tLoss: 0.601167\tGrad Norm: 1.108227\tLR: 0.030000\n",
      "Train Epoch: 280 [126976/194182 (65%)]\tLoss: 0.594465\tGrad Norm: 0.919163\tLR: 0.030000\n",
      "Train Epoch: 280 [147456/194182 (75%)]\tLoss: 0.603395\tGrad Norm: 1.104999\tLR: 0.030000\n",
      "Train Epoch: 280 [167936/194182 (85%)]\tLoss: 0.597528\tGrad Norm: 1.030807\tLR: 0.030000\n",
      "Train Epoch: 280 [188416/194182 (96%)]\tLoss: 0.597178\tGrad Norm: 1.710055\tLR: 0.030000\n",
      "Train set: Average loss: 0.5996\n",
      "Test set: Average loss: 0.2850, Average MAE: 0.3992\n",
      "Epoch 280: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 281 [4096/194182 (2%)]\tLoss: 0.597131\tGrad Norm: 1.288421\tLR: 0.030000\n",
      "Train Epoch: 281 [24576/194182 (12%)]\tLoss: 0.597204\tGrad Norm: 1.342674\tLR: 0.030000\n",
      "Train Epoch: 281 [45056/194182 (23%)]\tLoss: 0.602022\tGrad Norm: 1.359045\tLR: 0.030000\n",
      "Train Epoch: 281 [65536/194182 (33%)]\tLoss: 0.609381\tGrad Norm: 1.377083\tLR: 0.030000\n",
      "Train Epoch: 281 [86016/194182 (44%)]\tLoss: 0.593345\tGrad Norm: 1.233220\tLR: 0.030000\n",
      "Train Epoch: 281 [106496/194182 (54%)]\tLoss: 0.600868\tGrad Norm: 1.271323\tLR: 0.030000\n",
      "Train Epoch: 281 [126976/194182 (65%)]\tLoss: 0.596960\tGrad Norm: 1.143639\tLR: 0.030000\n",
      "Train Epoch: 281 [147456/194182 (75%)]\tLoss: 0.607588\tGrad Norm: 0.929694\tLR: 0.030000\n",
      "Train Epoch: 281 [167936/194182 (85%)]\tLoss: 0.594650\tGrad Norm: 1.029979\tLR: 0.030000\n",
      "Train Epoch: 281 [188416/194182 (96%)]\tLoss: 0.597610\tGrad Norm: 1.064109\tLR: 0.030000\n",
      "Train set: Average loss: 0.6001\n",
      "Test set: Average loss: 0.2851, Average MAE: 0.3780\n",
      "Train Epoch: 282 [4096/194182 (2%)]\tLoss: 0.594419\tGrad Norm: 1.308724\tLR: 0.030000\n",
      "Train Epoch: 282 [24576/194182 (12%)]\tLoss: 0.598566\tGrad Norm: 1.121862\tLR: 0.030000\n",
      "Train Epoch: 282 [45056/194182 (23%)]\tLoss: 0.594659\tGrad Norm: 1.092771\tLR: 0.030000\n",
      "Train Epoch: 282 [65536/194182 (33%)]\tLoss: 0.588882\tGrad Norm: 0.830886\tLR: 0.030000\n",
      "Train Epoch: 282 [86016/194182 (44%)]\tLoss: 0.599242\tGrad Norm: 0.880757\tLR: 0.030000\n",
      "Train Epoch: 282 [106496/194182 (54%)]\tLoss: 0.587543\tGrad Norm: 0.440816\tLR: 0.030000\n",
      "Train Epoch: 282 [126976/194182 (65%)]\tLoss: 0.603141\tGrad Norm: 0.939481\tLR: 0.030000\n",
      "Train Epoch: 282 [147456/194182 (75%)]\tLoss: 0.593869\tGrad Norm: 0.585817\tLR: 0.030000\n",
      "Train Epoch: 282 [167936/194182 (85%)]\tLoss: 0.595036\tGrad Norm: 0.781987\tLR: 0.030000\n",
      "Train Epoch: 282 [188416/194182 (96%)]\tLoss: 0.601753\tGrad Norm: 1.058231\tLR: 0.030000\n",
      "Train set: Average loss: 0.5955\n",
      "Test set: Average loss: 0.2818, Average MAE: 0.3871\n",
      "Train Epoch: 283 [4096/194182 (2%)]\tLoss: 0.604117\tGrad Norm: 1.191763\tLR: 0.030000\n",
      "Train Epoch: 283 [24576/194182 (12%)]\tLoss: 0.602067\tGrad Norm: 1.414533\tLR: 0.030000\n",
      "Train Epoch: 283 [45056/194182 (23%)]\tLoss: 0.610630\tGrad Norm: 1.284051\tLR: 0.030000\n",
      "Train Epoch: 283 [65536/194182 (33%)]\tLoss: 0.587539\tGrad Norm: 1.082583\tLR: 0.030000\n",
      "Train Epoch: 283 [86016/194182 (44%)]\tLoss: 0.597613\tGrad Norm: 0.896967\tLR: 0.030000\n",
      "Train Epoch: 283 [106496/194182 (54%)]\tLoss: 0.572919\tGrad Norm: 0.559257\tLR: 0.030000\n",
      "Train Epoch: 283 [126976/194182 (65%)]\tLoss: 0.598334\tGrad Norm: 0.989948\tLR: 0.030000\n",
      "Train Epoch: 283 [147456/194182 (75%)]\tLoss: 0.594479\tGrad Norm: 1.015855\tLR: 0.030000\n",
      "Train Epoch: 283 [167936/194182 (85%)]\tLoss: 0.611363\tGrad Norm: 1.200357\tLR: 0.030000\n",
      "Train Epoch: 283 [188416/194182 (96%)]\tLoss: 0.596995\tGrad Norm: 1.083144\tLR: 0.030000\n",
      "Train set: Average loss: 0.5966\n",
      "Test set: Average loss: 0.2794, Average MAE: 0.3718\n",
      "Train Epoch: 284 [4096/194182 (2%)]\tLoss: 0.591350\tGrad Norm: 0.955666\tLR: 0.030000\n",
      "Train Epoch: 284 [24576/194182 (12%)]\tLoss: 0.586163\tGrad Norm: 1.045399\tLR: 0.030000\n",
      "Train Epoch: 284 [45056/194182 (23%)]\tLoss: 0.588862\tGrad Norm: 0.811134\tLR: 0.030000\n",
      "Train Epoch: 284 [65536/194182 (33%)]\tLoss: 0.601944\tGrad Norm: 0.800594\tLR: 0.030000\n",
      "Train Epoch: 284 [86016/194182 (44%)]\tLoss: 0.587933\tGrad Norm: 0.672626\tLR: 0.030000\n",
      "Train Epoch: 284 [106496/194182 (54%)]\tLoss: 0.585240\tGrad Norm: 0.822046\tLR: 0.030000\n",
      "Train Epoch: 284 [126976/194182 (65%)]\tLoss: 0.590730\tGrad Norm: 0.832411\tLR: 0.030000\n",
      "Train Epoch: 284 [147456/194182 (75%)]\tLoss: 0.599475\tGrad Norm: 0.937997\tLR: 0.030000\n",
      "Train Epoch: 284 [167936/194182 (85%)]\tLoss: 0.590429\tGrad Norm: 0.946875\tLR: 0.030000\n",
      "Train Epoch: 284 [188416/194182 (96%)]\tLoss: 0.601014\tGrad Norm: 1.236243\tLR: 0.030000\n",
      "Train set: Average loss: 0.5937\n",
      "Test set: Average loss: 0.2809, Average MAE: 0.3920\n",
      "Train Epoch: 285 [4096/194182 (2%)]\tLoss: 0.586272\tGrad Norm: 1.036774\tLR: 0.030000\n",
      "Train Epoch: 285 [24576/194182 (12%)]\tLoss: 0.598674\tGrad Norm: 1.722348\tLR: 0.030000\n",
      "Train Epoch: 285 [45056/194182 (23%)]\tLoss: 0.591564\tGrad Norm: 1.048877\tLR: 0.030000\n",
      "Train Epoch: 285 [65536/194182 (33%)]\tLoss: 0.590125\tGrad Norm: 1.052402\tLR: 0.030000\n",
      "Train Epoch: 285 [86016/194182 (44%)]\tLoss: 0.601584\tGrad Norm: 1.428363\tLR: 0.030000\n",
      "Train Epoch: 285 [106496/194182 (54%)]\tLoss: 0.589085\tGrad Norm: 0.657904\tLR: 0.030000\n",
      "Train Epoch: 285 [126976/194182 (65%)]\tLoss: 0.600813\tGrad Norm: 1.138553\tLR: 0.030000\n",
      "Train Epoch: 285 [147456/194182 (75%)]\tLoss: 0.593739\tGrad Norm: 1.010352\tLR: 0.030000\n",
      "Train Epoch: 285 [167936/194182 (85%)]\tLoss: 0.604755\tGrad Norm: 1.317896\tLR: 0.030000\n",
      "Train Epoch: 285 [188416/194182 (96%)]\tLoss: 0.597086\tGrad Norm: 1.513310\tLR: 0.030000\n",
      "Train set: Average loss: 0.5974\n",
      "Test set: Average loss: 0.2835, Average MAE: 0.3724\n",
      "Epoch 285: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 286 [4096/194182 (2%)]\tLoss: 0.593169\tGrad Norm: 1.242233\tLR: 0.030000\n",
      "Train Epoch: 286 [24576/194182 (12%)]\tLoss: 0.604490\tGrad Norm: 1.312117\tLR: 0.030000\n",
      "Train Epoch: 286 [45056/194182 (23%)]\tLoss: 0.599324\tGrad Norm: 1.316290\tLR: 0.030000\n",
      "Train Epoch: 286 [65536/194182 (33%)]\tLoss: 0.600628\tGrad Norm: 1.190873\tLR: 0.030000\n",
      "Train Epoch: 286 [86016/194182 (44%)]\tLoss: 0.593533\tGrad Norm: 1.103200\tLR: 0.030000\n",
      "Train Epoch: 286 [106496/194182 (54%)]\tLoss: 0.587533\tGrad Norm: 1.004840\tLR: 0.030000\n",
      "Train Epoch: 286 [126976/194182 (65%)]\tLoss: 0.582611\tGrad Norm: 1.091721\tLR: 0.030000\n",
      "Train Epoch: 286 [147456/194182 (75%)]\tLoss: 0.599793\tGrad Norm: 0.771107\tLR: 0.030000\n",
      "Train Epoch: 286 [167936/194182 (85%)]\tLoss: 0.597624\tGrad Norm: 0.739685\tLR: 0.030000\n",
      "Train Epoch: 286 [188416/194182 (96%)]\tLoss: 0.588915\tGrad Norm: 0.966166\tLR: 0.030000\n",
      "Train set: Average loss: 0.5949\n",
      "Test set: Average loss: 0.2803, Average MAE: 0.3660\n",
      "Train Epoch: 287 [4096/194182 (2%)]\tLoss: 0.584650\tGrad Norm: 1.020594\tLR: 0.030000\n",
      "Train Epoch: 287 [24576/194182 (12%)]\tLoss: 0.599436\tGrad Norm: 1.495076\tLR: 0.030000\n",
      "Train Epoch: 287 [45056/194182 (23%)]\tLoss: 0.594933\tGrad Norm: 1.249756\tLR: 0.030000\n",
      "Train Epoch: 287 [65536/194182 (33%)]\tLoss: 0.594968\tGrad Norm: 1.068647\tLR: 0.030000\n",
      "Train Epoch: 287 [86016/194182 (44%)]\tLoss: 0.597776\tGrad Norm: 1.060154\tLR: 0.030000\n",
      "Train Epoch: 287 [106496/194182 (54%)]\tLoss: 0.592905\tGrad Norm: 1.211125\tLR: 0.030000\n",
      "Train Epoch: 287 [126976/194182 (65%)]\tLoss: 0.592622\tGrad Norm: 0.928045\tLR: 0.030000\n",
      "Train Epoch: 287 [147456/194182 (75%)]\tLoss: 0.585659\tGrad Norm: 0.920110\tLR: 0.030000\n",
      "Train Epoch: 287 [167936/194182 (85%)]\tLoss: 0.586689\tGrad Norm: 0.791130\tLR: 0.030000\n",
      "Train Epoch: 287 [188416/194182 (96%)]\tLoss: 0.590776\tGrad Norm: 1.072121\tLR: 0.030000\n",
      "Train set: Average loss: 0.5941\n",
      "Test set: Average loss: 0.2800, Average MAE: 0.3878\n",
      "Train Epoch: 288 [4096/194182 (2%)]\tLoss: 0.591140\tGrad Norm: 1.044292\tLR: 0.030000\n",
      "Train Epoch: 288 [24576/194182 (12%)]\tLoss: 0.598885\tGrad Norm: 1.089181\tLR: 0.030000\n",
      "Train Epoch: 288 [45056/194182 (23%)]\tLoss: 0.587869\tGrad Norm: 1.142886\tLR: 0.030000\n",
      "Train Epoch: 288 [65536/194182 (33%)]\tLoss: 0.600070\tGrad Norm: 1.237649\tLR: 0.030000\n",
      "Train Epoch: 288 [86016/194182 (44%)]\tLoss: 0.585395\tGrad Norm: 1.341462\tLR: 0.030000\n",
      "Train Epoch: 288 [106496/194182 (54%)]\tLoss: 0.599886\tGrad Norm: 1.390962\tLR: 0.030000\n",
      "Train Epoch: 288 [126976/194182 (65%)]\tLoss: 0.601799\tGrad Norm: 1.226069\tLR: 0.030000\n",
      "Train Epoch: 288 [147456/194182 (75%)]\tLoss: 0.604603\tGrad Norm: 1.533680\tLR: 0.030000\n",
      "Train Epoch: 288 [167936/194182 (85%)]\tLoss: 0.599771\tGrad Norm: 1.228164\tLR: 0.030000\n",
      "Train Epoch: 288 [188416/194182 (96%)]\tLoss: 0.598606\tGrad Norm: 1.150171\tLR: 0.030000\n",
      "Train set: Average loss: 0.5954\n",
      "Test set: Average loss: 0.2810, Average MAE: 0.3760\n",
      "Train Epoch: 289 [4096/194182 (2%)]\tLoss: 0.591775\tGrad Norm: 1.204182\tLR: 0.030000\n",
      "Train Epoch: 289 [24576/194182 (12%)]\tLoss: 0.586476\tGrad Norm: 1.032756\tLR: 0.030000\n",
      "Train Epoch: 289 [45056/194182 (23%)]\tLoss: 0.596671\tGrad Norm: 1.097596\tLR: 0.030000\n",
      "Train Epoch: 289 [65536/194182 (33%)]\tLoss: 0.587197\tGrad Norm: 1.150882\tLR: 0.030000\n",
      "Train Epoch: 289 [86016/194182 (44%)]\tLoss: 0.581541\tGrad Norm: 0.725237\tLR: 0.030000\n",
      "Train Epoch: 289 [106496/194182 (54%)]\tLoss: 0.582307\tGrad Norm: 0.809244\tLR: 0.030000\n",
      "Train Epoch: 289 [126976/194182 (65%)]\tLoss: 0.583000\tGrad Norm: 0.851332\tLR: 0.030000\n",
      "Train Epoch: 289 [147456/194182 (75%)]\tLoss: 0.603767\tGrad Norm: 1.224150\tLR: 0.030000\n",
      "Train Epoch: 289 [167936/194182 (85%)]\tLoss: 0.596029\tGrad Norm: 1.175232\tLR: 0.030000\n",
      "Train Epoch: 289 [188416/194182 (96%)]\tLoss: 0.593837\tGrad Norm: 1.236075\tLR: 0.030000\n",
      "Train set: Average loss: 0.5920\n",
      "Test set: Average loss: 0.2808, Average MAE: 0.3859\n",
      "Train Epoch: 290 [4096/194182 (2%)]\tLoss: 0.600685\tGrad Norm: 0.985601\tLR: 0.030000\n",
      "Train Epoch: 290 [24576/194182 (12%)]\tLoss: 0.600703\tGrad Norm: 1.322834\tLR: 0.030000\n",
      "Train Epoch: 290 [45056/194182 (23%)]\tLoss: 0.593000\tGrad Norm: 1.302330\tLR: 0.030000\n",
      "Train Epoch: 290 [65536/194182 (33%)]\tLoss: 0.603567\tGrad Norm: 1.041944\tLR: 0.030000\n",
      "Train Epoch: 290 [86016/194182 (44%)]\tLoss: 0.593087\tGrad Norm: 1.122429\tLR: 0.030000\n",
      "Train Epoch: 290 [106496/194182 (54%)]\tLoss: 0.589479\tGrad Norm: 1.044817\tLR: 0.030000\n",
      "Train Epoch: 290 [126976/194182 (65%)]\tLoss: 0.599771\tGrad Norm: 0.994922\tLR: 0.030000\n",
      "Train Epoch: 290 [147456/194182 (75%)]\tLoss: 0.592493\tGrad Norm: 1.026198\tLR: 0.030000\n",
      "Train Epoch: 290 [167936/194182 (85%)]\tLoss: 0.595327\tGrad Norm: 0.962042\tLR: 0.030000\n",
      "Train Epoch: 290 [188416/194182 (96%)]\tLoss: 0.586896\tGrad Norm: 0.929564\tLR: 0.030000\n",
      "Train set: Average loss: 0.5924\n",
      "Test set: Average loss: 0.2778, Average MAE: 0.3657\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 290: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 291 [4096/194182 (2%)]\tLoss: 0.587354\tGrad Norm: 0.880906\tLR: 0.030000\n",
      "Train Epoch: 291 [24576/194182 (12%)]\tLoss: 0.585032\tGrad Norm: 0.899725\tLR: 0.030000\n",
      "Train Epoch: 291 [45056/194182 (23%)]\tLoss: 0.592992\tGrad Norm: 1.277154\tLR: 0.030000\n",
      "Train Epoch: 291 [65536/194182 (33%)]\tLoss: 0.587814\tGrad Norm: 1.225694\tLR: 0.030000\n",
      "Train Epoch: 291 [86016/194182 (44%)]\tLoss: 0.593377\tGrad Norm: 1.432860\tLR: 0.030000\n",
      "Train Epoch: 291 [106496/194182 (54%)]\tLoss: 0.598161\tGrad Norm: 1.531387\tLR: 0.030000\n",
      "Train Epoch: 291 [126976/194182 (65%)]\tLoss: 0.573104\tGrad Norm: 0.829925\tLR: 0.030000\n",
      "Train Epoch: 291 [147456/194182 (75%)]\tLoss: 0.595081\tGrad Norm: 0.708138\tLR: 0.030000\n",
      "Train Epoch: 291 [167936/194182 (85%)]\tLoss: 0.583324\tGrad Norm: 0.957719\tLR: 0.030000\n",
      "Train Epoch: 291 [188416/194182 (96%)]\tLoss: 0.588585\tGrad Norm: 0.889031\tLR: 0.030000\n",
      "Train set: Average loss: 0.5914\n",
      "Test set: Average loss: 0.2783, Average MAE: 0.3728\n",
      "Train Epoch: 292 [4096/194182 (2%)]\tLoss: 0.596410\tGrad Norm: 0.882392\tLR: 0.030000\n",
      "Train Epoch: 292 [24576/194182 (12%)]\tLoss: 0.594442\tGrad Norm: 1.316199\tLR: 0.030000\n",
      "Train Epoch: 292 [45056/194182 (23%)]\tLoss: 0.595385\tGrad Norm: 1.371057\tLR: 0.030000\n",
      "Train Epoch: 292 [65536/194182 (33%)]\tLoss: 0.587451\tGrad Norm: 0.808620\tLR: 0.030000\n",
      "Train Epoch: 292 [86016/194182 (44%)]\tLoss: 0.583489\tGrad Norm: 0.849642\tLR: 0.030000\n",
      "Train Epoch: 292 [106496/194182 (54%)]\tLoss: 0.590482\tGrad Norm: 1.135343\tLR: 0.030000\n",
      "Train Epoch: 292 [126976/194182 (65%)]\tLoss: 0.585577\tGrad Norm: 0.894003\tLR: 0.030000\n",
      "Train Epoch: 292 [147456/194182 (75%)]\tLoss: 0.593372\tGrad Norm: 0.976142\tLR: 0.030000\n",
      "Train Epoch: 292 [167936/194182 (85%)]\tLoss: 0.595974\tGrad Norm: 1.343248\tLR: 0.030000\n",
      "Train Epoch: 292 [188416/194182 (96%)]\tLoss: 0.586114\tGrad Norm: 1.300311\tLR: 0.030000\n",
      "Train set: Average loss: 0.5909\n",
      "Test set: Average loss: 0.2797, Average MAE: 0.3795\n",
      "Train Epoch: 293 [4096/194182 (2%)]\tLoss: 0.595743\tGrad Norm: 0.998675\tLR: 0.030000\n",
      "Train Epoch: 293 [24576/194182 (12%)]\tLoss: 0.584147\tGrad Norm: 0.802239\tLR: 0.030000\n",
      "Train Epoch: 293 [45056/194182 (23%)]\tLoss: 0.598526\tGrad Norm: 1.201766\tLR: 0.030000\n",
      "Train Epoch: 293 [65536/194182 (33%)]\tLoss: 0.593109\tGrad Norm: 1.443796\tLR: 0.030000\n",
      "Train Epoch: 293 [86016/194182 (44%)]\tLoss: 0.591911\tGrad Norm: 0.718893\tLR: 0.030000\n",
      "Train Epoch: 293 [106496/194182 (54%)]\tLoss: 0.584614\tGrad Norm: 1.212449\tLR: 0.030000\n",
      "Train Epoch: 293 [126976/194182 (65%)]\tLoss: 0.597448\tGrad Norm: 1.190357\tLR: 0.030000\n",
      "Train Epoch: 293 [147456/194182 (75%)]\tLoss: 0.588244\tGrad Norm: 1.098629\tLR: 0.030000\n",
      "Train Epoch: 293 [167936/194182 (85%)]\tLoss: 0.587557\tGrad Norm: 0.903047\tLR: 0.030000\n",
      "Train Epoch: 293 [188416/194182 (96%)]\tLoss: 0.594831\tGrad Norm: 1.124942\tLR: 0.030000\n",
      "Train set: Average loss: 0.5902\n",
      "Test set: Average loss: 0.2832, Average MAE: 0.3781\n",
      "Train Epoch: 294 [4096/194182 (2%)]\tLoss: 0.603454\tGrad Norm: 1.566065\tLR: 0.030000\n",
      "Train Epoch: 294 [24576/194182 (12%)]\tLoss: 0.597044\tGrad Norm: 1.062007\tLR: 0.030000\n",
      "Train Epoch: 294 [45056/194182 (23%)]\tLoss: 0.592761\tGrad Norm: 0.688226\tLR: 0.030000\n",
      "Train Epoch: 294 [65536/194182 (33%)]\tLoss: 0.584793\tGrad Norm: 1.330768\tLR: 0.030000\n",
      "Train Epoch: 294 [86016/194182 (44%)]\tLoss: 0.583957\tGrad Norm: 0.933750\tLR: 0.030000\n",
      "Train Epoch: 294 [106496/194182 (54%)]\tLoss: 0.591562\tGrad Norm: 1.083313\tLR: 0.030000\n",
      "Train Epoch: 294 [126976/194182 (65%)]\tLoss: 0.598936\tGrad Norm: 1.044113\tLR: 0.030000\n",
      "Train Epoch: 294 [147456/194182 (75%)]\tLoss: 0.591299\tGrad Norm: 0.896202\tLR: 0.030000\n",
      "Train Epoch: 294 [167936/194182 (85%)]\tLoss: 0.577372\tGrad Norm: 1.065993\tLR: 0.030000\n",
      "Train Epoch: 294 [188416/194182 (96%)]\tLoss: 0.586848\tGrad Norm: 0.846024\tLR: 0.030000\n",
      "Train set: Average loss: 0.5884\n",
      "Test set: Average loss: 0.2756, Average MAE: 0.3787\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 295 [4096/194182 (2%)]\tLoss: 0.580476\tGrad Norm: 1.086479\tLR: 0.030000\n",
      "Train Epoch: 295 [24576/194182 (12%)]\tLoss: 0.579562\tGrad Norm: 1.153782\tLR: 0.030000\n",
      "Train Epoch: 295 [45056/194182 (23%)]\tLoss: 0.584505\tGrad Norm: 1.082021\tLR: 0.030000\n",
      "Train Epoch: 295 [65536/194182 (33%)]\tLoss: 0.583615\tGrad Norm: 0.848372\tLR: 0.030000\n",
      "Train Epoch: 295 [86016/194182 (44%)]\tLoss: 0.591462\tGrad Norm: 1.070336\tLR: 0.030000\n",
      "Train Epoch: 295 [106496/194182 (54%)]\tLoss: 0.586130\tGrad Norm: 1.227600\tLR: 0.030000\n",
      "Train Epoch: 295 [126976/194182 (65%)]\tLoss: 0.593544\tGrad Norm: 1.185293\tLR: 0.030000\n",
      "Train Epoch: 295 [147456/194182 (75%)]\tLoss: 0.584568\tGrad Norm: 1.333786\tLR: 0.030000\n",
      "Train Epoch: 295 [167936/194182 (85%)]\tLoss: 0.590649\tGrad Norm: 1.429395\tLR: 0.030000\n",
      "Train Epoch: 295 [188416/194182 (96%)]\tLoss: 0.595967\tGrad Norm: 1.521904\tLR: 0.030000\n",
      "Train set: Average loss: 0.5904\n",
      "Test set: Average loss: 0.2818, Average MAE: 0.3894\n",
      "Epoch 295: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 296 [4096/194182 (2%)]\tLoss: 0.594702\tGrad Norm: 1.200813\tLR: 0.030000\n",
      "Train Epoch: 296 [24576/194182 (12%)]\tLoss: 0.589249\tGrad Norm: 1.168988\tLR: 0.030000\n",
      "Train Epoch: 296 [45056/194182 (23%)]\tLoss: 0.577049\tGrad Norm: 1.014033\tLR: 0.030000\n",
      "Train Epoch: 296 [65536/194182 (33%)]\tLoss: 0.589399\tGrad Norm: 1.188693\tLR: 0.030000\n",
      "Train Epoch: 296 [86016/194182 (44%)]\tLoss: 0.575660\tGrad Norm: 1.180624\tLR: 0.030000\n",
      "Train Epoch: 296 [106496/194182 (54%)]\tLoss: 0.582971\tGrad Norm: 1.045117\tLR: 0.030000\n",
      "Train Epoch: 296 [126976/194182 (65%)]\tLoss: 0.598208\tGrad Norm: 1.242864\tLR: 0.030000\n",
      "Train Epoch: 296 [147456/194182 (75%)]\tLoss: 0.587389\tGrad Norm: 0.723835\tLR: 0.030000\n",
      "Train Epoch: 296 [167936/194182 (85%)]\tLoss: 0.595933\tGrad Norm: 0.863709\tLR: 0.030000\n",
      "Train Epoch: 296 [188416/194182 (96%)]\tLoss: 0.593807\tGrad Norm: 0.978018\tLR: 0.030000\n",
      "Train set: Average loss: 0.5876\n",
      "Test set: Average loss: 0.2761, Average MAE: 0.3683\n",
      "Train Epoch: 297 [4096/194182 (2%)]\tLoss: 0.583350\tGrad Norm: 0.958192\tLR: 0.030000\n",
      "Train Epoch: 297 [24576/194182 (12%)]\tLoss: 0.593683\tGrad Norm: 0.992392\tLR: 0.030000\n",
      "Train Epoch: 297 [45056/194182 (23%)]\tLoss: 0.580866\tGrad Norm: 1.015631\tLR: 0.030000\n",
      "Train Epoch: 297 [65536/194182 (33%)]\tLoss: 0.587277\tGrad Norm: 1.026037\tLR: 0.030000\n",
      "Train Epoch: 297 [86016/194182 (44%)]\tLoss: 0.583571\tGrad Norm: 0.824708\tLR: 0.030000\n",
      "Train Epoch: 297 [106496/194182 (54%)]\tLoss: 0.590049\tGrad Norm: 1.211343\tLR: 0.030000\n",
      "Train Epoch: 297 [126976/194182 (65%)]\tLoss: 0.595498\tGrad Norm: 1.490530\tLR: 0.030000\n",
      "Train Epoch: 297 [147456/194182 (75%)]\tLoss: 0.585029\tGrad Norm: 1.090780\tLR: 0.030000\n",
      "Train Epoch: 297 [167936/194182 (85%)]\tLoss: 0.589625\tGrad Norm: 1.260950\tLR: 0.030000\n",
      "Train Epoch: 297 [188416/194182 (96%)]\tLoss: 0.587703\tGrad Norm: 1.152602\tLR: 0.030000\n",
      "Train set: Average loss: 0.5874\n",
      "Test set: Average loss: 0.2763, Average MAE: 0.3765\n",
      "Train Epoch: 298 [4096/194182 (2%)]\tLoss: 0.583166\tGrad Norm: 0.833877\tLR: 0.030000\n",
      "Train Epoch: 298 [24576/194182 (12%)]\tLoss: 0.589401\tGrad Norm: 0.862426\tLR: 0.030000\n",
      "Train Epoch: 298 [45056/194182 (23%)]\tLoss: 0.578187\tGrad Norm: 0.871485\tLR: 0.030000\n",
      "Train Epoch: 298 [65536/194182 (33%)]\tLoss: 0.588242\tGrad Norm: 0.993990\tLR: 0.030000\n",
      "Train Epoch: 298 [86016/194182 (44%)]\tLoss: 0.589580\tGrad Norm: 1.003074\tLR: 0.030000\n",
      "Train Epoch: 298 [106496/194182 (54%)]\tLoss: 0.577763\tGrad Norm: 0.792652\tLR: 0.030000\n",
      "Train Epoch: 298 [126976/194182 (65%)]\tLoss: 0.585117\tGrad Norm: 1.007305\tLR: 0.030000\n",
      "Train Epoch: 298 [147456/194182 (75%)]\tLoss: 0.590980\tGrad Norm: 1.195223\tLR: 0.030000\n",
      "Train Epoch: 298 [167936/194182 (85%)]\tLoss: 0.592882\tGrad Norm: 1.352800\tLR: 0.030000\n",
      "Train Epoch: 298 [188416/194182 (96%)]\tLoss: 0.589661\tGrad Norm: 1.195497\tLR: 0.030000\n",
      "Train set: Average loss: 0.5858\n",
      "Test set: Average loss: 0.2833, Average MAE: 0.3865\n",
      "Train Epoch: 299 [4096/194182 (2%)]\tLoss: 0.584006\tGrad Norm: 1.423320\tLR: 0.030000\n",
      "Train Epoch: 299 [24576/194182 (12%)]\tLoss: 0.582221\tGrad Norm: 0.913097\tLR: 0.030000\n",
      "Train Epoch: 299 [45056/194182 (23%)]\tLoss: 0.575880\tGrad Norm: 1.100925\tLR: 0.030000\n",
      "Train Epoch: 299 [65536/194182 (33%)]\tLoss: 0.593986\tGrad Norm: 0.946376\tLR: 0.030000\n",
      "Train Epoch: 299 [86016/194182 (44%)]\tLoss: 0.590104\tGrad Norm: 1.089503\tLR: 0.030000\n",
      "Train Epoch: 299 [106496/194182 (54%)]\tLoss: 0.577144\tGrad Norm: 0.861435\tLR: 0.030000\n",
      "Train Epoch: 299 [126976/194182 (65%)]\tLoss: 0.585625\tGrad Norm: 0.717984\tLR: 0.030000\n",
      "Train Epoch: 299 [147456/194182 (75%)]\tLoss: 0.584768\tGrad Norm: 1.136331\tLR: 0.030000\n",
      "Train Epoch: 299 [167936/194182 (85%)]\tLoss: 0.586250\tGrad Norm: 0.940383\tLR: 0.030000\n",
      "Train Epoch: 299 [188416/194182 (96%)]\tLoss: 0.595094\tGrad Norm: 1.097444\tLR: 0.030000\n",
      "Train set: Average loss: 0.5845\n",
      "Test set: Average loss: 0.2798, Average MAE: 0.3821\n",
      "Train Epoch: 300 [4096/194182 (2%)]\tLoss: 0.589991\tGrad Norm: 1.255257\tLR: 0.030000\n",
      "Train Epoch: 300 [24576/194182 (12%)]\tLoss: 0.593567\tGrad Norm: 1.338974\tLR: 0.030000\n",
      "Train Epoch: 300 [45056/194182 (23%)]\tLoss: 0.602494\tGrad Norm: 1.214469\tLR: 0.030000\n",
      "Train Epoch: 300 [65536/194182 (33%)]\tLoss: 0.577405\tGrad Norm: 0.818735\tLR: 0.030000\n",
      "Train Epoch: 300 [86016/194182 (44%)]\tLoss: 0.578026\tGrad Norm: 1.016444\tLR: 0.030000\n",
      "Train Epoch: 300 [106496/194182 (54%)]\tLoss: 0.595591\tGrad Norm: 1.561027\tLR: 0.030000\n",
      "Train Epoch: 300 [126976/194182 (65%)]\tLoss: 0.593311\tGrad Norm: 1.453965\tLR: 0.030000\n",
      "Train Epoch: 300 [147456/194182 (75%)]\tLoss: 0.583869\tGrad Norm: 1.233191\tLR: 0.030000\n",
      "Train Epoch: 300 [167936/194182 (85%)]\tLoss: 0.586076\tGrad Norm: 1.307516\tLR: 0.030000\n",
      "Train Epoch: 300 [188416/194182 (96%)]\tLoss: 0.579137\tGrad Norm: 0.867248\tLR: 0.030000\n",
      "Train set: Average loss: 0.5869\n",
      "Test set: Average loss: 0.2755, Average MAE: 0.3864\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 300: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 301 [4096/194182 (2%)]\tLoss: 0.581477\tGrad Norm: 0.760728\tLR: 0.030000\n",
      "Train Epoch: 301 [24576/194182 (12%)]\tLoss: 0.576595\tGrad Norm: 0.992807\tLR: 0.030000\n",
      "Train Epoch: 301 [45056/194182 (23%)]\tLoss: 0.581466\tGrad Norm: 0.860729\tLR: 0.030000\n",
      "Train Epoch: 301 [65536/194182 (33%)]\tLoss: 0.592121\tGrad Norm: 1.041086\tLR: 0.030000\n",
      "Train Epoch: 301 [86016/194182 (44%)]\tLoss: 0.589162\tGrad Norm: 1.293555\tLR: 0.030000\n",
      "Train Epoch: 301 [106496/194182 (54%)]\tLoss: 0.586537\tGrad Norm: 1.237679\tLR: 0.030000\n",
      "Train Epoch: 301 [126976/194182 (65%)]\tLoss: 0.585672\tGrad Norm: 1.186081\tLR: 0.030000\n",
      "Train Epoch: 301 [147456/194182 (75%)]\tLoss: 0.584120\tGrad Norm: 1.427488\tLR: 0.030000\n",
      "Train Epoch: 301 [167936/194182 (85%)]\tLoss: 0.583903\tGrad Norm: 0.955474\tLR: 0.030000\n",
      "Train Epoch: 301 [188416/194182 (96%)]\tLoss: 0.580931\tGrad Norm: 0.679519\tLR: 0.030000\n",
      "Train set: Average loss: 0.5839\n",
      "Test set: Average loss: 0.2730, Average MAE: 0.3729\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 302 [4096/194182 (2%)]\tLoss: 0.586340\tGrad Norm: 0.643042\tLR: 0.030000\n",
      "Train Epoch: 302 [24576/194182 (12%)]\tLoss: 0.582938\tGrad Norm: 0.992873\tLR: 0.030000\n",
      "Train Epoch: 302 [45056/194182 (23%)]\tLoss: 0.594826\tGrad Norm: 1.380483\tLR: 0.030000\n",
      "Train Epoch: 302 [65536/194182 (33%)]\tLoss: 0.590977\tGrad Norm: 1.373446\tLR: 0.030000\n",
      "Train Epoch: 302 [86016/194182 (44%)]\tLoss: 0.586460\tGrad Norm: 1.204061\tLR: 0.030000\n",
      "Train Epoch: 302 [106496/194182 (54%)]\tLoss: 0.572505\tGrad Norm: 0.860378\tLR: 0.030000\n",
      "Train Epoch: 302 [126976/194182 (65%)]\tLoss: 0.582860\tGrad Norm: 1.077432\tLR: 0.030000\n",
      "Train Epoch: 302 [147456/194182 (75%)]\tLoss: 0.589109\tGrad Norm: 1.130875\tLR: 0.030000\n",
      "Train Epoch: 302 [167936/194182 (85%)]\tLoss: 0.580230\tGrad Norm: 1.099476\tLR: 0.030000\n",
      "Train Epoch: 302 [188416/194182 (96%)]\tLoss: 0.575953\tGrad Norm: 0.635995\tLR: 0.030000\n",
      "Train set: Average loss: 0.5835\n",
      "Test set: Average loss: 0.2753, Average MAE: 0.3653\n",
      "Train Epoch: 303 [4096/194182 (2%)]\tLoss: 0.584236\tGrad Norm: 1.176861\tLR: 0.030000\n",
      "Train Epoch: 303 [24576/194182 (12%)]\tLoss: 0.578924\tGrad Norm: 0.914423\tLR: 0.030000\n",
      "Train Epoch: 303 [45056/194182 (23%)]\tLoss: 0.588792\tGrad Norm: 0.990406\tLR: 0.030000\n",
      "Train Epoch: 303 [65536/194182 (33%)]\tLoss: 0.579355\tGrad Norm: 1.023663\tLR: 0.030000\n",
      "Train Epoch: 303 [86016/194182 (44%)]\tLoss: 0.577779\tGrad Norm: 0.871550\tLR: 0.030000\n",
      "Train Epoch: 303 [106496/194182 (54%)]\tLoss: 0.582861\tGrad Norm: 1.059677\tLR: 0.030000\n",
      "Train Epoch: 303 [126976/194182 (65%)]\tLoss: 0.582261\tGrad Norm: 0.946015\tLR: 0.030000\n",
      "Train Epoch: 303 [147456/194182 (75%)]\tLoss: 0.582048\tGrad Norm: 1.220451\tLR: 0.030000\n",
      "Train Epoch: 303 [167936/194182 (85%)]\tLoss: 0.593219\tGrad Norm: 1.329873\tLR: 0.030000\n",
      "Train Epoch: 303 [188416/194182 (96%)]\tLoss: 0.585318\tGrad Norm: 1.405440\tLR: 0.030000\n",
      "Train set: Average loss: 0.5834\n",
      "Test set: Average loss: 0.2791, Average MAE: 0.3813\n",
      "Train Epoch: 304 [4096/194182 (2%)]\tLoss: 0.592158\tGrad Norm: 0.989941\tLR: 0.030000\n",
      "Train Epoch: 304 [24576/194182 (12%)]\tLoss: 0.584367\tGrad Norm: 0.915044\tLR: 0.030000\n",
      "Train Epoch: 304 [45056/194182 (23%)]\tLoss: 0.581266\tGrad Norm: 0.554894\tLR: 0.030000\n",
      "Train Epoch: 304 [65536/194182 (33%)]\tLoss: 0.574353\tGrad Norm: 0.736669\tLR: 0.030000\n",
      "Train Epoch: 304 [86016/194182 (44%)]\tLoss: 0.592200\tGrad Norm: 1.087404\tLR: 0.030000\n",
      "Train Epoch: 304 [106496/194182 (54%)]\tLoss: 0.582197\tGrad Norm: 1.364023\tLR: 0.030000\n",
      "Train Epoch: 304 [126976/194182 (65%)]\tLoss: 0.585542\tGrad Norm: 1.124533\tLR: 0.030000\n",
      "Train Epoch: 304 [147456/194182 (75%)]\tLoss: 0.584938\tGrad Norm: 1.655733\tLR: 0.030000\n",
      "Train Epoch: 304 [167936/194182 (85%)]\tLoss: 0.588537\tGrad Norm: 1.483718\tLR: 0.030000\n",
      "Train Epoch: 304 [188416/194182 (96%)]\tLoss: 0.591897\tGrad Norm: 1.304932\tLR: 0.030000\n",
      "Train set: Average loss: 0.5837\n",
      "Test set: Average loss: 0.2818, Average MAE: 0.3727\n",
      "Train Epoch: 305 [4096/194182 (2%)]\tLoss: 0.592159\tGrad Norm: 1.423878\tLR: 0.030000\n",
      "Train Epoch: 305 [24576/194182 (12%)]\tLoss: 0.581273\tGrad Norm: 0.897901\tLR: 0.030000\n",
      "Train Epoch: 305 [45056/194182 (23%)]\tLoss: 0.573439\tGrad Norm: 0.821457\tLR: 0.030000\n",
      "Train Epoch: 305 [65536/194182 (33%)]\tLoss: 0.581327\tGrad Norm: 0.744713\tLR: 0.030000\n",
      "Train Epoch: 305 [86016/194182 (44%)]\tLoss: 0.586660\tGrad Norm: 0.959539\tLR: 0.030000\n",
      "Train Epoch: 305 [106496/194182 (54%)]\tLoss: 0.599579\tGrad Norm: 1.244752\tLR: 0.030000\n",
      "Train Epoch: 305 [126976/194182 (65%)]\tLoss: 0.574112\tGrad Norm: 0.971812\tLR: 0.030000\n",
      "Train Epoch: 305 [147456/194182 (75%)]\tLoss: 0.581092\tGrad Norm: 0.843180\tLR: 0.030000\n",
      "Train Epoch: 305 [167936/194182 (85%)]\tLoss: 0.580461\tGrad Norm: 0.850299\tLR: 0.030000\n",
      "Train Epoch: 305 [188416/194182 (96%)]\tLoss: 0.582280\tGrad Norm: 1.006667\tLR: 0.030000\n",
      "Train set: Average loss: 0.5802\n",
      "Test set: Average loss: 0.2754, Average MAE: 0.3663\n",
      "Epoch 305: Mean reward = 0.045 +/- 0.000\n",
      "Train Epoch: 306 [4096/194182 (2%)]\tLoss: 0.576050\tGrad Norm: 0.932618\tLR: 0.030000\n",
      "Train Epoch: 306 [24576/194182 (12%)]\tLoss: 0.588503\tGrad Norm: 1.159426\tLR: 0.030000\n",
      "Train Epoch: 306 [45056/194182 (23%)]\tLoss: 0.584920\tGrad Norm: 0.944420\tLR: 0.030000\n",
      "Train Epoch: 306 [65536/194182 (33%)]\tLoss: 0.585935\tGrad Norm: 1.278133\tLR: 0.030000\n",
      "Train Epoch: 306 [86016/194182 (44%)]\tLoss: 0.588779\tGrad Norm: 1.507863\tLR: 0.030000\n",
      "Train Epoch: 306 [106496/194182 (54%)]\tLoss: 0.583394\tGrad Norm: 1.397037\tLR: 0.030000\n",
      "Train Epoch: 306 [126976/194182 (65%)]\tLoss: 0.595304\tGrad Norm: 1.331564\tLR: 0.030000\n",
      "Train Epoch: 306 [147456/194182 (75%)]\tLoss: 0.580844\tGrad Norm: 1.412134\tLR: 0.030000\n",
      "Train Epoch: 306 [167936/194182 (85%)]\tLoss: 0.575907\tGrad Norm: 0.948165\tLR: 0.030000\n",
      "Train Epoch: 306 [188416/194182 (96%)]\tLoss: 0.582385\tGrad Norm: 0.875028\tLR: 0.030000\n",
      "Train set: Average loss: 0.5822\n",
      "Test set: Average loss: 0.2723, Average MAE: 0.3719\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 307 [4096/194182 (2%)]\tLoss: 0.573511\tGrad Norm: 0.620977\tLR: 0.030000\n",
      "Train Epoch: 307 [24576/194182 (12%)]\tLoss: 0.581557\tGrad Norm: 1.099884\tLR: 0.030000\n",
      "Train Epoch: 307 [45056/194182 (23%)]\tLoss: 0.579875\tGrad Norm: 0.906200\tLR: 0.030000\n",
      "Train Epoch: 307 [65536/194182 (33%)]\tLoss: 0.574964\tGrad Norm: 1.146046\tLR: 0.030000\n",
      "Train Epoch: 307 [86016/194182 (44%)]\tLoss: 0.578280\tGrad Norm: 0.992282\tLR: 0.030000\n",
      "Train Epoch: 307 [106496/194182 (54%)]\tLoss: 0.583377\tGrad Norm: 0.941125\tLR: 0.030000\n",
      "Train Epoch: 307 [126976/194182 (65%)]\tLoss: 0.569145\tGrad Norm: 0.650648\tLR: 0.030000\n",
      "Train Epoch: 307 [147456/194182 (75%)]\tLoss: 0.578061\tGrad Norm: 0.963433\tLR: 0.030000\n",
      "Train Epoch: 307 [167936/194182 (85%)]\tLoss: 0.579108\tGrad Norm: 1.259011\tLR: 0.030000\n",
      "Train Epoch: 307 [188416/194182 (96%)]\tLoss: 0.577385\tGrad Norm: 0.785696\tLR: 0.030000\n",
      "Train set: Average loss: 0.5784\n",
      "Test set: Average loss: 0.2704, Average MAE: 0.3736\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 308 [4096/194182 (2%)]\tLoss: 0.570458\tGrad Norm: 0.536881\tLR: 0.030000\n",
      "Train Epoch: 308 [24576/194182 (12%)]\tLoss: 0.570419\tGrad Norm: 0.789687\tLR: 0.030000\n",
      "Train Epoch: 308 [45056/194182 (23%)]\tLoss: 0.579028\tGrad Norm: 1.232249\tLR: 0.030000\n",
      "Train Epoch: 308 [65536/194182 (33%)]\tLoss: 0.588850\tGrad Norm: 1.483104\tLR: 0.030000\n",
      "Train Epoch: 308 [86016/194182 (44%)]\tLoss: 0.585659\tGrad Norm: 1.630391\tLR: 0.030000\n",
      "Train Epoch: 308 [106496/194182 (54%)]\tLoss: 0.584551\tGrad Norm: 1.123787\tLR: 0.030000\n",
      "Train Epoch: 308 [126976/194182 (65%)]\tLoss: 0.577213\tGrad Norm: 1.210968\tLR: 0.030000\n",
      "Train Epoch: 308 [147456/194182 (75%)]\tLoss: 0.566688\tGrad Norm: 0.858851\tLR: 0.030000\n",
      "Train Epoch: 308 [167936/194182 (85%)]\tLoss: 0.571196\tGrad Norm: 1.227530\tLR: 0.030000\n",
      "Train Epoch: 308 [188416/194182 (96%)]\tLoss: 0.576149\tGrad Norm: 1.073946\tLR: 0.030000\n",
      "Train set: Average loss: 0.5803\n",
      "Test set: Average loss: 0.2731, Average MAE: 0.3623\n",
      "Train Epoch: 309 [4096/194182 (2%)]\tLoss: 0.574111\tGrad Norm: 0.781146\tLR: 0.030000\n",
      "Train Epoch: 309 [24576/194182 (12%)]\tLoss: 0.580327\tGrad Norm: 1.156224\tLR: 0.030000\n",
      "Train Epoch: 309 [45056/194182 (23%)]\tLoss: 0.570118\tGrad Norm: 0.870828\tLR: 0.030000\n",
      "Train Epoch: 309 [65536/194182 (33%)]\tLoss: 0.572301\tGrad Norm: 0.970705\tLR: 0.030000\n",
      "Train Epoch: 309 [86016/194182 (44%)]\tLoss: 0.580310\tGrad Norm: 1.069117\tLR: 0.030000\n",
      "Train Epoch: 309 [106496/194182 (54%)]\tLoss: 0.582143\tGrad Norm: 1.069241\tLR: 0.030000\n",
      "Train Epoch: 309 [126976/194182 (65%)]\tLoss: 0.573731\tGrad Norm: 0.980992\tLR: 0.030000\n",
      "Train Epoch: 309 [147456/194182 (75%)]\tLoss: 0.584698\tGrad Norm: 1.206175\tLR: 0.030000\n",
      "Train Epoch: 309 [167936/194182 (85%)]\tLoss: 0.587307\tGrad Norm: 1.082296\tLR: 0.030000\n",
      "Train Epoch: 309 [188416/194182 (96%)]\tLoss: 0.575654\tGrad Norm: 0.752448\tLR: 0.030000\n",
      "Train set: Average loss: 0.5779\n",
      "Test set: Average loss: 0.2767, Average MAE: 0.3857\n",
      "Train Epoch: 310 [4096/194182 (2%)]\tLoss: 0.577743\tGrad Norm: 1.132232\tLR: 0.030000\n",
      "Train Epoch: 310 [24576/194182 (12%)]\tLoss: 0.589484\tGrad Norm: 1.163604\tLR: 0.030000\n",
      "Train Epoch: 310 [45056/194182 (23%)]\tLoss: 0.579083\tGrad Norm: 1.424153\tLR: 0.030000\n",
      "Train Epoch: 310 [65536/194182 (33%)]\tLoss: 0.596471\tGrad Norm: 1.250188\tLR: 0.030000\n",
      "Train Epoch: 310 [86016/194182 (44%)]\tLoss: 0.577597\tGrad Norm: 1.340903\tLR: 0.030000\n",
      "Train Epoch: 310 [106496/194182 (54%)]\tLoss: 0.576490\tGrad Norm: 1.320937\tLR: 0.030000\n",
      "Train Epoch: 310 [126976/194182 (65%)]\tLoss: 0.589733\tGrad Norm: 1.359500\tLR: 0.030000\n",
      "Train Epoch: 310 [147456/194182 (75%)]\tLoss: 0.581943\tGrad Norm: 1.194419\tLR: 0.030000\n",
      "Train Epoch: 310 [167936/194182 (85%)]\tLoss: 0.579408\tGrad Norm: 0.877896\tLR: 0.030000\n",
      "Train Epoch: 310 [188416/194182 (96%)]\tLoss: 0.573466\tGrad Norm: 0.769395\tLR: 0.030000\n",
      "Train set: Average loss: 0.5802\n",
      "Test set: Average loss: 0.2781, Average MAE: 0.3804\n",
      "Epoch 310: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 311 [4096/194182 (2%)]\tLoss: 0.577380\tGrad Norm: 1.311093\tLR: 0.030000\n",
      "Train Epoch: 311 [24576/194182 (12%)]\tLoss: 0.575574\tGrad Norm: 0.871291\tLR: 0.030000\n",
      "Train Epoch: 311 [45056/194182 (23%)]\tLoss: 0.582530\tGrad Norm: 1.020883\tLR: 0.030000\n",
      "Train Epoch: 311 [65536/194182 (33%)]\tLoss: 0.580939\tGrad Norm: 0.914875\tLR: 0.030000\n",
      "Train Epoch: 311 [86016/194182 (44%)]\tLoss: 0.573639\tGrad Norm: 0.644167\tLR: 0.030000\n",
      "Train Epoch: 311 [106496/194182 (54%)]\tLoss: 0.576361\tGrad Norm: 1.395040\tLR: 0.030000\n",
      "Train Epoch: 311 [126976/194182 (65%)]\tLoss: 0.577730\tGrad Norm: 1.032869\tLR: 0.030000\n",
      "Train Epoch: 311 [147456/194182 (75%)]\tLoss: 0.571045\tGrad Norm: 0.941675\tLR: 0.030000\n",
      "Train Epoch: 311 [167936/194182 (85%)]\tLoss: 0.592049\tGrad Norm: 1.606353\tLR: 0.030000\n",
      "Train Epoch: 311 [188416/194182 (96%)]\tLoss: 0.578187\tGrad Norm: 1.330270\tLR: 0.030000\n",
      "Train set: Average loss: 0.5773\n",
      "Test set: Average loss: 0.2794, Average MAE: 0.3689\n",
      "Train Epoch: 312 [4096/194182 (2%)]\tLoss: 0.591042\tGrad Norm: 1.841632\tLR: 0.030000\n",
      "Train Epoch: 312 [24576/194182 (12%)]\tLoss: 0.574643\tGrad Norm: 1.094683\tLR: 0.030000\n",
      "Train Epoch: 312 [45056/194182 (23%)]\tLoss: 0.565652\tGrad Norm: 1.054730\tLR: 0.030000\n",
      "Train Epoch: 312 [65536/194182 (33%)]\tLoss: 0.577845\tGrad Norm: 1.089989\tLR: 0.030000\n",
      "Train Epoch: 312 [86016/194182 (44%)]\tLoss: 0.572067\tGrad Norm: 1.053305\tLR: 0.030000\n",
      "Train Epoch: 312 [106496/194182 (54%)]\tLoss: 0.586016\tGrad Norm: 1.139270\tLR: 0.030000\n",
      "Train Epoch: 312 [126976/194182 (65%)]\tLoss: 0.581886\tGrad Norm: 1.218038\tLR: 0.030000\n",
      "Train Epoch: 312 [147456/194182 (75%)]\tLoss: 0.572805\tGrad Norm: 0.808018\tLR: 0.030000\n",
      "Train Epoch: 312 [167936/194182 (85%)]\tLoss: 0.572211\tGrad Norm: 0.804457\tLR: 0.030000\n",
      "Train Epoch: 312 [188416/194182 (96%)]\tLoss: 0.579591\tGrad Norm: 1.159732\tLR: 0.030000\n",
      "Train set: Average loss: 0.5776\n",
      "Test set: Average loss: 0.2772, Average MAE: 0.3894\n",
      "Train Epoch: 313 [4096/194182 (2%)]\tLoss: 0.576612\tGrad Norm: 1.086630\tLR: 0.030000\n",
      "Train Epoch: 313 [24576/194182 (12%)]\tLoss: 0.574725\tGrad Norm: 1.017144\tLR: 0.030000\n",
      "Train Epoch: 313 [45056/194182 (23%)]\tLoss: 0.569378\tGrad Norm: 1.047420\tLR: 0.030000\n",
      "Train Epoch: 313 [65536/194182 (33%)]\tLoss: 0.570881\tGrad Norm: 1.053946\tLR: 0.030000\n",
      "Train Epoch: 313 [86016/194182 (44%)]\tLoss: 0.574412\tGrad Norm: 1.225010\tLR: 0.030000\n",
      "Train Epoch: 313 [106496/194182 (54%)]\tLoss: 0.575234\tGrad Norm: 1.003674\tLR: 0.030000\n",
      "Train Epoch: 313 [126976/194182 (65%)]\tLoss: 0.579808\tGrad Norm: 1.327460\tLR: 0.030000\n",
      "Train Epoch: 313 [147456/194182 (75%)]\tLoss: 0.579234\tGrad Norm: 1.309235\tLR: 0.030000\n",
      "Train Epoch: 313 [167936/194182 (85%)]\tLoss: 0.577889\tGrad Norm: 0.933515\tLR: 0.030000\n",
      "Train Epoch: 313 [188416/194182 (96%)]\tLoss: 0.576811\tGrad Norm: 0.838496\tLR: 0.030000\n",
      "Train set: Average loss: 0.5758\n",
      "Test set: Average loss: 0.2727, Average MAE: 0.3692\n",
      "Train Epoch: 314 [4096/194182 (2%)]\tLoss: 0.570219\tGrad Norm: 0.947386\tLR: 0.030000\n",
      "Train Epoch: 314 [24576/194182 (12%)]\tLoss: 0.575600\tGrad Norm: 0.806243\tLR: 0.030000\n",
      "Train Epoch: 314 [45056/194182 (23%)]\tLoss: 0.578304\tGrad Norm: 0.895751\tLR: 0.030000\n",
      "Train Epoch: 314 [65536/194182 (33%)]\tLoss: 0.566079\tGrad Norm: 0.556214\tLR: 0.030000\n",
      "Train Epoch: 314 [86016/194182 (44%)]\tLoss: 0.565554\tGrad Norm: 0.613269\tLR: 0.030000\n",
      "Train Epoch: 314 [106496/194182 (54%)]\tLoss: 0.568904\tGrad Norm: 1.111156\tLR: 0.030000\n",
      "Train Epoch: 314 [126976/194182 (65%)]\tLoss: 0.588498\tGrad Norm: 1.350882\tLR: 0.030000\n",
      "Train Epoch: 314 [147456/194182 (75%)]\tLoss: 0.586440\tGrad Norm: 1.468218\tLR: 0.030000\n",
      "Train Epoch: 314 [167936/194182 (85%)]\tLoss: 0.566120\tGrad Norm: 1.085181\tLR: 0.030000\n",
      "Train Epoch: 314 [188416/194182 (96%)]\tLoss: 0.577656\tGrad Norm: 0.734692\tLR: 0.030000\n",
      "Train set: Average loss: 0.5741\n",
      "Test set: Average loss: 0.2798, Average MAE: 0.3931\n",
      "Train Epoch: 315 [4096/194182 (2%)]\tLoss: 0.581763\tGrad Norm: 1.372233\tLR: 0.030000\n",
      "Train Epoch: 315 [24576/194182 (12%)]\tLoss: 0.570397\tGrad Norm: 1.132740\tLR: 0.030000\n",
      "Train Epoch: 315 [45056/194182 (23%)]\tLoss: 0.575075\tGrad Norm: 0.723491\tLR: 0.030000\n",
      "Train Epoch: 315 [65536/194182 (33%)]\tLoss: 0.572304\tGrad Norm: 0.897820\tLR: 0.030000\n",
      "Train Epoch: 315 [86016/194182 (44%)]\tLoss: 0.570374\tGrad Norm: 0.850957\tLR: 0.030000\n",
      "Train Epoch: 315 [106496/194182 (54%)]\tLoss: 0.572341\tGrad Norm: 1.172197\tLR: 0.030000\n",
      "Train Epoch: 315 [126976/194182 (65%)]\tLoss: 0.564902\tGrad Norm: 0.791932\tLR: 0.030000\n",
      "Train Epoch: 315 [147456/194182 (75%)]\tLoss: 0.578605\tGrad Norm: 1.229796\tLR: 0.030000\n",
      "Train Epoch: 315 [167936/194182 (85%)]\tLoss: 0.575410\tGrad Norm: 1.534887\tLR: 0.030000\n",
      "Train Epoch: 315 [188416/194182 (96%)]\tLoss: 0.579151\tGrad Norm: 1.454427\tLR: 0.030000\n",
      "Train set: Average loss: 0.5746\n",
      "Test set: Average loss: 0.2792, Average MAE: 0.3839\n",
      "Epoch 315: Mean reward = 0.036 +/- 0.018\n",
      "Train Epoch: 316 [4096/194182 (2%)]\tLoss: 0.573141\tGrad Norm: 1.256064\tLR: 0.030000\n",
      "Train Epoch: 316 [24576/194182 (12%)]\tLoss: 0.578571\tGrad Norm: 1.436122\tLR: 0.030000\n",
      "Train Epoch: 316 [45056/194182 (23%)]\tLoss: 0.577229\tGrad Norm: 1.064674\tLR: 0.030000\n",
      "Train Epoch: 316 [65536/194182 (33%)]\tLoss: 0.583178\tGrad Norm: 1.016225\tLR: 0.030000\n",
      "Train Epoch: 316 [86016/194182 (44%)]\tLoss: 0.576961\tGrad Norm: 1.288341\tLR: 0.030000\n",
      "Train Epoch: 316 [106496/194182 (54%)]\tLoss: 0.574372\tGrad Norm: 0.969410\tLR: 0.030000\n",
      "Train Epoch: 316 [126976/194182 (65%)]\tLoss: 0.578967\tGrad Norm: 0.978909\tLR: 0.030000\n",
      "Train Epoch: 316 [147456/194182 (75%)]\tLoss: 0.581873\tGrad Norm: 1.162754\tLR: 0.030000\n",
      "Train Epoch: 316 [167936/194182 (85%)]\tLoss: 0.562854\tGrad Norm: 0.699349\tLR: 0.030000\n",
      "Train Epoch: 316 [188416/194182 (96%)]\tLoss: 0.573557\tGrad Norm: 0.770695\tLR: 0.030000\n",
      "Train set: Average loss: 0.5731\n",
      "Test set: Average loss: 0.2700, Average MAE: 0.3702\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 317 [4096/194182 (2%)]\tLoss: 0.566444\tGrad Norm: 0.946138\tLR: 0.030000\n",
      "Train Epoch: 317 [24576/194182 (12%)]\tLoss: 0.569894\tGrad Norm: 1.081884\tLR: 0.030000\n",
      "Train Epoch: 317 [45056/194182 (23%)]\tLoss: 0.567393\tGrad Norm: 1.223421\tLR: 0.030000\n",
      "Train Epoch: 317 [65536/194182 (33%)]\tLoss: 0.575857\tGrad Norm: 1.376337\tLR: 0.030000\n",
      "Train Epoch: 317 [86016/194182 (44%)]\tLoss: 0.583587\tGrad Norm: 1.465456\tLR: 0.030000\n",
      "Train Epoch: 317 [106496/194182 (54%)]\tLoss: 0.564935\tGrad Norm: 1.088045\tLR: 0.030000\n",
      "Train Epoch: 317 [126976/194182 (65%)]\tLoss: 0.570580\tGrad Norm: 0.924269\tLR: 0.030000\n",
      "Train Epoch: 317 [147456/194182 (75%)]\tLoss: 0.577226\tGrad Norm: 1.584577\tLR: 0.030000\n",
      "Train Epoch: 317 [167936/194182 (85%)]\tLoss: 0.583414\tGrad Norm: 1.601271\tLR: 0.030000\n",
      "Train Epoch: 317 [188416/194182 (96%)]\tLoss: 0.587376\tGrad Norm: 1.637576\tLR: 0.030000\n",
      "Train set: Average loss: 0.5758\n",
      "Test set: Average loss: 0.2777, Average MAE: 0.3858\n",
      "Train Epoch: 318 [4096/194182 (2%)]\tLoss: 0.575313\tGrad Norm: 1.205012\tLR: 0.030000\n",
      "Train Epoch: 318 [24576/194182 (12%)]\tLoss: 0.584519\tGrad Norm: 1.296824\tLR: 0.030000\n",
      "Train Epoch: 318 [45056/194182 (23%)]\tLoss: 0.574259\tGrad Norm: 1.033791\tLR: 0.030000\n",
      "Train Epoch: 318 [65536/194182 (33%)]\tLoss: 0.577190\tGrad Norm: 1.060076\tLR: 0.030000\n",
      "Train Epoch: 318 [86016/194182 (44%)]\tLoss: 0.568616\tGrad Norm: 0.679158\tLR: 0.030000\n",
      "Train Epoch: 318 [106496/194182 (54%)]\tLoss: 0.565152\tGrad Norm: 0.931658\tLR: 0.030000\n",
      "Train Epoch: 318 [126976/194182 (65%)]\tLoss: 0.579935\tGrad Norm: 1.028482\tLR: 0.030000\n",
      "Train Epoch: 318 [147456/194182 (75%)]\tLoss: 0.568494\tGrad Norm: 1.105294\tLR: 0.030000\n",
      "Train Epoch: 318 [167936/194182 (85%)]\tLoss: 0.573862\tGrad Norm: 0.916747\tLR: 0.030000\n",
      "Train Epoch: 318 [188416/194182 (96%)]\tLoss: 0.564860\tGrad Norm: 1.096223\tLR: 0.030000\n",
      "Train set: Average loss: 0.5722\n",
      "Test set: Average loss: 0.2716, Average MAE: 0.3644\n",
      "Train Epoch: 319 [4096/194182 (2%)]\tLoss: 0.568245\tGrad Norm: 1.062789\tLR: 0.030000\n",
      "Train Epoch: 319 [24576/194182 (12%)]\tLoss: 0.570583\tGrad Norm: 1.410300\tLR: 0.030000\n",
      "Train Epoch: 319 [45056/194182 (23%)]\tLoss: 0.571747\tGrad Norm: 1.039674\tLR: 0.030000\n",
      "Train Epoch: 319 [65536/194182 (33%)]\tLoss: 0.572593\tGrad Norm: 1.072311\tLR: 0.030000\n",
      "Train Epoch: 319 [86016/194182 (44%)]\tLoss: 0.571547\tGrad Norm: 1.122517\tLR: 0.030000\n",
      "Train Epoch: 319 [106496/194182 (54%)]\tLoss: 0.569749\tGrad Norm: 1.280739\tLR: 0.030000\n",
      "Train Epoch: 319 [126976/194182 (65%)]\tLoss: 0.576744\tGrad Norm: 1.377491\tLR: 0.030000\n",
      "Train Epoch: 319 [147456/194182 (75%)]\tLoss: 0.573484\tGrad Norm: 1.350384\tLR: 0.030000\n",
      "Train Epoch: 319 [167936/194182 (85%)]\tLoss: 0.572756\tGrad Norm: 1.026515\tLR: 0.030000\n",
      "Train Epoch: 319 [188416/194182 (96%)]\tLoss: 0.566282\tGrad Norm: 0.662778\tLR: 0.030000\n",
      "Train set: Average loss: 0.5721\n",
      "Test set: Average loss: 0.2689, Average MAE: 0.3719\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 320 [4096/194182 (2%)]\tLoss: 0.563159\tGrad Norm: 0.617933\tLR: 0.030000\n",
      "Train Epoch: 320 [24576/194182 (12%)]\tLoss: 0.563979\tGrad Norm: 0.740378\tLR: 0.030000\n",
      "Train Epoch: 320 [45056/194182 (23%)]\tLoss: 0.568694\tGrad Norm: 0.867897\tLR: 0.030000\n",
      "Train Epoch: 320 [65536/194182 (33%)]\tLoss: 0.568135\tGrad Norm: 1.290475\tLR: 0.030000\n",
      "Train Epoch: 320 [86016/194182 (44%)]\tLoss: 0.571391\tGrad Norm: 1.063777\tLR: 0.030000\n",
      "Train Epoch: 320 [106496/194182 (54%)]\tLoss: 0.569157\tGrad Norm: 1.269733\tLR: 0.030000\n",
      "Train Epoch: 320 [126976/194182 (65%)]\tLoss: 0.574522\tGrad Norm: 1.173878\tLR: 0.030000\n",
      "Train Epoch: 320 [147456/194182 (75%)]\tLoss: 0.566182\tGrad Norm: 0.862558\tLR: 0.030000\n",
      "Train Epoch: 320 [167936/194182 (85%)]\tLoss: 0.556050\tGrad Norm: 0.778469\tLR: 0.030000\n",
      "Train Epoch: 320 [188416/194182 (96%)]\tLoss: 0.574803\tGrad Norm: 0.901699\tLR: 0.030000\n",
      "Train set: Average loss: 0.5691\n",
      "Test set: Average loss: 0.2744, Average MAE: 0.3681\n",
      "Epoch 320: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 321 [4096/194182 (2%)]\tLoss: 0.565818\tGrad Norm: 1.202174\tLR: 0.030000\n",
      "Train Epoch: 321 [24576/194182 (12%)]\tLoss: 0.567525\tGrad Norm: 1.508189\tLR: 0.030000\n",
      "Train Epoch: 321 [45056/194182 (23%)]\tLoss: 0.578698\tGrad Norm: 1.343503\tLR: 0.030000\n",
      "Train Epoch: 321 [65536/194182 (33%)]\tLoss: 0.570035\tGrad Norm: 1.257381\tLR: 0.030000\n",
      "Train Epoch: 321 [86016/194182 (44%)]\tLoss: 0.582161\tGrad Norm: 1.399668\tLR: 0.030000\n",
      "Train Epoch: 321 [106496/194182 (54%)]\tLoss: 0.569952\tGrad Norm: 1.143094\tLR: 0.030000\n",
      "Train Epoch: 321 [126976/194182 (65%)]\tLoss: 0.567122\tGrad Norm: 0.813364\tLR: 0.030000\n",
      "Train Epoch: 321 [147456/194182 (75%)]\tLoss: 0.574266\tGrad Norm: 0.945833\tLR: 0.030000\n",
      "Train Epoch: 321 [167936/194182 (85%)]\tLoss: 0.572186\tGrad Norm: 1.248390\tLR: 0.030000\n",
      "Train Epoch: 321 [188416/194182 (96%)]\tLoss: 0.568318\tGrad Norm: 1.176672\tLR: 0.030000\n",
      "Train set: Average loss: 0.5723\n",
      "Test set: Average loss: 0.2820, Average MAE: 0.3907\n",
      "Train Epoch: 322 [4096/194182 (2%)]\tLoss: 0.572424\tGrad Norm: 1.481865\tLR: 0.030000\n",
      "Train Epoch: 322 [24576/194182 (12%)]\tLoss: 0.569137\tGrad Norm: 1.242895\tLR: 0.030000\n",
      "Train Epoch: 322 [45056/194182 (23%)]\tLoss: 0.567985\tGrad Norm: 0.783271\tLR: 0.030000\n",
      "Train Epoch: 322 [65536/194182 (33%)]\tLoss: 0.576311\tGrad Norm: 0.929732\tLR: 0.030000\n",
      "Train Epoch: 322 [86016/194182 (44%)]\tLoss: 0.566090\tGrad Norm: 1.189312\tLR: 0.030000\n",
      "Train Epoch: 322 [106496/194182 (54%)]\tLoss: 0.576035\tGrad Norm: 1.189100\tLR: 0.030000\n",
      "Train Epoch: 322 [126976/194182 (65%)]\tLoss: 0.586620\tGrad Norm: 1.175937\tLR: 0.030000\n",
      "Train Epoch: 322 [147456/194182 (75%)]\tLoss: 0.571741\tGrad Norm: 1.466349\tLR: 0.030000\n",
      "Train Epoch: 322 [167936/194182 (85%)]\tLoss: 0.571648\tGrad Norm: 1.179481\tLR: 0.030000\n",
      "Train Epoch: 322 [188416/194182 (96%)]\tLoss: 0.569218\tGrad Norm: 0.605193\tLR: 0.030000\n",
      "Train set: Average loss: 0.5705\n",
      "Test set: Average loss: 0.2688, Average MAE: 0.3599\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 323 [4096/194182 (2%)]\tLoss: 0.565953\tGrad Norm: 0.672454\tLR: 0.030000\n",
      "Train Epoch: 323 [24576/194182 (12%)]\tLoss: 0.559160\tGrad Norm: 0.731724\tLR: 0.030000\n",
      "Train Epoch: 323 [45056/194182 (23%)]\tLoss: 0.577678\tGrad Norm: 1.089519\tLR: 0.030000\n",
      "Train Epoch: 323 [65536/194182 (33%)]\tLoss: 0.568508\tGrad Norm: 1.196670\tLR: 0.030000\n",
      "Train Epoch: 323 [86016/194182 (44%)]\tLoss: 0.577939\tGrad Norm: 1.390549\tLR: 0.030000\n",
      "Train Epoch: 323 [106496/194182 (54%)]\tLoss: 0.570902\tGrad Norm: 1.218122\tLR: 0.030000\n",
      "Train Epoch: 323 [126976/194182 (65%)]\tLoss: 0.567648\tGrad Norm: 0.923017\tLR: 0.030000\n",
      "Train Epoch: 323 [147456/194182 (75%)]\tLoss: 0.563321\tGrad Norm: 1.279409\tLR: 0.030000\n",
      "Train Epoch: 323 [167936/194182 (85%)]\tLoss: 0.572099\tGrad Norm: 1.394127\tLR: 0.030000\n",
      "Train Epoch: 323 [188416/194182 (96%)]\tLoss: 0.579932\tGrad Norm: 1.598721\tLR: 0.030000\n",
      "Train set: Average loss: 0.5711\n",
      "Test set: Average loss: 0.2778, Average MAE: 0.3660\n",
      "Train Epoch: 324 [4096/194182 (2%)]\tLoss: 0.575003\tGrad Norm: 1.542591\tLR: 0.030000\n",
      "Train Epoch: 324 [24576/194182 (12%)]\tLoss: 0.571466\tGrad Norm: 1.127971\tLR: 0.030000\n",
      "Train Epoch: 324 [45056/194182 (23%)]\tLoss: 0.570379\tGrad Norm: 1.253770\tLR: 0.030000\n",
      "Train Epoch: 324 [65536/194182 (33%)]\tLoss: 0.572487\tGrad Norm: 1.094612\tLR: 0.030000\n",
      "Train Epoch: 324 [86016/194182 (44%)]\tLoss: 0.569069\tGrad Norm: 1.087347\tLR: 0.030000\n",
      "Train Epoch: 324 [106496/194182 (54%)]\tLoss: 0.557707\tGrad Norm: 0.895359\tLR: 0.030000\n",
      "Train Epoch: 324 [126976/194182 (65%)]\tLoss: 0.565897\tGrad Norm: 0.965722\tLR: 0.030000\n",
      "Train Epoch: 324 [147456/194182 (75%)]\tLoss: 0.567781\tGrad Norm: 0.999901\tLR: 0.030000\n",
      "Train Epoch: 324 [167936/194182 (85%)]\tLoss: 0.563224\tGrad Norm: 1.034166\tLR: 0.030000\n",
      "Train Epoch: 324 [188416/194182 (96%)]\tLoss: 0.572220\tGrad Norm: 0.874077\tLR: 0.030000\n",
      "Train set: Average loss: 0.5681\n",
      "Test set: Average loss: 0.2690, Average MAE: 0.3633\n",
      "Train Epoch: 325 [4096/194182 (2%)]\tLoss: 0.564472\tGrad Norm: 0.790398\tLR: 0.030000\n",
      "Train Epoch: 325 [24576/194182 (12%)]\tLoss: 0.560053\tGrad Norm: 0.636074\tLR: 0.030000\n",
      "Train Epoch: 325 [45056/194182 (23%)]\tLoss: 0.561667\tGrad Norm: 0.863414\tLR: 0.030000\n",
      "Train Epoch: 325 [65536/194182 (33%)]\tLoss: 0.569506\tGrad Norm: 0.999954\tLR: 0.030000\n",
      "Train Epoch: 325 [86016/194182 (44%)]\tLoss: 0.565017\tGrad Norm: 1.186063\tLR: 0.030000\n",
      "Train Epoch: 325 [106496/194182 (54%)]\tLoss: 0.560597\tGrad Norm: 1.316404\tLR: 0.030000\n",
      "Train Epoch: 325 [126976/194182 (65%)]\tLoss: 0.585208\tGrad Norm: 1.726693\tLR: 0.030000\n",
      "Train Epoch: 325 [147456/194182 (75%)]\tLoss: 0.575412\tGrad Norm: 0.740999\tLR: 0.030000\n",
      "Train Epoch: 325 [167936/194182 (85%)]\tLoss: 0.573101\tGrad Norm: 0.811994\tLR: 0.030000\n",
      "Train Epoch: 325 [188416/194182 (96%)]\tLoss: 0.573048\tGrad Norm: 0.991655\tLR: 0.030000\n",
      "Train set: Average loss: 0.5669\n",
      "Test set: Average loss: 0.2748, Average MAE: 0.3630\n",
      "Epoch 325: Mean reward = 0.036 +/- 0.018\n",
      "Train Epoch: 326 [4096/194182 (2%)]\tLoss: 0.566108\tGrad Norm: 1.252582\tLR: 0.030000\n",
      "Train Epoch: 326 [24576/194182 (12%)]\tLoss: 0.577175\tGrad Norm: 1.501795\tLR: 0.030000\n",
      "Train Epoch: 326 [45056/194182 (23%)]\tLoss: 0.577982\tGrad Norm: 1.322451\tLR: 0.030000\n",
      "Train Epoch: 326 [65536/194182 (33%)]\tLoss: 0.578357\tGrad Norm: 1.285059\tLR: 0.030000\n",
      "Train Epoch: 326 [86016/194182 (44%)]\tLoss: 0.562724\tGrad Norm: 1.317895\tLR: 0.030000\n",
      "Train Epoch: 326 [106496/194182 (54%)]\tLoss: 0.566723\tGrad Norm: 0.937961\tLR: 0.030000\n",
      "Train Epoch: 326 [126976/194182 (65%)]\tLoss: 0.573573\tGrad Norm: 1.153646\tLR: 0.030000\n",
      "Train Epoch: 326 [147456/194182 (75%)]\tLoss: 0.570178\tGrad Norm: 1.311182\tLR: 0.030000\n",
      "Train Epoch: 326 [167936/194182 (85%)]\tLoss: 0.565913\tGrad Norm: 1.066272\tLR: 0.030000\n",
      "Train Epoch: 326 [188416/194182 (96%)]\tLoss: 0.566603\tGrad Norm: 0.899240\tLR: 0.030000\n",
      "Train set: Average loss: 0.5686\n",
      "Test set: Average loss: 0.2726, Average MAE: 0.3719\n",
      "Train Epoch: 327 [4096/194182 (2%)]\tLoss: 0.568294\tGrad Norm: 0.954316\tLR: 0.030000\n",
      "Train Epoch: 327 [24576/194182 (12%)]\tLoss: 0.561831\tGrad Norm: 0.719662\tLR: 0.030000\n",
      "Train Epoch: 327 [45056/194182 (23%)]\tLoss: 0.563740\tGrad Norm: 0.906176\tLR: 0.030000\n",
      "Train Epoch: 327 [65536/194182 (33%)]\tLoss: 0.563692\tGrad Norm: 1.134593\tLR: 0.030000\n",
      "Train Epoch: 327 [86016/194182 (44%)]\tLoss: 0.560448\tGrad Norm: 0.571758\tLR: 0.030000\n",
      "Train Epoch: 327 [106496/194182 (54%)]\tLoss: 0.557290\tGrad Norm: 0.973516\tLR: 0.030000\n",
      "Train Epoch: 327 [126976/194182 (65%)]\tLoss: 0.567837\tGrad Norm: 1.213484\tLR: 0.030000\n",
      "Train Epoch: 327 [147456/194182 (75%)]\tLoss: 0.560782\tGrad Norm: 1.083866\tLR: 0.030000\n",
      "Train Epoch: 327 [167936/194182 (85%)]\tLoss: 0.566675\tGrad Norm: 1.112351\tLR: 0.030000\n",
      "Train Epoch: 327 [188416/194182 (96%)]\tLoss: 0.573542\tGrad Norm: 1.319041\tLR: 0.030000\n",
      "Train set: Average loss: 0.5652\n",
      "Test set: Average loss: 0.2793, Average MAE: 0.3634\n",
      "Train Epoch: 328 [4096/194182 (2%)]\tLoss: 0.572130\tGrad Norm: 1.634700\tLR: 0.030000\n",
      "Train Epoch: 328 [24576/194182 (12%)]\tLoss: 0.567816\tGrad Norm: 1.464841\tLR: 0.030000\n",
      "Train Epoch: 328 [45056/194182 (23%)]\tLoss: 0.566425\tGrad Norm: 1.409419\tLR: 0.030000\n",
      "Train Epoch: 328 [65536/194182 (33%)]\tLoss: 0.567551\tGrad Norm: 1.292499\tLR: 0.030000\n",
      "Train Epoch: 328 [86016/194182 (44%)]\tLoss: 0.566897\tGrad Norm: 1.131943\tLR: 0.030000\n",
      "Train Epoch: 328 [106496/194182 (54%)]\tLoss: 0.559128\tGrad Norm: 0.739721\tLR: 0.030000\n",
      "Train Epoch: 328 [126976/194182 (65%)]\tLoss: 0.559375\tGrad Norm: 0.839995\tLR: 0.030000\n",
      "Train Epoch: 328 [147456/194182 (75%)]\tLoss: 0.558126\tGrad Norm: 0.593524\tLR: 0.030000\n",
      "Train Epoch: 328 [167936/194182 (85%)]\tLoss: 0.568028\tGrad Norm: 1.002239\tLR: 0.030000\n",
      "Train Epoch: 328 [188416/194182 (96%)]\tLoss: 0.574396\tGrad Norm: 1.355192\tLR: 0.030000\n",
      "Train set: Average loss: 0.5662\n",
      "Test set: Average loss: 0.2738, Average MAE: 0.3576\n",
      "Train Epoch: 329 [4096/194182 (2%)]\tLoss: 0.565705\tGrad Norm: 1.244939\tLR: 0.030000\n",
      "Train Epoch: 329 [24576/194182 (12%)]\tLoss: 0.557839\tGrad Norm: 1.160423\tLR: 0.030000\n",
      "Train Epoch: 329 [45056/194182 (23%)]\tLoss: 0.573446\tGrad Norm: 1.441214\tLR: 0.030000\n",
      "Train Epoch: 329 [65536/194182 (33%)]\tLoss: 0.570751\tGrad Norm: 0.886458\tLR: 0.030000\n",
      "Train Epoch: 329 [86016/194182 (44%)]\tLoss: 0.556136\tGrad Norm: 0.642984\tLR: 0.030000\n",
      "Train Epoch: 329 [106496/194182 (54%)]\tLoss: 0.560597\tGrad Norm: 0.692207\tLR: 0.030000\n",
      "Train Epoch: 329 [126976/194182 (65%)]\tLoss: 0.558545\tGrad Norm: 0.389597\tLR: 0.030000\n",
      "Train Epoch: 329 [147456/194182 (75%)]\tLoss: 0.568573\tGrad Norm: 1.025980\tLR: 0.030000\n",
      "Train Epoch: 329 [167936/194182 (85%)]\tLoss: 0.570920\tGrad Norm: 1.570079\tLR: 0.030000\n",
      "Train Epoch: 329 [188416/194182 (96%)]\tLoss: 0.574276\tGrad Norm: 1.637265\tLR: 0.030000\n",
      "Train set: Average loss: 0.5649\n",
      "Test set: Average loss: 0.2822, Average MAE: 0.3912\n",
      "Train Epoch: 330 [4096/194182 (2%)]\tLoss: 0.565887\tGrad Norm: 1.573377\tLR: 0.030000\n",
      "Train Epoch: 330 [24576/194182 (12%)]\tLoss: 0.573599\tGrad Norm: 1.171817\tLR: 0.030000\n",
      "Train Epoch: 330 [45056/194182 (23%)]\tLoss: 0.563406\tGrad Norm: 1.195810\tLR: 0.030000\n",
      "Train Epoch: 330 [65536/194182 (33%)]\tLoss: 0.571806\tGrad Norm: 1.127126\tLR: 0.030000\n",
      "Train Epoch: 330 [86016/194182 (44%)]\tLoss: 0.559227\tGrad Norm: 1.276113\tLR: 0.030000\n",
      "Train Epoch: 330 [106496/194182 (54%)]\tLoss: 0.561355\tGrad Norm: 1.237753\tLR: 0.030000\n",
      "Train Epoch: 330 [126976/194182 (65%)]\tLoss: 0.575517\tGrad Norm: 1.059698\tLR: 0.030000\n",
      "Train Epoch: 330 [147456/194182 (75%)]\tLoss: 0.577153\tGrad Norm: 1.399206\tLR: 0.030000\n",
      "Train Epoch: 330 [167936/194182 (85%)]\tLoss: 0.560842\tGrad Norm: 0.856521\tLR: 0.030000\n",
      "Train Epoch: 330 [188416/194182 (96%)]\tLoss: 0.555738\tGrad Norm: 0.680554\tLR: 0.030000\n",
      "Train set: Average loss: 0.5652\n",
      "Test set: Average loss: 0.2670, Average MAE: 0.3686\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 330: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 331 [4096/194182 (2%)]\tLoss: 0.555337\tGrad Norm: 0.696482\tLR: 0.030000\n",
      "Train Epoch: 331 [24576/194182 (12%)]\tLoss: 0.563701\tGrad Norm: 0.896665\tLR: 0.030000\n",
      "Train Epoch: 331 [45056/194182 (23%)]\tLoss: 0.569157\tGrad Norm: 0.974386\tLR: 0.030000\n",
      "Train Epoch: 331 [65536/194182 (33%)]\tLoss: 0.563154\tGrad Norm: 1.207288\tLR: 0.030000\n",
      "Train Epoch: 331 [86016/194182 (44%)]\tLoss: 0.564323\tGrad Norm: 1.305850\tLR: 0.030000\n",
      "Train Epoch: 331 [106496/194182 (54%)]\tLoss: 0.559998\tGrad Norm: 1.118662\tLR: 0.030000\n",
      "Train Epoch: 331 [126976/194182 (65%)]\tLoss: 0.568187\tGrad Norm: 1.340527\tLR: 0.030000\n",
      "Train Epoch: 331 [147456/194182 (75%)]\tLoss: 0.566564\tGrad Norm: 1.234063\tLR: 0.030000\n",
      "Train Epoch: 331 [167936/194182 (85%)]\tLoss: 0.564049\tGrad Norm: 1.494504\tLR: 0.030000\n",
      "Train Epoch: 331 [188416/194182 (96%)]\tLoss: 0.570293\tGrad Norm: 1.197703\tLR: 0.030000\n",
      "Train set: Average loss: 0.5647\n",
      "Test set: Average loss: 0.2685, Average MAE: 0.3691\n",
      "Train Epoch: 332 [4096/194182 (2%)]\tLoss: 0.563759\tGrad Norm: 0.798209\tLR: 0.030000\n",
      "Train Epoch: 332 [24576/194182 (12%)]\tLoss: 0.555787\tGrad Norm: 0.718121\tLR: 0.030000\n",
      "Train Epoch: 332 [45056/194182 (23%)]\tLoss: 0.558699\tGrad Norm: 1.191417\tLR: 0.030000\n",
      "Train Epoch: 332 [65536/194182 (33%)]\tLoss: 0.569098\tGrad Norm: 1.463185\tLR: 0.030000\n",
      "Train Epoch: 332 [86016/194182 (44%)]\tLoss: 0.555233\tGrad Norm: 1.092152\tLR: 0.030000\n",
      "Train Epoch: 332 [106496/194182 (54%)]\tLoss: 0.570448\tGrad Norm: 1.444057\tLR: 0.030000\n",
      "Train Epoch: 332 [126976/194182 (65%)]\tLoss: 0.560578\tGrad Norm: 1.018482\tLR: 0.030000\n",
      "Train Epoch: 332 [147456/194182 (75%)]\tLoss: 0.558771\tGrad Norm: 1.042131\tLR: 0.030000\n",
      "Train Epoch: 332 [167936/194182 (85%)]\tLoss: 0.559951\tGrad Norm: 0.965281\tLR: 0.030000\n",
      "Train Epoch: 332 [188416/194182 (96%)]\tLoss: 0.568665\tGrad Norm: 1.077733\tLR: 0.030000\n",
      "Train set: Average loss: 0.5632\n",
      "Test set: Average loss: 0.2728, Average MAE: 0.3674\n",
      "Train Epoch: 333 [4096/194182 (2%)]\tLoss: 0.561371\tGrad Norm: 1.331187\tLR: 0.030000\n",
      "Train Epoch: 333 [24576/194182 (12%)]\tLoss: 0.569162\tGrad Norm: 1.077913\tLR: 0.030000\n",
      "Train Epoch: 333 [45056/194182 (23%)]\tLoss: 0.561529\tGrad Norm: 0.916151\tLR: 0.030000\n",
      "Train Epoch: 333 [65536/194182 (33%)]\tLoss: 0.555842\tGrad Norm: 1.019081\tLR: 0.030000\n",
      "Train Epoch: 333 [86016/194182 (44%)]\tLoss: 0.556670\tGrad Norm: 0.859062\tLR: 0.030000\n",
      "Train Epoch: 333 [106496/194182 (54%)]\tLoss: 0.555411\tGrad Norm: 1.074437\tLR: 0.030000\n",
      "Train Epoch: 333 [126976/194182 (65%)]\tLoss: 0.560656\tGrad Norm: 1.233321\tLR: 0.030000\n",
      "Train Epoch: 333 [147456/194182 (75%)]\tLoss: 0.572139\tGrad Norm: 1.671515\tLR: 0.030000\n",
      "Train Epoch: 333 [167936/194182 (85%)]\tLoss: 0.567363\tGrad Norm: 1.447423\tLR: 0.030000\n",
      "Train Epoch: 333 [188416/194182 (96%)]\tLoss: 0.565979\tGrad Norm: 1.542686\tLR: 0.030000\n",
      "Train set: Average loss: 0.5641\n",
      "Test set: Average loss: 0.2765, Average MAE: 0.3596\n",
      "Train Epoch: 334 [4096/194182 (2%)]\tLoss: 0.566300\tGrad Norm: 1.400265\tLR: 0.030000\n",
      "Train Epoch: 334 [24576/194182 (12%)]\tLoss: 0.568258\tGrad Norm: 1.263392\tLR: 0.030000\n",
      "Train Epoch: 334 [45056/194182 (23%)]\tLoss: 0.559684\tGrad Norm: 1.003528\tLR: 0.030000\n",
      "Train Epoch: 334 [65536/194182 (33%)]\tLoss: 0.562795\tGrad Norm: 0.957728\tLR: 0.030000\n",
      "Train Epoch: 334 [86016/194182 (44%)]\tLoss: 0.556603\tGrad Norm: 0.770723\tLR: 0.030000\n",
      "Train Epoch: 334 [106496/194182 (54%)]\tLoss: 0.561674\tGrad Norm: 0.778095\tLR: 0.030000\n",
      "Train Epoch: 334 [126976/194182 (65%)]\tLoss: 0.560360\tGrad Norm: 0.905347\tLR: 0.030000\n",
      "Train Epoch: 334 [147456/194182 (75%)]\tLoss: 0.562036\tGrad Norm: 1.141221\tLR: 0.030000\n",
      "Train Epoch: 334 [167936/194182 (85%)]\tLoss: 0.559101\tGrad Norm: 1.227412\tLR: 0.030000\n",
      "Train Epoch: 334 [188416/194182 (96%)]\tLoss: 0.565973\tGrad Norm: 1.345207\tLR: 0.030000\n",
      "Train set: Average loss: 0.5616\n",
      "Test set: Average loss: 0.2750, Average MAE: 0.3740\n",
      "Train Epoch: 335 [4096/194182 (2%)]\tLoss: 0.565612\tGrad Norm: 1.281945\tLR: 0.030000\n",
      "Train Epoch: 335 [24576/194182 (12%)]\tLoss: 0.556077\tGrad Norm: 0.995246\tLR: 0.030000\n",
      "Train Epoch: 335 [45056/194182 (23%)]\tLoss: 0.561743\tGrad Norm: 1.293831\tLR: 0.030000\n",
      "Train Epoch: 335 [65536/194182 (33%)]\tLoss: 0.564449\tGrad Norm: 1.150139\tLR: 0.030000\n",
      "Train Epoch: 335 [86016/194182 (44%)]\tLoss: 0.562911\tGrad Norm: 1.245080\tLR: 0.030000\n",
      "Train Epoch: 335 [106496/194182 (54%)]\tLoss: 0.560770\tGrad Norm: 1.129050\tLR: 0.030000\n",
      "Train Epoch: 335 [126976/194182 (65%)]\tLoss: 0.558339\tGrad Norm: 0.767652\tLR: 0.030000\n",
      "Train Epoch: 335 [147456/194182 (75%)]\tLoss: 0.559476\tGrad Norm: 0.790075\tLR: 0.030000\n",
      "Train Epoch: 335 [167936/194182 (85%)]\tLoss: 0.564051\tGrad Norm: 1.384258\tLR: 0.030000\n",
      "Train Epoch: 335 [188416/194182 (96%)]\tLoss: 0.562704\tGrad Norm: 1.201227\tLR: 0.030000\n",
      "Train set: Average loss: 0.5613\n",
      "Test set: Average loss: 0.2744, Average MAE: 0.3632\n",
      "Epoch 335: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 336 [4096/194182 (2%)]\tLoss: 0.559718\tGrad Norm: 1.361462\tLR: 0.030000\n",
      "Train Epoch: 336 [24576/194182 (12%)]\tLoss: 0.560414\tGrad Norm: 1.210830\tLR: 0.030000\n",
      "Train Epoch: 336 [45056/194182 (23%)]\tLoss: 0.565400\tGrad Norm: 1.368492\tLR: 0.030000\n",
      "Train Epoch: 336 [65536/194182 (33%)]\tLoss: 0.557751\tGrad Norm: 1.056874\tLR: 0.030000\n",
      "Train Epoch: 336 [86016/194182 (44%)]\tLoss: 0.564772\tGrad Norm: 1.175325\tLR: 0.030000\n",
      "Train Epoch: 336 [106496/194182 (54%)]\tLoss: 0.561312\tGrad Norm: 1.014418\tLR: 0.030000\n",
      "Train Epoch: 336 [126976/194182 (65%)]\tLoss: 0.552930\tGrad Norm: 0.777455\tLR: 0.030000\n",
      "Train Epoch: 336 [147456/194182 (75%)]\tLoss: 0.557697\tGrad Norm: 0.842181\tLR: 0.030000\n",
      "Train Epoch: 336 [167936/194182 (85%)]\tLoss: 0.559819\tGrad Norm: 1.083168\tLR: 0.030000\n",
      "Train Epoch: 336 [188416/194182 (96%)]\tLoss: 0.555360\tGrad Norm: 1.096344\tLR: 0.030000\n",
      "Train set: Average loss: 0.5612\n",
      "Test set: Average loss: 0.2703, Average MAE: 0.3745\n",
      "Train Epoch: 337 [4096/194182 (2%)]\tLoss: 0.559008\tGrad Norm: 1.093036\tLR: 0.030000\n",
      "Train Epoch: 337 [24576/194182 (12%)]\tLoss: 0.556338\tGrad Norm: 0.766283\tLR: 0.030000\n",
      "Train Epoch: 337 [45056/194182 (23%)]\tLoss: 0.560030\tGrad Norm: 0.721957\tLR: 0.030000\n",
      "Train Epoch: 337 [65536/194182 (33%)]\tLoss: 0.558202\tGrad Norm: 0.952021\tLR: 0.030000\n",
      "Train Epoch: 337 [86016/194182 (44%)]\tLoss: 0.558849\tGrad Norm: 1.180660\tLR: 0.030000\n",
      "Train Epoch: 337 [106496/194182 (54%)]\tLoss: 0.564201\tGrad Norm: 1.260237\tLR: 0.030000\n",
      "Train Epoch: 337 [126976/194182 (65%)]\tLoss: 0.566126\tGrad Norm: 1.353762\tLR: 0.030000\n",
      "Train Epoch: 337 [147456/194182 (75%)]\tLoss: 0.570125\tGrad Norm: 1.388535\tLR: 0.030000\n",
      "Train Epoch: 337 [167936/194182 (85%)]\tLoss: 0.556053\tGrad Norm: 0.913822\tLR: 0.030000\n",
      "Train Epoch: 337 [188416/194182 (96%)]\tLoss: 0.563560\tGrad Norm: 0.997476\tLR: 0.030000\n",
      "Train set: Average loss: 0.5592\n",
      "Test set: Average loss: 0.2706, Average MAE: 0.3745\n",
      "Train Epoch: 338 [4096/194182 (2%)]\tLoss: 0.558720\tGrad Norm: 1.098221\tLR: 0.030000\n",
      "Train Epoch: 338 [24576/194182 (12%)]\tLoss: 0.558451\tGrad Norm: 1.108118\tLR: 0.030000\n",
      "Train Epoch: 338 [45056/194182 (23%)]\tLoss: 0.554565\tGrad Norm: 1.077735\tLR: 0.030000\n",
      "Train Epoch: 338 [65536/194182 (33%)]\tLoss: 0.560867\tGrad Norm: 1.015874\tLR: 0.030000\n",
      "Train Epoch: 338 [86016/194182 (44%)]\tLoss: 0.563146\tGrad Norm: 1.120480\tLR: 0.030000\n",
      "Train Epoch: 338 [106496/194182 (54%)]\tLoss: 0.559611\tGrad Norm: 1.223314\tLR: 0.030000\n",
      "Train Epoch: 338 [126976/194182 (65%)]\tLoss: 0.566235\tGrad Norm: 1.363294\tLR: 0.030000\n",
      "Train Epoch: 338 [147456/194182 (75%)]\tLoss: 0.556733\tGrad Norm: 1.420341\tLR: 0.030000\n",
      "Train Epoch: 338 [167936/194182 (85%)]\tLoss: 0.557902\tGrad Norm: 0.922132\tLR: 0.030000\n",
      "Train Epoch: 338 [188416/194182 (96%)]\tLoss: 0.562761\tGrad Norm: 1.340085\tLR: 0.030000\n",
      "Train set: Average loss: 0.5603\n",
      "Test set: Average loss: 0.2755, Average MAE: 0.3910\n",
      "Train Epoch: 339 [4096/194182 (2%)]\tLoss: 0.565931\tGrad Norm: 1.413998\tLR: 0.030000\n",
      "Train Epoch: 339 [24576/194182 (12%)]\tLoss: 0.556125\tGrad Norm: 1.164531\tLR: 0.030000\n",
      "Train Epoch: 339 [45056/194182 (23%)]\tLoss: 0.560969\tGrad Norm: 1.341285\tLR: 0.030000\n",
      "Train Epoch: 339 [65536/194182 (33%)]\tLoss: 0.554616\tGrad Norm: 1.074300\tLR: 0.030000\n",
      "Train Epoch: 339 [86016/194182 (44%)]\tLoss: 0.561091\tGrad Norm: 1.002979\tLR: 0.030000\n",
      "Train Epoch: 339 [106496/194182 (54%)]\tLoss: 0.560290\tGrad Norm: 1.149907\tLR: 0.030000\n",
      "Train Epoch: 339 [126976/194182 (65%)]\tLoss: 0.554952\tGrad Norm: 0.983910\tLR: 0.030000\n",
      "Train Epoch: 339 [147456/194182 (75%)]\tLoss: 0.568376\tGrad Norm: 1.211593\tLR: 0.030000\n",
      "Train Epoch: 339 [167936/194182 (85%)]\tLoss: 0.560864\tGrad Norm: 1.450097\tLR: 0.030000\n",
      "Train Epoch: 339 [188416/194182 (96%)]\tLoss: 0.556791\tGrad Norm: 1.211101\tLR: 0.030000\n",
      "Train set: Average loss: 0.5602\n",
      "Test set: Average loss: 0.2732, Average MAE: 0.3614\n",
      "Train Epoch: 340 [4096/194182 (2%)]\tLoss: 0.551622\tGrad Norm: 1.276663\tLR: 0.030000\n",
      "Train Epoch: 340 [24576/194182 (12%)]\tLoss: 0.562774\tGrad Norm: 1.352634\tLR: 0.030000\n",
      "Train Epoch: 340 [45056/194182 (23%)]\tLoss: 0.559160\tGrad Norm: 1.050277\tLR: 0.030000\n",
      "Train Epoch: 340 [65536/194182 (33%)]\tLoss: 0.555696\tGrad Norm: 0.964957\tLR: 0.030000\n",
      "Train Epoch: 340 [86016/194182 (44%)]\tLoss: 0.553259\tGrad Norm: 0.769563\tLR: 0.030000\n",
      "Train Epoch: 340 [106496/194182 (54%)]\tLoss: 0.558973\tGrad Norm: 0.494632\tLR: 0.030000\n",
      "Train Epoch: 340 [126976/194182 (65%)]\tLoss: 0.544937\tGrad Norm: 0.744378\tLR: 0.030000\n",
      "Train Epoch: 340 [147456/194182 (75%)]\tLoss: 0.555985\tGrad Norm: 0.810606\tLR: 0.030000\n",
      "Train Epoch: 340 [167936/194182 (85%)]\tLoss: 0.556204\tGrad Norm: 1.040333\tLR: 0.030000\n",
      "Train Epoch: 340 [188416/194182 (96%)]\tLoss: 0.557630\tGrad Norm: 1.032677\tLR: 0.030000\n",
      "Train set: Average loss: 0.5559\n",
      "Test set: Average loss: 0.2689, Average MAE: 0.3560\n",
      "Epoch 340: Mean reward = 0.036 +/- 0.018\n",
      "Train Epoch: 341 [4096/194182 (2%)]\tLoss: 0.556998\tGrad Norm: 1.113508\tLR: 0.030000\n",
      "Train Epoch: 341 [24576/194182 (12%)]\tLoss: 0.558119\tGrad Norm: 1.255148\tLR: 0.030000\n",
      "Train Epoch: 341 [45056/194182 (23%)]\tLoss: 0.561965\tGrad Norm: 1.114485\tLR: 0.030000\n",
      "Train Epoch: 341 [65536/194182 (33%)]\tLoss: 0.569631\tGrad Norm: 1.480796\tLR: 0.030000\n",
      "Train Epoch: 341 [86016/194182 (44%)]\tLoss: 0.562687\tGrad Norm: 1.188169\tLR: 0.030000\n",
      "Train Epoch: 341 [106496/194182 (54%)]\tLoss: 0.558409\tGrad Norm: 1.018666\tLR: 0.030000\n",
      "Train Epoch: 341 [126976/194182 (65%)]\tLoss: 0.555504\tGrad Norm: 1.279355\tLR: 0.030000\n",
      "Train Epoch: 341 [147456/194182 (75%)]\tLoss: 0.560798\tGrad Norm: 1.315993\tLR: 0.030000\n",
      "Train Epoch: 341 [167936/194182 (85%)]\tLoss: 0.564041\tGrad Norm: 0.914724\tLR: 0.030000\n",
      "Train Epoch: 341 [188416/194182 (96%)]\tLoss: 0.547014\tGrad Norm: 0.773758\tLR: 0.030000\n",
      "Train set: Average loss: 0.5579\n",
      "Test set: Average loss: 0.2651, Average MAE: 0.3623\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 342 [4096/194182 (2%)]\tLoss: 0.551161\tGrad Norm: 0.745373\tLR: 0.030000\n",
      "Train Epoch: 342 [24576/194182 (12%)]\tLoss: 0.556308\tGrad Norm: 1.096076\tLR: 0.030000\n",
      "Train Epoch: 342 [45056/194182 (23%)]\tLoss: 0.551613\tGrad Norm: 1.107432\tLR: 0.030000\n",
      "Train Epoch: 342 [65536/194182 (33%)]\tLoss: 0.556552\tGrad Norm: 1.314361\tLR: 0.030000\n",
      "Train Epoch: 342 [86016/194182 (44%)]\tLoss: 0.547739\tGrad Norm: 1.081771\tLR: 0.030000\n",
      "Train Epoch: 342 [106496/194182 (54%)]\tLoss: 0.563725\tGrad Norm: 1.474520\tLR: 0.030000\n",
      "Train Epoch: 342 [126976/194182 (65%)]\tLoss: 0.555924\tGrad Norm: 1.333761\tLR: 0.030000\n",
      "Train Epoch: 342 [147456/194182 (75%)]\tLoss: 0.554997\tGrad Norm: 0.718501\tLR: 0.030000\n",
      "Train Epoch: 342 [167936/194182 (85%)]\tLoss: 0.548641\tGrad Norm: 0.910668\tLR: 0.030000\n",
      "Train Epoch: 342 [188416/194182 (96%)]\tLoss: 0.564563\tGrad Norm: 1.190418\tLR: 0.030000\n",
      "Train set: Average loss: 0.5566\n",
      "Test set: Average loss: 0.2770, Average MAE: 0.3560\n",
      "Train Epoch: 343 [4096/194182 (2%)]\tLoss: 0.569946\tGrad Norm: 1.780419\tLR: 0.030000\n",
      "Train Epoch: 343 [24576/194182 (12%)]\tLoss: 0.553439\tGrad Norm: 1.102443\tLR: 0.030000\n",
      "Train Epoch: 343 [45056/194182 (23%)]\tLoss: 0.564845\tGrad Norm: 1.423919\tLR: 0.030000\n",
      "Train Epoch: 343 [65536/194182 (33%)]\tLoss: 0.545357\tGrad Norm: 0.790761\tLR: 0.030000\n",
      "Train Epoch: 343 [86016/194182 (44%)]\tLoss: 0.550606\tGrad Norm: 0.609707\tLR: 0.030000\n",
      "Train Epoch: 343 [106496/194182 (54%)]\tLoss: 0.550908\tGrad Norm: 1.192080\tLR: 0.030000\n",
      "Train Epoch: 343 [126976/194182 (65%)]\tLoss: 0.549257\tGrad Norm: 1.286749\tLR: 0.030000\n",
      "Train Epoch: 343 [147456/194182 (75%)]\tLoss: 0.560356\tGrad Norm: 1.513797\tLR: 0.030000\n",
      "Train Epoch: 343 [167936/194182 (85%)]\tLoss: 0.567142\tGrad Norm: 1.198779\tLR: 0.030000\n",
      "Train Epoch: 343 [188416/194182 (96%)]\tLoss: 0.554652\tGrad Norm: 0.951740\tLR: 0.030000\n",
      "Train set: Average loss: 0.5569\n",
      "Test set: Average loss: 0.2649, Average MAE: 0.3700\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 344 [4096/194182 (2%)]\tLoss: 0.545820\tGrad Norm: 0.741081\tLR: 0.030000\n",
      "Train Epoch: 344 [24576/194182 (12%)]\tLoss: 0.547584\tGrad Norm: 0.955183\tLR: 0.030000\n",
      "Train Epoch: 344 [45056/194182 (23%)]\tLoss: 0.553826\tGrad Norm: 1.072175\tLR: 0.030000\n",
      "Train Epoch: 344 [65536/194182 (33%)]\tLoss: 0.556305\tGrad Norm: 1.139625\tLR: 0.030000\n",
      "Train Epoch: 344 [86016/194182 (44%)]\tLoss: 0.553386\tGrad Norm: 1.175335\tLR: 0.030000\n",
      "Train Epoch: 344 [106496/194182 (54%)]\tLoss: 0.563938\tGrad Norm: 1.131211\tLR: 0.030000\n",
      "Train Epoch: 344 [126976/194182 (65%)]\tLoss: 0.561188\tGrad Norm: 1.358034\tLR: 0.030000\n",
      "Train Epoch: 344 [147456/194182 (75%)]\tLoss: 0.553831\tGrad Norm: 0.686281\tLR: 0.030000\n",
      "Train Epoch: 344 [167936/194182 (85%)]\tLoss: 0.552590\tGrad Norm: 0.920313\tLR: 0.030000\n",
      "Train Epoch: 344 [188416/194182 (96%)]\tLoss: 0.548395\tGrad Norm: 1.109799\tLR: 0.030000\n",
      "Train set: Average loss: 0.5545\n",
      "Test set: Average loss: 0.2704, Average MAE: 0.3755\n",
      "Train Epoch: 345 [4096/194182 (2%)]\tLoss: 0.559638\tGrad Norm: 1.183568\tLR: 0.030000\n",
      "Train Epoch: 345 [24576/194182 (12%)]\tLoss: 0.551950\tGrad Norm: 1.114781\tLR: 0.030000\n",
      "Train Epoch: 345 [45056/194182 (23%)]\tLoss: 0.557588\tGrad Norm: 1.324798\tLR: 0.030000\n",
      "Train Epoch: 345 [65536/194182 (33%)]\tLoss: 0.546994\tGrad Norm: 0.895139\tLR: 0.030000\n",
      "Train Epoch: 345 [86016/194182 (44%)]\tLoss: 0.554120\tGrad Norm: 0.892133\tLR: 0.030000\n",
      "Train Epoch: 345 [106496/194182 (54%)]\tLoss: 0.557283\tGrad Norm: 1.219615\tLR: 0.030000\n",
      "Train Epoch: 345 [126976/194182 (65%)]\tLoss: 0.553315\tGrad Norm: 1.178508\tLR: 0.030000\n",
      "Train Epoch: 345 [147456/194182 (75%)]\tLoss: 0.554098\tGrad Norm: 1.140231\tLR: 0.030000\n",
      "Train Epoch: 345 [167936/194182 (85%)]\tLoss: 0.562935\tGrad Norm: 1.286226\tLR: 0.030000\n",
      "Train Epoch: 345 [188416/194182 (96%)]\tLoss: 0.556220\tGrad Norm: 1.226165\tLR: 0.030000\n",
      "Train set: Average loss: 0.5557\n",
      "Test set: Average loss: 0.2671, Average MAE: 0.3617\n",
      "Epoch 345: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 346 [4096/194182 (2%)]\tLoss: 0.552620\tGrad Norm: 1.015617\tLR: 0.030000\n",
      "Train Epoch: 346 [24576/194182 (12%)]\tLoss: 0.553098\tGrad Norm: 0.895270\tLR: 0.030000\n",
      "Train Epoch: 346 [45056/194182 (23%)]\tLoss: 0.557299\tGrad Norm: 1.229196\tLR: 0.030000\n",
      "Train Epoch: 346 [65536/194182 (33%)]\tLoss: 0.557134\tGrad Norm: 1.222122\tLR: 0.030000\n",
      "Train Epoch: 346 [86016/194182 (44%)]\tLoss: 0.553933\tGrad Norm: 0.924208\tLR: 0.030000\n",
      "Train Epoch: 346 [106496/194182 (54%)]\tLoss: 0.549309\tGrad Norm: 1.274096\tLR: 0.030000\n",
      "Train Epoch: 346 [126976/194182 (65%)]\tLoss: 0.548132\tGrad Norm: 1.103365\tLR: 0.030000\n",
      "Train Epoch: 346 [147456/194182 (75%)]\tLoss: 0.554111\tGrad Norm: 1.242571\tLR: 0.030000\n",
      "Train Epoch: 346 [167936/194182 (85%)]\tLoss: 0.557242\tGrad Norm: 1.145402\tLR: 0.030000\n",
      "Train Epoch: 346 [188416/194182 (96%)]\tLoss: 0.559287\tGrad Norm: 1.095334\tLR: 0.030000\n",
      "Train set: Average loss: 0.5540\n",
      "Test set: Average loss: 0.2688, Average MAE: 0.3697\n",
      "Train Epoch: 347 [4096/194182 (2%)]\tLoss: 0.545723\tGrad Norm: 1.071655\tLR: 0.030000\n",
      "Train Epoch: 347 [24576/194182 (12%)]\tLoss: 0.552221\tGrad Norm: 1.025023\tLR: 0.030000\n",
      "Train Epoch: 347 [45056/194182 (23%)]\tLoss: 0.559268\tGrad Norm: 1.058510\tLR: 0.030000\n",
      "Train Epoch: 347 [65536/194182 (33%)]\tLoss: 0.556464\tGrad Norm: 1.142231\tLR: 0.030000\n",
      "Train Epoch: 347 [86016/194182 (44%)]\tLoss: 0.559653\tGrad Norm: 1.329037\tLR: 0.030000\n",
      "Train Epoch: 347 [106496/194182 (54%)]\tLoss: 0.557683\tGrad Norm: 1.496348\tLR: 0.030000\n",
      "Train Epoch: 347 [126976/194182 (65%)]\tLoss: 0.552315\tGrad Norm: 1.251859\tLR: 0.030000\n",
      "Train Epoch: 347 [147456/194182 (75%)]\tLoss: 0.538472\tGrad Norm: 0.844224\tLR: 0.030000\n",
      "Train Epoch: 347 [167936/194182 (85%)]\tLoss: 0.546316\tGrad Norm: 0.952482\tLR: 0.030000\n",
      "Train Epoch: 347 [188416/194182 (96%)]\tLoss: 0.543180\tGrad Norm: 0.726951\tLR: 0.030000\n",
      "Train set: Average loss: 0.5529\n",
      "Test set: Average loss: 0.2672, Average MAE: 0.3788\n",
      "Train Epoch: 348 [4096/194182 (2%)]\tLoss: 0.548553\tGrad Norm: 1.094883\tLR: 0.030000\n",
      "Train Epoch: 348 [24576/194182 (12%)]\tLoss: 0.555477\tGrad Norm: 1.410376\tLR: 0.030000\n",
      "Train Epoch: 348 [45056/194182 (23%)]\tLoss: 0.552172\tGrad Norm: 1.144651\tLR: 0.030000\n",
      "Train Epoch: 348 [65536/194182 (33%)]\tLoss: 0.546495\tGrad Norm: 0.999992\tLR: 0.030000\n",
      "Train Epoch: 348 [86016/194182 (44%)]\tLoss: 0.557400\tGrad Norm: 0.968563\tLR: 0.030000\n",
      "Train Epoch: 348 [106496/194182 (54%)]\tLoss: 0.557942\tGrad Norm: 1.233239\tLR: 0.030000\n",
      "Train Epoch: 348 [126976/194182 (65%)]\tLoss: 0.550341\tGrad Norm: 1.404735\tLR: 0.030000\n",
      "Train Epoch: 348 [147456/194182 (75%)]\tLoss: 0.553954\tGrad Norm: 1.162096\tLR: 0.030000\n",
      "Train Epoch: 348 [167936/194182 (85%)]\tLoss: 0.552747\tGrad Norm: 1.375074\tLR: 0.030000\n",
      "Train Epoch: 348 [188416/194182 (96%)]\tLoss: 0.564389\tGrad Norm: 1.203402\tLR: 0.030000\n",
      "Train set: Average loss: 0.5539\n",
      "Test set: Average loss: 0.2688, Average MAE: 0.3781\n",
      "Train Epoch: 349 [4096/194182 (2%)]\tLoss: 0.558995\tGrad Norm: 1.007447\tLR: 0.030000\n",
      "Train Epoch: 349 [24576/194182 (12%)]\tLoss: 0.557591\tGrad Norm: 1.093373\tLR: 0.030000\n",
      "Train Epoch: 349 [45056/194182 (23%)]\tLoss: 0.546772\tGrad Norm: 1.107760\tLR: 0.030000\n",
      "Train Epoch: 349 [65536/194182 (33%)]\tLoss: 0.545212\tGrad Norm: 1.151893\tLR: 0.030000\n",
      "Train Epoch: 349 [86016/194182 (44%)]\tLoss: 0.545273\tGrad Norm: 1.049280\tLR: 0.030000\n",
      "Train Epoch: 349 [106496/194182 (54%)]\tLoss: 0.546349\tGrad Norm: 0.892423\tLR: 0.030000\n",
      "Train Epoch: 349 [126976/194182 (65%)]\tLoss: 0.542862\tGrad Norm: 1.016196\tLR: 0.030000\n",
      "Train Epoch: 349 [147456/194182 (75%)]\tLoss: 0.557902\tGrad Norm: 1.253365\tLR: 0.030000\n",
      "Train Epoch: 349 [167936/194182 (85%)]\tLoss: 0.551000\tGrad Norm: 1.197282\tLR: 0.030000\n",
      "Train Epoch: 349 [188416/194182 (96%)]\tLoss: 0.560276\tGrad Norm: 1.242156\tLR: 0.030000\n",
      "Train set: Average loss: 0.5522\n",
      "Test set: Average loss: 0.2692, Average MAE: 0.3830\n",
      "Train Epoch: 350 [4096/194182 (2%)]\tLoss: 0.545525\tGrad Norm: 1.308163\tLR: 0.030000\n",
      "Train Epoch: 350 [24576/194182 (12%)]\tLoss: 0.545724\tGrad Norm: 1.022213\tLR: 0.030000\n",
      "Train Epoch: 350 [45056/194182 (23%)]\tLoss: 0.546907\tGrad Norm: 1.136205\tLR: 0.030000\n",
      "Train Epoch: 350 [65536/194182 (33%)]\tLoss: 0.550470\tGrad Norm: 0.835906\tLR: 0.030000\n",
      "Train Epoch: 350 [86016/194182 (44%)]\tLoss: 0.549839\tGrad Norm: 1.054456\tLR: 0.030000\n",
      "Train Epoch: 350 [106496/194182 (54%)]\tLoss: 0.542582\tGrad Norm: 0.728059\tLR: 0.030000\n",
      "Train Epoch: 350 [126976/194182 (65%)]\tLoss: 0.552879\tGrad Norm: 0.880502\tLR: 0.030000\n",
      "Train Epoch: 350 [147456/194182 (75%)]\tLoss: 0.544090\tGrad Norm: 0.874655\tLR: 0.030000\n",
      "Train Epoch: 350 [167936/194182 (85%)]\tLoss: 0.553916\tGrad Norm: 1.047442\tLR: 0.030000\n",
      "Train Epoch: 350 [188416/194182 (96%)]\tLoss: 0.556461\tGrad Norm: 1.396605\tLR: 0.030000\n",
      "Train set: Average loss: 0.5503\n",
      "Test set: Average loss: 0.2708, Average MAE: 0.3809\n",
      "Epoch 350: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 351 [4096/194182 (2%)]\tLoss: 0.554564\tGrad Norm: 1.167636\tLR: 0.030000\n",
      "Train Epoch: 351 [24576/194182 (12%)]\tLoss: 0.557534\tGrad Norm: 1.248267\tLR: 0.030000\n",
      "Train Epoch: 351 [45056/194182 (23%)]\tLoss: 0.550304\tGrad Norm: 1.343310\tLR: 0.030000\n",
      "Train Epoch: 351 [65536/194182 (33%)]\tLoss: 0.545237\tGrad Norm: 1.031090\tLR: 0.030000\n",
      "Train Epoch: 351 [86016/194182 (44%)]\tLoss: 0.549939\tGrad Norm: 1.223193\tLR: 0.030000\n",
      "Train Epoch: 351 [106496/194182 (54%)]\tLoss: 0.556032\tGrad Norm: 1.313635\tLR: 0.030000\n",
      "Train Epoch: 351 [126976/194182 (65%)]\tLoss: 0.545554\tGrad Norm: 1.181733\tLR: 0.030000\n",
      "Train Epoch: 351 [147456/194182 (75%)]\tLoss: 0.550159\tGrad Norm: 1.236197\tLR: 0.030000\n",
      "Train Epoch: 351 [167936/194182 (85%)]\tLoss: 0.548119\tGrad Norm: 1.138389\tLR: 0.030000\n",
      "Train Epoch: 351 [188416/194182 (96%)]\tLoss: 0.554169\tGrad Norm: 1.292072\tLR: 0.030000\n",
      "Train set: Average loss: 0.5523\n",
      "Test set: Average loss: 0.2655, Average MAE: 0.3542\n",
      "Train Epoch: 352 [4096/194182 (2%)]\tLoss: 0.544461\tGrad Norm: 0.821846\tLR: 0.030000\n",
      "Train Epoch: 352 [24576/194182 (12%)]\tLoss: 0.557099\tGrad Norm: 0.677424\tLR: 0.030000\n",
      "Train Epoch: 352 [45056/194182 (23%)]\tLoss: 0.540968\tGrad Norm: 0.635841\tLR: 0.030000\n",
      "Train Epoch: 352 [65536/194182 (33%)]\tLoss: 0.544161\tGrad Norm: 0.798878\tLR: 0.030000\n",
      "Train Epoch: 352 [86016/194182 (44%)]\tLoss: 0.546311\tGrad Norm: 0.965728\tLR: 0.030000\n",
      "Train Epoch: 352 [106496/194182 (54%)]\tLoss: 0.552090\tGrad Norm: 0.760419\tLR: 0.030000\n",
      "Train Epoch: 352 [126976/194182 (65%)]\tLoss: 0.549793\tGrad Norm: 1.213825\tLR: 0.030000\n",
      "Train Epoch: 352 [147456/194182 (75%)]\tLoss: 0.552159\tGrad Norm: 1.302689\tLR: 0.030000\n",
      "Train Epoch: 352 [167936/194182 (85%)]\tLoss: 0.548846\tGrad Norm: 1.197473\tLR: 0.030000\n",
      "Train Epoch: 352 [188416/194182 (96%)]\tLoss: 0.546854\tGrad Norm: 1.041616\tLR: 0.030000\n",
      "Train set: Average loss: 0.5479\n",
      "Test set: Average loss: 0.2688, Average MAE: 0.3799\n",
      "Train Epoch: 353 [4096/194182 (2%)]\tLoss: 0.551119\tGrad Norm: 1.280020\tLR: 0.030000\n",
      "Train Epoch: 353 [24576/194182 (12%)]\tLoss: 0.552572\tGrad Norm: 0.904324\tLR: 0.030000\n",
      "Train Epoch: 353 [45056/194182 (23%)]\tLoss: 0.546734\tGrad Norm: 0.897013\tLR: 0.030000\n",
      "Train Epoch: 353 [65536/194182 (33%)]\tLoss: 0.550633\tGrad Norm: 1.250371\tLR: 0.030000\n",
      "Train Epoch: 353 [86016/194182 (44%)]\tLoss: 0.552481\tGrad Norm: 1.235895\tLR: 0.030000\n",
      "Train Epoch: 353 [106496/194182 (54%)]\tLoss: 0.548767\tGrad Norm: 1.272072\tLR: 0.030000\n",
      "Train Epoch: 353 [126976/194182 (65%)]\tLoss: 0.551508\tGrad Norm: 1.609208\tLR: 0.030000\n",
      "Train Epoch: 353 [147456/194182 (75%)]\tLoss: 0.550179\tGrad Norm: 0.989997\tLR: 0.030000\n",
      "Train Epoch: 353 [167936/194182 (85%)]\tLoss: 0.546291\tGrad Norm: 1.049043\tLR: 0.030000\n",
      "Train Epoch: 353 [188416/194182 (96%)]\tLoss: 0.549015\tGrad Norm: 1.054625\tLR: 0.030000\n",
      "Train set: Average loss: 0.5497\n",
      "Test set: Average loss: 0.2686, Average MAE: 0.3758\n",
      "Train Epoch: 354 [4096/194182 (2%)]\tLoss: 0.550574\tGrad Norm: 1.169152\tLR: 0.030000\n",
      "Train Epoch: 354 [24576/194182 (12%)]\tLoss: 0.551321\tGrad Norm: 1.027899\tLR: 0.030000\n",
      "Train Epoch: 354 [45056/194182 (23%)]\tLoss: 0.547924\tGrad Norm: 1.251409\tLR: 0.030000\n",
      "Train Epoch: 354 [65536/194182 (33%)]\tLoss: 0.544821\tGrad Norm: 1.161124\tLR: 0.030000\n",
      "Train Epoch: 354 [86016/194182 (44%)]\tLoss: 0.551837\tGrad Norm: 1.155775\tLR: 0.030000\n",
      "Train Epoch: 354 [106496/194182 (54%)]\tLoss: 0.563631\tGrad Norm: 1.516769\tLR: 0.030000\n",
      "Train Epoch: 354 [126976/194182 (65%)]\tLoss: 0.544235\tGrad Norm: 0.870642\tLR: 0.030000\n",
      "Train Epoch: 354 [147456/194182 (75%)]\tLoss: 0.544550\tGrad Norm: 0.614927\tLR: 0.030000\n",
      "Train Epoch: 354 [167936/194182 (85%)]\tLoss: 0.536781\tGrad Norm: 0.550895\tLR: 0.030000\n",
      "Train Epoch: 354 [188416/194182 (96%)]\tLoss: 0.545161\tGrad Norm: 0.759031\tLR: 0.030000\n",
      "Train set: Average loss: 0.5472\n",
      "Test set: Average loss: 0.2661, Average MAE: 0.3603\n",
      "Train Epoch: 355 [4096/194182 (2%)]\tLoss: 0.551678\tGrad Norm: 1.531479\tLR: 0.030000\n",
      "Train Epoch: 355 [24576/194182 (12%)]\tLoss: 0.546925\tGrad Norm: 1.036254\tLR: 0.030000\n",
      "Train Epoch: 355 [45056/194182 (23%)]\tLoss: 0.539304\tGrad Norm: 0.892214\tLR: 0.030000\n",
      "Train Epoch: 355 [65536/194182 (33%)]\tLoss: 0.543910\tGrad Norm: 0.774848\tLR: 0.030000\n",
      "Train Epoch: 355 [86016/194182 (44%)]\tLoss: 0.547405\tGrad Norm: 0.780784\tLR: 0.030000\n",
      "Train Epoch: 355 [106496/194182 (54%)]\tLoss: 0.547145\tGrad Norm: 1.317607\tLR: 0.030000\n",
      "Train Epoch: 355 [126976/194182 (65%)]\tLoss: 0.548302\tGrad Norm: 1.613384\tLR: 0.030000\n",
      "Train Epoch: 355 [147456/194182 (75%)]\tLoss: 0.562187\tGrad Norm: 1.873538\tLR: 0.030000\n",
      "Train Epoch: 355 [167936/194182 (85%)]\tLoss: 0.549243\tGrad Norm: 1.313628\tLR: 0.030000\n",
      "Train Epoch: 355 [188416/194182 (96%)]\tLoss: 0.566689\tGrad Norm: 1.545234\tLR: 0.030000\n",
      "Train set: Average loss: 0.5502\n",
      "Test set: Average loss: 0.2659, Average MAE: 0.3760\n",
      "Epoch 355: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 356 [4096/194182 (2%)]\tLoss: 0.551786\tGrad Norm: 1.038767\tLR: 0.030000\n",
      "Train Epoch: 356 [24576/194182 (12%)]\tLoss: 0.556005\tGrad Norm: 1.196742\tLR: 0.030000\n",
      "Train Epoch: 356 [45056/194182 (23%)]\tLoss: 0.544593\tGrad Norm: 0.946144\tLR: 0.030000\n",
      "Train Epoch: 356 [65536/194182 (33%)]\tLoss: 0.542371\tGrad Norm: 0.644335\tLR: 0.030000\n",
      "Train Epoch: 356 [86016/194182 (44%)]\tLoss: 0.548715\tGrad Norm: 1.081485\tLR: 0.030000\n",
      "Train Epoch: 356 [106496/194182 (54%)]\tLoss: 0.547577\tGrad Norm: 1.161837\tLR: 0.030000\n",
      "Train Epoch: 356 [126976/194182 (65%)]\tLoss: 0.554837\tGrad Norm: 1.073382\tLR: 0.030000\n",
      "Train Epoch: 356 [147456/194182 (75%)]\tLoss: 0.546823\tGrad Norm: 1.070803\tLR: 0.030000\n",
      "Train Epoch: 356 [167936/194182 (85%)]\tLoss: 0.544003\tGrad Norm: 0.964589\tLR: 0.030000\n",
      "Train Epoch: 356 [188416/194182 (96%)]\tLoss: 0.543956\tGrad Norm: 0.870697\tLR: 0.030000\n",
      "Train set: Average loss: 0.5456\n",
      "Test set: Average loss: 0.2652, Average MAE: 0.3701\n",
      "Train Epoch: 357 [4096/194182 (2%)]\tLoss: 0.546256\tGrad Norm: 0.988997\tLR: 0.030000\n",
      "Train Epoch: 357 [24576/194182 (12%)]\tLoss: 0.544345\tGrad Norm: 1.362059\tLR: 0.030000\n",
      "Train Epoch: 357 [45056/194182 (23%)]\tLoss: 0.547982\tGrad Norm: 1.375572\tLR: 0.030000\n",
      "Train Epoch: 357 [65536/194182 (33%)]\tLoss: 0.547006\tGrad Norm: 1.142240\tLR: 0.030000\n",
      "Train Epoch: 357 [86016/194182 (44%)]\tLoss: 0.552405\tGrad Norm: 1.515426\tLR: 0.030000\n",
      "Train Epoch: 357 [106496/194182 (54%)]\tLoss: 0.562175\tGrad Norm: 1.641665\tLR: 0.030000\n",
      "Train Epoch: 357 [126976/194182 (65%)]\tLoss: 0.549185\tGrad Norm: 0.958526\tLR: 0.030000\n",
      "Train Epoch: 357 [147456/194182 (75%)]\tLoss: 0.541932\tGrad Norm: 0.749551\tLR: 0.030000\n",
      "Train Epoch: 357 [167936/194182 (85%)]\tLoss: 0.546113\tGrad Norm: 0.931344\tLR: 0.030000\n",
      "Train Epoch: 357 [188416/194182 (96%)]\tLoss: 0.543336\tGrad Norm: 1.018965\tLR: 0.030000\n",
      "Train set: Average loss: 0.5472\n",
      "Test set: Average loss: 0.2636, Average MAE: 0.3622\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 358 [4096/194182 (2%)]\tLoss: 0.536890\tGrad Norm: 0.846174\tLR: 0.030000\n",
      "Train Epoch: 358 [24576/194182 (12%)]\tLoss: 0.550018\tGrad Norm: 1.266182\tLR: 0.030000\n",
      "Train Epoch: 358 [45056/194182 (23%)]\tLoss: 0.544086\tGrad Norm: 1.234791\tLR: 0.030000\n",
      "Train Epoch: 358 [65536/194182 (33%)]\tLoss: 0.550292\tGrad Norm: 1.401813\tLR: 0.030000\n",
      "Train Epoch: 358 [86016/194182 (44%)]\tLoss: 0.550234\tGrad Norm: 0.799777\tLR: 0.030000\n",
      "Train Epoch: 358 [106496/194182 (54%)]\tLoss: 0.550600\tGrad Norm: 1.080541\tLR: 0.030000\n",
      "Train Epoch: 358 [126976/194182 (65%)]\tLoss: 0.550970\tGrad Norm: 1.483360\tLR: 0.030000\n",
      "Train Epoch: 358 [147456/194182 (75%)]\tLoss: 0.555242\tGrad Norm: 1.472926\tLR: 0.030000\n",
      "Train Epoch: 358 [167936/194182 (85%)]\tLoss: 0.535679\tGrad Norm: 0.839468\tLR: 0.030000\n",
      "Train Epoch: 358 [188416/194182 (96%)]\tLoss: 0.552869\tGrad Norm: 1.220247\tLR: 0.030000\n",
      "Train set: Average loss: 0.5470\n",
      "Test set: Average loss: 0.2686, Average MAE: 0.3832\n",
      "Train Epoch: 359 [4096/194182 (2%)]\tLoss: 0.547641\tGrad Norm: 1.254258\tLR: 0.030000\n",
      "Train Epoch: 359 [24576/194182 (12%)]\tLoss: 0.550321\tGrad Norm: 1.352074\tLR: 0.030000\n",
      "Train Epoch: 359 [45056/194182 (23%)]\tLoss: 0.546987\tGrad Norm: 1.176483\tLR: 0.030000\n",
      "Train Epoch: 359 [65536/194182 (33%)]\tLoss: 0.541219\tGrad Norm: 0.798974\tLR: 0.030000\n",
      "Train Epoch: 359 [86016/194182 (44%)]\tLoss: 0.546446\tGrad Norm: 0.865004\tLR: 0.030000\n",
      "Train Epoch: 359 [106496/194182 (54%)]\tLoss: 0.539505\tGrad Norm: 1.195718\tLR: 0.030000\n",
      "Train Epoch: 359 [126976/194182 (65%)]\tLoss: 0.536465\tGrad Norm: 0.996332\tLR: 0.030000\n",
      "Train Epoch: 359 [147456/194182 (75%)]\tLoss: 0.554502\tGrad Norm: 1.447252\tLR: 0.030000\n",
      "Train Epoch: 359 [167936/194182 (85%)]\tLoss: 0.541966\tGrad Norm: 1.020225\tLR: 0.030000\n",
      "Train Epoch: 359 [188416/194182 (96%)]\tLoss: 0.546449\tGrad Norm: 0.935846\tLR: 0.030000\n",
      "Train set: Average loss: 0.5453\n",
      "Test set: Average loss: 0.2705, Average MAE: 0.3682\n",
      "Train Epoch: 360 [4096/194182 (2%)]\tLoss: 0.544838\tGrad Norm: 1.403904\tLR: 0.030000\n",
      "Train Epoch: 360 [24576/194182 (12%)]\tLoss: 0.550167\tGrad Norm: 1.088305\tLR: 0.030000\n",
      "Train Epoch: 360 [45056/194182 (23%)]\tLoss: 0.542641\tGrad Norm: 0.956518\tLR: 0.030000\n",
      "Train Epoch: 360 [65536/194182 (33%)]\tLoss: 0.545766\tGrad Norm: 0.670867\tLR: 0.030000\n",
      "Train Epoch: 360 [86016/194182 (44%)]\tLoss: 0.538853\tGrad Norm: 0.891037\tLR: 0.030000\n",
      "Train Epoch: 360 [106496/194182 (54%)]\tLoss: 0.547951\tGrad Norm: 1.050429\tLR: 0.030000\n",
      "Train Epoch: 360 [126976/194182 (65%)]\tLoss: 0.540928\tGrad Norm: 1.146702\tLR: 0.030000\n",
      "Train Epoch: 360 [147456/194182 (75%)]\tLoss: 0.537477\tGrad Norm: 1.059468\tLR: 0.030000\n",
      "Train Epoch: 360 [167936/194182 (85%)]\tLoss: 0.542046\tGrad Norm: 1.150114\tLR: 0.030000\n",
      "Train Epoch: 360 [188416/194182 (96%)]\tLoss: 0.546082\tGrad Norm: 1.018581\tLR: 0.030000\n",
      "Train set: Average loss: 0.5437\n",
      "Test set: Average loss: 0.2696, Average MAE: 0.3511\n",
      "Epoch 360: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 361 [4096/194182 (2%)]\tLoss: 0.541087\tGrad Norm: 1.459604\tLR: 0.030000\n",
      "Train Epoch: 361 [24576/194182 (12%)]\tLoss: 0.543224\tGrad Norm: 1.383362\tLR: 0.030000\n",
      "Train Epoch: 361 [45056/194182 (23%)]\tLoss: 0.542555\tGrad Norm: 1.227096\tLR: 0.030000\n",
      "Train Epoch: 361 [65536/194182 (33%)]\tLoss: 0.548650\tGrad Norm: 1.483878\tLR: 0.030000\n",
      "Train Epoch: 361 [86016/194182 (44%)]\tLoss: 0.548466\tGrad Norm: 1.379033\tLR: 0.030000\n",
      "Train Epoch: 361 [106496/194182 (54%)]\tLoss: 0.539311\tGrad Norm: 0.935183\tLR: 0.030000\n",
      "Train Epoch: 361 [126976/194182 (65%)]\tLoss: 0.543295\tGrad Norm: 0.980838\tLR: 0.030000\n",
      "Train Epoch: 361 [147456/194182 (75%)]\tLoss: 0.543069\tGrad Norm: 1.045176\tLR: 0.030000\n",
      "Train Epoch: 361 [167936/194182 (85%)]\tLoss: 0.536762\tGrad Norm: 1.090429\tLR: 0.030000\n",
      "Train Epoch: 361 [188416/194182 (96%)]\tLoss: 0.538767\tGrad Norm: 0.774895\tLR: 0.030000\n",
      "Train set: Average loss: 0.5447\n",
      "Test set: Average loss: 0.2620, Average MAE: 0.3676\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 362 [4096/194182 (2%)]\tLoss: 0.540049\tGrad Norm: 1.111711\tLR: 0.030000\n",
      "Train Epoch: 362 [24576/194182 (12%)]\tLoss: 0.544952\tGrad Norm: 0.905429\tLR: 0.030000\n",
      "Train Epoch: 362 [45056/194182 (23%)]\tLoss: 0.538154\tGrad Norm: 0.910623\tLR: 0.030000\n",
      "Train Epoch: 362 [65536/194182 (33%)]\tLoss: 0.543237\tGrad Norm: 1.269217\tLR: 0.030000\n",
      "Train Epoch: 362 [86016/194182 (44%)]\tLoss: 0.552684\tGrad Norm: 1.452027\tLR: 0.030000\n",
      "Train Epoch: 362 [106496/194182 (54%)]\tLoss: 0.544751\tGrad Norm: 1.299379\tLR: 0.030000\n",
      "Train Epoch: 362 [126976/194182 (65%)]\tLoss: 0.534640\tGrad Norm: 0.888875\tLR: 0.030000\n",
      "Train Epoch: 362 [147456/194182 (75%)]\tLoss: 0.540489\tGrad Norm: 1.230554\tLR: 0.030000\n",
      "Train Epoch: 362 [167936/194182 (85%)]\tLoss: 0.543633\tGrad Norm: 1.343106\tLR: 0.030000\n",
      "Train Epoch: 362 [188416/194182 (96%)]\tLoss: 0.552140\tGrad Norm: 1.264499\tLR: 0.030000\n",
      "Train set: Average loss: 0.5443\n",
      "Test set: Average loss: 0.2692, Average MAE: 0.3612\n",
      "Train Epoch: 363 [4096/194182 (2%)]\tLoss: 0.545370\tGrad Norm: 1.358899\tLR: 0.030000\n",
      "Train Epoch: 363 [24576/194182 (12%)]\tLoss: 0.547994\tGrad Norm: 1.325998\tLR: 0.030000\n",
      "Train Epoch: 363 [45056/194182 (23%)]\tLoss: 0.543757\tGrad Norm: 1.306899\tLR: 0.030000\n",
      "Train Epoch: 363 [65536/194182 (33%)]\tLoss: 0.541341\tGrad Norm: 1.086960\tLR: 0.030000\n",
      "Train Epoch: 363 [86016/194182 (44%)]\tLoss: 0.548293\tGrad Norm: 1.203999\tLR: 0.030000\n",
      "Train Epoch: 363 [106496/194182 (54%)]\tLoss: 0.533846\tGrad Norm: 1.015452\tLR: 0.030000\n",
      "Train Epoch: 363 [126976/194182 (65%)]\tLoss: 0.540066\tGrad Norm: 0.687444\tLR: 0.030000\n",
      "Train Epoch: 363 [147456/194182 (75%)]\tLoss: 0.543900\tGrad Norm: 0.801663\tLR: 0.030000\n",
      "Train Epoch: 363 [167936/194182 (85%)]\tLoss: 0.538309\tGrad Norm: 0.485380\tLR: 0.030000\n",
      "Train Epoch: 363 [188416/194182 (96%)]\tLoss: 0.531128\tGrad Norm: 0.645886\tLR: 0.030000\n",
      "Train set: Average loss: 0.5414\n",
      "Test set: Average loss: 0.2618, Average MAE: 0.3519\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 364 [4096/194182 (2%)]\tLoss: 0.540687\tGrad Norm: 0.921657\tLR: 0.030000\n",
      "Train Epoch: 364 [24576/194182 (12%)]\tLoss: 0.537750\tGrad Norm: 1.213575\tLR: 0.030000\n",
      "Train Epoch: 364 [45056/194182 (23%)]\tLoss: 0.542974\tGrad Norm: 1.409034\tLR: 0.030000\n",
      "Train Epoch: 364 [65536/194182 (33%)]\tLoss: 0.546426\tGrad Norm: 1.585039\tLR: 0.030000\n",
      "Train Epoch: 364 [86016/194182 (44%)]\tLoss: 0.536993\tGrad Norm: 0.966724\tLR: 0.030000\n",
      "Train Epoch: 364 [106496/194182 (54%)]\tLoss: 0.538976\tGrad Norm: 0.783673\tLR: 0.030000\n",
      "Train Epoch: 364 [126976/194182 (65%)]\tLoss: 0.547964\tGrad Norm: 1.243750\tLR: 0.030000\n",
      "Train Epoch: 364 [147456/194182 (75%)]\tLoss: 0.543001\tGrad Norm: 1.215886\tLR: 0.030000\n",
      "Train Epoch: 364 [167936/194182 (85%)]\tLoss: 0.544110\tGrad Norm: 1.348086\tLR: 0.030000\n",
      "Train Epoch: 364 [188416/194182 (96%)]\tLoss: 0.550234\tGrad Norm: 1.576994\tLR: 0.030000\n",
      "Train set: Average loss: 0.5440\n",
      "Test set: Average loss: 0.2726, Average MAE: 0.3663\n",
      "Train Epoch: 365 [4096/194182 (2%)]\tLoss: 0.549980\tGrad Norm: 1.614758\tLR: 0.030000\n",
      "Train Epoch: 365 [24576/194182 (12%)]\tLoss: 0.536414\tGrad Norm: 1.152200\tLR: 0.030000\n",
      "Train Epoch: 365 [45056/194182 (23%)]\tLoss: 0.538478\tGrad Norm: 1.062048\tLR: 0.030000\n",
      "Train Epoch: 365 [65536/194182 (33%)]\tLoss: 0.541727\tGrad Norm: 1.228648\tLR: 0.030000\n",
      "Train Epoch: 365 [86016/194182 (44%)]\tLoss: 0.543756\tGrad Norm: 1.236474\tLR: 0.030000\n",
      "Train Epoch: 365 [106496/194182 (54%)]\tLoss: 0.541372\tGrad Norm: 1.365111\tLR: 0.030000\n",
      "Train Epoch: 365 [126976/194182 (65%)]\tLoss: 0.546717\tGrad Norm: 1.217638\tLR: 0.030000\n",
      "Train Epoch: 365 [147456/194182 (75%)]\tLoss: 0.534298\tGrad Norm: 0.817947\tLR: 0.030000\n",
      "Train Epoch: 365 [167936/194182 (85%)]\tLoss: 0.532667\tGrad Norm: 0.875718\tLR: 0.030000\n",
      "Train Epoch: 365 [188416/194182 (96%)]\tLoss: 0.539342\tGrad Norm: 0.898054\tLR: 0.030000\n",
      "Train set: Average loss: 0.5421\n",
      "Test set: Average loss: 0.2621, Average MAE: 0.3595\n",
      "Epoch 365: Mean reward = 0.036 +/- 0.018\n",
      "Train Epoch: 366 [4096/194182 (2%)]\tLoss: 0.531887\tGrad Norm: 0.928111\tLR: 0.030000\n",
      "Train Epoch: 366 [24576/194182 (12%)]\tLoss: 0.539095\tGrad Norm: 0.813079\tLR: 0.030000\n",
      "Train Epoch: 366 [45056/194182 (23%)]\tLoss: 0.537174\tGrad Norm: 1.252584\tLR: 0.030000\n",
      "Train Epoch: 366 [65536/194182 (33%)]\tLoss: 0.546500\tGrad Norm: 1.305202\tLR: 0.030000\n",
      "Train Epoch: 366 [86016/194182 (44%)]\tLoss: 0.539100\tGrad Norm: 0.863358\tLR: 0.030000\n",
      "Train Epoch: 366 [106496/194182 (54%)]\tLoss: 0.543600\tGrad Norm: 1.160760\tLR: 0.030000\n",
      "Train Epoch: 366 [126976/194182 (65%)]\tLoss: 0.532915\tGrad Norm: 1.251760\tLR: 0.030000\n",
      "Train Epoch: 366 [147456/194182 (75%)]\tLoss: 0.543676\tGrad Norm: 1.128966\tLR: 0.030000\n",
      "Train Epoch: 366 [167936/194182 (85%)]\tLoss: 0.541726\tGrad Norm: 1.169461\tLR: 0.030000\n",
      "Train Epoch: 366 [188416/194182 (96%)]\tLoss: 0.532272\tGrad Norm: 0.953782\tLR: 0.030000\n",
      "Train set: Average loss: 0.5406\n",
      "Test set: Average loss: 0.2653, Average MAE: 0.3611\n",
      "Train Epoch: 367 [4096/194182 (2%)]\tLoss: 0.542127\tGrad Norm: 1.184691\tLR: 0.030000\n",
      "Train Epoch: 367 [24576/194182 (12%)]\tLoss: 0.551779\tGrad Norm: 1.293354\tLR: 0.030000\n",
      "Train Epoch: 367 [45056/194182 (23%)]\tLoss: 0.534507\tGrad Norm: 1.015469\tLR: 0.030000\n",
      "Train Epoch: 367 [65536/194182 (33%)]\tLoss: 0.543904\tGrad Norm: 0.773772\tLR: 0.030000\n",
      "Train Epoch: 367 [86016/194182 (44%)]\tLoss: 0.538105\tGrad Norm: 0.852771\tLR: 0.030000\n",
      "Train Epoch: 367 [106496/194182 (54%)]\tLoss: 0.531105\tGrad Norm: 0.837355\tLR: 0.030000\n",
      "Train Epoch: 367 [126976/194182 (65%)]\tLoss: 0.533989\tGrad Norm: 1.061292\tLR: 0.030000\n",
      "Train Epoch: 367 [147456/194182 (75%)]\tLoss: 0.535864\tGrad Norm: 1.243747\tLR: 0.030000\n",
      "Train Epoch: 367 [167936/194182 (85%)]\tLoss: 0.545184\tGrad Norm: 1.106230\tLR: 0.030000\n",
      "Train Epoch: 367 [188416/194182 (96%)]\tLoss: 0.536262\tGrad Norm: 1.090236\tLR: 0.030000\n",
      "Train set: Average loss: 0.5396\n",
      "Test set: Average loss: 0.2667, Average MAE: 0.3786\n",
      "Train Epoch: 368 [4096/194182 (2%)]\tLoss: 0.549463\tGrad Norm: 1.170626\tLR: 0.030000\n",
      "Train Epoch: 368 [24576/194182 (12%)]\tLoss: 0.531667\tGrad Norm: 1.129571\tLR: 0.030000\n",
      "Train Epoch: 368 [45056/194182 (23%)]\tLoss: 0.536437\tGrad Norm: 0.861420\tLR: 0.030000\n",
      "Train Epoch: 368 [65536/194182 (33%)]\tLoss: 0.545544\tGrad Norm: 1.284301\tLR: 0.030000\n",
      "Train Epoch: 368 [86016/194182 (44%)]\tLoss: 0.554719\tGrad Norm: 1.584495\tLR: 0.030000\n",
      "Train Epoch: 368 [106496/194182 (54%)]\tLoss: 0.543307\tGrad Norm: 0.951978\tLR: 0.030000\n",
      "Train Epoch: 368 [126976/194182 (65%)]\tLoss: 0.556600\tGrad Norm: 1.482967\tLR: 0.030000\n",
      "Train Epoch: 368 [147456/194182 (75%)]\tLoss: 0.532640\tGrad Norm: 1.094533\tLR: 0.030000\n",
      "Train Epoch: 368 [167936/194182 (85%)]\tLoss: 0.536016\tGrad Norm: 1.004035\tLR: 0.030000\n",
      "Train Epoch: 368 [188416/194182 (96%)]\tLoss: 0.546782\tGrad Norm: 0.939163\tLR: 0.030000\n",
      "Train set: Average loss: 0.5404\n",
      "Test set: Average loss: 0.2644, Average MAE: 0.3747\n",
      "Train Epoch: 369 [4096/194182 (2%)]\tLoss: 0.536114\tGrad Norm: 1.094125\tLR: 0.030000\n",
      "Train Epoch: 369 [24576/194182 (12%)]\tLoss: 0.547744\tGrad Norm: 0.906183\tLR: 0.030000\n",
      "Train Epoch: 369 [45056/194182 (23%)]\tLoss: 0.534295\tGrad Norm: 0.840773\tLR: 0.030000\n",
      "Train Epoch: 369 [65536/194182 (33%)]\tLoss: 0.531358\tGrad Norm: 0.927568\tLR: 0.030000\n",
      "Train Epoch: 369 [86016/194182 (44%)]\tLoss: 0.542233\tGrad Norm: 1.235280\tLR: 0.030000\n",
      "Train Epoch: 369 [106496/194182 (54%)]\tLoss: 0.539343\tGrad Norm: 1.286839\tLR: 0.030000\n",
      "Train Epoch: 369 [126976/194182 (65%)]\tLoss: 0.536061\tGrad Norm: 1.043319\tLR: 0.030000\n",
      "Train Epoch: 369 [147456/194182 (75%)]\tLoss: 0.537017\tGrad Norm: 1.016434\tLR: 0.030000\n",
      "Train Epoch: 369 [167936/194182 (85%)]\tLoss: 0.536078\tGrad Norm: 1.318165\tLR: 0.030000\n",
      "Train Epoch: 369 [188416/194182 (96%)]\tLoss: 0.541045\tGrad Norm: 1.297084\tLR: 0.030000\n",
      "Train set: Average loss: 0.5388\n",
      "Test set: Average loss: 0.2663, Average MAE: 0.3631\n",
      "Train Epoch: 370 [4096/194182 (2%)]\tLoss: 0.537672\tGrad Norm: 1.178615\tLR: 0.030000\n",
      "Train Epoch: 370 [24576/194182 (12%)]\tLoss: 0.534025\tGrad Norm: 1.264964\tLR: 0.030000\n",
      "Train Epoch: 370 [45056/194182 (23%)]\tLoss: 0.545771\tGrad Norm: 1.328612\tLR: 0.030000\n",
      "Train Epoch: 370 [65536/194182 (33%)]\tLoss: 0.530976\tGrad Norm: 1.099604\tLR: 0.030000\n",
      "Train Epoch: 370 [86016/194182 (44%)]\tLoss: 0.538877\tGrad Norm: 1.198395\tLR: 0.030000\n",
      "Train Epoch: 370 [106496/194182 (54%)]\tLoss: 0.547828\tGrad Norm: 1.296210\tLR: 0.030000\n",
      "Train Epoch: 370 [126976/194182 (65%)]\tLoss: 0.532167\tGrad Norm: 1.207182\tLR: 0.030000\n",
      "Train Epoch: 370 [147456/194182 (75%)]\tLoss: 0.534299\tGrad Norm: 1.153386\tLR: 0.030000\n",
      "Train Epoch: 370 [167936/194182 (85%)]\tLoss: 0.538630\tGrad Norm: 1.109492\tLR: 0.030000\n",
      "Train Epoch: 370 [188416/194182 (96%)]\tLoss: 0.535041\tGrad Norm: 0.899816\tLR: 0.030000\n",
      "Train set: Average loss: 0.5391\n",
      "Test set: Average loss: 0.2600, Average MAE: 0.3674\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 370: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 371 [4096/194182 (2%)]\tLoss: 0.533927\tGrad Norm: 0.773415\tLR: 0.030000\n",
      "Train Epoch: 371 [24576/194182 (12%)]\tLoss: 0.540873\tGrad Norm: 0.996298\tLR: 0.030000\n",
      "Train Epoch: 371 [45056/194182 (23%)]\tLoss: 0.534311\tGrad Norm: 0.836886\tLR: 0.030000\n",
      "Train Epoch: 371 [65536/194182 (33%)]\tLoss: 0.526126\tGrad Norm: 0.900004\tLR: 0.030000\n",
      "Train Epoch: 371 [86016/194182 (44%)]\tLoss: 0.539351\tGrad Norm: 1.315563\tLR: 0.030000\n",
      "Train Epoch: 371 [106496/194182 (54%)]\tLoss: 0.542594\tGrad Norm: 1.266926\tLR: 0.030000\n",
      "Train Epoch: 371 [126976/194182 (65%)]\tLoss: 0.539226\tGrad Norm: 1.407660\tLR: 0.030000\n",
      "Train Epoch: 371 [147456/194182 (75%)]\tLoss: 0.546230\tGrad Norm: 1.268304\tLR: 0.030000\n",
      "Train Epoch: 371 [167936/194182 (85%)]\tLoss: 0.536730\tGrad Norm: 1.004546\tLR: 0.030000\n",
      "Train Epoch: 371 [188416/194182 (96%)]\tLoss: 0.538177\tGrad Norm: 1.226544\tLR: 0.030000\n",
      "Train set: Average loss: 0.5375\n",
      "Test set: Average loss: 0.2697, Average MAE: 0.3866\n",
      "Train Epoch: 372 [4096/194182 (2%)]\tLoss: 0.542339\tGrad Norm: 1.307530\tLR: 0.030000\n",
      "Train Epoch: 372 [24576/194182 (12%)]\tLoss: 0.532201\tGrad Norm: 0.909617\tLR: 0.030000\n",
      "Train Epoch: 372 [45056/194182 (23%)]\tLoss: 0.532087\tGrad Norm: 0.813858\tLR: 0.030000\n",
      "Train Epoch: 372 [65536/194182 (33%)]\tLoss: 0.537652\tGrad Norm: 1.165511\tLR: 0.030000\n",
      "Train Epoch: 372 [86016/194182 (44%)]\tLoss: 0.534025\tGrad Norm: 0.980618\tLR: 0.030000\n",
      "Train Epoch: 372 [106496/194182 (54%)]\tLoss: 0.542673\tGrad Norm: 1.085404\tLR: 0.030000\n",
      "Train Epoch: 372 [126976/194182 (65%)]\tLoss: 0.538426\tGrad Norm: 1.334941\tLR: 0.030000\n",
      "Train Epoch: 372 [147456/194182 (75%)]\tLoss: 0.549433\tGrad Norm: 1.378710\tLR: 0.030000\n",
      "Train Epoch: 372 [167936/194182 (85%)]\tLoss: 0.540338\tGrad Norm: 1.201720\tLR: 0.030000\n",
      "Train Epoch: 372 [188416/194182 (96%)]\tLoss: 0.533508\tGrad Norm: 1.399399\tLR: 0.030000\n",
      "Train set: Average loss: 0.5369\n",
      "Test set: Average loss: 0.2686, Average MAE: 0.3805\n",
      "Train Epoch: 373 [4096/194182 (2%)]\tLoss: 0.542935\tGrad Norm: 1.484078\tLR: 0.030000\n",
      "Train Epoch: 373 [24576/194182 (12%)]\tLoss: 0.544751\tGrad Norm: 1.766730\tLR: 0.030000\n",
      "Train Epoch: 373 [45056/194182 (23%)]\tLoss: 0.537327\tGrad Norm: 1.430424\tLR: 0.030000\n",
      "Train Epoch: 373 [65536/194182 (33%)]\tLoss: 0.536437\tGrad Norm: 0.912194\tLR: 0.030000\n",
      "Train Epoch: 373 [86016/194182 (44%)]\tLoss: 0.540564\tGrad Norm: 1.022371\tLR: 0.030000\n",
      "Train Epoch: 373 [106496/194182 (54%)]\tLoss: 0.547007\tGrad Norm: 1.340732\tLR: 0.030000\n",
      "Train Epoch: 373 [126976/194182 (65%)]\tLoss: 0.535644\tGrad Norm: 1.189594\tLR: 0.030000\n",
      "Train Epoch: 373 [147456/194182 (75%)]\tLoss: 0.534328\tGrad Norm: 1.039402\tLR: 0.030000\n",
      "Train Epoch: 373 [167936/194182 (85%)]\tLoss: 0.536111\tGrad Norm: 1.179247\tLR: 0.030000\n",
      "Train Epoch: 373 [188416/194182 (96%)]\tLoss: 0.541113\tGrad Norm: 1.222783\tLR: 0.030000\n",
      "Train set: Average loss: 0.5392\n",
      "Test set: Average loss: 0.2658, Average MAE: 0.3728\n",
      "Train Epoch: 374 [4096/194182 (2%)]\tLoss: 0.541042\tGrad Norm: 1.259584\tLR: 0.030000\n",
      "Train Epoch: 374 [24576/194182 (12%)]\tLoss: 0.535239\tGrad Norm: 1.064628\tLR: 0.030000\n",
      "Train Epoch: 374 [45056/194182 (23%)]\tLoss: 0.533714\tGrad Norm: 1.053932\tLR: 0.030000\n",
      "Train Epoch: 374 [65536/194182 (33%)]\tLoss: 0.534888\tGrad Norm: 0.805594\tLR: 0.030000\n",
      "Train Epoch: 374 [86016/194182 (44%)]\tLoss: 0.535074\tGrad Norm: 0.855259\tLR: 0.030000\n",
      "Train Epoch: 374 [106496/194182 (54%)]\tLoss: 0.541094\tGrad Norm: 1.185651\tLR: 0.030000\n",
      "Train Epoch: 374 [126976/194182 (65%)]\tLoss: 0.537931\tGrad Norm: 0.880280\tLR: 0.030000\n",
      "Train Epoch: 374 [147456/194182 (75%)]\tLoss: 0.526017\tGrad Norm: 0.815295\tLR: 0.030000\n",
      "Train Epoch: 374 [167936/194182 (85%)]\tLoss: 0.526281\tGrad Norm: 0.839662\tLR: 0.030000\n",
      "Train Epoch: 374 [188416/194182 (96%)]\tLoss: 0.524781\tGrad Norm: 0.997953\tLR: 0.030000\n",
      "Train set: Average loss: 0.5336\n",
      "Test set: Average loss: 0.2713, Average MAE: 0.3853\n",
      "Train Epoch: 375 [4096/194182 (2%)]\tLoss: 0.537570\tGrad Norm: 1.542248\tLR: 0.030000\n",
      "Train Epoch: 375 [24576/194182 (12%)]\tLoss: 0.532705\tGrad Norm: 1.275988\tLR: 0.030000\n",
      "Train Epoch: 375 [45056/194182 (23%)]\tLoss: 0.537122\tGrad Norm: 1.255232\tLR: 0.030000\n",
      "Train Epoch: 375 [65536/194182 (33%)]\tLoss: 0.531370\tGrad Norm: 1.145160\tLR: 0.030000\n",
      "Train Epoch: 375 [86016/194182 (44%)]\tLoss: 0.535092\tGrad Norm: 0.897071\tLR: 0.030000\n",
      "Train Epoch: 375 [106496/194182 (54%)]\tLoss: 0.524750\tGrad Norm: 0.762173\tLR: 0.030000\n",
      "Train Epoch: 375 [126976/194182 (65%)]\tLoss: 0.532105\tGrad Norm: 0.901588\tLR: 0.030000\n",
      "Train Epoch: 375 [147456/194182 (75%)]\tLoss: 0.531335\tGrad Norm: 0.955259\tLR: 0.030000\n",
      "Train Epoch: 375 [167936/194182 (85%)]\tLoss: 0.545430\tGrad Norm: 1.322899\tLR: 0.030000\n",
      "Train Epoch: 375 [188416/194182 (96%)]\tLoss: 0.546452\tGrad Norm: 1.454582\tLR: 0.030000\n",
      "Train set: Average loss: 0.5360\n",
      "Test set: Average loss: 0.2722, Average MAE: 0.3918\n",
      "Epoch 375: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 376 [4096/194182 (2%)]\tLoss: 0.541150\tGrad Norm: 1.518013\tLR: 0.030000\n",
      "Train Epoch: 376 [24576/194182 (12%)]\tLoss: 0.528707\tGrad Norm: 1.187109\tLR: 0.030000\n",
      "Train Epoch: 376 [45056/194182 (23%)]\tLoss: 0.542757\tGrad Norm: 1.260184\tLR: 0.030000\n",
      "Train Epoch: 376 [65536/194182 (33%)]\tLoss: 0.536568\tGrad Norm: 1.024574\tLR: 0.030000\n",
      "Train Epoch: 376 [86016/194182 (44%)]\tLoss: 0.535356\tGrad Norm: 0.902259\tLR: 0.030000\n",
      "Train Epoch: 376 [106496/194182 (54%)]\tLoss: 0.544346\tGrad Norm: 1.142682\tLR: 0.030000\n",
      "Train Epoch: 376 [126976/194182 (65%)]\tLoss: 0.532804\tGrad Norm: 0.840723\tLR: 0.030000\n",
      "Train Epoch: 376 [147456/194182 (75%)]\tLoss: 0.534285\tGrad Norm: 0.974224\tLR: 0.030000\n",
      "Train Epoch: 376 [167936/194182 (85%)]\tLoss: 0.524881\tGrad Norm: 1.455190\tLR: 0.030000\n",
      "Train Epoch: 376 [188416/194182 (96%)]\tLoss: 0.543966\tGrad Norm: 1.573998\tLR: 0.030000\n",
      "Train set: Average loss: 0.5355\n",
      "Test set: Average loss: 0.2652, Average MAE: 0.3702\n",
      "Train Epoch: 377 [4096/194182 (2%)]\tLoss: 0.547858\tGrad Norm: 1.548499\tLR: 0.030000\n",
      "Train Epoch: 377 [24576/194182 (12%)]\tLoss: 0.533919\tGrad Norm: 1.234065\tLR: 0.030000\n",
      "Train Epoch: 377 [45056/194182 (23%)]\tLoss: 0.536492\tGrad Norm: 1.295024\tLR: 0.030000\n",
      "Train Epoch: 377 [65536/194182 (33%)]\tLoss: 0.533933\tGrad Norm: 0.999386\tLR: 0.030000\n",
      "Train Epoch: 377 [86016/194182 (44%)]\tLoss: 0.531391\tGrad Norm: 1.163022\tLR: 0.030000\n",
      "Train Epoch: 377 [106496/194182 (54%)]\tLoss: 0.532573\tGrad Norm: 1.268449\tLR: 0.030000\n",
      "Train Epoch: 377 [126976/194182 (65%)]\tLoss: 0.535237\tGrad Norm: 1.313540\tLR: 0.030000\n",
      "Train Epoch: 377 [147456/194182 (75%)]\tLoss: 0.535617\tGrad Norm: 1.456019\tLR: 0.030000\n",
      "Train Epoch: 377 [167936/194182 (85%)]\tLoss: 0.530488\tGrad Norm: 0.953619\tLR: 0.030000\n",
      "Train Epoch: 377 [188416/194182 (96%)]\tLoss: 0.537581\tGrad Norm: 1.086707\tLR: 0.030000\n",
      "Train set: Average loss: 0.5357\n",
      "Test set: Average loss: 0.2619, Average MAE: 0.3533\n",
      "Train Epoch: 378 [4096/194182 (2%)]\tLoss: 0.530330\tGrad Norm: 1.146018\tLR: 0.030000\n",
      "Train Epoch: 378 [24576/194182 (12%)]\tLoss: 0.536558\tGrad Norm: 1.061067\tLR: 0.030000\n",
      "Train Epoch: 378 [45056/194182 (23%)]\tLoss: 0.534352\tGrad Norm: 1.114226\tLR: 0.030000\n",
      "Train Epoch: 378 [65536/194182 (33%)]\tLoss: 0.540773\tGrad Norm: 1.269605\tLR: 0.030000\n",
      "Train Epoch: 378 [86016/194182 (44%)]\tLoss: 0.529293\tGrad Norm: 1.006926\tLR: 0.030000\n",
      "Train Epoch: 378 [106496/194182 (54%)]\tLoss: 0.528977\tGrad Norm: 0.937845\tLR: 0.030000\n",
      "Train Epoch: 378 [126976/194182 (65%)]\tLoss: 0.526731\tGrad Norm: 0.942968\tLR: 0.030000\n",
      "Train Epoch: 378 [147456/194182 (75%)]\tLoss: 0.535265\tGrad Norm: 1.120396\tLR: 0.030000\n",
      "Train Epoch: 378 [167936/194182 (85%)]\tLoss: 0.522945\tGrad Norm: 0.781277\tLR: 0.030000\n",
      "Train Epoch: 378 [188416/194182 (96%)]\tLoss: 0.527560\tGrad Norm: 1.259785\tLR: 0.030000\n",
      "Train set: Average loss: 0.5321\n",
      "Test set: Average loss: 0.2641, Average MAE: 0.3620\n",
      "Train Epoch: 379 [4096/194182 (2%)]\tLoss: 0.535826\tGrad Norm: 1.176267\tLR: 0.030000\n",
      "Train Epoch: 379 [24576/194182 (12%)]\tLoss: 0.526367\tGrad Norm: 0.902141\tLR: 0.030000\n",
      "Train Epoch: 379 [45056/194182 (23%)]\tLoss: 0.529160\tGrad Norm: 1.196702\tLR: 0.030000\n",
      "Train Epoch: 379 [65536/194182 (33%)]\tLoss: 0.540807\tGrad Norm: 1.237859\tLR: 0.030000\n",
      "Train Epoch: 379 [86016/194182 (44%)]\tLoss: 0.543824\tGrad Norm: 1.628447\tLR: 0.030000\n",
      "Train Epoch: 379 [106496/194182 (54%)]\tLoss: 0.533891\tGrad Norm: 1.205721\tLR: 0.030000\n",
      "Train Epoch: 379 [126976/194182 (65%)]\tLoss: 0.521230\tGrad Norm: 0.749652\tLR: 0.030000\n",
      "Train Epoch: 379 [147456/194182 (75%)]\tLoss: 0.521433\tGrad Norm: 0.611226\tLR: 0.030000\n",
      "Train Epoch: 379 [167936/194182 (85%)]\tLoss: 0.525962\tGrad Norm: 0.827211\tLR: 0.030000\n",
      "Train Epoch: 379 [188416/194182 (96%)]\tLoss: 0.522840\tGrad Norm: 0.780711\tLR: 0.030000\n",
      "Train set: Average loss: 0.5316\n",
      "Test set: Average loss: 0.2579, Average MAE: 0.3531\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 380 [4096/194182 (2%)]\tLoss: 0.534568\tGrad Norm: 0.757310\tLR: 0.030000\n",
      "Train Epoch: 380 [24576/194182 (12%)]\tLoss: 0.534435\tGrad Norm: 1.064163\tLR: 0.030000\n",
      "Train Epoch: 380 [45056/194182 (23%)]\tLoss: 0.528540\tGrad Norm: 1.024193\tLR: 0.030000\n",
      "Train Epoch: 380 [65536/194182 (33%)]\tLoss: 0.542457\tGrad Norm: 1.180226\tLR: 0.030000\n",
      "Train Epoch: 380 [86016/194182 (44%)]\tLoss: 0.530985\tGrad Norm: 1.133623\tLR: 0.030000\n",
      "Train Epoch: 380 [106496/194182 (54%)]\tLoss: 0.533586\tGrad Norm: 1.119512\tLR: 0.030000\n",
      "Train Epoch: 380 [126976/194182 (65%)]\tLoss: 0.530364\tGrad Norm: 0.995297\tLR: 0.030000\n",
      "Train Epoch: 380 [147456/194182 (75%)]\tLoss: 0.537433\tGrad Norm: 1.165388\tLR: 0.030000\n",
      "Train Epoch: 380 [167936/194182 (85%)]\tLoss: 0.527007\tGrad Norm: 1.236808\tLR: 0.030000\n",
      "Train Epoch: 380 [188416/194182 (96%)]\tLoss: 0.542523\tGrad Norm: 1.308019\tLR: 0.030000\n",
      "Train set: Average loss: 0.5317\n",
      "Test set: Average loss: 0.2591, Average MAE: 0.3557\n",
      "Epoch 380: Mean reward = 0.036 +/- 0.018\n",
      "Train Epoch: 381 [4096/194182 (2%)]\tLoss: 0.533504\tGrad Norm: 1.140196\tLR: 0.030000\n",
      "Train Epoch: 381 [24576/194182 (12%)]\tLoss: 0.530381\tGrad Norm: 0.846731\tLR: 0.030000\n",
      "Train Epoch: 381 [45056/194182 (23%)]\tLoss: 0.535520\tGrad Norm: 0.913501\tLR: 0.030000\n",
      "Train Epoch: 381 [65536/194182 (33%)]\tLoss: 0.533942\tGrad Norm: 0.824373\tLR: 0.030000\n",
      "Train Epoch: 381 [86016/194182 (44%)]\tLoss: 0.527041\tGrad Norm: 0.974178\tLR: 0.030000\n",
      "Train Epoch: 381 [106496/194182 (54%)]\tLoss: 0.531384\tGrad Norm: 1.251236\tLR: 0.030000\n",
      "Train Epoch: 381 [126976/194182 (65%)]\tLoss: 0.537333\tGrad Norm: 1.486049\tLR: 0.030000\n",
      "Train Epoch: 381 [147456/194182 (75%)]\tLoss: 0.535138\tGrad Norm: 1.385101\tLR: 0.030000\n",
      "Train Epoch: 381 [167936/194182 (85%)]\tLoss: 0.531231\tGrad Norm: 1.321797\tLR: 0.030000\n",
      "Train Epoch: 381 [188416/194182 (96%)]\tLoss: 0.540279\tGrad Norm: 1.632076\tLR: 0.030000\n",
      "Train set: Average loss: 0.5328\n",
      "Test set: Average loss: 0.2610, Average MAE: 0.3524\n",
      "Train Epoch: 382 [4096/194182 (2%)]\tLoss: 0.525229\tGrad Norm: 0.983511\tLR: 0.030000\n",
      "Train Epoch: 382 [24576/194182 (12%)]\tLoss: 0.526122\tGrad Norm: 0.631625\tLR: 0.030000\n",
      "Train Epoch: 382 [45056/194182 (23%)]\tLoss: 0.528915\tGrad Norm: 0.683009\tLR: 0.030000\n",
      "Train Epoch: 382 [65536/194182 (33%)]\tLoss: 0.520035\tGrad Norm: 1.021493\tLR: 0.030000\n",
      "Train Epoch: 382 [86016/194182 (44%)]\tLoss: 0.526376\tGrad Norm: 0.956645\tLR: 0.030000\n",
      "Train Epoch: 382 [106496/194182 (54%)]\tLoss: 0.540122\tGrad Norm: 1.342899\tLR: 0.030000\n",
      "Train Epoch: 382 [126976/194182 (65%)]\tLoss: 0.532883\tGrad Norm: 1.245769\tLR: 0.030000\n",
      "Train Epoch: 382 [147456/194182 (75%)]\tLoss: 0.527614\tGrad Norm: 0.966967\tLR: 0.030000\n",
      "Train Epoch: 382 [167936/194182 (85%)]\tLoss: 0.533878\tGrad Norm: 1.167917\tLR: 0.030000\n",
      "Train Epoch: 382 [188416/194182 (96%)]\tLoss: 0.534300\tGrad Norm: 1.103387\tLR: 0.030000\n",
      "Train set: Average loss: 0.5297\n",
      "Test set: Average loss: 0.2599, Average MAE: 0.3551\n",
      "Train Epoch: 383 [4096/194182 (2%)]\tLoss: 0.529046\tGrad Norm: 0.984920\tLR: 0.030000\n",
      "Train Epoch: 383 [24576/194182 (12%)]\tLoss: 0.522966\tGrad Norm: 1.021414\tLR: 0.030000\n",
      "Train Epoch: 383 [45056/194182 (23%)]\tLoss: 0.526098\tGrad Norm: 0.937732\tLR: 0.030000\n",
      "Train Epoch: 383 [65536/194182 (33%)]\tLoss: 0.523876\tGrad Norm: 1.119092\tLR: 0.030000\n",
      "Train Epoch: 383 [86016/194182 (44%)]\tLoss: 0.529262\tGrad Norm: 1.273246\tLR: 0.030000\n",
      "Train Epoch: 383 [106496/194182 (54%)]\tLoss: 0.528717\tGrad Norm: 1.446014\tLR: 0.030000\n",
      "Train Epoch: 383 [126976/194182 (65%)]\tLoss: 0.537611\tGrad Norm: 1.446190\tLR: 0.030000\n",
      "Train Epoch: 383 [147456/194182 (75%)]\tLoss: 0.538476\tGrad Norm: 1.181017\tLR: 0.030000\n",
      "Train Epoch: 383 [167936/194182 (85%)]\tLoss: 0.523025\tGrad Norm: 1.061051\tLR: 0.030000\n",
      "Train Epoch: 383 [188416/194182 (96%)]\tLoss: 0.526573\tGrad Norm: 1.060922\tLR: 0.030000\n",
      "Train set: Average loss: 0.5309\n",
      "Test set: Average loss: 0.2643, Average MAE: 0.3662\n",
      "Train Epoch: 384 [4096/194182 (2%)]\tLoss: 0.525009\tGrad Norm: 1.140513\tLR: 0.030000\n",
      "Train Epoch: 384 [24576/194182 (12%)]\tLoss: 0.524815\tGrad Norm: 1.047998\tLR: 0.030000\n",
      "Train Epoch: 384 [45056/194182 (23%)]\tLoss: 0.521810\tGrad Norm: 1.103616\tLR: 0.030000\n",
      "Train Epoch: 384 [65536/194182 (33%)]\tLoss: 0.537018\tGrad Norm: 1.455774\tLR: 0.030000\n",
      "Train Epoch: 384 [86016/194182 (44%)]\tLoss: 0.529366\tGrad Norm: 0.960425\tLR: 0.030000\n",
      "Train Epoch: 384 [106496/194182 (54%)]\tLoss: 0.525197\tGrad Norm: 0.933760\tLR: 0.030000\n",
      "Train Epoch: 384 [126976/194182 (65%)]\tLoss: 0.524271\tGrad Norm: 0.850529\tLR: 0.030000\n",
      "Train Epoch: 384 [147456/194182 (75%)]\tLoss: 0.526101\tGrad Norm: 1.070891\tLR: 0.030000\n",
      "Train Epoch: 384 [167936/194182 (85%)]\tLoss: 0.536653\tGrad Norm: 1.310368\tLR: 0.030000\n",
      "Train Epoch: 384 [188416/194182 (96%)]\tLoss: 0.534515\tGrad Norm: 1.366125\tLR: 0.030000\n",
      "Train set: Average loss: 0.5295\n",
      "Test set: Average loss: 0.2608, Average MAE: 0.3739\n",
      "Train Epoch: 385 [4096/194182 (2%)]\tLoss: 0.527361\tGrad Norm: 1.051350\tLR: 0.030000\n",
      "Train Epoch: 385 [24576/194182 (12%)]\tLoss: 0.537408\tGrad Norm: 1.242561\tLR: 0.030000\n",
      "Train Epoch: 385 [45056/194182 (23%)]\tLoss: 0.530810\tGrad Norm: 0.983831\tLR: 0.030000\n",
      "Train Epoch: 385 [65536/194182 (33%)]\tLoss: 0.529251\tGrad Norm: 1.207487\tLR: 0.030000\n",
      "Train Epoch: 385 [86016/194182 (44%)]\tLoss: 0.526634\tGrad Norm: 1.217727\tLR: 0.030000\n",
      "Train Epoch: 385 [106496/194182 (54%)]\tLoss: 0.528326\tGrad Norm: 1.083901\tLR: 0.030000\n",
      "Train Epoch: 385 [126976/194182 (65%)]\tLoss: 0.520977\tGrad Norm: 0.930499\tLR: 0.030000\n",
      "Train Epoch: 385 [147456/194182 (75%)]\tLoss: 0.525172\tGrad Norm: 1.055093\tLR: 0.030000\n",
      "Train Epoch: 385 [167936/194182 (85%)]\tLoss: 0.522781\tGrad Norm: 0.953655\tLR: 0.030000\n",
      "Train Epoch: 385 [188416/194182 (96%)]\tLoss: 0.529921\tGrad Norm: 1.101752\tLR: 0.030000\n",
      "Train set: Average loss: 0.5279\n",
      "Test set: Average loss: 0.2626, Average MAE: 0.3766\n",
      "Epoch 385: Mean reward = 0.045 +/- 0.001\n",
      "Train Epoch: 386 [4096/194182 (2%)]\tLoss: 0.525069\tGrad Norm: 1.269798\tLR: 0.030000\n",
      "Train Epoch: 386 [24576/194182 (12%)]\tLoss: 0.529108\tGrad Norm: 1.246329\tLR: 0.030000\n",
      "Train Epoch: 386 [45056/194182 (23%)]\tLoss: 0.527863\tGrad Norm: 1.285108\tLR: 0.030000\n",
      "Train Epoch: 386 [65536/194182 (33%)]\tLoss: 0.525844\tGrad Norm: 1.085347\tLR: 0.030000\n",
      "Train Epoch: 386 [86016/194182 (44%)]\tLoss: 0.523495\tGrad Norm: 0.872082\tLR: 0.030000\n",
      "Train Epoch: 386 [106496/194182 (54%)]\tLoss: 0.531354\tGrad Norm: 1.014979\tLR: 0.030000\n",
      "Train Epoch: 386 [126976/194182 (65%)]\tLoss: 0.533659\tGrad Norm: 1.035340\tLR: 0.030000\n",
      "Train Epoch: 386 [147456/194182 (75%)]\tLoss: 0.529383\tGrad Norm: 1.121106\tLR: 0.030000\n",
      "Train Epoch: 386 [167936/194182 (85%)]\tLoss: 0.526210\tGrad Norm: 0.981558\tLR: 0.030000\n",
      "Train Epoch: 386 [188416/194182 (96%)]\tLoss: 0.537408\tGrad Norm: 1.304545\tLR: 0.030000\n",
      "Train set: Average loss: 0.5281\n",
      "Test set: Average loss: 0.2622, Average MAE: 0.3726\n",
      "Train Epoch: 387 [4096/194182 (2%)]\tLoss: 0.534578\tGrad Norm: 1.124601\tLR: 0.030000\n",
      "Train Epoch: 387 [24576/194182 (12%)]\tLoss: 0.527091\tGrad Norm: 1.058308\tLR: 0.030000\n",
      "Train Epoch: 387 [45056/194182 (23%)]\tLoss: 0.526984\tGrad Norm: 1.208183\tLR: 0.030000\n",
      "Train Epoch: 387 [65536/194182 (33%)]\tLoss: 0.527076\tGrad Norm: 0.901857\tLR: 0.030000\n",
      "Train Epoch: 387 [86016/194182 (44%)]\tLoss: 0.525749\tGrad Norm: 1.207145\tLR: 0.030000\n",
      "Train Epoch: 387 [106496/194182 (54%)]\tLoss: 0.529644\tGrad Norm: 1.556324\tLR: 0.030000\n",
      "Train Epoch: 387 [126976/194182 (65%)]\tLoss: 0.524564\tGrad Norm: 1.347520\tLR: 0.030000\n",
      "Train Epoch: 387 [147456/194182 (75%)]\tLoss: 0.536853\tGrad Norm: 1.692302\tLR: 0.030000\n",
      "Train Epoch: 387 [167936/194182 (85%)]\tLoss: 0.537243\tGrad Norm: 1.411215\tLR: 0.030000\n",
      "Train Epoch: 387 [188416/194182 (96%)]\tLoss: 0.525776\tGrad Norm: 1.379677\tLR: 0.030000\n",
      "Train set: Average loss: 0.5296\n",
      "Test set: Average loss: 0.2608, Average MAE: 0.3504\n",
      "Train Epoch: 388 [4096/194182 (2%)]\tLoss: 0.528450\tGrad Norm: 1.148755\tLR: 0.030000\n",
      "Train Epoch: 388 [24576/194182 (12%)]\tLoss: 0.526164\tGrad Norm: 0.939193\tLR: 0.030000\n",
      "Train Epoch: 388 [45056/194182 (23%)]\tLoss: 0.516862\tGrad Norm: 0.803504\tLR: 0.030000\n",
      "Train Epoch: 388 [65536/194182 (33%)]\tLoss: 0.519733\tGrad Norm: 0.732029\tLR: 0.030000\n",
      "Train Epoch: 388 [86016/194182 (44%)]\tLoss: 0.533136\tGrad Norm: 1.291880\tLR: 0.030000\n",
      "Train Epoch: 388 [106496/194182 (54%)]\tLoss: 0.535903\tGrad Norm: 1.195451\tLR: 0.030000\n",
      "Train Epoch: 388 [126976/194182 (65%)]\tLoss: 0.537332\tGrad Norm: 1.321009\tLR: 0.030000\n",
      "Train Epoch: 388 [147456/194182 (75%)]\tLoss: 0.522578\tGrad Norm: 0.851682\tLR: 0.030000\n",
      "Train Epoch: 388 [167936/194182 (85%)]\tLoss: 0.528848\tGrad Norm: 1.069498\tLR: 0.030000\n",
      "Train Epoch: 388 [188416/194182 (96%)]\tLoss: 0.526395\tGrad Norm: 0.878821\tLR: 0.030000\n",
      "Train set: Average loss: 0.5257\n",
      "Test set: Average loss: 0.2604, Average MAE: 0.3595\n",
      "Train Epoch: 389 [4096/194182 (2%)]\tLoss: 0.527824\tGrad Norm: 1.012237\tLR: 0.030000\n",
      "Train Epoch: 389 [24576/194182 (12%)]\tLoss: 0.528018\tGrad Norm: 1.023359\tLR: 0.030000\n",
      "Train Epoch: 389 [45056/194182 (23%)]\tLoss: 0.525076\tGrad Norm: 0.918070\tLR: 0.030000\n",
      "Train Epoch: 389 [65536/194182 (33%)]\tLoss: 0.531154\tGrad Norm: 1.277832\tLR: 0.030000\n",
      "Train Epoch: 389 [86016/194182 (44%)]\tLoss: 0.537808\tGrad Norm: 1.334932\tLR: 0.030000\n",
      "Train Epoch: 389 [106496/194182 (54%)]\tLoss: 0.525516\tGrad Norm: 1.393431\tLR: 0.030000\n",
      "Train Epoch: 389 [126976/194182 (65%)]\tLoss: 0.525511\tGrad Norm: 1.185094\tLR: 0.030000\n",
      "Train Epoch: 389 [147456/194182 (75%)]\tLoss: 0.535678\tGrad Norm: 1.333430\tLR: 0.030000\n",
      "Train Epoch: 389 [167936/194182 (85%)]\tLoss: 0.527025\tGrad Norm: 1.147169\tLR: 0.030000\n",
      "Train Epoch: 389 [188416/194182 (96%)]\tLoss: 0.521115\tGrad Norm: 0.991342\tLR: 0.030000\n",
      "Train set: Average loss: 0.5274\n",
      "Test set: Average loss: 0.2582, Average MAE: 0.3515\n",
      "Train Epoch: 390 [4096/194182 (2%)]\tLoss: 0.520835\tGrad Norm: 0.927076\tLR: 0.030000\n",
      "Train Epoch: 390 [24576/194182 (12%)]\tLoss: 0.519871\tGrad Norm: 1.501898\tLR: 0.030000\n",
      "Train Epoch: 390 [45056/194182 (23%)]\tLoss: 0.543404\tGrad Norm: 1.576760\tLR: 0.030000\n",
      "Train Epoch: 390 [65536/194182 (33%)]\tLoss: 0.530888\tGrad Norm: 1.047755\tLR: 0.030000\n",
      "Train Epoch: 390 [86016/194182 (44%)]\tLoss: 0.528605\tGrad Norm: 1.127684\tLR: 0.030000\n",
      "Train Epoch: 390 [106496/194182 (54%)]\tLoss: 0.524732\tGrad Norm: 1.102750\tLR: 0.030000\n",
      "Train Epoch: 390 [126976/194182 (65%)]\tLoss: 0.530893\tGrad Norm: 0.953013\tLR: 0.030000\n",
      "Train Epoch: 390 [147456/194182 (75%)]\tLoss: 0.519675\tGrad Norm: 0.958342\tLR: 0.030000\n",
      "Train Epoch: 390 [167936/194182 (85%)]\tLoss: 0.527232\tGrad Norm: 1.126975\tLR: 0.030000\n",
      "Train Epoch: 390 [188416/194182 (96%)]\tLoss: 0.529737\tGrad Norm: 1.612334\tLR: 0.030000\n",
      "Train set: Average loss: 0.5275\n",
      "Test set: Average loss: 0.2629, Average MAE: 0.3469\n",
      "Epoch 390: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 391 [4096/194182 (2%)]\tLoss: 0.529241\tGrad Norm: 1.268189\tLR: 0.030000\n",
      "Train Epoch: 391 [24576/194182 (12%)]\tLoss: 0.523481\tGrad Norm: 0.988125\tLR: 0.030000\n",
      "Train Epoch: 391 [45056/194182 (23%)]\tLoss: 0.535082\tGrad Norm: 1.432720\tLR: 0.030000\n",
      "Train Epoch: 391 [65536/194182 (33%)]\tLoss: 0.530178\tGrad Norm: 1.014373\tLR: 0.030000\n",
      "Train Epoch: 391 [86016/194182 (44%)]\tLoss: 0.520303\tGrad Norm: 0.791029\tLR: 0.030000\n",
      "Train Epoch: 391 [106496/194182 (54%)]\tLoss: 0.519406\tGrad Norm: 0.899426\tLR: 0.030000\n",
      "Train Epoch: 391 [126976/194182 (65%)]\tLoss: 0.521857\tGrad Norm: 1.185431\tLR: 0.030000\n",
      "Train Epoch: 391 [147456/194182 (75%)]\tLoss: 0.529391\tGrad Norm: 1.259329\tLR: 0.030000\n",
      "Train Epoch: 391 [167936/194182 (85%)]\tLoss: 0.519766\tGrad Norm: 0.896247\tLR: 0.030000\n",
      "Train Epoch: 391 [188416/194182 (96%)]\tLoss: 0.527446\tGrad Norm: 1.132910\tLR: 0.030000\n",
      "Train set: Average loss: 0.5243\n",
      "Test set: Average loss: 0.2614, Average MAE: 0.3631\n",
      "Train Epoch: 392 [4096/194182 (2%)]\tLoss: 0.533441\tGrad Norm: 1.084106\tLR: 0.030000\n",
      "Train Epoch: 392 [24576/194182 (12%)]\tLoss: 0.524742\tGrad Norm: 1.219310\tLR: 0.030000\n",
      "Train Epoch: 392 [45056/194182 (23%)]\tLoss: 0.526440\tGrad Norm: 1.357053\tLR: 0.030000\n",
      "Train Epoch: 392 [65536/194182 (33%)]\tLoss: 0.532761\tGrad Norm: 1.349234\tLR: 0.030000\n",
      "Train Epoch: 392 [86016/194182 (44%)]\tLoss: 0.526843\tGrad Norm: 1.324004\tLR: 0.030000\n",
      "Train Epoch: 392 [106496/194182 (54%)]\tLoss: 0.522824\tGrad Norm: 1.239499\tLR: 0.030000\n",
      "Train Epoch: 392 [126976/194182 (65%)]\tLoss: 0.527839\tGrad Norm: 0.998427\tLR: 0.030000\n",
      "Train Epoch: 392 [147456/194182 (75%)]\tLoss: 0.519468\tGrad Norm: 1.063107\tLR: 0.030000\n",
      "Train Epoch: 392 [167936/194182 (85%)]\tLoss: 0.526424\tGrad Norm: 1.611071\tLR: 0.030000\n",
      "Train Epoch: 392 [188416/194182 (96%)]\tLoss: 0.531630\tGrad Norm: 1.207216\tLR: 0.030000\n",
      "Train set: Average loss: 0.5267\n",
      "Test set: Average loss: 0.2657, Average MAE: 0.3469\n",
      "Train Epoch: 393 [4096/194182 (2%)]\tLoss: 0.531139\tGrad Norm: 1.474542\tLR: 0.030000\n",
      "Train Epoch: 393 [24576/194182 (12%)]\tLoss: 0.527493\tGrad Norm: 1.076337\tLR: 0.030000\n",
      "Train Epoch: 393 [45056/194182 (23%)]\tLoss: 0.517936\tGrad Norm: 0.740546\tLR: 0.030000\n",
      "Train Epoch: 393 [65536/194182 (33%)]\tLoss: 0.513738\tGrad Norm: 0.990114\tLR: 0.030000\n",
      "Train Epoch: 393 [86016/194182 (44%)]\tLoss: 0.521350\tGrad Norm: 1.132767\tLR: 0.030000\n",
      "Train Epoch: 393 [106496/194182 (54%)]\tLoss: 0.530554\tGrad Norm: 1.395697\tLR: 0.030000\n",
      "Train Epoch: 393 [126976/194182 (65%)]\tLoss: 0.525248\tGrad Norm: 1.366427\tLR: 0.030000\n",
      "Train Epoch: 393 [147456/194182 (75%)]\tLoss: 0.514203\tGrad Norm: 1.039928\tLR: 0.030000\n",
      "Train Epoch: 393 [167936/194182 (85%)]\tLoss: 0.527402\tGrad Norm: 1.011859\tLR: 0.030000\n",
      "Train Epoch: 393 [188416/194182 (96%)]\tLoss: 0.524478\tGrad Norm: 0.979324\tLR: 0.030000\n",
      "Train set: Average loss: 0.5238\n",
      "Test set: Average loss: 0.2623, Average MAE: 0.3709\n",
      "Train Epoch: 394 [4096/194182 (2%)]\tLoss: 0.523621\tGrad Norm: 1.180975\tLR: 0.030000\n",
      "Train Epoch: 394 [24576/194182 (12%)]\tLoss: 0.531886\tGrad Norm: 1.098394\tLR: 0.030000\n",
      "Train Epoch: 394 [45056/194182 (23%)]\tLoss: 0.517691\tGrad Norm: 1.028434\tLR: 0.030000\n",
      "Train Epoch: 394 [65536/194182 (33%)]\tLoss: 0.516582\tGrad Norm: 0.841269\tLR: 0.030000\n",
      "Train Epoch: 394 [86016/194182 (44%)]\tLoss: 0.522940\tGrad Norm: 1.150167\tLR: 0.030000\n",
      "Train Epoch: 394 [106496/194182 (54%)]\tLoss: 0.533407\tGrad Norm: 1.605660\tLR: 0.030000\n",
      "Train Epoch: 394 [126976/194182 (65%)]\tLoss: 0.522709\tGrad Norm: 0.826937\tLR: 0.030000\n",
      "Train Epoch: 394 [147456/194182 (75%)]\tLoss: 0.507006\tGrad Norm: 0.835994\tLR: 0.030000\n",
      "Train Epoch: 394 [167936/194182 (85%)]\tLoss: 0.518618\tGrad Norm: 0.633947\tLR: 0.030000\n",
      "Train Epoch: 394 [188416/194182 (96%)]\tLoss: 0.517953\tGrad Norm: 0.617198\tLR: 0.030000\n",
      "Train set: Average loss: 0.5220\n",
      "Test set: Average loss: 0.2557, Average MAE: 0.3551\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 395 [4096/194182 (2%)]\tLoss: 0.516896\tGrad Norm: 0.754488\tLR: 0.030000\n",
      "Train Epoch: 395 [24576/194182 (12%)]\tLoss: 0.515635\tGrad Norm: 0.721423\tLR: 0.030000\n",
      "Train Epoch: 395 [45056/194182 (23%)]\tLoss: 0.520541\tGrad Norm: 0.929878\tLR: 0.030000\n",
      "Train Epoch: 395 [65536/194182 (33%)]\tLoss: 0.522545\tGrad Norm: 1.130989\tLR: 0.030000\n",
      "Train Epoch: 395 [86016/194182 (44%)]\tLoss: 0.516112\tGrad Norm: 0.702478\tLR: 0.030000\n",
      "Train Epoch: 395 [106496/194182 (54%)]\tLoss: 0.520051\tGrad Norm: 0.595698\tLR: 0.030000\n",
      "Train Epoch: 395 [126976/194182 (65%)]\tLoss: 0.519602\tGrad Norm: 0.658534\tLR: 0.030000\n",
      "Train Epoch: 395 [147456/194182 (75%)]\tLoss: 0.521512\tGrad Norm: 1.185833\tLR: 0.030000\n",
      "Train Epoch: 395 [167936/194182 (85%)]\tLoss: 0.531560\tGrad Norm: 1.294677\tLR: 0.030000\n",
      "Train Epoch: 395 [188416/194182 (96%)]\tLoss: 0.530368\tGrad Norm: 1.502919\tLR: 0.030000\n",
      "Train set: Average loss: 0.5208\n",
      "Test set: Average loss: 0.2633, Average MAE: 0.3535\n",
      "Epoch 395: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 396 [4096/194182 (2%)]\tLoss: 0.531804\tGrad Norm: 1.518081\tLR: 0.030000\n",
      "Train Epoch: 396 [24576/194182 (12%)]\tLoss: 0.519936\tGrad Norm: 1.492205\tLR: 0.030000\n",
      "Train Epoch: 396 [45056/194182 (23%)]\tLoss: 0.523887\tGrad Norm: 1.250546\tLR: 0.030000\n",
      "Train Epoch: 396 [65536/194182 (33%)]\tLoss: 0.522426\tGrad Norm: 1.082453\tLR: 0.030000\n",
      "Train Epoch: 396 [86016/194182 (44%)]\tLoss: 0.538979\tGrad Norm: 1.464200\tLR: 0.030000\n",
      "Train Epoch: 396 [106496/194182 (54%)]\tLoss: 0.531788\tGrad Norm: 1.343985\tLR: 0.030000\n",
      "Train Epoch: 396 [126976/194182 (65%)]\tLoss: 0.526462\tGrad Norm: 1.146923\tLR: 0.030000\n",
      "Train Epoch: 396 [147456/194182 (75%)]\tLoss: 0.533616\tGrad Norm: 1.291280\tLR: 0.030000\n",
      "Train Epoch: 396 [167936/194182 (85%)]\tLoss: 0.517368\tGrad Norm: 1.110808\tLR: 0.030000\n",
      "Train Epoch: 396 [188416/194182 (96%)]\tLoss: 0.515846\tGrad Norm: 0.807507\tLR: 0.030000\n",
      "Train set: Average loss: 0.5241\n",
      "Test set: Average loss: 0.2563, Average MAE: 0.3542\n",
      "Train Epoch: 397 [4096/194182 (2%)]\tLoss: 0.510698\tGrad Norm: 0.900512\tLR: 0.030000\n",
      "Train Epoch: 397 [24576/194182 (12%)]\tLoss: 0.523353\tGrad Norm: 0.891044\tLR: 0.030000\n",
      "Train Epoch: 397 [45056/194182 (23%)]\tLoss: 0.523236\tGrad Norm: 1.210267\tLR: 0.030000\n",
      "Train Epoch: 397 [65536/194182 (33%)]\tLoss: 0.517762\tGrad Norm: 1.078902\tLR: 0.030000\n",
      "Train Epoch: 397 [86016/194182 (44%)]\tLoss: 0.518095\tGrad Norm: 1.007406\tLR: 0.030000\n",
      "Train Epoch: 397 [106496/194182 (54%)]\tLoss: 0.518048\tGrad Norm: 0.884620\tLR: 0.030000\n",
      "Train Epoch: 397 [126976/194182 (65%)]\tLoss: 0.524188\tGrad Norm: 1.093968\tLR: 0.030000\n",
      "Train Epoch: 397 [147456/194182 (75%)]\tLoss: 0.518757\tGrad Norm: 0.695834\tLR: 0.030000\n",
      "Train Epoch: 397 [167936/194182 (85%)]\tLoss: 0.521504\tGrad Norm: 0.677864\tLR: 0.030000\n",
      "Train Epoch: 397 [188416/194182 (96%)]\tLoss: 0.516931\tGrad Norm: 1.010719\tLR: 0.030000\n",
      "Train set: Average loss: 0.5196\n",
      "Test set: Average loss: 0.2581, Average MAE: 0.3511\n",
      "Train Epoch: 398 [4096/194182 (2%)]\tLoss: 0.523417\tGrad Norm: 1.070776\tLR: 0.030000\n",
      "Train Epoch: 398 [24576/194182 (12%)]\tLoss: 0.523685\tGrad Norm: 1.351167\tLR: 0.030000\n",
      "Train Epoch: 398 [45056/194182 (23%)]\tLoss: 0.523624\tGrad Norm: 1.583619\tLR: 0.030000\n",
      "Train Epoch: 398 [65536/194182 (33%)]\tLoss: 0.521295\tGrad Norm: 1.242224\tLR: 0.030000\n",
      "Train Epoch: 398 [86016/194182 (44%)]\tLoss: 0.518105\tGrad Norm: 1.109958\tLR: 0.030000\n",
      "Train Epoch: 398 [106496/194182 (54%)]\tLoss: 0.528966\tGrad Norm: 1.602172\tLR: 0.030000\n",
      "Train Epoch: 398 [126976/194182 (65%)]\tLoss: 0.526678\tGrad Norm: 1.540968\tLR: 0.030000\n",
      "Train Epoch: 398 [147456/194182 (75%)]\tLoss: 0.523828\tGrad Norm: 1.450661\tLR: 0.030000\n",
      "Train Epoch: 398 [167936/194182 (85%)]\tLoss: 0.519259\tGrad Norm: 0.700054\tLR: 0.030000\n",
      "Train Epoch: 398 [188416/194182 (96%)]\tLoss: 0.521943\tGrad Norm: 1.027884\tLR: 0.030000\n",
      "Train set: Average loss: 0.5236\n",
      "Test set: Average loss: 0.2601, Average MAE: 0.3450\n",
      "Train Epoch: 399 [4096/194182 (2%)]\tLoss: 0.527552\tGrad Norm: 1.376424\tLR: 0.030000\n",
      "Train Epoch: 399 [24576/194182 (12%)]\tLoss: 0.523032\tGrad Norm: 1.312865\tLR: 0.030000\n",
      "Train Epoch: 399 [45056/194182 (23%)]\tLoss: 0.516819\tGrad Norm: 1.302406\tLR: 0.030000\n",
      "Train Epoch: 399 [65536/194182 (33%)]\tLoss: 0.514359\tGrad Norm: 0.685439\tLR: 0.030000\n",
      "Train Epoch: 399 [86016/194182 (44%)]\tLoss: 0.524132\tGrad Norm: 0.933576\tLR: 0.030000\n",
      "Train Epoch: 399 [106496/194182 (54%)]\tLoss: 0.516627\tGrad Norm: 1.097666\tLR: 0.030000\n",
      "Train Epoch: 399 [126976/194182 (65%)]\tLoss: 0.514403\tGrad Norm: 1.151789\tLR: 0.030000\n",
      "Train Epoch: 399 [147456/194182 (75%)]\tLoss: 0.522057\tGrad Norm: 1.298498\tLR: 0.030000\n",
      "Train Epoch: 399 [167936/194182 (85%)]\tLoss: 0.516677\tGrad Norm: 0.981370\tLR: 0.030000\n",
      "Train Epoch: 399 [188416/194182 (96%)]\tLoss: 0.523336\tGrad Norm: 1.035918\tLR: 0.030000\n",
      "Train set: Average loss: 0.5205\n",
      "Test set: Average loss: 0.2571, Average MAE: 0.3616\n",
      "Train Epoch: 400 [4096/194182 (2%)]\tLoss: 0.515103\tGrad Norm: 0.869654\tLR: 0.030000\n",
      "Train Epoch: 400 [24576/194182 (12%)]\tLoss: 0.521394\tGrad Norm: 1.087659\tLR: 0.030000\n",
      "Train Epoch: 400 [45056/194182 (23%)]\tLoss: 0.518204\tGrad Norm: 1.495365\tLR: 0.030000\n",
      "Train Epoch: 400 [65536/194182 (33%)]\tLoss: 0.524117\tGrad Norm: 1.401219\tLR: 0.030000\n",
      "Train Epoch: 400 [86016/194182 (44%)]\tLoss: 0.530345\tGrad Norm: 1.326698\tLR: 0.030000\n",
      "Train Epoch: 400 [106496/194182 (54%)]\tLoss: 0.516577\tGrad Norm: 1.276777\tLR: 0.030000\n",
      "Train Epoch: 400 [126976/194182 (65%)]\tLoss: 0.525953\tGrad Norm: 0.788527\tLR: 0.030000\n",
      "Train Epoch: 400 [147456/194182 (75%)]\tLoss: 0.518696\tGrad Norm: 0.732170\tLR: 0.030000\n",
      "Train Epoch: 400 [167936/194182 (85%)]\tLoss: 0.519787\tGrad Norm: 1.043773\tLR: 0.030000\n",
      "Train Epoch: 400 [188416/194182 (96%)]\tLoss: 0.512179\tGrad Norm: 0.952352\tLR: 0.030000\n",
      "Train set: Average loss: 0.5200\n",
      "Test set: Average loss: 0.2545, Average MAE: 0.3656\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 400: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 401 [4096/194182 (2%)]\tLoss: 0.516301\tGrad Norm: 0.769859\tLR: 0.030000\n",
      "Train Epoch: 401 [24576/194182 (12%)]\tLoss: 0.507207\tGrad Norm: 0.813491\tLR: 0.030000\n",
      "Train Epoch: 401 [45056/194182 (23%)]\tLoss: 0.521267\tGrad Norm: 1.063658\tLR: 0.030000\n",
      "Train Epoch: 401 [65536/194182 (33%)]\tLoss: 0.514473\tGrad Norm: 0.750796\tLR: 0.030000\n",
      "Train Epoch: 401 [86016/194182 (44%)]\tLoss: 0.514659\tGrad Norm: 0.882527\tLR: 0.030000\n",
      "Train Epoch: 401 [106496/194182 (54%)]\tLoss: 0.523879\tGrad Norm: 1.197310\tLR: 0.030000\n",
      "Train Epoch: 401 [126976/194182 (65%)]\tLoss: 0.526612\tGrad Norm: 1.648144\tLR: 0.030000\n",
      "Train Epoch: 401 [147456/194182 (75%)]\tLoss: 0.529451\tGrad Norm: 1.313439\tLR: 0.030000\n",
      "Train Epoch: 401 [167936/194182 (85%)]\tLoss: 0.520301\tGrad Norm: 1.068750\tLR: 0.030000\n",
      "Train Epoch: 401 [188416/194182 (96%)]\tLoss: 0.519447\tGrad Norm: 1.158991\tLR: 0.030000\n",
      "Train set: Average loss: 0.5196\n",
      "Test set: Average loss: 0.2625, Average MAE: 0.3701\n",
      "Train Epoch: 402 [4096/194182 (2%)]\tLoss: 0.520954\tGrad Norm: 1.308939\tLR: 0.030000\n",
      "Train Epoch: 402 [24576/194182 (12%)]\tLoss: 0.523627\tGrad Norm: 1.314879\tLR: 0.030000\n",
      "Train Epoch: 402 [45056/194182 (23%)]\tLoss: 0.523858\tGrad Norm: 1.235272\tLR: 0.030000\n",
      "Train Epoch: 402 [65536/194182 (33%)]\tLoss: 0.512947\tGrad Norm: 0.857049\tLR: 0.030000\n",
      "Train Epoch: 402 [86016/194182 (44%)]\tLoss: 0.527380\tGrad Norm: 0.652384\tLR: 0.030000\n",
      "Train Epoch: 402 [106496/194182 (54%)]\tLoss: 0.522310\tGrad Norm: 0.683049\tLR: 0.030000\n",
      "Train Epoch: 402 [126976/194182 (65%)]\tLoss: 0.521501\tGrad Norm: 1.062994\tLR: 0.030000\n",
      "Train Epoch: 402 [147456/194182 (75%)]\tLoss: 0.520976\tGrad Norm: 1.075424\tLR: 0.030000\n",
      "Train Epoch: 402 [167936/194182 (85%)]\tLoss: 0.517099\tGrad Norm: 1.134323\tLR: 0.030000\n",
      "Train Epoch: 402 [188416/194182 (96%)]\tLoss: 0.521235\tGrad Norm: 1.145477\tLR: 0.030000\n",
      "Train set: Average loss: 0.5186\n",
      "Test set: Average loss: 0.2624, Average MAE: 0.3704\n",
      "Train Epoch: 403 [4096/194182 (2%)]\tLoss: 0.521894\tGrad Norm: 1.328267\tLR: 0.030000\n",
      "Train Epoch: 403 [24576/194182 (12%)]\tLoss: 0.522281\tGrad Norm: 1.226392\tLR: 0.030000\n",
      "Train Epoch: 403 [45056/194182 (23%)]\tLoss: 0.515899\tGrad Norm: 1.081757\tLR: 0.030000\n",
      "Train Epoch: 403 [65536/194182 (33%)]\tLoss: 0.529886\tGrad Norm: 1.426574\tLR: 0.030000\n",
      "Train Epoch: 403 [86016/194182 (44%)]\tLoss: 0.518766\tGrad Norm: 1.167109\tLR: 0.030000\n",
      "Train Epoch: 403 [106496/194182 (54%)]\tLoss: 0.523270\tGrad Norm: 1.233408\tLR: 0.030000\n",
      "Train Epoch: 403 [126976/194182 (65%)]\tLoss: 0.516795\tGrad Norm: 1.193789\tLR: 0.030000\n",
      "Train Epoch: 403 [147456/194182 (75%)]\tLoss: 0.517114\tGrad Norm: 1.240570\tLR: 0.030000\n",
      "Train Epoch: 403 [167936/194182 (85%)]\tLoss: 0.526446\tGrad Norm: 1.385434\tLR: 0.030000\n",
      "Train Epoch: 403 [188416/194182 (96%)]\tLoss: 0.520977\tGrad Norm: 1.318802\tLR: 0.030000\n",
      "Train set: Average loss: 0.5194\n",
      "Test set: Average loss: 0.2568, Average MAE: 0.3678\n",
      "Train Epoch: 404 [4096/194182 (2%)]\tLoss: 0.514829\tGrad Norm: 0.923823\tLR: 0.030000\n",
      "Train Epoch: 404 [24576/194182 (12%)]\tLoss: 0.515442\tGrad Norm: 1.169411\tLR: 0.030000\n",
      "Train Epoch: 404 [45056/194182 (23%)]\tLoss: 0.511662\tGrad Norm: 0.775966\tLR: 0.030000\n",
      "Train Epoch: 404 [65536/194182 (33%)]\tLoss: 0.515966\tGrad Norm: 0.872659\tLR: 0.030000\n",
      "Train Epoch: 404 [86016/194182 (44%)]\tLoss: 0.506844\tGrad Norm: 0.759060\tLR: 0.030000\n",
      "Train Epoch: 404 [106496/194182 (54%)]\tLoss: 0.522127\tGrad Norm: 0.846511\tLR: 0.030000\n",
      "Train Epoch: 404 [126976/194182 (65%)]\tLoss: 0.508646\tGrad Norm: 1.231017\tLR: 0.030000\n",
      "Train Epoch: 404 [147456/194182 (75%)]\tLoss: 0.519865\tGrad Norm: 1.147247\tLR: 0.030000\n",
      "Train Epoch: 404 [167936/194182 (85%)]\tLoss: 0.519706\tGrad Norm: 1.344015\tLR: 0.030000\n",
      "Train Epoch: 404 [188416/194182 (96%)]\tLoss: 0.516402\tGrad Norm: 1.125736\tLR: 0.030000\n",
      "Train set: Average loss: 0.5163\n",
      "Test set: Average loss: 0.2614, Average MAE: 0.3756\n",
      "Train Epoch: 405 [4096/194182 (2%)]\tLoss: 0.523260\tGrad Norm: 1.178494\tLR: 0.030000\n",
      "Train Epoch: 405 [24576/194182 (12%)]\tLoss: 0.517941\tGrad Norm: 1.035989\tLR: 0.030000\n",
      "Train Epoch: 405 [45056/194182 (23%)]\tLoss: 0.521572\tGrad Norm: 1.479393\tLR: 0.030000\n",
      "Train Epoch: 405 [65536/194182 (33%)]\tLoss: 0.523312\tGrad Norm: 1.405150\tLR: 0.030000\n",
      "Train Epoch: 405 [86016/194182 (44%)]\tLoss: 0.516266\tGrad Norm: 1.138314\tLR: 0.030000\n",
      "Train Epoch: 405 [106496/194182 (54%)]\tLoss: 0.516380\tGrad Norm: 0.728851\tLR: 0.030000\n",
      "Train Epoch: 405 [126976/194182 (65%)]\tLoss: 0.513323\tGrad Norm: 0.707477\tLR: 0.030000\n",
      "Train Epoch: 405 [147456/194182 (75%)]\tLoss: 0.518129\tGrad Norm: 1.028634\tLR: 0.030000\n",
      "Train Epoch: 405 [167936/194182 (85%)]\tLoss: 0.519200\tGrad Norm: 0.884245\tLR: 0.030000\n",
      "Train Epoch: 405 [188416/194182 (96%)]\tLoss: 0.519099\tGrad Norm: 0.968157\tLR: 0.030000\n",
      "Train set: Average loss: 0.5168\n",
      "Test set: Average loss: 0.2540, Average MAE: 0.3556\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 405: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 406 [4096/194182 (2%)]\tLoss: 0.511122\tGrad Norm: 0.792481\tLR: 0.030000\n",
      "Train Epoch: 406 [24576/194182 (12%)]\tLoss: 0.514617\tGrad Norm: 0.809271\tLR: 0.030000\n",
      "Train Epoch: 406 [45056/194182 (23%)]\tLoss: 0.515871\tGrad Norm: 0.981743\tLR: 0.030000\n",
      "Train Epoch: 406 [65536/194182 (33%)]\tLoss: 0.516904\tGrad Norm: 1.141264\tLR: 0.030000\n",
      "Train Epoch: 406 [86016/194182 (44%)]\tLoss: 0.504616\tGrad Norm: 0.839603\tLR: 0.030000\n",
      "Train Epoch: 406 [106496/194182 (54%)]\tLoss: 0.511752\tGrad Norm: 1.398118\tLR: 0.030000\n",
      "Train Epoch: 406 [126976/194182 (65%)]\tLoss: 0.531476\tGrad Norm: 1.490080\tLR: 0.030000\n",
      "Train Epoch: 406 [147456/194182 (75%)]\tLoss: 0.519528\tGrad Norm: 1.468626\tLR: 0.030000\n",
      "Train Epoch: 406 [167936/194182 (85%)]\tLoss: 0.522275\tGrad Norm: 1.276237\tLR: 0.030000\n",
      "Train Epoch: 406 [188416/194182 (96%)]\tLoss: 0.513964\tGrad Norm: 0.982252\tLR: 0.030000\n",
      "Train set: Average loss: 0.5165\n",
      "Test set: Average loss: 0.2556, Average MAE: 0.3462\n",
      "Train Epoch: 407 [4096/194182 (2%)]\tLoss: 0.519369\tGrad Norm: 1.035527\tLR: 0.030000\n",
      "Train Epoch: 407 [24576/194182 (12%)]\tLoss: 0.518906\tGrad Norm: 0.964023\tLR: 0.030000\n",
      "Train Epoch: 407 [45056/194182 (23%)]\tLoss: 0.525568\tGrad Norm: 1.225856\tLR: 0.030000\n",
      "Train Epoch: 407 [65536/194182 (33%)]\tLoss: 0.516197\tGrad Norm: 1.138372\tLR: 0.030000\n",
      "Train Epoch: 407 [86016/194182 (44%)]\tLoss: 0.522550\tGrad Norm: 1.191742\tLR: 0.030000\n",
      "Train Epoch: 407 [106496/194182 (54%)]\tLoss: 0.509708\tGrad Norm: 1.106738\tLR: 0.030000\n",
      "Train Epoch: 407 [126976/194182 (65%)]\tLoss: 0.516992\tGrad Norm: 1.126330\tLR: 0.030000\n",
      "Train Epoch: 407 [147456/194182 (75%)]\tLoss: 0.517526\tGrad Norm: 1.329652\tLR: 0.030000\n",
      "Train Epoch: 407 [167936/194182 (85%)]\tLoss: 0.517212\tGrad Norm: 1.478210\tLR: 0.030000\n",
      "Train Epoch: 407 [188416/194182 (96%)]\tLoss: 0.526208\tGrad Norm: 1.420337\tLR: 0.030000\n",
      "Train set: Average loss: 0.5173\n",
      "Test set: Average loss: 0.2623, Average MAE: 0.3515\n",
      "Train Epoch: 408 [4096/194182 (2%)]\tLoss: 0.511434\tGrad Norm: 1.315632\tLR: 0.030000\n",
      "Train Epoch: 408 [24576/194182 (12%)]\tLoss: 0.519735\tGrad Norm: 1.177977\tLR: 0.030000\n",
      "Train Epoch: 408 [45056/194182 (23%)]\tLoss: 0.520257\tGrad Norm: 1.119102\tLR: 0.030000\n",
      "Train Epoch: 408 [65536/194182 (33%)]\tLoss: 0.511592\tGrad Norm: 1.269273\tLR: 0.030000\n",
      "Train Epoch: 408 [86016/194182 (44%)]\tLoss: 0.525385\tGrad Norm: 1.265999\tLR: 0.030000\n",
      "Train Epoch: 408 [106496/194182 (54%)]\tLoss: 0.517330\tGrad Norm: 0.725875\tLR: 0.030000\n",
      "Train Epoch: 408 [126976/194182 (65%)]\tLoss: 0.521781\tGrad Norm: 1.071919\tLR: 0.030000\n",
      "Train Epoch: 408 [147456/194182 (75%)]\tLoss: 0.517964\tGrad Norm: 0.978339\tLR: 0.030000\n",
      "Train Epoch: 408 [167936/194182 (85%)]\tLoss: 0.514751\tGrad Norm: 1.283383\tLR: 0.030000\n",
      "Train Epoch: 408 [188416/194182 (96%)]\tLoss: 0.512218\tGrad Norm: 1.006923\tLR: 0.030000\n",
      "Train set: Average loss: 0.5153\n",
      "Test set: Average loss: 0.2580, Average MAE: 0.3592\n",
      "Train Epoch: 409 [4096/194182 (2%)]\tLoss: 0.508253\tGrad Norm: 1.172035\tLR: 0.030000\n",
      "Train Epoch: 409 [24576/194182 (12%)]\tLoss: 0.516480\tGrad Norm: 1.096804\tLR: 0.030000\n",
      "Train Epoch: 409 [45056/194182 (23%)]\tLoss: 0.510497\tGrad Norm: 1.082574\tLR: 0.030000\n",
      "Train Epoch: 409 [65536/194182 (33%)]\tLoss: 0.517220\tGrad Norm: 0.976484\tLR: 0.030000\n",
      "Train Epoch: 409 [86016/194182 (44%)]\tLoss: 0.505385\tGrad Norm: 0.922764\tLR: 0.030000\n",
      "Train Epoch: 409 [106496/194182 (54%)]\tLoss: 0.510690\tGrad Norm: 0.887676\tLR: 0.030000\n",
      "Train Epoch: 409 [126976/194182 (65%)]\tLoss: 0.520994\tGrad Norm: 1.487017\tLR: 0.030000\n",
      "Train Epoch: 409 [147456/194182 (75%)]\tLoss: 0.519500\tGrad Norm: 1.335040\tLR: 0.030000\n",
      "Train Epoch: 409 [167936/194182 (85%)]\tLoss: 0.504460\tGrad Norm: 0.927734\tLR: 0.030000\n",
      "Train Epoch: 409 [188416/194182 (96%)]\tLoss: 0.528166\tGrad Norm: 1.161678\tLR: 0.030000\n",
      "Train set: Average loss: 0.5145\n",
      "Test set: Average loss: 0.2564, Average MAE: 0.3700\n",
      "Train Epoch: 410 [4096/194182 (2%)]\tLoss: 0.509240\tGrad Norm: 1.004276\tLR: 0.030000\n",
      "Train Epoch: 410 [24576/194182 (12%)]\tLoss: 0.521967\tGrad Norm: 1.302734\tLR: 0.030000\n",
      "Train Epoch: 410 [45056/194182 (23%)]\tLoss: 0.514511\tGrad Norm: 0.674322\tLR: 0.030000\n",
      "Train Epoch: 410 [65536/194182 (33%)]\tLoss: 0.504473\tGrad Norm: 0.790702\tLR: 0.030000\n",
      "Train Epoch: 410 [86016/194182 (44%)]\tLoss: 0.507470\tGrad Norm: 1.142508\tLR: 0.030000\n",
      "Train Epoch: 410 [106496/194182 (54%)]\tLoss: 0.510633\tGrad Norm: 1.193773\tLR: 0.030000\n",
      "Train Epoch: 410 [126976/194182 (65%)]\tLoss: 0.507387\tGrad Norm: 1.117277\tLR: 0.030000\n",
      "Train Epoch: 410 [147456/194182 (75%)]\tLoss: 0.518248\tGrad Norm: 1.183384\tLR: 0.030000\n",
      "Train Epoch: 410 [167936/194182 (85%)]\tLoss: 0.515422\tGrad Norm: 0.857592\tLR: 0.030000\n",
      "Train Epoch: 410 [188416/194182 (96%)]\tLoss: 0.506484\tGrad Norm: 0.848028\tLR: 0.030000\n",
      "Train set: Average loss: 0.5130\n",
      "Test set: Average loss: 0.2579, Average MAE: 0.3627\n",
      "Epoch 410: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 411 [4096/194182 (2%)]\tLoss: 0.511088\tGrad Norm: 0.978694\tLR: 0.030000\n",
      "Train Epoch: 411 [24576/194182 (12%)]\tLoss: 0.522954\tGrad Norm: 0.962048\tLR: 0.030000\n",
      "Train Epoch: 411 [45056/194182 (23%)]\tLoss: 0.513255\tGrad Norm: 1.266995\tLR: 0.030000\n",
      "Train Epoch: 411 [65536/194182 (33%)]\tLoss: 0.517936\tGrad Norm: 1.238705\tLR: 0.030000\n",
      "Train Epoch: 411 [86016/194182 (44%)]\tLoss: 0.518773\tGrad Norm: 1.339203\tLR: 0.030000\n",
      "Train Epoch: 411 [106496/194182 (54%)]\tLoss: 0.517465\tGrad Norm: 1.312956\tLR: 0.030000\n",
      "Train Epoch: 411 [126976/194182 (65%)]\tLoss: 0.510295\tGrad Norm: 1.235344\tLR: 0.030000\n",
      "Train Epoch: 411 [147456/194182 (75%)]\tLoss: 0.505324\tGrad Norm: 0.921519\tLR: 0.030000\n",
      "Train Epoch: 411 [167936/194182 (85%)]\tLoss: 0.514151\tGrad Norm: 0.980435\tLR: 0.030000\n",
      "Train Epoch: 411 [188416/194182 (96%)]\tLoss: 0.511144\tGrad Norm: 1.371311\tLR: 0.030000\n",
      "Train set: Average loss: 0.5141\n",
      "Test set: Average loss: 0.2595, Average MAE: 0.3757\n",
      "Train Epoch: 412 [4096/194182 (2%)]\tLoss: 0.515067\tGrad Norm: 1.251537\tLR: 0.030000\n",
      "Train Epoch: 412 [24576/194182 (12%)]\tLoss: 0.509010\tGrad Norm: 0.936329\tLR: 0.030000\n",
      "Train Epoch: 412 [45056/194182 (23%)]\tLoss: 0.515525\tGrad Norm: 0.975469\tLR: 0.030000\n",
      "Train Epoch: 412 [65536/194182 (33%)]\tLoss: 0.522134\tGrad Norm: 1.357395\tLR: 0.030000\n",
      "Train Epoch: 412 [86016/194182 (44%)]\tLoss: 0.514774\tGrad Norm: 1.014736\tLR: 0.030000\n",
      "Train Epoch: 412 [106496/194182 (54%)]\tLoss: 0.508391\tGrad Norm: 1.096453\tLR: 0.030000\n",
      "Train Epoch: 412 [126976/194182 (65%)]\tLoss: 0.511270\tGrad Norm: 1.092737\tLR: 0.030000\n",
      "Train Epoch: 412 [147456/194182 (75%)]\tLoss: 0.506881\tGrad Norm: 1.128282\tLR: 0.030000\n",
      "Train Epoch: 412 [167936/194182 (85%)]\tLoss: 0.511343\tGrad Norm: 1.289812\tLR: 0.030000\n",
      "Train Epoch: 412 [188416/194182 (96%)]\tLoss: 0.517999\tGrad Norm: 1.343765\tLR: 0.030000\n",
      "Train set: Average loss: 0.5139\n",
      "Test set: Average loss: 0.2572, Average MAE: 0.3476\n",
      "Train Epoch: 413 [4096/194182 (2%)]\tLoss: 0.509859\tGrad Norm: 1.087988\tLR: 0.030000\n",
      "Train Epoch: 413 [24576/194182 (12%)]\tLoss: 0.516797\tGrad Norm: 1.176580\tLR: 0.030000\n",
      "Train Epoch: 413 [45056/194182 (23%)]\tLoss: 0.512010\tGrad Norm: 0.810959\tLR: 0.030000\n",
      "Train Epoch: 413 [65536/194182 (33%)]\tLoss: 0.505157\tGrad Norm: 0.976627\tLR: 0.030000\n",
      "Train Epoch: 413 [86016/194182 (44%)]\tLoss: 0.515182\tGrad Norm: 1.110126\tLR: 0.030000\n",
      "Train Epoch: 413 [106496/194182 (54%)]\tLoss: 0.506791\tGrad Norm: 0.974146\tLR: 0.030000\n",
      "Train Epoch: 413 [126976/194182 (65%)]\tLoss: 0.510268\tGrad Norm: 0.806048\tLR: 0.030000\n",
      "Train Epoch: 413 [147456/194182 (75%)]\tLoss: 0.512629\tGrad Norm: 0.849570\tLR: 0.030000\n",
      "Train Epoch: 413 [167936/194182 (85%)]\tLoss: 0.508879\tGrad Norm: 1.332552\tLR: 0.030000\n",
      "Train Epoch: 413 [188416/194182 (96%)]\tLoss: 0.518091\tGrad Norm: 1.224950\tLR: 0.030000\n",
      "Train set: Average loss: 0.5117\n",
      "Test set: Average loss: 0.2642, Average MAE: 0.3786\n",
      "Train Epoch: 414 [4096/194182 (2%)]\tLoss: 0.508705\tGrad Norm: 1.441198\tLR: 0.030000\n",
      "Train Epoch: 414 [24576/194182 (12%)]\tLoss: 0.510670\tGrad Norm: 1.056127\tLR: 0.030000\n",
      "Train Epoch: 414 [45056/194182 (23%)]\tLoss: 0.511225\tGrad Norm: 1.133894\tLR: 0.030000\n",
      "Train Epoch: 414 [65536/194182 (33%)]\tLoss: 0.505053\tGrad Norm: 0.797119\tLR: 0.030000\n",
      "Train Epoch: 414 [86016/194182 (44%)]\tLoss: 0.509077\tGrad Norm: 0.798071\tLR: 0.030000\n",
      "Train Epoch: 414 [106496/194182 (54%)]\tLoss: 0.502462\tGrad Norm: 0.781870\tLR: 0.030000\n",
      "Train Epoch: 414 [126976/194182 (65%)]\tLoss: 0.507381\tGrad Norm: 0.863194\tLR: 0.030000\n",
      "Train Epoch: 414 [147456/194182 (75%)]\tLoss: 0.514516\tGrad Norm: 0.970007\tLR: 0.030000\n",
      "Train Epoch: 414 [167936/194182 (85%)]\tLoss: 0.513365\tGrad Norm: 1.012559\tLR: 0.030000\n",
      "Train Epoch: 414 [188416/194182 (96%)]\tLoss: 0.515693\tGrad Norm: 1.142017\tLR: 0.030000\n",
      "Train set: Average loss: 0.5104\n",
      "Test set: Average loss: 0.2524, Average MAE: 0.3529\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 415 [4096/194182 (2%)]\tLoss: 0.505539\tGrad Norm: 0.796918\tLR: 0.030000\n",
      "Train Epoch: 415 [24576/194182 (12%)]\tLoss: 0.513208\tGrad Norm: 0.461200\tLR: 0.030000\n",
      "Train Epoch: 415 [45056/194182 (23%)]\tLoss: 0.501644\tGrad Norm: 0.687731\tLR: 0.030000\n",
      "Train Epoch: 415 [65536/194182 (33%)]\tLoss: 0.503405\tGrad Norm: 0.808444\tLR: 0.030000\n",
      "Train Epoch: 415 [86016/194182 (44%)]\tLoss: 0.505925\tGrad Norm: 1.188390\tLR: 0.030000\n",
      "Train Epoch: 415 [106496/194182 (54%)]\tLoss: 0.514374\tGrad Norm: 1.489866\tLR: 0.030000\n",
      "Train Epoch: 415 [126976/194182 (65%)]\tLoss: 0.517151\tGrad Norm: 1.572578\tLR: 0.030000\n",
      "Train Epoch: 415 [147456/194182 (75%)]\tLoss: 0.514092\tGrad Norm: 1.214437\tLR: 0.030000\n",
      "Train Epoch: 415 [167936/194182 (85%)]\tLoss: 0.505455\tGrad Norm: 0.843576\tLR: 0.030000\n",
      "Train Epoch: 415 [188416/194182 (96%)]\tLoss: 0.531506\tGrad Norm: 1.643424\tLR: 0.030000\n",
      "Train set: Average loss: 0.5103\n",
      "Test set: Average loss: 0.2551, Average MAE: 0.3494\n",
      "Epoch 415: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 416 [4096/194182 (2%)]\tLoss: 0.508872\tGrad Norm: 1.055586\tLR: 0.030000\n",
      "Train Epoch: 416 [24576/194182 (12%)]\tLoss: 0.521191\tGrad Norm: 1.598166\tLR: 0.030000\n",
      "Train Epoch: 416 [45056/194182 (23%)]\tLoss: 0.520690\tGrad Norm: 1.696140\tLR: 0.030000\n",
      "Train Epoch: 416 [65536/194182 (33%)]\tLoss: 0.518244\tGrad Norm: 1.297475\tLR: 0.030000\n",
      "Train Epoch: 416 [86016/194182 (44%)]\tLoss: 0.503265\tGrad Norm: 1.071882\tLR: 0.030000\n",
      "Train Epoch: 416 [106496/194182 (54%)]\tLoss: 0.509845\tGrad Norm: 1.003617\tLR: 0.030000\n",
      "Train Epoch: 416 [126976/194182 (65%)]\tLoss: 0.511461\tGrad Norm: 1.104087\tLR: 0.030000\n",
      "Train Epoch: 416 [147456/194182 (75%)]\tLoss: 0.514708\tGrad Norm: 1.273039\tLR: 0.030000\n",
      "Train Epoch: 416 [167936/194182 (85%)]\tLoss: 0.520737\tGrad Norm: 1.227473\tLR: 0.030000\n",
      "Train Epoch: 416 [188416/194182 (96%)]\tLoss: 0.518614\tGrad Norm: 1.087268\tLR: 0.030000\n",
      "Train set: Average loss: 0.5126\n",
      "Test set: Average loss: 0.2563, Average MAE: 0.3432\n",
      "Train Epoch: 417 [4096/194182 (2%)]\tLoss: 0.510205\tGrad Norm: 1.233736\tLR: 0.030000\n",
      "Train Epoch: 417 [24576/194182 (12%)]\tLoss: 0.511672\tGrad Norm: 1.333767\tLR: 0.030000\n",
      "Train Epoch: 417 [45056/194182 (23%)]\tLoss: 0.507572\tGrad Norm: 1.125879\tLR: 0.030000\n",
      "Train Epoch: 417 [65536/194182 (33%)]\tLoss: 0.512244\tGrad Norm: 1.007528\tLR: 0.030000\n",
      "Train Epoch: 417 [86016/194182 (44%)]\tLoss: 0.509039\tGrad Norm: 1.084533\tLR: 0.030000\n",
      "Train Epoch: 417 [106496/194182 (54%)]\tLoss: 0.510574\tGrad Norm: 1.071403\tLR: 0.030000\n",
      "Train Epoch: 417 [126976/194182 (65%)]\tLoss: 0.505910\tGrad Norm: 1.016177\tLR: 0.030000\n",
      "Train Epoch: 417 [147456/194182 (75%)]\tLoss: 0.502608\tGrad Norm: 1.158377\tLR: 0.030000\n",
      "Train Epoch: 417 [167936/194182 (85%)]\tLoss: 0.519872\tGrad Norm: 1.412047\tLR: 0.030000\n",
      "Train Epoch: 417 [188416/194182 (96%)]\tLoss: 0.508836\tGrad Norm: 0.983813\tLR: 0.030000\n",
      "Train set: Average loss: 0.5106\n",
      "Test set: Average loss: 0.2548, Average MAE: 0.3477\n",
      "Train Epoch: 418 [4096/194182 (2%)]\tLoss: 0.514849\tGrad Norm: 1.064782\tLR: 0.030000\n",
      "Train Epoch: 418 [24576/194182 (12%)]\tLoss: 0.509101\tGrad Norm: 1.382374\tLR: 0.030000\n",
      "Train Epoch: 418 [45056/194182 (23%)]\tLoss: 0.508133\tGrad Norm: 1.317244\tLR: 0.030000\n",
      "Train Epoch: 418 [65536/194182 (33%)]\tLoss: 0.504905\tGrad Norm: 1.295062\tLR: 0.030000\n",
      "Train Epoch: 418 [86016/194182 (44%)]\tLoss: 0.504383\tGrad Norm: 1.236836\tLR: 0.030000\n",
      "Train Epoch: 418 [106496/194182 (54%)]\tLoss: 0.510368\tGrad Norm: 1.101055\tLR: 0.030000\n",
      "Train Epoch: 418 [126976/194182 (65%)]\tLoss: 0.513532\tGrad Norm: 1.000428\tLR: 0.030000\n",
      "Train Epoch: 418 [147456/194182 (75%)]\tLoss: 0.514400\tGrad Norm: 1.170208\tLR: 0.030000\n",
      "Train Epoch: 418 [167936/194182 (85%)]\tLoss: 0.506485\tGrad Norm: 1.046429\tLR: 0.030000\n",
      "Train Epoch: 418 [188416/194182 (96%)]\tLoss: 0.510629\tGrad Norm: 1.239806\tLR: 0.030000\n",
      "Train set: Average loss: 0.5111\n",
      "Test set: Average loss: 0.2580, Average MAE: 0.3528\n",
      "Train Epoch: 419 [4096/194182 (2%)]\tLoss: 0.513794\tGrad Norm: 1.238189\tLR: 0.030000\n",
      "Train Epoch: 419 [24576/194182 (12%)]\tLoss: 0.509537\tGrad Norm: 1.198851\tLR: 0.030000\n",
      "Train Epoch: 419 [45056/194182 (23%)]\tLoss: 0.504132\tGrad Norm: 0.959748\tLR: 0.030000\n",
      "Train Epoch: 419 [65536/194182 (33%)]\tLoss: 0.504009\tGrad Norm: 1.077488\tLR: 0.030000\n",
      "Train Epoch: 419 [86016/194182 (44%)]\tLoss: 0.495823\tGrad Norm: 0.910689\tLR: 0.030000\n",
      "Train Epoch: 419 [106496/194182 (54%)]\tLoss: 0.495748\tGrad Norm: 0.909229\tLR: 0.030000\n",
      "Train Epoch: 419 [126976/194182 (65%)]\tLoss: 0.516031\tGrad Norm: 1.558124\tLR: 0.030000\n",
      "Train Epoch: 419 [147456/194182 (75%)]\tLoss: 0.515520\tGrad Norm: 1.232545\tLR: 0.030000\n",
      "Train Epoch: 419 [167936/194182 (85%)]\tLoss: 0.517684\tGrad Norm: 1.401748\tLR: 0.030000\n",
      "Train Epoch: 419 [188416/194182 (96%)]\tLoss: 0.511065\tGrad Norm: 1.392425\tLR: 0.030000\n",
      "Train set: Average loss: 0.5106\n",
      "Test set: Average loss: 0.2645, Average MAE: 0.3783\n",
      "Train Epoch: 420 [4096/194182 (2%)]\tLoss: 0.513788\tGrad Norm: 1.484592\tLR: 0.030000\n",
      "Train Epoch: 420 [24576/194182 (12%)]\tLoss: 0.501364\tGrad Norm: 0.959425\tLR: 0.030000\n",
      "Train Epoch: 420 [45056/194182 (23%)]\tLoss: 0.502191\tGrad Norm: 0.725039\tLR: 0.030000\n",
      "Train Epoch: 420 [65536/194182 (33%)]\tLoss: 0.513426\tGrad Norm: 1.239427\tLR: 0.030000\n",
      "Train Epoch: 420 [86016/194182 (44%)]\tLoss: 0.511554\tGrad Norm: 1.195089\tLR: 0.030000\n",
      "Train Epoch: 420 [106496/194182 (54%)]\tLoss: 0.501253\tGrad Norm: 1.199527\tLR: 0.030000\n",
      "Train Epoch: 420 [126976/194182 (65%)]\tLoss: 0.504131\tGrad Norm: 0.840961\tLR: 0.030000\n",
      "Train Epoch: 420 [147456/194182 (75%)]\tLoss: 0.504115\tGrad Norm: 1.039303\tLR: 0.030000\n",
      "Train Epoch: 420 [167936/194182 (85%)]\tLoss: 0.502996\tGrad Norm: 1.118976\tLR: 0.030000\n",
      "Train Epoch: 420 [188416/194182 (96%)]\tLoss: 0.505672\tGrad Norm: 1.100665\tLR: 0.030000\n",
      "Train set: Average loss: 0.5082\n",
      "Test set: Average loss: 0.2563, Average MAE: 0.3625\n",
      "Epoch 420: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 421 [4096/194182 (2%)]\tLoss: 0.502251\tGrad Norm: 1.162810\tLR: 0.030000\n",
      "Train Epoch: 421 [24576/194182 (12%)]\tLoss: 0.504959\tGrad Norm: 1.116020\tLR: 0.030000\n",
      "Train Epoch: 421 [45056/194182 (23%)]\tLoss: 0.504305\tGrad Norm: 1.057589\tLR: 0.030000\n",
      "Train Epoch: 421 [65536/194182 (33%)]\tLoss: 0.506823\tGrad Norm: 1.143612\tLR: 0.030000\n",
      "Train Epoch: 421 [86016/194182 (44%)]\tLoss: 0.510967\tGrad Norm: 1.284765\tLR: 0.030000\n",
      "Train Epoch: 421 [106496/194182 (54%)]\tLoss: 0.505455\tGrad Norm: 1.019755\tLR: 0.030000\n",
      "Train Epoch: 421 [126976/194182 (65%)]\tLoss: 0.506157\tGrad Norm: 0.962646\tLR: 0.030000\n",
      "Train Epoch: 421 [147456/194182 (75%)]\tLoss: 0.500428\tGrad Norm: 1.031030\tLR: 0.030000\n",
      "Train Epoch: 421 [167936/194182 (85%)]\tLoss: 0.507252\tGrad Norm: 1.199817\tLR: 0.030000\n",
      "Train Epoch: 421 [188416/194182 (96%)]\tLoss: 0.510217\tGrad Norm: 1.166854\tLR: 0.030000\n",
      "Train set: Average loss: 0.5082\n",
      "Test set: Average loss: 0.2563, Average MAE: 0.3686\n",
      "Train Epoch: 422 [4096/194182 (2%)]\tLoss: 0.505352\tGrad Norm: 1.158217\tLR: 0.030000\n",
      "Train Epoch: 422 [24576/194182 (12%)]\tLoss: 0.510928\tGrad Norm: 1.217953\tLR: 0.030000\n",
      "Train Epoch: 422 [45056/194182 (23%)]\tLoss: 0.504803\tGrad Norm: 1.127982\tLR: 0.030000\n",
      "Train Epoch: 422 [65536/194182 (33%)]\tLoss: 0.506458\tGrad Norm: 1.290152\tLR: 0.030000\n",
      "Train Epoch: 422 [86016/194182 (44%)]\tLoss: 0.505605\tGrad Norm: 0.717957\tLR: 0.030000\n",
      "Train Epoch: 422 [106496/194182 (54%)]\tLoss: 0.507202\tGrad Norm: 0.836161\tLR: 0.030000\n",
      "Train Epoch: 422 [126976/194182 (65%)]\tLoss: 0.508767\tGrad Norm: 1.169212\tLR: 0.030000\n",
      "Train Epoch: 422 [147456/194182 (75%)]\tLoss: 0.499587\tGrad Norm: 1.418248\tLR: 0.030000\n",
      "Train Epoch: 422 [167936/194182 (85%)]\tLoss: 0.514502\tGrad Norm: 1.091063\tLR: 0.030000\n",
      "Train Epoch: 422 [188416/194182 (96%)]\tLoss: 0.503612\tGrad Norm: 1.117587\tLR: 0.030000\n",
      "Train set: Average loss: 0.5076\n",
      "Test set: Average loss: 0.2550, Average MAE: 0.3658\n",
      "Train Epoch: 423 [4096/194182 (2%)]\tLoss: 0.508985\tGrad Norm: 0.986445\tLR: 0.030000\n",
      "Train Epoch: 423 [24576/194182 (12%)]\tLoss: 0.503266\tGrad Norm: 1.062654\tLR: 0.030000\n",
      "Train Epoch: 423 [45056/194182 (23%)]\tLoss: 0.506947\tGrad Norm: 1.127579\tLR: 0.030000\n",
      "Train Epoch: 423 [65536/194182 (33%)]\tLoss: 0.510078\tGrad Norm: 1.121033\tLR: 0.030000\n",
      "Train Epoch: 423 [86016/194182 (44%)]\tLoss: 0.497936\tGrad Norm: 0.878777\tLR: 0.030000\n",
      "Train Epoch: 423 [106496/194182 (54%)]\tLoss: 0.499485\tGrad Norm: 0.645461\tLR: 0.030000\n",
      "Train Epoch: 423 [126976/194182 (65%)]\tLoss: 0.506524\tGrad Norm: 1.517795\tLR: 0.030000\n",
      "Train Epoch: 423 [147456/194182 (75%)]\tLoss: 0.513156\tGrad Norm: 1.368339\tLR: 0.030000\n",
      "Train Epoch: 423 [167936/194182 (85%)]\tLoss: 0.508459\tGrad Norm: 1.175473\tLR: 0.030000\n",
      "Train Epoch: 423 [188416/194182 (96%)]\tLoss: 0.510168\tGrad Norm: 1.273661\tLR: 0.030000\n",
      "Train set: Average loss: 0.5066\n",
      "Test set: Average loss: 0.2542, Average MAE: 0.3428\n",
      "Train Epoch: 424 [4096/194182 (2%)]\tLoss: 0.508749\tGrad Norm: 1.089748\tLR: 0.030000\n",
      "Train Epoch: 424 [24576/194182 (12%)]\tLoss: 0.504991\tGrad Norm: 1.238178\tLR: 0.030000\n",
      "Train Epoch: 424 [45056/194182 (23%)]\tLoss: 0.503078\tGrad Norm: 1.017481\tLR: 0.030000\n",
      "Train Epoch: 424 [65536/194182 (33%)]\tLoss: 0.496927\tGrad Norm: 0.693901\tLR: 0.030000\n",
      "Train Epoch: 424 [86016/194182 (44%)]\tLoss: 0.501964\tGrad Norm: 0.711344\tLR: 0.030000\n",
      "Train Epoch: 424 [106496/194182 (54%)]\tLoss: 0.512855\tGrad Norm: 1.243225\tLR: 0.030000\n",
      "Train Epoch: 424 [126976/194182 (65%)]\tLoss: 0.498325\tGrad Norm: 1.120549\tLR: 0.030000\n",
      "Train Epoch: 424 [147456/194182 (75%)]\tLoss: 0.501951\tGrad Norm: 0.618015\tLR: 0.030000\n",
      "Train Epoch: 424 [167936/194182 (85%)]\tLoss: 0.505831\tGrad Norm: 1.261816\tLR: 0.030000\n",
      "Train Epoch: 424 [188416/194182 (96%)]\tLoss: 0.503077\tGrad Norm: 1.158258\tLR: 0.030000\n",
      "Train set: Average loss: 0.5055\n",
      "Test set: Average loss: 0.2583, Average MAE: 0.3464\n",
      "Train Epoch: 425 [4096/194182 (2%)]\tLoss: 0.515470\tGrad Norm: 1.543489\tLR: 0.030000\n",
      "Train Epoch: 425 [24576/194182 (12%)]\tLoss: 0.514811\tGrad Norm: 1.442431\tLR: 0.030000\n",
      "Train Epoch: 425 [45056/194182 (23%)]\tLoss: 0.521110\tGrad Norm: 1.520913\tLR: 0.030000\n",
      "Train Epoch: 425 [65536/194182 (33%)]\tLoss: 0.506386\tGrad Norm: 1.284323\tLR: 0.030000\n",
      "Train Epoch: 425 [86016/194182 (44%)]\tLoss: 0.512635\tGrad Norm: 1.537271\tLR: 0.030000\n",
      "Train Epoch: 425 [106496/194182 (54%)]\tLoss: 0.502122\tGrad Norm: 1.093103\tLR: 0.030000\n",
      "Train Epoch: 425 [126976/194182 (65%)]\tLoss: 0.499988\tGrad Norm: 0.872217\tLR: 0.030000\n",
      "Train Epoch: 425 [147456/194182 (75%)]\tLoss: 0.500202\tGrad Norm: 0.773055\tLR: 0.030000\n",
      "Train Epoch: 425 [167936/194182 (85%)]\tLoss: 0.496529\tGrad Norm: 0.979920\tLR: 0.030000\n",
      "Train Epoch: 425 [188416/194182 (96%)]\tLoss: 0.508289\tGrad Norm: 0.836730\tLR: 0.030000\n",
      "Train set: Average loss: 0.5071\n",
      "Test set: Average loss: 0.2558, Average MAE: 0.3479\n",
      "Epoch 425: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 426 [4096/194182 (2%)]\tLoss: 0.510666\tGrad Norm: 1.201451\tLR: 0.030000\n",
      "Train Epoch: 426 [24576/194182 (12%)]\tLoss: 0.501852\tGrad Norm: 1.047752\tLR: 0.030000\n",
      "Train Epoch: 426 [45056/194182 (23%)]\tLoss: 0.503571\tGrad Norm: 1.242630\tLR: 0.030000\n",
      "Train Epoch: 426 [65536/194182 (33%)]\tLoss: 0.511916\tGrad Norm: 1.062936\tLR: 0.030000\n",
      "Train Epoch: 426 [86016/194182 (44%)]\tLoss: 0.506317\tGrad Norm: 1.135531\tLR: 0.030000\n",
      "Train Epoch: 426 [106496/194182 (54%)]\tLoss: 0.510715\tGrad Norm: 1.277914\tLR: 0.030000\n",
      "Train Epoch: 426 [126976/194182 (65%)]\tLoss: 0.507686\tGrad Norm: 0.958487\tLR: 0.030000\n",
      "Train Epoch: 426 [147456/194182 (75%)]\tLoss: 0.501409\tGrad Norm: 0.974661\tLR: 0.030000\n",
      "Train Epoch: 426 [167936/194182 (85%)]\tLoss: 0.513041\tGrad Norm: 1.136821\tLR: 0.030000\n",
      "Train Epoch: 426 [188416/194182 (96%)]\tLoss: 0.503804\tGrad Norm: 0.973022\tLR: 0.030000\n",
      "Train set: Average loss: 0.5053\n",
      "Test set: Average loss: 0.2511, Average MAE: 0.3536\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 427 [4096/194182 (2%)]\tLoss: 0.496329\tGrad Norm: 0.846336\tLR: 0.030000\n",
      "Train Epoch: 427 [24576/194182 (12%)]\tLoss: 0.508627\tGrad Norm: 0.920843\tLR: 0.030000\n",
      "Train Epoch: 427 [45056/194182 (23%)]\tLoss: 0.499418\tGrad Norm: 0.877480\tLR: 0.030000\n",
      "Train Epoch: 427 [65536/194182 (33%)]\tLoss: 0.501620\tGrad Norm: 1.173413\tLR: 0.030000\n",
      "Train Epoch: 427 [86016/194182 (44%)]\tLoss: 0.498857\tGrad Norm: 1.230092\tLR: 0.030000\n",
      "Train Epoch: 427 [106496/194182 (54%)]\tLoss: 0.510388\tGrad Norm: 1.019493\tLR: 0.030000\n",
      "Train Epoch: 427 [126976/194182 (65%)]\tLoss: 0.506403\tGrad Norm: 1.232316\tLR: 0.030000\n",
      "Train Epoch: 427 [147456/194182 (75%)]\tLoss: 0.509740\tGrad Norm: 1.029918\tLR: 0.030000\n",
      "Train Epoch: 427 [167936/194182 (85%)]\tLoss: 0.498506\tGrad Norm: 1.147949\tLR: 0.030000\n",
      "Train Epoch: 427 [188416/194182 (96%)]\tLoss: 0.497314\tGrad Norm: 1.235619\tLR: 0.030000\n",
      "Train set: Average loss: 0.5046\n",
      "Test set: Average loss: 0.2585, Average MAE: 0.3745\n",
      "Train Epoch: 428 [4096/194182 (2%)]\tLoss: 0.504999\tGrad Norm: 1.243929\tLR: 0.030000\n",
      "Train Epoch: 428 [24576/194182 (12%)]\tLoss: 0.504199\tGrad Norm: 1.264026\tLR: 0.030000\n",
      "Train Epoch: 428 [45056/194182 (23%)]\tLoss: 0.517351\tGrad Norm: 1.585032\tLR: 0.030000\n",
      "Train Epoch: 428 [65536/194182 (33%)]\tLoss: 0.499969\tGrad Norm: 1.079314\tLR: 0.030000\n",
      "Train Epoch: 428 [86016/194182 (44%)]\tLoss: 0.502728\tGrad Norm: 1.201651\tLR: 0.030000\n",
      "Train Epoch: 428 [106496/194182 (54%)]\tLoss: 0.500993\tGrad Norm: 1.143714\tLR: 0.030000\n",
      "Train Epoch: 428 [126976/194182 (65%)]\tLoss: 0.512480\tGrad Norm: 1.024141\tLR: 0.030000\n",
      "Train Epoch: 428 [147456/194182 (75%)]\tLoss: 0.509115\tGrad Norm: 1.001352\tLR: 0.030000\n",
      "Train Epoch: 428 [167936/194182 (85%)]\tLoss: 0.505208\tGrad Norm: 1.131683\tLR: 0.030000\n",
      "Train Epoch: 428 [188416/194182 (96%)]\tLoss: 0.507621\tGrad Norm: 1.010375\tLR: 0.030000\n",
      "Train set: Average loss: 0.5055\n",
      "Test set: Average loss: 0.2503, Average MAE: 0.3581\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 429 [4096/194182 (2%)]\tLoss: 0.505625\tGrad Norm: 0.639928\tLR: 0.030000\n",
      "Train Epoch: 429 [24576/194182 (12%)]\tLoss: 0.506185\tGrad Norm: 0.953886\tLR: 0.030000\n",
      "Train Epoch: 429 [45056/194182 (23%)]\tLoss: 0.504132\tGrad Norm: 1.302776\tLR: 0.030000\n",
      "Train Epoch: 429 [65536/194182 (33%)]\tLoss: 0.501744\tGrad Norm: 1.069476\tLR: 0.030000\n",
      "Train Epoch: 429 [86016/194182 (44%)]\tLoss: 0.500418\tGrad Norm: 1.004028\tLR: 0.030000\n",
      "Train Epoch: 429 [106496/194182 (54%)]\tLoss: 0.503773\tGrad Norm: 1.485780\tLR: 0.030000\n",
      "Train Epoch: 429 [126976/194182 (65%)]\tLoss: 0.503246\tGrad Norm: 0.716217\tLR: 0.030000\n",
      "Train Epoch: 429 [147456/194182 (75%)]\tLoss: 0.498373\tGrad Norm: 0.703876\tLR: 0.030000\n",
      "Train Epoch: 429 [167936/194182 (85%)]\tLoss: 0.498140\tGrad Norm: 0.699314\tLR: 0.030000\n",
      "Train Epoch: 429 [188416/194182 (96%)]\tLoss: 0.487365\tGrad Norm: 0.852306\tLR: 0.030000\n",
      "Train set: Average loss: 0.5018\n",
      "Test set: Average loss: 0.2518, Average MAE: 0.3603\n",
      "Train Epoch: 430 [4096/194182 (2%)]\tLoss: 0.502730\tGrad Norm: 0.744768\tLR: 0.030000\n",
      "Train Epoch: 430 [24576/194182 (12%)]\tLoss: 0.502617\tGrad Norm: 0.813459\tLR: 0.030000\n",
      "Train Epoch: 430 [45056/194182 (23%)]\tLoss: 0.497188\tGrad Norm: 0.965257\tLR: 0.030000\n",
      "Train Epoch: 430 [65536/194182 (33%)]\tLoss: 0.501860\tGrad Norm: 1.259733\tLR: 0.030000\n",
      "Train Epoch: 430 [86016/194182 (44%)]\tLoss: 0.503376\tGrad Norm: 1.404193\tLR: 0.030000\n",
      "Train Epoch: 430 [106496/194182 (54%)]\tLoss: 0.505543\tGrad Norm: 1.409756\tLR: 0.030000\n",
      "Train Epoch: 430 [126976/194182 (65%)]\tLoss: 0.508427\tGrad Norm: 1.409375\tLR: 0.030000\n",
      "Train Epoch: 430 [147456/194182 (75%)]\tLoss: 0.506475\tGrad Norm: 1.398343\tLR: 0.030000\n",
      "Train Epoch: 430 [167936/194182 (85%)]\tLoss: 0.506700\tGrad Norm: 1.473662\tLR: 0.030000\n",
      "Train Epoch: 430 [188416/194182 (96%)]\tLoss: 0.500888\tGrad Norm: 0.988015\tLR: 0.030000\n",
      "Train set: Average loss: 0.5050\n",
      "Test set: Average loss: 0.2559, Average MAE: 0.3687\n",
      "Epoch 430: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 431 [4096/194182 (2%)]\tLoss: 0.502348\tGrad Norm: 0.993679\tLR: 0.030000\n",
      "Train Epoch: 431 [24576/194182 (12%)]\tLoss: 0.494915\tGrad Norm: 0.632213\tLR: 0.030000\n",
      "Train Epoch: 431 [45056/194182 (23%)]\tLoss: 0.502308\tGrad Norm: 0.661537\tLR: 0.030000\n",
      "Train Epoch: 431 [65536/194182 (33%)]\tLoss: 0.500700\tGrad Norm: 0.980928\tLR: 0.030000\n",
      "Train Epoch: 431 [86016/194182 (44%)]\tLoss: 0.501234\tGrad Norm: 0.948530\tLR: 0.030000\n",
      "Train Epoch: 431 [106496/194182 (54%)]\tLoss: 0.497327\tGrad Norm: 1.089146\tLR: 0.030000\n",
      "Train Epoch: 431 [126976/194182 (65%)]\tLoss: 0.495740\tGrad Norm: 1.086976\tLR: 0.030000\n",
      "Train Epoch: 431 [147456/194182 (75%)]\tLoss: 0.506122\tGrad Norm: 1.176548\tLR: 0.030000\n",
      "Train Epoch: 431 [167936/194182 (85%)]\tLoss: 0.507747\tGrad Norm: 1.392223\tLR: 0.030000\n",
      "Train Epoch: 431 [188416/194182 (96%)]\tLoss: 0.501931\tGrad Norm: 1.070770\tLR: 0.030000\n",
      "Train set: Average loss: 0.5014\n",
      "Test set: Average loss: 0.2514, Average MAE: 0.3580\n",
      "Train Epoch: 432 [4096/194182 (2%)]\tLoss: 0.497764\tGrad Norm: 0.741649\tLR: 0.030000\n",
      "Train Epoch: 432 [24576/194182 (12%)]\tLoss: 0.497434\tGrad Norm: 0.849180\tLR: 0.030000\n",
      "Train Epoch: 432 [45056/194182 (23%)]\tLoss: 0.501207\tGrad Norm: 1.285399\tLR: 0.030000\n",
      "Train Epoch: 432 [65536/194182 (33%)]\tLoss: 0.505418\tGrad Norm: 1.293127\tLR: 0.030000\n",
      "Train Epoch: 432 [86016/194182 (44%)]\tLoss: 0.502660\tGrad Norm: 1.461117\tLR: 0.030000\n",
      "Train Epoch: 432 [106496/194182 (54%)]\tLoss: 0.493931\tGrad Norm: 1.252389\tLR: 0.030000\n",
      "Train Epoch: 432 [126976/194182 (65%)]\tLoss: 0.497008\tGrad Norm: 1.234129\tLR: 0.030000\n",
      "Train Epoch: 432 [147456/194182 (75%)]\tLoss: 0.510587\tGrad Norm: 1.409420\tLR: 0.030000\n",
      "Train Epoch: 432 [167936/194182 (85%)]\tLoss: 0.502796\tGrad Norm: 1.247495\tLR: 0.030000\n",
      "Train Epoch: 432 [188416/194182 (96%)]\tLoss: 0.505894\tGrad Norm: 1.203652\tLR: 0.030000\n",
      "Train set: Average loss: 0.5035\n",
      "Test set: Average loss: 0.2543, Average MAE: 0.3687\n",
      "Train Epoch: 433 [4096/194182 (2%)]\tLoss: 0.495446\tGrad Norm: 1.041962\tLR: 0.030000\n",
      "Train Epoch: 433 [24576/194182 (12%)]\tLoss: 0.500319\tGrad Norm: 1.096699\tLR: 0.030000\n",
      "Train Epoch: 433 [45056/194182 (23%)]\tLoss: 0.503592\tGrad Norm: 1.318438\tLR: 0.030000\n",
      "Train Epoch: 433 [65536/194182 (33%)]\tLoss: 0.495494\tGrad Norm: 0.813070\tLR: 0.030000\n",
      "Train Epoch: 433 [86016/194182 (44%)]\tLoss: 0.499470\tGrad Norm: 0.552965\tLR: 0.030000\n",
      "Train Epoch: 433 [106496/194182 (54%)]\tLoss: 0.491276\tGrad Norm: 0.850841\tLR: 0.030000\n",
      "Train Epoch: 433 [126976/194182 (65%)]\tLoss: 0.490646\tGrad Norm: 0.712117\tLR: 0.030000\n",
      "Train Epoch: 433 [147456/194182 (75%)]\tLoss: 0.495643\tGrad Norm: 0.733363\tLR: 0.030000\n",
      "Train Epoch: 433 [167936/194182 (85%)]\tLoss: 0.499640\tGrad Norm: 0.925579\tLR: 0.030000\n",
      "Train Epoch: 433 [188416/194182 (96%)]\tLoss: 0.502984\tGrad Norm: 1.018420\tLR: 0.030000\n",
      "Train set: Average loss: 0.4989\n",
      "Test set: Average loss: 0.2582, Average MAE: 0.3528\n",
      "Train Epoch: 434 [4096/194182 (2%)]\tLoss: 0.505759\tGrad Norm: 1.443045\tLR: 0.030000\n",
      "Train Epoch: 434 [24576/194182 (12%)]\tLoss: 0.505666\tGrad Norm: 1.512878\tLR: 0.030000\n",
      "Train Epoch: 434 [45056/194182 (23%)]\tLoss: 0.502063\tGrad Norm: 1.115008\tLR: 0.030000\n",
      "Train Epoch: 434 [65536/194182 (33%)]\tLoss: 0.492785\tGrad Norm: 0.832626\tLR: 0.030000\n",
      "Train Epoch: 434 [86016/194182 (44%)]\tLoss: 0.501630\tGrad Norm: 1.206677\tLR: 0.030000\n",
      "Train Epoch: 434 [106496/194182 (54%)]\tLoss: 0.499287\tGrad Norm: 1.163598\tLR: 0.030000\n",
      "Train Epoch: 434 [126976/194182 (65%)]\tLoss: 0.500594\tGrad Norm: 0.705749\tLR: 0.030000\n",
      "Train Epoch: 434 [147456/194182 (75%)]\tLoss: 0.506346\tGrad Norm: 1.485005\tLR: 0.030000\n",
      "Train Epoch: 434 [167936/194182 (85%)]\tLoss: 0.502050\tGrad Norm: 1.118848\tLR: 0.030000\n",
      "Train Epoch: 434 [188416/194182 (96%)]\tLoss: 0.500536\tGrad Norm: 1.018638\tLR: 0.030000\n",
      "Train set: Average loss: 0.5012\n",
      "Test set: Average loss: 0.2533, Average MAE: 0.3451\n",
      "Train Epoch: 435 [4096/194182 (2%)]\tLoss: 0.499275\tGrad Norm: 1.069176\tLR: 0.030000\n",
      "Train Epoch: 435 [24576/194182 (12%)]\tLoss: 0.500000\tGrad Norm: 1.035421\tLR: 0.030000\n",
      "Train Epoch: 435 [45056/194182 (23%)]\tLoss: 0.491743\tGrad Norm: 1.021622\tLR: 0.030000\n",
      "Train Epoch: 435 [65536/194182 (33%)]\tLoss: 0.499535\tGrad Norm: 1.109734\tLR: 0.030000\n",
      "Train Epoch: 435 [86016/194182 (44%)]\tLoss: 0.495800\tGrad Norm: 0.958240\tLR: 0.030000\n",
      "Train Epoch: 435 [106496/194182 (54%)]\tLoss: 0.499088\tGrad Norm: 1.279850\tLR: 0.030000\n",
      "Train Epoch: 435 [126976/194182 (65%)]\tLoss: 0.510612\tGrad Norm: 1.226741\tLR: 0.030000\n",
      "Train Epoch: 435 [147456/194182 (75%)]\tLoss: 0.504461\tGrad Norm: 1.329005\tLR: 0.030000\n",
      "Train Epoch: 435 [167936/194182 (85%)]\tLoss: 0.512488\tGrad Norm: 1.328473\tLR: 0.030000\n",
      "Train Epoch: 435 [188416/194182 (96%)]\tLoss: 0.497038\tGrad Norm: 0.572982\tLR: 0.030000\n",
      "Train set: Average loss: 0.5006\n",
      "Test set: Average loss: 0.2499, Average MAE: 0.3608\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 435: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 436 [4096/194182 (2%)]\tLoss: 0.494527\tGrad Norm: 0.851656\tLR: 0.030000\n",
      "Train Epoch: 436 [24576/194182 (12%)]\tLoss: 0.498968\tGrad Norm: 1.120253\tLR: 0.030000\n",
      "Train Epoch: 436 [45056/194182 (23%)]\tLoss: 0.505488\tGrad Norm: 1.309460\tLR: 0.030000\n",
      "Train Epoch: 436 [65536/194182 (33%)]\tLoss: 0.508005\tGrad Norm: 1.521070\tLR: 0.030000\n",
      "Train Epoch: 436 [86016/194182 (44%)]\tLoss: 0.508227\tGrad Norm: 1.401457\tLR: 0.030000\n",
      "Train Epoch: 436 [106496/194182 (54%)]\tLoss: 0.498865\tGrad Norm: 1.313083\tLR: 0.030000\n",
      "Train Epoch: 436 [126976/194182 (65%)]\tLoss: 0.501779\tGrad Norm: 1.231373\tLR: 0.030000\n",
      "Train Epoch: 436 [147456/194182 (75%)]\tLoss: 0.501177\tGrad Norm: 1.378269\tLR: 0.030000\n",
      "Train Epoch: 436 [167936/194182 (85%)]\tLoss: 0.498869\tGrad Norm: 1.123461\tLR: 0.030000\n",
      "Train Epoch: 436 [188416/194182 (96%)]\tLoss: 0.498820\tGrad Norm: 1.128481\tLR: 0.030000\n",
      "Train set: Average loss: 0.5022\n",
      "Test set: Average loss: 0.2561, Average MAE: 0.3561\n",
      "Train Epoch: 437 [4096/194182 (2%)]\tLoss: 0.492525\tGrad Norm: 1.164404\tLR: 0.030000\n",
      "Train Epoch: 437 [24576/194182 (12%)]\tLoss: 0.501601\tGrad Norm: 1.239512\tLR: 0.030000\n",
      "Train Epoch: 437 [45056/194182 (23%)]\tLoss: 0.500326\tGrad Norm: 1.331663\tLR: 0.030000\n",
      "Train Epoch: 437 [65536/194182 (33%)]\tLoss: 0.495531\tGrad Norm: 0.958170\tLR: 0.030000\n",
      "Train Epoch: 437 [86016/194182 (44%)]\tLoss: 0.498893\tGrad Norm: 0.801051\tLR: 0.030000\n",
      "Train Epoch: 437 [106496/194182 (54%)]\tLoss: 0.498929\tGrad Norm: 0.836732\tLR: 0.030000\n",
      "Train Epoch: 437 [126976/194182 (65%)]\tLoss: 0.499586\tGrad Norm: 1.281952\tLR: 0.030000\n",
      "Train Epoch: 437 [147456/194182 (75%)]\tLoss: 0.499278\tGrad Norm: 1.046603\tLR: 0.030000\n",
      "Train Epoch: 437 [167936/194182 (85%)]\tLoss: 0.493616\tGrad Norm: 0.917472\tLR: 0.030000\n",
      "Train Epoch: 437 [188416/194182 (96%)]\tLoss: 0.492560\tGrad Norm: 1.058317\tLR: 0.030000\n",
      "Train set: Average loss: 0.4986\n",
      "Test set: Average loss: 0.2636, Average MAE: 0.3444\n",
      "Train Epoch: 438 [4096/194182 (2%)]\tLoss: 0.508723\tGrad Norm: 2.084570\tLR: 0.030000\n",
      "Train Epoch: 438 [24576/194182 (12%)]\tLoss: 0.498674\tGrad Norm: 1.351750\tLR: 0.030000\n",
      "Train Epoch: 438 [45056/194182 (23%)]\tLoss: 0.514309\tGrad Norm: 1.446353\tLR: 0.030000\n",
      "Train Epoch: 438 [65536/194182 (33%)]\tLoss: 0.494355\tGrad Norm: 1.213948\tLR: 0.030000\n",
      "Train Epoch: 438 [86016/194182 (44%)]\tLoss: 0.503016\tGrad Norm: 0.867160\tLR: 0.030000\n",
      "Train Epoch: 438 [106496/194182 (54%)]\tLoss: 0.500911\tGrad Norm: 0.995546\tLR: 0.030000\n",
      "Train Epoch: 438 [126976/194182 (65%)]\tLoss: 0.507122\tGrad Norm: 1.100189\tLR: 0.030000\n",
      "Train Epoch: 438 [147456/194182 (75%)]\tLoss: 0.494112\tGrad Norm: 1.026556\tLR: 0.030000\n",
      "Train Epoch: 438 [167936/194182 (85%)]\tLoss: 0.499843\tGrad Norm: 0.935114\tLR: 0.030000\n",
      "Train Epoch: 438 [188416/194182 (96%)]\tLoss: 0.498852\tGrad Norm: 1.067130\tLR: 0.030000\n",
      "Train set: Average loss: 0.4999\n",
      "Test set: Average loss: 0.2520, Average MAE: 0.3526\n",
      "Train Epoch: 439 [4096/194182 (2%)]\tLoss: 0.509455\tGrad Norm: 1.046188\tLR: 0.030000\n",
      "Train Epoch: 439 [24576/194182 (12%)]\tLoss: 0.500793\tGrad Norm: 1.465897\tLR: 0.030000\n",
      "Train Epoch: 439 [45056/194182 (23%)]\tLoss: 0.500534\tGrad Norm: 1.146691\tLR: 0.030000\n",
      "Train Epoch: 439 [65536/194182 (33%)]\tLoss: 0.495290\tGrad Norm: 1.123240\tLR: 0.030000\n",
      "Train Epoch: 439 [86016/194182 (44%)]\tLoss: 0.494588\tGrad Norm: 0.903029\tLR: 0.030000\n",
      "Train Epoch: 439 [106496/194182 (54%)]\tLoss: 0.507729\tGrad Norm: 1.132638\tLR: 0.030000\n",
      "Train Epoch: 439 [126976/194182 (65%)]\tLoss: 0.501941\tGrad Norm: 1.226854\tLR: 0.030000\n",
      "Train Epoch: 439 [147456/194182 (75%)]\tLoss: 0.500393\tGrad Norm: 1.320195\tLR: 0.030000\n",
      "Train Epoch: 439 [167936/194182 (85%)]\tLoss: 0.504255\tGrad Norm: 1.220713\tLR: 0.030000\n",
      "Train Epoch: 439 [188416/194182 (96%)]\tLoss: 0.502930\tGrad Norm: 1.057855\tLR: 0.030000\n",
      "Train set: Average loss: 0.4996\n",
      "Test set: Average loss: 0.2561, Average MAE: 0.3729\n",
      "Train Epoch: 440 [4096/194182 (2%)]\tLoss: 0.499376\tGrad Norm: 1.236596\tLR: 0.030000\n",
      "Train Epoch: 440 [24576/194182 (12%)]\tLoss: 0.496637\tGrad Norm: 1.117542\tLR: 0.030000\n",
      "Train Epoch: 440 [45056/194182 (23%)]\tLoss: 0.505733\tGrad Norm: 1.299373\tLR: 0.030000\n",
      "Train Epoch: 440 [65536/194182 (33%)]\tLoss: 0.505203\tGrad Norm: 1.335646\tLR: 0.030000\n",
      "Train Epoch: 440 [86016/194182 (44%)]\tLoss: 0.495701\tGrad Norm: 0.831337\tLR: 0.030000\n",
      "Train Epoch: 440 [106496/194182 (54%)]\tLoss: 0.500768\tGrad Norm: 1.416496\tLR: 0.030000\n",
      "Train Epoch: 440 [126976/194182 (65%)]\tLoss: 0.499177\tGrad Norm: 1.024010\tLR: 0.030000\n",
      "Train Epoch: 440 [147456/194182 (75%)]\tLoss: 0.498908\tGrad Norm: 0.971625\tLR: 0.030000\n",
      "Train Epoch: 440 [167936/194182 (85%)]\tLoss: 0.492834\tGrad Norm: 0.886807\tLR: 0.030000\n",
      "Train Epoch: 440 [188416/194182 (96%)]\tLoss: 0.494095\tGrad Norm: 1.173496\tLR: 0.030000\n",
      "Train set: Average loss: 0.4983\n",
      "Test set: Average loss: 0.2632, Average MAE: 0.3792\n",
      "Epoch 440: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 441 [4096/194182 (2%)]\tLoss: 0.505223\tGrad Norm: 1.547510\tLR: 0.030000\n",
      "Train Epoch: 441 [24576/194182 (12%)]\tLoss: 0.493118\tGrad Norm: 0.776499\tLR: 0.030000\n",
      "Train Epoch: 441 [45056/194182 (23%)]\tLoss: 0.495346\tGrad Norm: 1.156545\tLR: 0.030000\n",
      "Train Epoch: 441 [65536/194182 (33%)]\tLoss: 0.493071\tGrad Norm: 0.925768\tLR: 0.030000\n",
      "Train Epoch: 441 [86016/194182 (44%)]\tLoss: 0.500605\tGrad Norm: 1.359455\tLR: 0.030000\n",
      "Train Epoch: 441 [106496/194182 (54%)]\tLoss: 0.498885\tGrad Norm: 1.476890\tLR: 0.030000\n",
      "Train Epoch: 441 [126976/194182 (65%)]\tLoss: 0.500910\tGrad Norm: 1.179653\tLR: 0.030000\n",
      "Train Epoch: 441 [147456/194182 (75%)]\tLoss: 0.500926\tGrad Norm: 1.134088\tLR: 0.030000\n",
      "Train Epoch: 441 [167936/194182 (85%)]\tLoss: 0.505142\tGrad Norm: 1.178015\tLR: 0.030000\n",
      "Train Epoch: 441 [188416/194182 (96%)]\tLoss: 0.499368\tGrad Norm: 1.074358\tLR: 0.030000\n",
      "Train set: Average loss: 0.4984\n",
      "Test set: Average loss: 0.2515, Average MAE: 0.3521\n",
      "Train Epoch: 442 [4096/194182 (2%)]\tLoss: 0.492808\tGrad Norm: 0.997523\tLR: 0.030000\n",
      "Train Epoch: 442 [24576/194182 (12%)]\tLoss: 0.497795\tGrad Norm: 1.043947\tLR: 0.030000\n",
      "Train Epoch: 442 [45056/194182 (23%)]\tLoss: 0.497194\tGrad Norm: 0.906567\tLR: 0.030000\n",
      "Train Epoch: 442 [65536/194182 (33%)]\tLoss: 0.497381\tGrad Norm: 1.161675\tLR: 0.030000\n",
      "Train Epoch: 442 [86016/194182 (44%)]\tLoss: 0.498951\tGrad Norm: 1.267975\tLR: 0.030000\n",
      "Train Epoch: 442 [106496/194182 (54%)]\tLoss: 0.496483\tGrad Norm: 1.224702\tLR: 0.030000\n",
      "Train Epoch: 442 [126976/194182 (65%)]\tLoss: 0.492138\tGrad Norm: 1.195106\tLR: 0.030000\n",
      "Train Epoch: 442 [147456/194182 (75%)]\tLoss: 0.497533\tGrad Norm: 1.129918\tLR: 0.030000\n",
      "Train Epoch: 442 [167936/194182 (85%)]\tLoss: 0.509447\tGrad Norm: 1.366233\tLR: 0.030000\n",
      "Train Epoch: 442 [188416/194182 (96%)]\tLoss: 0.504224\tGrad Norm: 1.422725\tLR: 0.030000\n",
      "Train set: Average loss: 0.4986\n",
      "Test set: Average loss: 0.2547, Average MAE: 0.3425\n",
      "Train Epoch: 443 [4096/194182 (2%)]\tLoss: 0.500954\tGrad Norm: 1.440955\tLR: 0.030000\n",
      "Train Epoch: 443 [24576/194182 (12%)]\tLoss: 0.485996\tGrad Norm: 1.049350\tLR: 0.030000\n",
      "Train Epoch: 443 [45056/194182 (23%)]\tLoss: 0.489790\tGrad Norm: 0.787434\tLR: 0.030000\n",
      "Train Epoch: 443 [65536/194182 (33%)]\tLoss: 0.484512\tGrad Norm: 0.581979\tLR: 0.030000\n",
      "Train Epoch: 443 [86016/194182 (44%)]\tLoss: 0.499416\tGrad Norm: 0.862370\tLR: 0.030000\n",
      "Train Epoch: 443 [106496/194182 (54%)]\tLoss: 0.491434\tGrad Norm: 0.801665\tLR: 0.030000\n",
      "Train Epoch: 443 [126976/194182 (65%)]\tLoss: 0.493915\tGrad Norm: 0.809083\tLR: 0.030000\n",
      "Train Epoch: 443 [147456/194182 (75%)]\tLoss: 0.496268\tGrad Norm: 0.906745\tLR: 0.030000\n",
      "Train Epoch: 443 [167936/194182 (85%)]\tLoss: 0.503792\tGrad Norm: 1.239935\tLR: 0.030000\n",
      "Train Epoch: 443 [188416/194182 (96%)]\tLoss: 0.497995\tGrad Norm: 1.215590\tLR: 0.030000\n",
      "Train set: Average loss: 0.4942\n",
      "Test set: Average loss: 0.2561, Average MAE: 0.3426\n",
      "Train Epoch: 444 [4096/194182 (2%)]\tLoss: 0.502191\tGrad Norm: 1.379198\tLR: 0.030000\n",
      "Train Epoch: 444 [24576/194182 (12%)]\tLoss: 0.497037\tGrad Norm: 1.156562\tLR: 0.030000\n",
      "Train Epoch: 444 [45056/194182 (23%)]\tLoss: 0.504649\tGrad Norm: 1.345668\tLR: 0.030000\n",
      "Train Epoch: 444 [65536/194182 (33%)]\tLoss: 0.508979\tGrad Norm: 1.470809\tLR: 0.030000\n",
      "Train Epoch: 444 [86016/194182 (44%)]\tLoss: 0.500444\tGrad Norm: 1.160138\tLR: 0.030000\n",
      "Train Epoch: 444 [106496/194182 (54%)]\tLoss: 0.502312\tGrad Norm: 1.162607\tLR: 0.030000\n",
      "Train Epoch: 444 [126976/194182 (65%)]\tLoss: 0.488450\tGrad Norm: 1.072200\tLR: 0.030000\n",
      "Train Epoch: 444 [147456/194182 (75%)]\tLoss: 0.501619\tGrad Norm: 1.177240\tLR: 0.030000\n",
      "Train Epoch: 444 [167936/194182 (85%)]\tLoss: 0.492342\tGrad Norm: 0.763803\tLR: 0.030000\n",
      "Train Epoch: 444 [188416/194182 (96%)]\tLoss: 0.493493\tGrad Norm: 0.478074\tLR: 0.030000\n",
      "Train set: Average loss: 0.4955\n",
      "Test set: Average loss: 0.2517, Average MAE: 0.3590\n",
      "Train Epoch: 445 [4096/194182 (2%)]\tLoss: 0.490888\tGrad Norm: 0.914597\tLR: 0.030000\n",
      "Train Epoch: 445 [24576/194182 (12%)]\tLoss: 0.497484\tGrad Norm: 1.156795\tLR: 0.030000\n",
      "Train Epoch: 445 [45056/194182 (23%)]\tLoss: 0.491548\tGrad Norm: 0.965955\tLR: 0.030000\n",
      "Train Epoch: 445 [65536/194182 (33%)]\tLoss: 0.495663\tGrad Norm: 1.106518\tLR: 0.030000\n",
      "Train Epoch: 445 [86016/194182 (44%)]\tLoss: 0.508668\tGrad Norm: 1.516324\tLR: 0.030000\n",
      "Train Epoch: 445 [106496/194182 (54%)]\tLoss: 0.500959\tGrad Norm: 1.399085\tLR: 0.030000\n",
      "Train Epoch: 445 [126976/194182 (65%)]\tLoss: 0.503931\tGrad Norm: 1.147410\tLR: 0.030000\n",
      "Train Epoch: 445 [147456/194182 (75%)]\tLoss: 0.494717\tGrad Norm: 0.896100\tLR: 0.030000\n",
      "Train Epoch: 445 [167936/194182 (85%)]\tLoss: 0.495814\tGrad Norm: 0.784854\tLR: 0.030000\n",
      "Train Epoch: 445 [188416/194182 (96%)]\tLoss: 0.488983\tGrad Norm: 0.634495\tLR: 0.030000\n",
      "Train set: Average loss: 0.4951\n",
      "Test set: Average loss: 0.2485, Average MAE: 0.3558\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Epoch 445: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 446 [4096/194182 (2%)]\tLoss: 0.489166\tGrad Norm: 0.723591\tLR: 0.030000\n",
      "Train Epoch: 446 [24576/194182 (12%)]\tLoss: 0.490075\tGrad Norm: 0.958338\tLR: 0.030000\n",
      "Train Epoch: 446 [45056/194182 (23%)]\tLoss: 0.493495\tGrad Norm: 1.037176\tLR: 0.030000\n",
      "Train Epoch: 446 [65536/194182 (33%)]\tLoss: 0.500819\tGrad Norm: 1.100832\tLR: 0.030000\n",
      "Train Epoch: 446 [86016/194182 (44%)]\tLoss: 0.499578\tGrad Norm: 1.517830\tLR: 0.030000\n",
      "Train Epoch: 446 [106496/194182 (54%)]\tLoss: 0.500014\tGrad Norm: 1.509770\tLR: 0.030000\n",
      "Train Epoch: 446 [126976/194182 (65%)]\tLoss: 0.505708\tGrad Norm: 1.548277\tLR: 0.030000\n",
      "Train Epoch: 446 [147456/194182 (75%)]\tLoss: 0.496714\tGrad Norm: 0.981771\tLR: 0.030000\n",
      "Train Epoch: 446 [167936/194182 (85%)]\tLoss: 0.486327\tGrad Norm: 0.909141\tLR: 0.030000\n",
      "Train Epoch: 446 [188416/194182 (96%)]\tLoss: 0.487084\tGrad Norm: 0.651867\tLR: 0.030000\n",
      "Train set: Average loss: 0.4951\n",
      "Test set: Average loss: 0.2487, Average MAE: 0.3409\n",
      "Train Epoch: 447 [4096/194182 (2%)]\tLoss: 0.490884\tGrad Norm: 0.849890\tLR: 0.030000\n",
      "Train Epoch: 447 [24576/194182 (12%)]\tLoss: 0.489782\tGrad Norm: 1.123818\tLR: 0.030000\n",
      "Train Epoch: 447 [45056/194182 (23%)]\tLoss: 0.484612\tGrad Norm: 0.949630\tLR: 0.030000\n",
      "Train Epoch: 447 [65536/194182 (33%)]\tLoss: 0.497502\tGrad Norm: 0.946871\tLR: 0.030000\n",
      "Train Epoch: 447 [86016/194182 (44%)]\tLoss: 0.485913\tGrad Norm: 0.923541\tLR: 0.030000\n",
      "Train Epoch: 447 [106496/194182 (54%)]\tLoss: 0.490270\tGrad Norm: 1.085255\tLR: 0.030000\n",
      "Train Epoch: 447 [126976/194182 (65%)]\tLoss: 0.498902\tGrad Norm: 1.292030\tLR: 0.030000\n",
      "Train Epoch: 447 [147456/194182 (75%)]\tLoss: 0.489306\tGrad Norm: 0.805652\tLR: 0.030000\n",
      "Train Epoch: 447 [167936/194182 (85%)]\tLoss: 0.493903\tGrad Norm: 1.082557\tLR: 0.030000\n",
      "Train Epoch: 447 [188416/194182 (96%)]\tLoss: 0.497498\tGrad Norm: 1.199589\tLR: 0.030000\n",
      "Train set: Average loss: 0.4931\n",
      "Test set: Average loss: 0.2594, Average MAE: 0.3630\n",
      "Train Epoch: 448 [4096/194182 (2%)]\tLoss: 0.494075\tGrad Norm: 1.466037\tLR: 0.030000\n",
      "Train Epoch: 448 [24576/194182 (12%)]\tLoss: 0.494373\tGrad Norm: 1.480102\tLR: 0.030000\n",
      "Train Epoch: 448 [45056/194182 (23%)]\tLoss: 0.497770\tGrad Norm: 1.432043\tLR: 0.030000\n",
      "Train Epoch: 448 [65536/194182 (33%)]\tLoss: 0.504796\tGrad Norm: 1.557619\tLR: 0.030000\n",
      "Train Epoch: 448 [86016/194182 (44%)]\tLoss: 0.493479\tGrad Norm: 1.102770\tLR: 0.030000\n",
      "Train Epoch: 448 [106496/194182 (54%)]\tLoss: 0.496614\tGrad Norm: 1.285225\tLR: 0.030000\n",
      "Train Epoch: 448 [126976/194182 (65%)]\tLoss: 0.484082\tGrad Norm: 1.164707\tLR: 0.030000\n",
      "Train Epoch: 448 [147456/194182 (75%)]\tLoss: 0.481840\tGrad Norm: 0.925392\tLR: 0.030000\n",
      "Train Epoch: 448 [167936/194182 (85%)]\tLoss: 0.496161\tGrad Norm: 0.837114\tLR: 0.030000\n",
      "Train Epoch: 448 [188416/194182 (96%)]\tLoss: 0.496496\tGrad Norm: 0.716762\tLR: 0.030000\n",
      "Train set: Average loss: 0.4952\n",
      "Test set: Average loss: 0.2504, Average MAE: 0.3449\n",
      "Train Epoch: 449 [4096/194182 (2%)]\tLoss: 0.487787\tGrad Norm: 0.855544\tLR: 0.030000\n",
      "Train Epoch: 449 [24576/194182 (12%)]\tLoss: 0.498649\tGrad Norm: 1.167874\tLR: 0.030000\n",
      "Train Epoch: 449 [45056/194182 (23%)]\tLoss: 0.491045\tGrad Norm: 0.692781\tLR: 0.030000\n",
      "Train Epoch: 449 [65536/194182 (33%)]\tLoss: 0.491640\tGrad Norm: 0.960926\tLR: 0.030000\n",
      "Train Epoch: 449 [86016/194182 (44%)]\tLoss: 0.494183\tGrad Norm: 1.380275\tLR: 0.030000\n",
      "Train Epoch: 449 [106496/194182 (54%)]\tLoss: 0.495378\tGrad Norm: 1.313365\tLR: 0.030000\n",
      "Train Epoch: 449 [126976/194182 (65%)]\tLoss: 0.497498\tGrad Norm: 1.167662\tLR: 0.030000\n",
      "Train Epoch: 449 [147456/194182 (75%)]\tLoss: 0.486473\tGrad Norm: 0.871341\tLR: 0.030000\n",
      "Train Epoch: 449 [167936/194182 (85%)]\tLoss: 0.492139\tGrad Norm: 0.863704\tLR: 0.030000\n",
      "Train Epoch: 449 [188416/194182 (96%)]\tLoss: 0.493496\tGrad Norm: 1.226721\tLR: 0.030000\n",
      "Train set: Average loss: 0.4925\n",
      "Test set: Average loss: 0.2555, Average MAE: 0.3492\n",
      "Train Epoch: 450 [4096/194182 (2%)]\tLoss: 0.503404\tGrad Norm: 1.421987\tLR: 0.030000\n",
      "Train Epoch: 450 [24576/194182 (12%)]\tLoss: 0.490897\tGrad Norm: 0.912769\tLR: 0.030000\n",
      "Train Epoch: 450 [45056/194182 (23%)]\tLoss: 0.487063\tGrad Norm: 0.848806\tLR: 0.030000\n",
      "Train Epoch: 450 [65536/194182 (33%)]\tLoss: 0.486189\tGrad Norm: 0.897586\tLR: 0.030000\n",
      "Train Epoch: 450 [86016/194182 (44%)]\tLoss: 0.496547\tGrad Norm: 1.211540\tLR: 0.030000\n",
      "Train Epoch: 450 [106496/194182 (54%)]\tLoss: 0.493731\tGrad Norm: 1.034595\tLR: 0.030000\n",
      "Train Epoch: 450 [126976/194182 (65%)]\tLoss: 0.491058\tGrad Norm: 0.903488\tLR: 0.030000\n",
      "Train Epoch: 450 [147456/194182 (75%)]\tLoss: 0.493025\tGrad Norm: 0.993871\tLR: 0.030000\n",
      "Train Epoch: 450 [167936/194182 (85%)]\tLoss: 0.489591\tGrad Norm: 0.881407\tLR: 0.030000\n",
      "Train Epoch: 450 [188416/194182 (96%)]\tLoss: 0.499514\tGrad Norm: 1.553123\tLR: 0.030000\n",
      "Train set: Average loss: 0.4918\n",
      "Test set: Average loss: 0.2541, Average MAE: 0.3721\n",
      "Epoch 450: Mean reward = 0.045 +/- 0.000\n",
      "Train Epoch: 451 [4096/194182 (2%)]\tLoss: 0.487831\tGrad Norm: 1.254277\tLR: 0.030000\n",
      "Train Epoch: 451 [24576/194182 (12%)]\tLoss: 0.484678\tGrad Norm: 1.050276\tLR: 0.030000\n",
      "Train Epoch: 451 [45056/194182 (23%)]\tLoss: 0.501425\tGrad Norm: 1.187876\tLR: 0.030000\n",
      "Train Epoch: 451 [65536/194182 (33%)]\tLoss: 0.496146\tGrad Norm: 0.949034\tLR: 0.030000\n",
      "Train Epoch: 451 [86016/194182 (44%)]\tLoss: 0.496500\tGrad Norm: 1.486039\tLR: 0.030000\n",
      "Train Epoch: 451 [106496/194182 (54%)]\tLoss: 0.488106\tGrad Norm: 1.296850\tLR: 0.030000\n",
      "Train Epoch: 451 [126976/194182 (65%)]\tLoss: 0.494057\tGrad Norm: 0.801567\tLR: 0.030000\n",
      "Train Epoch: 451 [147456/194182 (75%)]\tLoss: 0.487931\tGrad Norm: 0.817099\tLR: 0.030000\n",
      "Train Epoch: 451 [167936/194182 (85%)]\tLoss: 0.492590\tGrad Norm: 1.047910\tLR: 0.030000\n",
      "Train Epoch: 451 [188416/194182 (96%)]\tLoss: 0.501056\tGrad Norm: 1.137937\tLR: 0.030000\n",
      "Train set: Average loss: 0.4918\n",
      "Test set: Average loss: 0.2520, Average MAE: 0.3619\n",
      "Train Epoch: 452 [4096/194182 (2%)]\tLoss: 0.500720\tGrad Norm: 0.903174\tLR: 0.030000\n",
      "Train Epoch: 452 [24576/194182 (12%)]\tLoss: 0.488823\tGrad Norm: 1.008829\tLR: 0.030000\n",
      "Train Epoch: 452 [45056/194182 (23%)]\tLoss: 0.484526\tGrad Norm: 1.015852\tLR: 0.030000\n",
      "Train Epoch: 452 [65536/194182 (33%)]\tLoss: 0.491201\tGrad Norm: 1.192200\tLR: 0.030000\n",
      "Train Epoch: 452 [86016/194182 (44%)]\tLoss: 0.500598\tGrad Norm: 1.583139\tLR: 0.030000\n",
      "Train Epoch: 452 [106496/194182 (54%)]\tLoss: 0.495256\tGrad Norm: 1.313617\tLR: 0.030000\n",
      "Train Epoch: 452 [126976/194182 (65%)]\tLoss: 0.499993\tGrad Norm: 1.329022\tLR: 0.030000\n",
      "Train Epoch: 452 [147456/194182 (75%)]\tLoss: 0.495190\tGrad Norm: 0.972975\tLR: 0.030000\n",
      "Train Epoch: 452 [167936/194182 (85%)]\tLoss: 0.483964\tGrad Norm: 1.147819\tLR: 0.030000\n",
      "Train Epoch: 452 [188416/194182 (96%)]\tLoss: 0.488668\tGrad Norm: 1.024018\tLR: 0.030000\n",
      "Train set: Average loss: 0.4922\n",
      "Test set: Average loss: 0.2491, Average MAE: 0.3582\n",
      "Train Epoch: 453 [4096/194182 (2%)]\tLoss: 0.488071\tGrad Norm: 0.811036\tLR: 0.030000\n",
      "Train Epoch: 453 [24576/194182 (12%)]\tLoss: 0.491153\tGrad Norm: 0.924102\tLR: 0.030000\n",
      "Train Epoch: 453 [45056/194182 (23%)]\tLoss: 0.486106\tGrad Norm: 0.975613\tLR: 0.030000\n",
      "Train Epoch: 453 [65536/194182 (33%)]\tLoss: 0.485736\tGrad Norm: 1.180181\tLR: 0.030000\n",
      "Train Epoch: 453 [86016/194182 (44%)]\tLoss: 0.487947\tGrad Norm: 1.103855\tLR: 0.030000\n",
      "Train Epoch: 453 [106496/194182 (54%)]\tLoss: 0.489895\tGrad Norm: 1.007590\tLR: 0.030000\n",
      "Train Epoch: 453 [126976/194182 (65%)]\tLoss: 0.483656\tGrad Norm: 1.267287\tLR: 0.030000\n",
      "Train Epoch: 453 [147456/194182 (75%)]\tLoss: 0.484877\tGrad Norm: 1.158299\tLR: 0.030000\n",
      "Train Epoch: 453 [167936/194182 (85%)]\tLoss: 0.491228\tGrad Norm: 0.914895\tLR: 0.030000\n",
      "Train Epoch: 453 [188416/194182 (96%)]\tLoss: 0.496751\tGrad Norm: 1.392781\tLR: 0.030000\n",
      "Train set: Average loss: 0.4910\n",
      "Test set: Average loss: 0.2533, Average MAE: 0.3443\n",
      "Train Epoch: 454 [4096/194182 (2%)]\tLoss: 0.498036\tGrad Norm: 1.223402\tLR: 0.030000\n",
      "Train Epoch: 454 [24576/194182 (12%)]\tLoss: 0.487087\tGrad Norm: 1.312283\tLR: 0.030000\n",
      "Train Epoch: 454 [45056/194182 (23%)]\tLoss: 0.503705\tGrad Norm: 1.198943\tLR: 0.030000\n",
      "Train Epoch: 454 [65536/194182 (33%)]\tLoss: 0.489445\tGrad Norm: 0.879800\tLR: 0.030000\n",
      "Train Epoch: 454 [86016/194182 (44%)]\tLoss: 0.485657\tGrad Norm: 1.207592\tLR: 0.030000\n",
      "Train Epoch: 454 [106496/194182 (54%)]\tLoss: 0.492924\tGrad Norm: 1.272512\tLR: 0.030000\n",
      "Train Epoch: 454 [126976/194182 (65%)]\tLoss: 0.489267\tGrad Norm: 1.113132\tLR: 0.030000\n",
      "Train Epoch: 454 [147456/194182 (75%)]\tLoss: 0.498033\tGrad Norm: 1.514646\tLR: 0.030000\n",
      "Train Epoch: 454 [167936/194182 (85%)]\tLoss: 0.494182\tGrad Norm: 1.180344\tLR: 0.030000\n",
      "Train Epoch: 454 [188416/194182 (96%)]\tLoss: 0.492169\tGrad Norm: 1.169366\tLR: 0.030000\n",
      "Train set: Average loss: 0.4922\n",
      "Test set: Average loss: 0.2529, Average MAE: 0.3410\n",
      "Train Epoch: 455 [4096/194182 (2%)]\tLoss: 0.490666\tGrad Norm: 1.586146\tLR: 0.030000\n",
      "Train Epoch: 455 [24576/194182 (12%)]\tLoss: 0.494715\tGrad Norm: 1.320424\tLR: 0.030000\n",
      "Train Epoch: 455 [45056/194182 (23%)]\tLoss: 0.479514\tGrad Norm: 0.766951\tLR: 0.030000\n",
      "Train Epoch: 455 [65536/194182 (33%)]\tLoss: 0.490300\tGrad Norm: 1.028421\tLR: 0.030000\n",
      "Train Epoch: 455 [86016/194182 (44%)]\tLoss: 0.486067\tGrad Norm: 1.008143\tLR: 0.030000\n",
      "Train Epoch: 455 [106496/194182 (54%)]\tLoss: 0.484400\tGrad Norm: 0.859486\tLR: 0.030000\n",
      "Train Epoch: 455 [126976/194182 (65%)]\tLoss: 0.487494\tGrad Norm: 0.833200\tLR: 0.030000\n",
      "Train Epoch: 455 [147456/194182 (75%)]\tLoss: 0.483375\tGrad Norm: 0.834883\tLR: 0.030000\n",
      "Train Epoch: 455 [167936/194182 (85%)]\tLoss: 0.484066\tGrad Norm: 0.902778\tLR: 0.030000\n",
      "Train Epoch: 455 [188416/194182 (96%)]\tLoss: 0.493079\tGrad Norm: 1.121617\tLR: 0.030000\n",
      "Train set: Average loss: 0.4886\n",
      "Test set: Average loss: 0.2495, Average MAE: 0.3589\n",
      "Epoch 455: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 456 [4096/194182 (2%)]\tLoss: 0.484454\tGrad Norm: 0.956039\tLR: 0.030000\n",
      "Train Epoch: 456 [24576/194182 (12%)]\tLoss: 0.484493\tGrad Norm: 1.110881\tLR: 0.030000\n",
      "Train Epoch: 456 [45056/194182 (23%)]\tLoss: 0.495562\tGrad Norm: 1.516359\tLR: 0.030000\n",
      "Train Epoch: 456 [65536/194182 (33%)]\tLoss: 0.495607\tGrad Norm: 1.659752\tLR: 0.030000\n",
      "Train Epoch: 456 [86016/194182 (44%)]\tLoss: 0.501613\tGrad Norm: 1.521217\tLR: 0.030000\n",
      "Train Epoch: 456 [106496/194182 (54%)]\tLoss: 0.490285\tGrad Norm: 1.101731\tLR: 0.030000\n",
      "Train Epoch: 456 [126976/194182 (65%)]\tLoss: 0.471424\tGrad Norm: 0.527255\tLR: 0.030000\n",
      "Train Epoch: 456 [147456/194182 (75%)]\tLoss: 0.484201\tGrad Norm: 0.498015\tLR: 0.030000\n",
      "Train Epoch: 456 [167936/194182 (85%)]\tLoss: 0.490791\tGrad Norm: 0.855156\tLR: 0.030000\n",
      "Train Epoch: 456 [188416/194182 (96%)]\tLoss: 0.489372\tGrad Norm: 1.088316\tLR: 0.030000\n",
      "Train set: Average loss: 0.4897\n",
      "Test set: Average loss: 0.2535, Average MAE: 0.3658\n",
      "Train Epoch: 457 [4096/194182 (2%)]\tLoss: 0.483839\tGrad Norm: 1.177077\tLR: 0.030000\n",
      "Train Epoch: 457 [24576/194182 (12%)]\tLoss: 0.475882\tGrad Norm: 0.939835\tLR: 0.030000\n",
      "Train Epoch: 457 [45056/194182 (23%)]\tLoss: 0.489215\tGrad Norm: 1.253828\tLR: 0.030000\n",
      "Train Epoch: 457 [65536/194182 (33%)]\tLoss: 0.481447\tGrad Norm: 0.812063\tLR: 0.030000\n",
      "Train Epoch: 457 [86016/194182 (44%)]\tLoss: 0.483180\tGrad Norm: 0.659927\tLR: 0.030000\n",
      "Train Epoch: 457 [106496/194182 (54%)]\tLoss: 0.490014\tGrad Norm: 0.958713\tLR: 0.030000\n",
      "Train Epoch: 457 [126976/194182 (65%)]\tLoss: 0.498346\tGrad Norm: 1.347093\tLR: 0.030000\n",
      "Train Epoch: 457 [147456/194182 (75%)]\tLoss: 0.488626\tGrad Norm: 1.336051\tLR: 0.030000\n",
      "Train Epoch: 457 [167936/194182 (85%)]\tLoss: 0.490217\tGrad Norm: 1.469137\tLR: 0.030000\n",
      "Train Epoch: 457 [188416/194182 (96%)]\tLoss: 0.492934\tGrad Norm: 1.322020\tLR: 0.030000\n",
      "Train set: Average loss: 0.4901\n",
      "Test set: Average loss: 0.2556, Average MAE: 0.3675\n",
      "Train Epoch: 458 [4096/194182 (2%)]\tLoss: 0.493126\tGrad Norm: 1.150694\tLR: 0.030000\n",
      "Train Epoch: 458 [24576/194182 (12%)]\tLoss: 0.493085\tGrad Norm: 1.129064\tLR: 0.030000\n",
      "Train Epoch: 458 [45056/194182 (23%)]\tLoss: 0.485448\tGrad Norm: 0.974003\tLR: 0.030000\n",
      "Train Epoch: 458 [65536/194182 (33%)]\tLoss: 0.485637\tGrad Norm: 0.943144\tLR: 0.030000\n",
      "Train Epoch: 458 [86016/194182 (44%)]\tLoss: 0.493416\tGrad Norm: 1.111158\tLR: 0.030000\n",
      "Train Epoch: 458 [106496/194182 (54%)]\tLoss: 0.487764\tGrad Norm: 0.955720\tLR: 0.030000\n",
      "Train Epoch: 458 [126976/194182 (65%)]\tLoss: 0.496686\tGrad Norm: 1.056555\tLR: 0.030000\n",
      "Train Epoch: 458 [147456/194182 (75%)]\tLoss: 0.481870\tGrad Norm: 0.777246\tLR: 0.030000\n",
      "Train Epoch: 458 [167936/194182 (85%)]\tLoss: 0.489696\tGrad Norm: 1.000461\tLR: 0.030000\n",
      "Train Epoch: 458 [188416/194182 (96%)]\tLoss: 0.485645\tGrad Norm: 1.140083\tLR: 0.030000\n",
      "Train set: Average loss: 0.4874\n",
      "Test set: Average loss: 0.2512, Average MAE: 0.3409\n",
      "Train Epoch: 459 [4096/194182 (2%)]\tLoss: 0.488683\tGrad Norm: 1.082314\tLR: 0.030000\n",
      "Train Epoch: 459 [24576/194182 (12%)]\tLoss: 0.482669\tGrad Norm: 0.976505\tLR: 0.030000\n",
      "Train Epoch: 459 [45056/194182 (23%)]\tLoss: 0.488785\tGrad Norm: 1.303801\tLR: 0.030000\n",
      "Train Epoch: 459 [65536/194182 (33%)]\tLoss: 0.497313\tGrad Norm: 1.757255\tLR: 0.030000\n",
      "Train Epoch: 459 [86016/194182 (44%)]\tLoss: 0.478103\tGrad Norm: 1.391330\tLR: 0.030000\n",
      "Train Epoch: 459 [106496/194182 (54%)]\tLoss: 0.481507\tGrad Norm: 1.024710\tLR: 0.030000\n",
      "Train Epoch: 459 [126976/194182 (65%)]\tLoss: 0.502499\tGrad Norm: 1.353007\tLR: 0.030000\n",
      "Train Epoch: 459 [147456/194182 (75%)]\tLoss: 0.497639\tGrad Norm: 1.224584\tLR: 0.030000\n",
      "Train Epoch: 459 [167936/194182 (85%)]\tLoss: 0.489674\tGrad Norm: 1.188861\tLR: 0.030000\n",
      "Train Epoch: 459 [188416/194182 (96%)]\tLoss: 0.488443\tGrad Norm: 1.403872\tLR: 0.030000\n",
      "Train set: Average loss: 0.4902\n",
      "Test set: Average loss: 0.2548, Average MAE: 0.3534\n",
      "Train Epoch: 460 [4096/194182 (2%)]\tLoss: 0.488886\tGrad Norm: 1.152560\tLR: 0.030000\n",
      "Train Epoch: 460 [24576/194182 (12%)]\tLoss: 0.487438\tGrad Norm: 0.830361\tLR: 0.030000\n",
      "Train Epoch: 460 [45056/194182 (23%)]\tLoss: 0.487078\tGrad Norm: 0.968212\tLR: 0.030000\n",
      "Train Epoch: 460 [65536/194182 (33%)]\tLoss: 0.482457\tGrad Norm: 0.859845\tLR: 0.030000\n",
      "Train Epoch: 460 [86016/194182 (44%)]\tLoss: 0.480625\tGrad Norm: 0.776323\tLR: 0.030000\n",
      "Train Epoch: 460 [106496/194182 (54%)]\tLoss: 0.486405\tGrad Norm: 0.895290\tLR: 0.030000\n",
      "Train Epoch: 460 [126976/194182 (65%)]\tLoss: 0.485412\tGrad Norm: 1.212822\tLR: 0.030000\n",
      "Train Epoch: 460 [147456/194182 (75%)]\tLoss: 0.487989\tGrad Norm: 1.353666\tLR: 0.030000\n",
      "Train Epoch: 460 [167936/194182 (85%)]\tLoss: 0.512250\tGrad Norm: 1.770470\tLR: 0.030000\n",
      "Train Epoch: 460 [188416/194182 (96%)]\tLoss: 0.490518\tGrad Norm: 1.152788\tLR: 0.030000\n",
      "Train set: Average loss: 0.4870\n",
      "Test set: Average loss: 0.2544, Average MAE: 0.3408\n",
      "Epoch 460: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 461 [4096/194182 (2%)]\tLoss: 0.496106\tGrad Norm: 1.352622\tLR: 0.030000\n",
      "Train Epoch: 461 [24576/194182 (12%)]\tLoss: 0.491302\tGrad Norm: 1.070500\tLR: 0.030000\n",
      "Train Epoch: 461 [45056/194182 (23%)]\tLoss: 0.481840\tGrad Norm: 0.758782\tLR: 0.030000\n",
      "Train Epoch: 461 [65536/194182 (33%)]\tLoss: 0.484800\tGrad Norm: 0.724830\tLR: 0.030000\n",
      "Train Epoch: 461 [86016/194182 (44%)]\tLoss: 0.488243\tGrad Norm: 1.103815\tLR: 0.030000\n",
      "Train Epoch: 461 [106496/194182 (54%)]\tLoss: 0.483719\tGrad Norm: 0.935895\tLR: 0.030000\n",
      "Train Epoch: 461 [126976/194182 (65%)]\tLoss: 0.482156\tGrad Norm: 1.022062\tLR: 0.030000\n",
      "Train Epoch: 461 [147456/194182 (75%)]\tLoss: 0.490144\tGrad Norm: 1.272707\tLR: 0.030000\n",
      "Train Epoch: 461 [167936/194182 (85%)]\tLoss: 0.487351\tGrad Norm: 1.479505\tLR: 0.030000\n",
      "Train Epoch: 461 [188416/194182 (96%)]\tLoss: 0.477367\tGrad Norm: 0.969083\tLR: 0.030000\n",
      "Train set: Average loss: 0.4865\n",
      "Test set: Average loss: 0.2556, Average MAE: 0.3692\n",
      "Train Epoch: 462 [4096/194182 (2%)]\tLoss: 0.487727\tGrad Norm: 1.126127\tLR: 0.030000\n",
      "Train Epoch: 462 [24576/194182 (12%)]\tLoss: 0.478896\tGrad Norm: 0.905355\tLR: 0.030000\n",
      "Train Epoch: 462 [45056/194182 (23%)]\tLoss: 0.486610\tGrad Norm: 0.870015\tLR: 0.030000\n",
      "Train Epoch: 462 [65536/194182 (33%)]\tLoss: 0.489832\tGrad Norm: 0.982227\tLR: 0.030000\n",
      "Train Epoch: 462 [86016/194182 (44%)]\tLoss: 0.494357\tGrad Norm: 1.446717\tLR: 0.030000\n",
      "Train Epoch: 462 [106496/194182 (54%)]\tLoss: 0.490500\tGrad Norm: 1.214048\tLR: 0.030000\n",
      "Train Epoch: 462 [126976/194182 (65%)]\tLoss: 0.493351\tGrad Norm: 1.291370\tLR: 0.030000\n",
      "Train Epoch: 462 [147456/194182 (75%)]\tLoss: 0.492025\tGrad Norm: 1.034737\tLR: 0.030000\n",
      "Train Epoch: 462 [167936/194182 (85%)]\tLoss: 0.485430\tGrad Norm: 1.074586\tLR: 0.030000\n",
      "Train Epoch: 462 [188416/194182 (96%)]\tLoss: 0.496723\tGrad Norm: 1.433754\tLR: 0.030000\n",
      "Train set: Average loss: 0.4867\n",
      "Test set: Average loss: 0.2565, Average MAE: 0.3408\n",
      "Train Epoch: 463 [4096/194182 (2%)]\tLoss: 0.492916\tGrad Norm: 1.581093\tLR: 0.030000\n",
      "Train Epoch: 463 [24576/194182 (12%)]\tLoss: 0.486983\tGrad Norm: 1.233123\tLR: 0.030000\n",
      "Train Epoch: 463 [45056/194182 (23%)]\tLoss: 0.478966\tGrad Norm: 0.989444\tLR: 0.030000\n",
      "Train Epoch: 463 [65536/194182 (33%)]\tLoss: 0.483100\tGrad Norm: 1.236688\tLR: 0.030000\n",
      "Train Epoch: 463 [86016/194182 (44%)]\tLoss: 0.482681\tGrad Norm: 1.294368\tLR: 0.030000\n",
      "Train Epoch: 463 [106496/194182 (54%)]\tLoss: 0.483059\tGrad Norm: 0.982251\tLR: 0.030000\n",
      "Train Epoch: 463 [126976/194182 (65%)]\tLoss: 0.483837\tGrad Norm: 0.683260\tLR: 0.030000\n",
      "Train Epoch: 463 [147456/194182 (75%)]\tLoss: 0.490060\tGrad Norm: 1.135439\tLR: 0.030000\n",
      "Train Epoch: 463 [167936/194182 (85%)]\tLoss: 0.492617\tGrad Norm: 1.613130\tLR: 0.030000\n",
      "Train Epoch: 463 [188416/194182 (96%)]\tLoss: 0.492991\tGrad Norm: 1.226372\tLR: 0.030000\n",
      "Train set: Average loss: 0.4877\n",
      "Test set: Average loss: 0.2536, Average MAE: 0.3423\n",
      "Train Epoch: 464 [4096/194182 (2%)]\tLoss: 0.488527\tGrad Norm: 1.300874\tLR: 0.030000\n",
      "Train Epoch: 464 [24576/194182 (12%)]\tLoss: 0.479872\tGrad Norm: 1.250746\tLR: 0.030000\n",
      "Train Epoch: 464 [45056/194182 (23%)]\tLoss: 0.492017\tGrad Norm: 1.415663\tLR: 0.030000\n",
      "Train Epoch: 464 [65536/194182 (33%)]\tLoss: 0.483256\tGrad Norm: 0.834402\tLR: 0.030000\n",
      "Train Epoch: 464 [86016/194182 (44%)]\tLoss: 0.482874\tGrad Norm: 0.948733\tLR: 0.030000\n",
      "Train Epoch: 464 [106496/194182 (54%)]\tLoss: 0.476130\tGrad Norm: 1.041012\tLR: 0.030000\n",
      "Train Epoch: 464 [126976/194182 (65%)]\tLoss: 0.478356\tGrad Norm: 0.975713\tLR: 0.030000\n",
      "Train Epoch: 464 [147456/194182 (75%)]\tLoss: 0.487001\tGrad Norm: 0.619689\tLR: 0.030000\n",
      "Train Epoch: 464 [167936/194182 (85%)]\tLoss: 0.485915\tGrad Norm: 0.948275\tLR: 0.030000\n",
      "Train Epoch: 464 [188416/194182 (96%)]\tLoss: 0.491432\tGrad Norm: 1.219535\tLR: 0.030000\n",
      "Train set: Average loss: 0.4845\n",
      "Test set: Average loss: 0.2515, Average MAE: 0.3626\n",
      "Train Epoch: 465 [4096/194182 (2%)]\tLoss: 0.489079\tGrad Norm: 1.028607\tLR: 0.030000\n",
      "Train Epoch: 465 [24576/194182 (12%)]\tLoss: 0.488419\tGrad Norm: 1.341257\tLR: 0.030000\n",
      "Train Epoch: 465 [45056/194182 (23%)]\tLoss: 0.488490\tGrad Norm: 0.931480\tLR: 0.030000\n",
      "Train Epoch: 465 [65536/194182 (33%)]\tLoss: 0.484537\tGrad Norm: 0.995525\tLR: 0.030000\n",
      "Train Epoch: 465 [86016/194182 (44%)]\tLoss: 0.484681\tGrad Norm: 1.077425\tLR: 0.030000\n",
      "Train Epoch: 465 [106496/194182 (54%)]\tLoss: 0.480067\tGrad Norm: 0.999047\tLR: 0.030000\n",
      "Train Epoch: 465 [126976/194182 (65%)]\tLoss: 0.479718\tGrad Norm: 1.309214\tLR: 0.030000\n",
      "Train Epoch: 465 [147456/194182 (75%)]\tLoss: 0.489156\tGrad Norm: 1.256939\tLR: 0.030000\n",
      "Train Epoch: 465 [167936/194182 (85%)]\tLoss: 0.482230\tGrad Norm: 1.488420\tLR: 0.030000\n",
      "Train Epoch: 465 [188416/194182 (96%)]\tLoss: 0.487581\tGrad Norm: 1.299651\tLR: 0.030000\n",
      "Train set: Average loss: 0.4855\n",
      "Test set: Average loss: 0.2533, Average MAE: 0.3661\n",
      "Epoch 465: Mean reward = 0.042 +/- 0.014\n",
      "Train Epoch: 466 [4096/194182 (2%)]\tLoss: 0.494975\tGrad Norm: 1.228143\tLR: 0.030000\n",
      "Train Epoch: 466 [24576/194182 (12%)]\tLoss: 0.483831\tGrad Norm: 1.294416\tLR: 0.030000\n",
      "Train Epoch: 466 [45056/194182 (23%)]\tLoss: 0.491109\tGrad Norm: 1.197587\tLR: 0.030000\n",
      "Train Epoch: 466 [65536/194182 (33%)]\tLoss: 0.488251\tGrad Norm: 1.239151\tLR: 0.030000\n",
      "Train Epoch: 466 [86016/194182 (44%)]\tLoss: 0.495526\tGrad Norm: 1.297719\tLR: 0.030000\n",
      "Train Epoch: 466 [106496/194182 (54%)]\tLoss: 0.488205\tGrad Norm: 0.944672\tLR: 0.030000\n",
      "Train Epoch: 466 [126976/194182 (65%)]\tLoss: 0.477462\tGrad Norm: 0.942547\tLR: 0.030000\n",
      "Train Epoch: 466 [147456/194182 (75%)]\tLoss: 0.482455\tGrad Norm: 1.074227\tLR: 0.030000\n",
      "Train Epoch: 466 [167936/194182 (85%)]\tLoss: 0.493157\tGrad Norm: 1.587894\tLR: 0.030000\n",
      "Train Epoch: 466 [188416/194182 (96%)]\tLoss: 0.487205\tGrad Norm: 1.209530\tLR: 0.030000\n",
      "Train set: Average loss: 0.4862\n",
      "Test set: Average loss: 0.2548, Average MAE: 0.3668\n",
      "Train Epoch: 467 [4096/194182 (2%)]\tLoss: 0.491679\tGrad Norm: 1.280604\tLR: 0.030000\n",
      "Train Epoch: 467 [24576/194182 (12%)]\tLoss: 0.491176\tGrad Norm: 1.295236\tLR: 0.030000\n",
      "Train Epoch: 467 [45056/194182 (23%)]\tLoss: 0.489107\tGrad Norm: 1.027438\tLR: 0.030000\n",
      "Train Epoch: 467 [65536/194182 (33%)]\tLoss: 0.476671\tGrad Norm: 0.751271\tLR: 0.030000\n",
      "Train Epoch: 467 [86016/194182 (44%)]\tLoss: 0.484429\tGrad Norm: 0.778068\tLR: 0.030000\n",
      "Train Epoch: 467 [106496/194182 (54%)]\tLoss: 0.482219\tGrad Norm: 1.092762\tLR: 0.030000\n",
      "Train Epoch: 467 [126976/194182 (65%)]\tLoss: 0.487466\tGrad Norm: 1.119830\tLR: 0.030000\n",
      "Train Epoch: 467 [147456/194182 (75%)]\tLoss: 0.476105\tGrad Norm: 1.171807\tLR: 0.030000\n",
      "Train Epoch: 467 [167936/194182 (85%)]\tLoss: 0.482830\tGrad Norm: 1.302987\tLR: 0.030000\n",
      "Train Epoch: 467 [188416/194182 (96%)]\tLoss: 0.485942\tGrad Norm: 0.796877\tLR: 0.030000\n",
      "Train set: Average loss: 0.4839\n",
      "Test set: Average loss: 0.2468, Average MAE: 0.3463\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 468 [4096/194182 (2%)]\tLoss: 0.479402\tGrad Norm: 0.817455\tLR: 0.030000\n",
      "Train Epoch: 468 [24576/194182 (12%)]\tLoss: 0.486360\tGrad Norm: 0.982124\tLR: 0.030000\n",
      "Train Epoch: 468 [45056/194182 (23%)]\tLoss: 0.474449\tGrad Norm: 0.934463\tLR: 0.030000\n",
      "Train Epoch: 468 [65536/194182 (33%)]\tLoss: 0.489003\tGrad Norm: 0.907643\tLR: 0.030000\n",
      "Train Epoch: 468 [86016/194182 (44%)]\tLoss: 0.484552\tGrad Norm: 0.937920\tLR: 0.030000\n",
      "Train Epoch: 468 [106496/194182 (54%)]\tLoss: 0.479593\tGrad Norm: 1.362957\tLR: 0.030000\n",
      "Train Epoch: 468 [126976/194182 (65%)]\tLoss: 0.493143\tGrad Norm: 1.391822\tLR: 0.030000\n",
      "Train Epoch: 468 [147456/194182 (75%)]\tLoss: 0.483947\tGrad Norm: 1.337212\tLR: 0.030000\n",
      "Train Epoch: 468 [167936/194182 (85%)]\tLoss: 0.474306\tGrad Norm: 0.923098\tLR: 0.030000\n",
      "Train Epoch: 468 [188416/194182 (96%)]\tLoss: 0.476721\tGrad Norm: 0.486152\tLR: 0.030000\n",
      "Train set: Average loss: 0.4833\n",
      "Test set: Average loss: 0.2459, Average MAE: 0.3448\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 469 [4096/194182 (2%)]\tLoss: 0.478615\tGrad Norm: 0.695203\tLR: 0.030000\n",
      "Train Epoch: 469 [24576/194182 (12%)]\tLoss: 0.480992\tGrad Norm: 1.121039\tLR: 0.030000\n",
      "Train Epoch: 469 [45056/194182 (23%)]\tLoss: 0.479533\tGrad Norm: 1.046637\tLR: 0.030000\n",
      "Train Epoch: 469 [65536/194182 (33%)]\tLoss: 0.476880\tGrad Norm: 1.155925\tLR: 0.030000\n",
      "Train Epoch: 469 [86016/194182 (44%)]\tLoss: 0.483745\tGrad Norm: 1.166358\tLR: 0.030000\n",
      "Train Epoch: 469 [106496/194182 (54%)]\tLoss: 0.488660\tGrad Norm: 1.253775\tLR: 0.030000\n",
      "Train Epoch: 469 [126976/194182 (65%)]\tLoss: 0.472430\tGrad Norm: 0.768316\tLR: 0.030000\n",
      "Train Epoch: 469 [147456/194182 (75%)]\tLoss: 0.499756\tGrad Norm: 0.967903\tLR: 0.030000\n",
      "Train Epoch: 469 [167936/194182 (85%)]\tLoss: 0.484269\tGrad Norm: 0.701807\tLR: 0.030000\n",
      "Train Epoch: 469 [188416/194182 (96%)]\tLoss: 0.485834\tGrad Norm: 0.910887\tLR: 0.030000\n",
      "Train set: Average loss: 0.4822\n",
      "Test set: Average loss: 0.2502, Average MAE: 0.3443\n",
      "Train Epoch: 470 [4096/194182 (2%)]\tLoss: 0.490292\tGrad Norm: 1.201657\tLR: 0.030000\n",
      "Train Epoch: 470 [24576/194182 (12%)]\tLoss: 0.476633\tGrad Norm: 0.938004\tLR: 0.030000\n",
      "Train Epoch: 470 [45056/194182 (23%)]\tLoss: 0.480419\tGrad Norm: 1.314750\tLR: 0.030000\n",
      "Train Epoch: 470 [65536/194182 (33%)]\tLoss: 0.481638\tGrad Norm: 1.326499\tLR: 0.030000\n",
      "Train Epoch: 470 [86016/194182 (44%)]\tLoss: 0.473630\tGrad Norm: 0.945364\tLR: 0.030000\n",
      "Train Epoch: 470 [106496/194182 (54%)]\tLoss: 0.481101\tGrad Norm: 0.931760\tLR: 0.030000\n",
      "Train Epoch: 470 [126976/194182 (65%)]\tLoss: 0.481322\tGrad Norm: 1.016796\tLR: 0.030000\n",
      "Train Epoch: 470 [147456/194182 (75%)]\tLoss: 0.484466\tGrad Norm: 1.340462\tLR: 0.030000\n",
      "Train Epoch: 470 [167936/194182 (85%)]\tLoss: 0.478956\tGrad Norm: 1.126492\tLR: 0.030000\n",
      "Train Epoch: 470 [188416/194182 (96%)]\tLoss: 0.478699\tGrad Norm: 1.091674\tLR: 0.030000\n",
      "Train set: Average loss: 0.4827\n",
      "Test set: Average loss: 0.2484, Average MAE: 0.3506\n",
      "Epoch 470: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 471 [4096/194182 (2%)]\tLoss: 0.478998\tGrad Norm: 1.027113\tLR: 0.030000\n",
      "Train Epoch: 471 [24576/194182 (12%)]\tLoss: 0.486267\tGrad Norm: 1.132648\tLR: 0.030000\n",
      "Train Epoch: 471 [45056/194182 (23%)]\tLoss: 0.477558\tGrad Norm: 1.140093\tLR: 0.030000\n",
      "Train Epoch: 471 [65536/194182 (33%)]\tLoss: 0.485337\tGrad Norm: 1.047439\tLR: 0.030000\n",
      "Train Epoch: 471 [86016/194182 (44%)]\tLoss: 0.474504\tGrad Norm: 0.943502\tLR: 0.030000\n",
      "Train Epoch: 471 [106496/194182 (54%)]\tLoss: 0.486249\tGrad Norm: 1.094981\tLR: 0.030000\n",
      "Train Epoch: 471 [126976/194182 (65%)]\tLoss: 0.488529\tGrad Norm: 1.291457\tLR: 0.030000\n",
      "Train Epoch: 471 [147456/194182 (75%)]\tLoss: 0.483361\tGrad Norm: 1.476310\tLR: 0.030000\n",
      "Train Epoch: 471 [167936/194182 (85%)]\tLoss: 0.487073\tGrad Norm: 1.166572\tLR: 0.030000\n",
      "Train Epoch: 471 [188416/194182 (96%)]\tLoss: 0.485336\tGrad Norm: 1.096817\tLR: 0.030000\n",
      "Train set: Average loss: 0.4833\n",
      "Test set: Average loss: 0.2494, Average MAE: 0.3385\n",
      "Train Epoch: 472 [4096/194182 (2%)]\tLoss: 0.488120\tGrad Norm: 1.173157\tLR: 0.030000\n",
      "Train Epoch: 472 [24576/194182 (12%)]\tLoss: 0.478136\tGrad Norm: 1.268838\tLR: 0.030000\n",
      "Train Epoch: 472 [45056/194182 (23%)]\tLoss: 0.477319\tGrad Norm: 1.025218\tLR: 0.030000\n",
      "Train Epoch: 472 [65536/194182 (33%)]\tLoss: 0.483598\tGrad Norm: 0.981897\tLR: 0.030000\n",
      "Train Epoch: 472 [86016/194182 (44%)]\tLoss: 0.476478\tGrad Norm: 1.176573\tLR: 0.030000\n",
      "Train Epoch: 472 [106496/194182 (54%)]\tLoss: 0.480875\tGrad Norm: 0.850476\tLR: 0.030000\n",
      "Train Epoch: 472 [126976/194182 (65%)]\tLoss: 0.480381\tGrad Norm: 0.964559\tLR: 0.030000\n",
      "Train Epoch: 472 [147456/194182 (75%)]\tLoss: 0.480849\tGrad Norm: 0.939690\tLR: 0.030000\n",
      "Train Epoch: 472 [167936/194182 (85%)]\tLoss: 0.482496\tGrad Norm: 1.015018\tLR: 0.030000\n",
      "Train Epoch: 472 [188416/194182 (96%)]\tLoss: 0.498049\tGrad Norm: 1.260647\tLR: 0.030000\n",
      "Train set: Average loss: 0.4819\n",
      "Test set: Average loss: 0.2492, Average MAE: 0.3377\n",
      "Train Epoch: 473 [4096/194182 (2%)]\tLoss: 0.485188\tGrad Norm: 1.074078\tLR: 0.030000\n",
      "Train Epoch: 473 [24576/194182 (12%)]\tLoss: 0.476192\tGrad Norm: 1.389139\tLR: 0.030000\n",
      "Train Epoch: 473 [45056/194182 (23%)]\tLoss: 0.489387\tGrad Norm: 1.485411\tLR: 0.030000\n",
      "Train Epoch: 473 [65536/194182 (33%)]\tLoss: 0.496043\tGrad Norm: 1.666956\tLR: 0.030000\n",
      "Train Epoch: 473 [86016/194182 (44%)]\tLoss: 0.490844\tGrad Norm: 1.106138\tLR: 0.030000\n",
      "Train Epoch: 473 [106496/194182 (54%)]\tLoss: 0.483767\tGrad Norm: 1.080295\tLR: 0.030000\n",
      "Train Epoch: 473 [126976/194182 (65%)]\tLoss: 0.478171\tGrad Norm: 1.193754\tLR: 0.030000\n",
      "Train Epoch: 473 [147456/194182 (75%)]\tLoss: 0.480133\tGrad Norm: 1.336501\tLR: 0.030000\n",
      "Train Epoch: 473 [167936/194182 (85%)]\tLoss: 0.472341\tGrad Norm: 0.810626\tLR: 0.030000\n",
      "Train Epoch: 473 [188416/194182 (96%)]\tLoss: 0.472105\tGrad Norm: 0.758898\tLR: 0.030000\n",
      "Train set: Average loss: 0.4834\n",
      "Test set: Average loss: 0.2471, Average MAE: 0.3500\n",
      "Train Epoch: 474 [4096/194182 (2%)]\tLoss: 0.477869\tGrad Norm: 1.062918\tLR: 0.030000\n",
      "Train Epoch: 474 [24576/194182 (12%)]\tLoss: 0.486967\tGrad Norm: 1.118822\tLR: 0.030000\n",
      "Train Epoch: 474 [45056/194182 (23%)]\tLoss: 0.480663\tGrad Norm: 0.844953\tLR: 0.030000\n",
      "Train Epoch: 474 [65536/194182 (33%)]\tLoss: 0.478752\tGrad Norm: 0.773512\tLR: 0.030000\n",
      "Train Epoch: 474 [86016/194182 (44%)]\tLoss: 0.482582\tGrad Norm: 1.418996\tLR: 0.030000\n",
      "Train Epoch: 474 [106496/194182 (54%)]\tLoss: 0.482433\tGrad Norm: 1.336747\tLR: 0.030000\n",
      "Train Epoch: 474 [126976/194182 (65%)]\tLoss: 0.472762\tGrad Norm: 0.809177\tLR: 0.030000\n",
      "Train Epoch: 474 [147456/194182 (75%)]\tLoss: 0.480951\tGrad Norm: 1.202867\tLR: 0.030000\n",
      "Train Epoch: 474 [167936/194182 (85%)]\tLoss: 0.486682\tGrad Norm: 1.380495\tLR: 0.030000\n",
      "Train Epoch: 474 [188416/194182 (96%)]\tLoss: 0.480488\tGrad Norm: 1.222100\tLR: 0.030000\n",
      "Train set: Average loss: 0.4816\n",
      "Test set: Average loss: 0.2513, Average MAE: 0.3386\n",
      "Train Epoch: 475 [4096/194182 (2%)]\tLoss: 0.491206\tGrad Norm: 1.554988\tLR: 0.030000\n",
      "Train Epoch: 475 [24576/194182 (12%)]\tLoss: 0.482548\tGrad Norm: 1.385705\tLR: 0.030000\n",
      "Train Epoch: 475 [45056/194182 (23%)]\tLoss: 0.478667\tGrad Norm: 1.153938\tLR: 0.030000\n",
      "Train Epoch: 475 [65536/194182 (33%)]\tLoss: 0.482677\tGrad Norm: 0.762097\tLR: 0.030000\n",
      "Train Epoch: 475 [86016/194182 (44%)]\tLoss: 0.477001\tGrad Norm: 0.755221\tLR: 0.030000\n",
      "Train Epoch: 475 [106496/194182 (54%)]\tLoss: 0.478282\tGrad Norm: 0.896998\tLR: 0.030000\n",
      "Train Epoch: 475 [126976/194182 (65%)]\tLoss: 0.478945\tGrad Norm: 0.837371\tLR: 0.030000\n",
      "Train Epoch: 475 [147456/194182 (75%)]\tLoss: 0.486209\tGrad Norm: 1.158378\tLR: 0.030000\n",
      "Train Epoch: 475 [167936/194182 (85%)]\tLoss: 0.492194\tGrad Norm: 1.252840\tLR: 0.030000\n",
      "Train Epoch: 475 [188416/194182 (96%)]\tLoss: 0.478364\tGrad Norm: 1.032532\tLR: 0.030000\n",
      "Train set: Average loss: 0.4805\n",
      "Test set: Average loss: 0.2517, Average MAE: 0.3449\n",
      "Epoch 475: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 476 [4096/194182 (2%)]\tLoss: 0.482129\tGrad Norm: 1.195203\tLR: 0.030000\n",
      "Train Epoch: 476 [24576/194182 (12%)]\tLoss: 0.478219\tGrad Norm: 1.205841\tLR: 0.030000\n",
      "Train Epoch: 476 [45056/194182 (23%)]\tLoss: 0.477415\tGrad Norm: 1.063964\tLR: 0.030000\n",
      "Train Epoch: 476 [65536/194182 (33%)]\tLoss: 0.485763\tGrad Norm: 1.093466\tLR: 0.030000\n",
      "Train Epoch: 476 [86016/194182 (44%)]\tLoss: 0.473586\tGrad Norm: 0.910292\tLR: 0.030000\n",
      "Train Epoch: 476 [106496/194182 (54%)]\tLoss: 0.479267\tGrad Norm: 1.082247\tLR: 0.030000\n",
      "Train Epoch: 476 [126976/194182 (65%)]\tLoss: 0.484695\tGrad Norm: 0.987169\tLR: 0.030000\n",
      "Train Epoch: 476 [147456/194182 (75%)]\tLoss: 0.477330\tGrad Norm: 1.146116\tLR: 0.030000\n",
      "Train Epoch: 476 [167936/194182 (85%)]\tLoss: 0.467282\tGrad Norm: 0.780138\tLR: 0.030000\n",
      "Train Epoch: 476 [188416/194182 (96%)]\tLoss: 0.469378\tGrad Norm: 0.850475\tLR: 0.030000\n",
      "Train set: Average loss: 0.4787\n",
      "Test set: Average loss: 0.2485, Average MAE: 0.3418\n",
      "Train Epoch: 477 [4096/194182 (2%)]\tLoss: 0.473056\tGrad Norm: 1.175556\tLR: 0.030000\n",
      "Train Epoch: 477 [24576/194182 (12%)]\tLoss: 0.477863\tGrad Norm: 1.384086\tLR: 0.030000\n",
      "Train Epoch: 477 [45056/194182 (23%)]\tLoss: 0.481886\tGrad Norm: 1.240489\tLR: 0.030000\n",
      "Train Epoch: 477 [65536/194182 (33%)]\tLoss: 0.481648\tGrad Norm: 1.012935\tLR: 0.030000\n",
      "Train Epoch: 477 [86016/194182 (44%)]\tLoss: 0.488790\tGrad Norm: 1.153991\tLR: 0.030000\n",
      "Train Epoch: 477 [106496/194182 (54%)]\tLoss: 0.482936\tGrad Norm: 0.936548\tLR: 0.030000\n",
      "Train Epoch: 477 [126976/194182 (65%)]\tLoss: 0.487829\tGrad Norm: 1.481772\tLR: 0.030000\n",
      "Train Epoch: 477 [147456/194182 (75%)]\tLoss: 0.484899\tGrad Norm: 1.248108\tLR: 0.030000\n",
      "Train Epoch: 477 [167936/194182 (85%)]\tLoss: 0.483895\tGrad Norm: 1.459923\tLR: 0.030000\n",
      "Train Epoch: 477 [188416/194182 (96%)]\tLoss: 0.471037\tGrad Norm: 0.798847\tLR: 0.030000\n",
      "Train set: Average loss: 0.4811\n",
      "Test set: Average loss: 0.2533, Average MAE: 0.3422\n",
      "Train Epoch: 478 [4096/194182 (2%)]\tLoss: 0.489611\tGrad Norm: 1.486209\tLR: 0.030000\n",
      "Train Epoch: 478 [24576/194182 (12%)]\tLoss: 0.473476\tGrad Norm: 1.218768\tLR: 0.030000\n",
      "Train Epoch: 478 [45056/194182 (23%)]\tLoss: 0.475260\tGrad Norm: 1.261581\tLR: 0.030000\n",
      "Train Epoch: 478 [65536/194182 (33%)]\tLoss: 0.487903\tGrad Norm: 1.534201\tLR: 0.030000\n",
      "Train Epoch: 478 [86016/194182 (44%)]\tLoss: 0.495109\tGrad Norm: 1.441041\tLR: 0.030000\n",
      "Train Epoch: 478 [106496/194182 (54%)]\tLoss: 0.470644\tGrad Norm: 1.014899\tLR: 0.030000\n",
      "Train Epoch: 478 [126976/194182 (65%)]\tLoss: 0.479065\tGrad Norm: 1.393742\tLR: 0.030000\n",
      "Train Epoch: 478 [147456/194182 (75%)]\tLoss: 0.473649\tGrad Norm: 1.085242\tLR: 0.030000\n",
      "Train Epoch: 478 [167936/194182 (85%)]\tLoss: 0.474277\tGrad Norm: 0.610095\tLR: 0.030000\n",
      "Train Epoch: 478 [188416/194182 (96%)]\tLoss: 0.479882\tGrad Norm: 0.618352\tLR: 0.030000\n",
      "Train set: Average loss: 0.4797\n",
      "Test set: Average loss: 0.2477, Average MAE: 0.3528\n",
      "Train Epoch: 479 [4096/194182 (2%)]\tLoss: 0.472210\tGrad Norm: 1.109482\tLR: 0.030000\n",
      "Train Epoch: 479 [24576/194182 (12%)]\tLoss: 0.487481\tGrad Norm: 1.056029\tLR: 0.030000\n",
      "Train Epoch: 479 [45056/194182 (23%)]\tLoss: 0.476331\tGrad Norm: 0.939477\tLR: 0.030000\n",
      "Train Epoch: 479 [65536/194182 (33%)]\tLoss: 0.482500\tGrad Norm: 0.923465\tLR: 0.030000\n",
      "Train Epoch: 479 [86016/194182 (44%)]\tLoss: 0.472211\tGrad Norm: 0.829254\tLR: 0.030000\n",
      "Train Epoch: 479 [106496/194182 (54%)]\tLoss: 0.483192\tGrad Norm: 1.261561\tLR: 0.030000\n",
      "Train Epoch: 479 [126976/194182 (65%)]\tLoss: 0.477457\tGrad Norm: 1.148986\tLR: 0.030000\n",
      "Train Epoch: 479 [147456/194182 (75%)]\tLoss: 0.482448\tGrad Norm: 1.305712\tLR: 0.030000\n",
      "Train Epoch: 479 [167936/194182 (85%)]\tLoss: 0.480167\tGrad Norm: 1.448443\tLR: 0.030000\n",
      "Train Epoch: 479 [188416/194182 (96%)]\tLoss: 0.486740\tGrad Norm: 1.459395\tLR: 0.030000\n",
      "Train set: Average loss: 0.4791\n",
      "Test set: Average loss: 0.2491, Average MAE: 0.3384\n",
      "Train Epoch: 480 [4096/194182 (2%)]\tLoss: 0.479330\tGrad Norm: 1.043150\tLR: 0.030000\n",
      "Train Epoch: 480 [24576/194182 (12%)]\tLoss: 0.482234\tGrad Norm: 0.800854\tLR: 0.030000\n",
      "Train Epoch: 480 [45056/194182 (23%)]\tLoss: 0.473296\tGrad Norm: 0.801561\tLR: 0.030000\n",
      "Train Epoch: 480 [65536/194182 (33%)]\tLoss: 0.480720\tGrad Norm: 1.076895\tLR: 0.030000\n",
      "Train Epoch: 480 [86016/194182 (44%)]\tLoss: 0.481617\tGrad Norm: 1.285417\tLR: 0.030000\n",
      "Train Epoch: 480 [106496/194182 (54%)]\tLoss: 0.477283\tGrad Norm: 1.261510\tLR: 0.030000\n",
      "Train Epoch: 480 [126976/194182 (65%)]\tLoss: 0.481751\tGrad Norm: 1.438453\tLR: 0.030000\n",
      "Train Epoch: 480 [147456/194182 (75%)]\tLoss: 0.475500\tGrad Norm: 1.090585\tLR: 0.030000\n",
      "Train Epoch: 480 [167936/194182 (85%)]\tLoss: 0.483983\tGrad Norm: 1.325549\tLR: 0.030000\n",
      "Train Epoch: 480 [188416/194182 (96%)]\tLoss: 0.468351\tGrad Norm: 0.991064\tLR: 0.030000\n",
      "Train set: Average loss: 0.4782\n",
      "Test set: Average loss: 0.2514, Average MAE: 0.3379\n",
      "Epoch 480: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 481 [4096/194182 (2%)]\tLoss: 0.489066\tGrad Norm: 1.337334\tLR: 0.030000\n",
      "Train Epoch: 481 [24576/194182 (12%)]\tLoss: 0.477134\tGrad Norm: 1.186301\tLR: 0.030000\n",
      "Train Epoch: 481 [45056/194182 (23%)]\tLoss: 0.467638\tGrad Norm: 0.959767\tLR: 0.030000\n",
      "Train Epoch: 481 [65536/194182 (33%)]\tLoss: 0.464489\tGrad Norm: 0.670820\tLR: 0.030000\n",
      "Train Epoch: 481 [86016/194182 (44%)]\tLoss: 0.472350\tGrad Norm: 1.030737\tLR: 0.030000\n",
      "Train Epoch: 481 [106496/194182 (54%)]\tLoss: 0.483528\tGrad Norm: 1.412656\tLR: 0.030000\n",
      "Train Epoch: 481 [126976/194182 (65%)]\tLoss: 0.486611\tGrad Norm: 1.267535\tLR: 0.030000\n",
      "Train Epoch: 481 [147456/194182 (75%)]\tLoss: 0.484157\tGrad Norm: 1.365282\tLR: 0.030000\n",
      "Train Epoch: 481 [167936/194182 (85%)]\tLoss: 0.487749\tGrad Norm: 1.495288\tLR: 0.030000\n",
      "Train Epoch: 481 [188416/194182 (96%)]\tLoss: 0.474401\tGrad Norm: 0.920694\tLR: 0.030000\n",
      "Train set: Average loss: 0.4790\n",
      "Test set: Average loss: 0.2448, Average MAE: 0.3472\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 482 [4096/194182 (2%)]\tLoss: 0.465444\tGrad Norm: 0.683097\tLR: 0.030000\n",
      "Train Epoch: 482 [24576/194182 (12%)]\tLoss: 0.475032\tGrad Norm: 0.987651\tLR: 0.030000\n",
      "Train Epoch: 482 [45056/194182 (23%)]\tLoss: 0.476603\tGrad Norm: 0.989018\tLR: 0.030000\n",
      "Train Epoch: 482 [65536/194182 (33%)]\tLoss: 0.468496\tGrad Norm: 0.923745\tLR: 0.030000\n",
      "Train Epoch: 482 [86016/194182 (44%)]\tLoss: 0.475468\tGrad Norm: 1.026990\tLR: 0.030000\n",
      "Train Epoch: 482 [106496/194182 (54%)]\tLoss: 0.468132\tGrad Norm: 0.766999\tLR: 0.030000\n",
      "Train Epoch: 482 [126976/194182 (65%)]\tLoss: 0.474724\tGrad Norm: 1.137752\tLR: 0.030000\n",
      "Train Epoch: 482 [147456/194182 (75%)]\tLoss: 0.478029\tGrad Norm: 1.157492\tLR: 0.030000\n",
      "Train Epoch: 482 [167936/194182 (85%)]\tLoss: 0.482250\tGrad Norm: 1.246411\tLR: 0.030000\n",
      "Train Epoch: 482 [188416/194182 (96%)]\tLoss: 0.476990\tGrad Norm: 1.132914\tLR: 0.030000\n",
      "Train set: Average loss: 0.4760\n",
      "Test set: Average loss: 0.2535, Average MAE: 0.3408\n",
      "Train Epoch: 483 [4096/194182 (2%)]\tLoss: 0.475042\tGrad Norm: 1.405590\tLR: 0.030000\n",
      "Train Epoch: 483 [24576/194182 (12%)]\tLoss: 0.472739\tGrad Norm: 1.152252\tLR: 0.030000\n",
      "Train Epoch: 483 [45056/194182 (23%)]\tLoss: 0.481055\tGrad Norm: 0.995440\tLR: 0.030000\n",
      "Train Epoch: 483 [65536/194182 (33%)]\tLoss: 0.477829\tGrad Norm: 0.894069\tLR: 0.030000\n",
      "Train Epoch: 483 [86016/194182 (44%)]\tLoss: 0.477249\tGrad Norm: 0.974893\tLR: 0.030000\n",
      "Train Epoch: 483 [106496/194182 (54%)]\tLoss: 0.473036\tGrad Norm: 0.950209\tLR: 0.030000\n",
      "Train Epoch: 483 [126976/194182 (65%)]\tLoss: 0.480731\tGrad Norm: 1.246636\tLR: 0.030000\n",
      "Train Epoch: 483 [147456/194182 (75%)]\tLoss: 0.481423\tGrad Norm: 1.039837\tLR: 0.030000\n",
      "Train Epoch: 483 [167936/194182 (85%)]\tLoss: 0.477910\tGrad Norm: 0.925004\tLR: 0.030000\n",
      "Train Epoch: 483 [188416/194182 (96%)]\tLoss: 0.477003\tGrad Norm: 1.020673\tLR: 0.030000\n",
      "Train set: Average loss: 0.4758\n",
      "Test set: Average loss: 0.2473, Average MAE: 0.3436\n",
      "Train Epoch: 484 [4096/194182 (2%)]\tLoss: 0.469200\tGrad Norm: 0.923224\tLR: 0.030000\n",
      "Train Epoch: 484 [24576/194182 (12%)]\tLoss: 0.480533\tGrad Norm: 1.205910\tLR: 0.030000\n",
      "Train Epoch: 484 [45056/194182 (23%)]\tLoss: 0.476794\tGrad Norm: 1.135313\tLR: 0.030000\n",
      "Train Epoch: 484 [65536/194182 (33%)]\tLoss: 0.486563\tGrad Norm: 1.434589\tLR: 0.030000\n",
      "Train Epoch: 484 [86016/194182 (44%)]\tLoss: 0.475599\tGrad Norm: 1.289677\tLR: 0.030000\n",
      "Train Epoch: 484 [106496/194182 (54%)]\tLoss: 0.479755\tGrad Norm: 1.251827\tLR: 0.030000\n",
      "Train Epoch: 484 [126976/194182 (65%)]\tLoss: 0.470066\tGrad Norm: 1.059675\tLR: 0.030000\n",
      "Train Epoch: 484 [147456/194182 (75%)]\tLoss: 0.472795\tGrad Norm: 1.116072\tLR: 0.030000\n",
      "Train Epoch: 484 [167936/194182 (85%)]\tLoss: 0.482732\tGrad Norm: 1.320176\tLR: 0.030000\n",
      "Train Epoch: 484 [188416/194182 (96%)]\tLoss: 0.480725\tGrad Norm: 1.219477\tLR: 0.030000\n",
      "Train set: Average loss: 0.4782\n",
      "Test set: Average loss: 0.2536, Average MAE: 0.3413\n",
      "Train Epoch: 485 [4096/194182 (2%)]\tLoss: 0.484422\tGrad Norm: 1.494146\tLR: 0.030000\n",
      "Train Epoch: 485 [24576/194182 (12%)]\tLoss: 0.480723\tGrad Norm: 1.009511\tLR: 0.030000\n",
      "Train Epoch: 485 [45056/194182 (23%)]\tLoss: 0.464922\tGrad Norm: 0.957771\tLR: 0.030000\n",
      "Train Epoch: 485 [65536/194182 (33%)]\tLoss: 0.479019\tGrad Norm: 1.066149\tLR: 0.030000\n",
      "Train Epoch: 485 [86016/194182 (44%)]\tLoss: 0.478088\tGrad Norm: 0.900211\tLR: 0.030000\n",
      "Train Epoch: 485 [106496/194182 (54%)]\tLoss: 0.473975\tGrad Norm: 1.067034\tLR: 0.030000\n",
      "Train Epoch: 485 [126976/194182 (65%)]\tLoss: 0.473048\tGrad Norm: 1.011449\tLR: 0.030000\n",
      "Train Epoch: 485 [147456/194182 (75%)]\tLoss: 0.473248\tGrad Norm: 1.041865\tLR: 0.030000\n",
      "Train Epoch: 485 [167936/194182 (85%)]\tLoss: 0.466922\tGrad Norm: 1.053080\tLR: 0.030000\n",
      "Train Epoch: 485 [188416/194182 (96%)]\tLoss: 0.472018\tGrad Norm: 1.213566\tLR: 0.030000\n",
      "Train set: Average loss: 0.4751\n",
      "Test set: Average loss: 0.2526, Average MAE: 0.3674\n",
      "Epoch 485: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 486 [4096/194182 (2%)]\tLoss: 0.471279\tGrad Norm: 1.306166\tLR: 0.030000\n",
      "Train Epoch: 486 [24576/194182 (12%)]\tLoss: 0.482194\tGrad Norm: 1.284535\tLR: 0.030000\n",
      "Train Epoch: 486 [45056/194182 (23%)]\tLoss: 0.483471\tGrad Norm: 1.128845\tLR: 0.030000\n",
      "Train Epoch: 486 [65536/194182 (33%)]\tLoss: 0.478225\tGrad Norm: 1.055680\tLR: 0.030000\n",
      "Train Epoch: 486 [86016/194182 (44%)]\tLoss: 0.482412\tGrad Norm: 1.023161\tLR: 0.030000\n",
      "Train Epoch: 486 [106496/194182 (54%)]\tLoss: 0.477961\tGrad Norm: 1.220698\tLR: 0.030000\n",
      "Train Epoch: 486 [126976/194182 (65%)]\tLoss: 0.484084\tGrad Norm: 1.427701\tLR: 0.030000\n",
      "Train Epoch: 486 [147456/194182 (75%)]\tLoss: 0.474546\tGrad Norm: 1.193076\tLR: 0.030000\n",
      "Train Epoch: 486 [167936/194182 (85%)]\tLoss: 0.479392\tGrad Norm: 0.946632\tLR: 0.030000\n",
      "Train Epoch: 486 [188416/194182 (96%)]\tLoss: 0.473196\tGrad Norm: 1.150864\tLR: 0.030000\n",
      "Train set: Average loss: 0.4762\n",
      "Test set: Average loss: 0.2527, Average MAE: 0.3646\n",
      "Train Epoch: 487 [4096/194182 (2%)]\tLoss: 0.474918\tGrad Norm: 1.289686\tLR: 0.030000\n",
      "Train Epoch: 487 [24576/194182 (12%)]\tLoss: 0.469658\tGrad Norm: 1.266515\tLR: 0.030000\n",
      "Train Epoch: 487 [45056/194182 (23%)]\tLoss: 0.471622\tGrad Norm: 1.040144\tLR: 0.030000\n",
      "Train Epoch: 487 [65536/194182 (33%)]\tLoss: 0.477976\tGrad Norm: 1.207509\tLR: 0.030000\n",
      "Train Epoch: 487 [86016/194182 (44%)]\tLoss: 0.477204\tGrad Norm: 0.954940\tLR: 0.030000\n",
      "Train Epoch: 487 [106496/194182 (54%)]\tLoss: 0.479547\tGrad Norm: 1.380720\tLR: 0.030000\n",
      "Train Epoch: 487 [126976/194182 (65%)]\tLoss: 0.472245\tGrad Norm: 1.250429\tLR: 0.030000\n",
      "Train Epoch: 487 [147456/194182 (75%)]\tLoss: 0.477283\tGrad Norm: 1.310412\tLR: 0.030000\n",
      "Train Epoch: 487 [167936/194182 (85%)]\tLoss: 0.477792\tGrad Norm: 1.160251\tLR: 0.030000\n",
      "Train Epoch: 487 [188416/194182 (96%)]\tLoss: 0.466884\tGrad Norm: 1.163342\tLR: 0.030000\n",
      "Train set: Average loss: 0.4765\n",
      "Test set: Average loss: 0.2512, Average MAE: 0.3414\n",
      "Train Epoch: 488 [4096/194182 (2%)]\tLoss: 0.477272\tGrad Norm: 1.244687\tLR: 0.030000\n",
      "Train Epoch: 488 [24576/194182 (12%)]\tLoss: 0.473590\tGrad Norm: 1.303937\tLR: 0.030000\n",
      "Train Epoch: 488 [45056/194182 (23%)]\tLoss: 0.471195\tGrad Norm: 0.740283\tLR: 0.030000\n",
      "Train Epoch: 488 [65536/194182 (33%)]\tLoss: 0.471759\tGrad Norm: 0.980103\tLR: 0.030000\n",
      "Train Epoch: 488 [86016/194182 (44%)]\tLoss: 0.479212\tGrad Norm: 1.314945\tLR: 0.030000\n",
      "Train Epoch: 488 [106496/194182 (54%)]\tLoss: 0.478755\tGrad Norm: 1.195180\tLR: 0.030000\n",
      "Train Epoch: 488 [126976/194182 (65%)]\tLoss: 0.472490\tGrad Norm: 1.133514\tLR: 0.030000\n",
      "Train Epoch: 488 [147456/194182 (75%)]\tLoss: 0.471799\tGrad Norm: 1.080393\tLR: 0.030000\n",
      "Train Epoch: 488 [167936/194182 (85%)]\tLoss: 0.472462\tGrad Norm: 1.130785\tLR: 0.030000\n",
      "Train Epoch: 488 [188416/194182 (96%)]\tLoss: 0.479402\tGrad Norm: 1.065328\tLR: 0.030000\n",
      "Train set: Average loss: 0.4746\n",
      "Test set: Average loss: 0.2494, Average MAE: 0.3498\n",
      "Train Epoch: 489 [4096/194182 (2%)]\tLoss: 0.474793\tGrad Norm: 1.209420\tLR: 0.030000\n",
      "Train Epoch: 489 [24576/194182 (12%)]\tLoss: 0.481694\tGrad Norm: 1.230680\tLR: 0.030000\n",
      "Train Epoch: 489 [45056/194182 (23%)]\tLoss: 0.471296\tGrad Norm: 0.971376\tLR: 0.030000\n",
      "Train Epoch: 489 [65536/194182 (33%)]\tLoss: 0.468275\tGrad Norm: 1.027004\tLR: 0.030000\n",
      "Train Epoch: 489 [86016/194182 (44%)]\tLoss: 0.475388\tGrad Norm: 1.042095\tLR: 0.030000\n",
      "Train Epoch: 489 [106496/194182 (54%)]\tLoss: 0.475437\tGrad Norm: 1.029007\tLR: 0.030000\n",
      "Train Epoch: 489 [126976/194182 (65%)]\tLoss: 0.472068\tGrad Norm: 1.053922\tLR: 0.030000\n",
      "Train Epoch: 489 [147456/194182 (75%)]\tLoss: 0.473697\tGrad Norm: 0.739980\tLR: 0.030000\n",
      "Train Epoch: 489 [167936/194182 (85%)]\tLoss: 0.479798\tGrad Norm: 1.371553\tLR: 0.030000\n",
      "Train Epoch: 489 [188416/194182 (96%)]\tLoss: 0.482708\tGrad Norm: 1.420427\tLR: 0.030000\n",
      "Train set: Average loss: 0.4740\n",
      "Test set: Average loss: 0.2571, Average MAE: 0.3744\n",
      "Train Epoch: 490 [4096/194182 (2%)]\tLoss: 0.479527\tGrad Norm: 1.514266\tLR: 0.030000\n",
      "Train Epoch: 490 [24576/194182 (12%)]\tLoss: 0.477676\tGrad Norm: 1.365054\tLR: 0.030000\n",
      "Train Epoch: 490 [45056/194182 (23%)]\tLoss: 0.475048\tGrad Norm: 1.208736\tLR: 0.030000\n",
      "Train Epoch: 490 [65536/194182 (33%)]\tLoss: 0.473730\tGrad Norm: 0.996236\tLR: 0.030000\n",
      "Train Epoch: 490 [86016/194182 (44%)]\tLoss: 0.465378\tGrad Norm: 0.942301\tLR: 0.030000\n",
      "Train Epoch: 490 [106496/194182 (54%)]\tLoss: 0.474694\tGrad Norm: 0.737605\tLR: 0.030000\n",
      "Train Epoch: 490 [126976/194182 (65%)]\tLoss: 0.466560\tGrad Norm: 0.930584\tLR: 0.030000\n",
      "Train Epoch: 490 [147456/194182 (75%)]\tLoss: 0.470695\tGrad Norm: 1.064738\tLR: 0.030000\n",
      "Train Epoch: 490 [167936/194182 (85%)]\tLoss: 0.471769\tGrad Norm: 1.161339\tLR: 0.030000\n",
      "Train Epoch: 490 [188416/194182 (96%)]\tLoss: 0.463320\tGrad Norm: 1.084871\tLR: 0.030000\n",
      "Train set: Average loss: 0.4733\n",
      "Test set: Average loss: 0.2482, Average MAE: 0.3577\n",
      "Epoch 490: Mean reward = 0.063 +/- 0.069\n",
      "Train Epoch: 491 [4096/194182 (2%)]\tLoss: 0.463005\tGrad Norm: 1.028293\tLR: 0.030000\n",
      "Train Epoch: 491 [24576/194182 (12%)]\tLoss: 0.461757\tGrad Norm: 1.204785\tLR: 0.030000\n",
      "Train Epoch: 491 [45056/194182 (23%)]\tLoss: 0.475832\tGrad Norm: 1.167272\tLR: 0.030000\n",
      "Train Epoch: 491 [65536/194182 (33%)]\tLoss: 0.461274\tGrad Norm: 1.037384\tLR: 0.030000\n",
      "Train Epoch: 491 [86016/194182 (44%)]\tLoss: 0.475381\tGrad Norm: 1.174864\tLR: 0.030000\n",
      "Train Epoch: 491 [106496/194182 (54%)]\tLoss: 0.474579\tGrad Norm: 1.325375\tLR: 0.030000\n",
      "Train Epoch: 491 [126976/194182 (65%)]\tLoss: 0.474975\tGrad Norm: 1.010120\tLR: 0.030000\n",
      "Train Epoch: 491 [147456/194182 (75%)]\tLoss: 0.471888\tGrad Norm: 1.114329\tLR: 0.030000\n",
      "Train Epoch: 491 [167936/194182 (85%)]\tLoss: 0.487943\tGrad Norm: 1.127479\tLR: 0.030000\n",
      "Train Epoch: 491 [188416/194182 (96%)]\tLoss: 0.475209\tGrad Norm: 1.111391\tLR: 0.030000\n",
      "Train set: Average loss: 0.4735\n",
      "Test set: Average loss: 0.2487, Average MAE: 0.3396\n",
      "Train Epoch: 492 [4096/194182 (2%)]\tLoss: 0.475648\tGrad Norm: 1.161772\tLR: 0.030000\n",
      "Train Epoch: 492 [24576/194182 (12%)]\tLoss: 0.475437\tGrad Norm: 0.810816\tLR: 0.030000\n",
      "Train Epoch: 492 [45056/194182 (23%)]\tLoss: 0.471637\tGrad Norm: 0.984205\tLR: 0.030000\n",
      "Train Epoch: 492 [65536/194182 (33%)]\tLoss: 0.483474\tGrad Norm: 1.676006\tLR: 0.030000\n",
      "Train Epoch: 492 [86016/194182 (44%)]\tLoss: 0.477719\tGrad Norm: 1.396251\tLR: 0.030000\n",
      "Train Epoch: 492 [106496/194182 (54%)]\tLoss: 0.470420\tGrad Norm: 0.723627\tLR: 0.030000\n",
      "Train Epoch: 492 [126976/194182 (65%)]\tLoss: 0.471625\tGrad Norm: 0.724686\tLR: 0.030000\n",
      "Train Epoch: 492 [147456/194182 (75%)]\tLoss: 0.460633\tGrad Norm: 0.968367\tLR: 0.030000\n",
      "Train Epoch: 492 [167936/194182 (85%)]\tLoss: 0.476194\tGrad Norm: 1.310671\tLR: 0.030000\n",
      "Train Epoch: 492 [188416/194182 (96%)]\tLoss: 0.487866\tGrad Norm: 1.464327\tLR: 0.030000\n",
      "Train set: Average loss: 0.4736\n",
      "Test set: Average loss: 0.2502, Average MAE: 0.3367\n",
      "Train Epoch: 493 [4096/194182 (2%)]\tLoss: 0.482596\tGrad Norm: 1.224901\tLR: 0.030000\n",
      "Train Epoch: 493 [24576/194182 (12%)]\tLoss: 0.469955\tGrad Norm: 0.881271\tLR: 0.030000\n",
      "Train Epoch: 493 [45056/194182 (23%)]\tLoss: 0.476029\tGrad Norm: 1.144147\tLR: 0.030000\n",
      "Train Epoch: 493 [65536/194182 (33%)]\tLoss: 0.463126\tGrad Norm: 0.934905\tLR: 0.030000\n",
      "Train Epoch: 493 [86016/194182 (44%)]\tLoss: 0.477517\tGrad Norm: 1.392249\tLR: 0.030000\n",
      "Train Epoch: 493 [106496/194182 (54%)]\tLoss: 0.471176\tGrad Norm: 1.421892\tLR: 0.030000\n",
      "Train Epoch: 493 [126976/194182 (65%)]\tLoss: 0.477308\tGrad Norm: 1.113623\tLR: 0.030000\n",
      "Train Epoch: 493 [147456/194182 (75%)]\tLoss: 0.472611\tGrad Norm: 0.905215\tLR: 0.030000\n",
      "Train Epoch: 493 [167936/194182 (85%)]\tLoss: 0.474996\tGrad Norm: 0.833541\tLR: 0.030000\n",
      "Train Epoch: 493 [188416/194182 (96%)]\tLoss: 0.473118\tGrad Norm: 1.014771\tLR: 0.030000\n",
      "Train set: Average loss: 0.4724\n",
      "Test set: Average loss: 0.2481, Average MAE: 0.3376\n",
      "Train Epoch: 494 [4096/194182 (2%)]\tLoss: 0.466432\tGrad Norm: 1.097941\tLR: 0.030000\n",
      "Train Epoch: 494 [24576/194182 (12%)]\tLoss: 0.471840\tGrad Norm: 0.701822\tLR: 0.030000\n",
      "Train Epoch: 494 [45056/194182 (23%)]\tLoss: 0.472471\tGrad Norm: 0.749617\tLR: 0.030000\n",
      "Train Epoch: 494 [65536/194182 (33%)]\tLoss: 0.466265\tGrad Norm: 0.956503\tLR: 0.030000\n",
      "Train Epoch: 494 [86016/194182 (44%)]\tLoss: 0.482273\tGrad Norm: 1.342792\tLR: 0.030000\n",
      "Train Epoch: 494 [106496/194182 (54%)]\tLoss: 0.482991\tGrad Norm: 1.157946\tLR: 0.030000\n",
      "Train Epoch: 494 [126976/194182 (65%)]\tLoss: 0.464788\tGrad Norm: 0.683603\tLR: 0.030000\n",
      "Train Epoch: 494 [147456/194182 (75%)]\tLoss: 0.456975\tGrad Norm: 0.673018\tLR: 0.030000\n",
      "Train Epoch: 494 [167936/194182 (85%)]\tLoss: 0.466125\tGrad Norm: 1.023971\tLR: 0.030000\n",
      "Train Epoch: 494 [188416/194182 (96%)]\tLoss: 0.474332\tGrad Norm: 1.100845\tLR: 0.030000\n",
      "Train set: Average loss: 0.4691\n",
      "Test set: Average loss: 0.2469, Average MAE: 0.3380\n",
      "Train Epoch: 495 [4096/194182 (2%)]\tLoss: 0.468381\tGrad Norm: 1.195541\tLR: 0.030000\n",
      "Train Epoch: 495 [24576/194182 (12%)]\tLoss: 0.474074\tGrad Norm: 1.225230\tLR: 0.030000\n",
      "Train Epoch: 495 [45056/194182 (23%)]\tLoss: 0.467285\tGrad Norm: 1.125935\tLR: 0.030000\n",
      "Train Epoch: 495 [65536/194182 (33%)]\tLoss: 0.469729\tGrad Norm: 1.283280\tLR: 0.030000\n",
      "Train Epoch: 495 [86016/194182 (44%)]\tLoss: 0.470213\tGrad Norm: 1.251145\tLR: 0.030000\n",
      "Train Epoch: 495 [106496/194182 (54%)]\tLoss: 0.469416\tGrad Norm: 1.159372\tLR: 0.030000\n",
      "Train Epoch: 495 [126976/194182 (65%)]\tLoss: 0.482639\tGrad Norm: 1.423877\tLR: 0.030000\n",
      "Train Epoch: 495 [147456/194182 (75%)]\tLoss: 0.458865\tGrad Norm: 0.862891\tLR: 0.030000\n",
      "Train Epoch: 495 [167936/194182 (85%)]\tLoss: 0.467217\tGrad Norm: 0.871115\tLR: 0.030000\n",
      "Train Epoch: 495 [188416/194182 (96%)]\tLoss: 0.466845\tGrad Norm: 1.073008\tLR: 0.030000\n",
      "Train set: Average loss: 0.4720\n",
      "Test set: Average loss: 0.2560, Average MAE: 0.3711\n",
      "Epoch 495: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 496 [4096/194182 (2%)]\tLoss: 0.476707\tGrad Norm: 1.442493\tLR: 0.030000\n",
      "Train Epoch: 496 [24576/194182 (12%)]\tLoss: 0.474024\tGrad Norm: 1.258015\tLR: 0.030000\n",
      "Train Epoch: 496 [45056/194182 (23%)]\tLoss: 0.473046\tGrad Norm: 1.269216\tLR: 0.030000\n",
      "Train Epoch: 496 [65536/194182 (33%)]\tLoss: 0.461916\tGrad Norm: 0.916582\tLR: 0.030000\n",
      "Train Epoch: 496 [86016/194182 (44%)]\tLoss: 0.468931\tGrad Norm: 0.840755\tLR: 0.030000\n",
      "Train Epoch: 496 [106496/194182 (54%)]\tLoss: 0.470833\tGrad Norm: 0.990963\tLR: 0.030000\n",
      "Train Epoch: 496 [126976/194182 (65%)]\tLoss: 0.475178\tGrad Norm: 1.143082\tLR: 0.030000\n",
      "Train Epoch: 496 [147456/194182 (75%)]\tLoss: 0.469798\tGrad Norm: 1.081514\tLR: 0.030000\n",
      "Train Epoch: 496 [167936/194182 (85%)]\tLoss: 0.473061\tGrad Norm: 0.980815\tLR: 0.030000\n",
      "Train Epoch: 496 [188416/194182 (96%)]\tLoss: 0.468242\tGrad Norm: 0.801292\tLR: 0.030000\n",
      "Train set: Average loss: 0.4705\n",
      "Test set: Average loss: 0.2438, Average MAE: 0.3455\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 497 [4096/194182 (2%)]\tLoss: 0.462944\tGrad Norm: 0.848283\tLR: 0.030000\n",
      "Train Epoch: 497 [24576/194182 (12%)]\tLoss: 0.466537\tGrad Norm: 1.016789\tLR: 0.030000\n",
      "Train Epoch: 497 [45056/194182 (23%)]\tLoss: 0.467046\tGrad Norm: 1.156209\tLR: 0.030000\n",
      "Train Epoch: 497 [65536/194182 (33%)]\tLoss: 0.473464\tGrad Norm: 1.239652\tLR: 0.030000\n",
      "Train Epoch: 497 [86016/194182 (44%)]\tLoss: 0.476158\tGrad Norm: 1.387646\tLR: 0.030000\n",
      "Train Epoch: 497 [106496/194182 (54%)]\tLoss: 0.470843\tGrad Norm: 1.420253\tLR: 0.030000\n",
      "Train Epoch: 497 [126976/194182 (65%)]\tLoss: 0.470382\tGrad Norm: 1.501054\tLR: 0.030000\n",
      "Train Epoch: 497 [147456/194182 (75%)]\tLoss: 0.466270\tGrad Norm: 1.072491\tLR: 0.030000\n",
      "Train Epoch: 497 [167936/194182 (85%)]\tLoss: 0.461285\tGrad Norm: 0.997763\tLR: 0.030000\n",
      "Train Epoch: 497 [188416/194182 (96%)]\tLoss: 0.471914\tGrad Norm: 0.880345\tLR: 0.030000\n",
      "Train set: Average loss: 0.4716\n",
      "Test set: Average loss: 0.2499, Average MAE: 0.3479\n",
      "Train Epoch: 498 [4096/194182 (2%)]\tLoss: 0.477152\tGrad Norm: 1.256809\tLR: 0.030000\n",
      "Train Epoch: 498 [24576/194182 (12%)]\tLoss: 0.466299\tGrad Norm: 0.949161\tLR: 0.030000\n",
      "Train Epoch: 498 [45056/194182 (23%)]\tLoss: 0.464813\tGrad Norm: 0.418890\tLR: 0.030000\n",
      "Train Epoch: 498 [65536/194182 (33%)]\tLoss: 0.469660\tGrad Norm: 0.443643\tLR: 0.030000\n",
      "Train Epoch: 498 [86016/194182 (44%)]\tLoss: 0.480246\tGrad Norm: 1.258649\tLR: 0.030000\n",
      "Train Epoch: 498 [106496/194182 (54%)]\tLoss: 0.469691\tGrad Norm: 1.279650\tLR: 0.030000\n",
      "Train Epoch: 498 [126976/194182 (65%)]\tLoss: 0.472593\tGrad Norm: 1.193084\tLR: 0.030000\n",
      "Train Epoch: 498 [147456/194182 (75%)]\tLoss: 0.466827\tGrad Norm: 0.941027\tLR: 0.030000\n",
      "Train Epoch: 498 [167936/194182 (85%)]\tLoss: 0.476779\tGrad Norm: 1.133533\tLR: 0.030000\n",
      "Train Epoch: 498 [188416/194182 (96%)]\tLoss: 0.473230\tGrad Norm: 1.300796\tLR: 0.030000\n",
      "Train set: Average loss: 0.4686\n",
      "Test set: Average loss: 0.2442, Average MAE: 0.3385\n",
      "Train Epoch: 499 [4096/194182 (2%)]\tLoss: 0.458622\tGrad Norm: 0.682681\tLR: 0.030000\n",
      "Train Epoch: 499 [24576/194182 (12%)]\tLoss: 0.458999\tGrad Norm: 0.836585\tLR: 0.030000\n",
      "Train Epoch: 499 [45056/194182 (23%)]\tLoss: 0.468529\tGrad Norm: 0.902818\tLR: 0.030000\n",
      "Train Epoch: 499 [65536/194182 (33%)]\tLoss: 0.475726\tGrad Norm: 1.179217\tLR: 0.030000\n",
      "Train Epoch: 499 [86016/194182 (44%)]\tLoss: 0.476009\tGrad Norm: 1.634286\tLR: 0.030000\n",
      "Train Epoch: 499 [106496/194182 (54%)]\tLoss: 0.471301\tGrad Norm: 1.037301\tLR: 0.030000\n",
      "Train Epoch: 499 [126976/194182 (65%)]\tLoss: 0.471270\tGrad Norm: 1.257573\tLR: 0.030000\n",
      "Train Epoch: 499 [147456/194182 (75%)]\tLoss: 0.470633\tGrad Norm: 1.213285\tLR: 0.030000\n",
      "Train Epoch: 499 [167936/194182 (85%)]\tLoss: 0.470661\tGrad Norm: 1.068212\tLR: 0.030000\n",
      "Train Epoch: 499 [188416/194182 (96%)]\tLoss: 0.472591\tGrad Norm: 1.120572\tLR: 0.030000\n",
      "Train set: Average loss: 0.4689\n",
      "Test set: Average loss: 0.2504, Average MAE: 0.3458\n",
      "Train Epoch: 500 [4096/194182 (2%)]\tLoss: 0.470417\tGrad Norm: 1.252621\tLR: 0.030000\n",
      "Train Epoch: 500 [24576/194182 (12%)]\tLoss: 0.466002\tGrad Norm: 1.125250\tLR: 0.030000\n",
      "Train Epoch: 500 [45056/194182 (23%)]\tLoss: 0.467749\tGrad Norm: 1.175748\tLR: 0.030000\n",
      "Train Epoch: 500 [65536/194182 (33%)]\tLoss: 0.466443\tGrad Norm: 0.996102\tLR: 0.030000\n",
      "Train Epoch: 500 [86016/194182 (44%)]\tLoss: 0.476385\tGrad Norm: 1.329023\tLR: 0.030000\n",
      "Train Epoch: 500 [106496/194182 (54%)]\tLoss: 0.468410\tGrad Norm: 1.157441\tLR: 0.030000\n",
      "Train Epoch: 500 [126976/194182 (65%)]\tLoss: 0.461106\tGrad Norm: 1.170550\tLR: 0.030000\n",
      "Train Epoch: 500 [147456/194182 (75%)]\tLoss: 0.472819\tGrad Norm: 1.265363\tLR: 0.030000\n",
      "Train Epoch: 500 [167936/194182 (85%)]\tLoss: 0.466195\tGrad Norm: 1.114870\tLR: 0.030000\n",
      "Train Epoch: 500 [188416/194182 (96%)]\tLoss: 0.468085\tGrad Norm: 1.259406\tLR: 0.030000\n",
      "Train set: Average loss: 0.4702\n",
      "Test set: Average loss: 0.2519, Average MAE: 0.3375\n",
      "Epoch 500: Mean reward = 0.043 +/- 0.028\n",
      "Train Epoch: 501 [4096/194182 (2%)]\tLoss: 0.468469\tGrad Norm: 1.381786\tLR: 0.030000\n",
      "Train Epoch: 501 [24576/194182 (12%)]\tLoss: 0.482381\tGrad Norm: 1.558488\tLR: 0.030000\n",
      "Train Epoch: 501 [45056/194182 (23%)]\tLoss: 0.473341\tGrad Norm: 1.160554\tLR: 0.030000\n",
      "Train Epoch: 501 [65536/194182 (33%)]\tLoss: 0.464459\tGrad Norm: 1.063335\tLR: 0.030000\n",
      "Train Epoch: 501 [86016/194182 (44%)]\tLoss: 0.467668\tGrad Norm: 0.843368\tLR: 0.030000\n",
      "Train Epoch: 501 [106496/194182 (54%)]\tLoss: 0.474536\tGrad Norm: 1.247382\tLR: 0.030000\n",
      "Train Epoch: 501 [126976/194182 (65%)]\tLoss: 0.465301\tGrad Norm: 0.997585\tLR: 0.030000\n",
      "Train Epoch: 501 [147456/194182 (75%)]\tLoss: 0.473384\tGrad Norm: 1.341626\tLR: 0.030000\n",
      "Train Epoch: 501 [167936/194182 (85%)]\tLoss: 0.473899\tGrad Norm: 1.246579\tLR: 0.030000\n",
      "Train Epoch: 501 [188416/194182 (96%)]\tLoss: 0.464595\tGrad Norm: 1.048889\tLR: 0.030000\n",
      "Train set: Average loss: 0.4697\n",
      "Test set: Average loss: 0.2459, Average MAE: 0.3421\n",
      "Train Epoch: 502 [4096/194182 (2%)]\tLoss: 0.465001\tGrad Norm: 1.063775\tLR: 0.030000\n",
      "Train Epoch: 502 [24576/194182 (12%)]\tLoss: 0.458582\tGrad Norm: 0.765964\tLR: 0.030000\n",
      "Train Epoch: 502 [45056/194182 (23%)]\tLoss: 0.459114\tGrad Norm: 0.948445\tLR: 0.030000\n",
      "Train Epoch: 502 [65536/194182 (33%)]\tLoss: 0.468739\tGrad Norm: 0.921615\tLR: 0.030000\n",
      "Train Epoch: 502 [86016/194182 (44%)]\tLoss: 0.463912\tGrad Norm: 1.288898\tLR: 0.030000\n",
      "Train Epoch: 502 [106496/194182 (54%)]\tLoss: 0.480050\tGrad Norm: 1.657614\tLR: 0.030000\n",
      "Train Epoch: 502 [126976/194182 (65%)]\tLoss: 0.474421\tGrad Norm: 1.472685\tLR: 0.030000\n",
      "Train Epoch: 502 [147456/194182 (75%)]\tLoss: 0.477103\tGrad Norm: 1.291597\tLR: 0.030000\n",
      "Train Epoch: 502 [167936/194182 (85%)]\tLoss: 0.463730\tGrad Norm: 1.086707\tLR: 0.030000\n",
      "Train Epoch: 502 [188416/194182 (96%)]\tLoss: 0.471681\tGrad Norm: 1.055189\tLR: 0.030000\n",
      "Train set: Average loss: 0.4689\n",
      "Test set: Average loss: 0.2488, Average MAE: 0.3532\n",
      "Train Epoch: 503 [4096/194182 (2%)]\tLoss: 0.474137\tGrad Norm: 1.281383\tLR: 0.030000\n",
      "Train Epoch: 503 [24576/194182 (12%)]\tLoss: 0.467208\tGrad Norm: 0.789290\tLR: 0.030000\n",
      "Train Epoch: 503 [45056/194182 (23%)]\tLoss: 0.467662\tGrad Norm: 0.964877\tLR: 0.030000\n",
      "Train Epoch: 503 [65536/194182 (33%)]\tLoss: 0.480042\tGrad Norm: 1.278121\tLR: 0.030000\n",
      "Train Epoch: 503 [86016/194182 (44%)]\tLoss: 0.472517\tGrad Norm: 1.606379\tLR: 0.030000\n",
      "Train Epoch: 503 [106496/194182 (54%)]\tLoss: 0.475208\tGrad Norm: 1.136122\tLR: 0.030000\n",
      "Train Epoch: 503 [126976/194182 (65%)]\tLoss: 0.467090\tGrad Norm: 1.401784\tLR: 0.030000\n",
      "Train Epoch: 503 [147456/194182 (75%)]\tLoss: 0.460788\tGrad Norm: 1.184189\tLR: 0.030000\n",
      "Train Epoch: 503 [167936/194182 (85%)]\tLoss: 0.468620\tGrad Norm: 0.904854\tLR: 0.030000\n",
      "Train Epoch: 503 [188416/194182 (96%)]\tLoss: 0.458627\tGrad Norm: 0.849650\tLR: 0.030000\n",
      "Train set: Average loss: 0.4684\n",
      "Test set: Average loss: 0.2476, Average MAE: 0.3516\n",
      "Train Epoch: 504 [4096/194182 (2%)]\tLoss: 0.474485\tGrad Norm: 1.254893\tLR: 0.030000\n",
      "Train Epoch: 504 [24576/194182 (12%)]\tLoss: 0.460009\tGrad Norm: 1.197636\tLR: 0.030000\n",
      "Train Epoch: 504 [45056/194182 (23%)]\tLoss: 0.467030\tGrad Norm: 1.051510\tLR: 0.030000\n",
      "Train Epoch: 504 [65536/194182 (33%)]\tLoss: 0.477215\tGrad Norm: 1.168741\tLR: 0.030000\n",
      "Train Epoch: 504 [86016/194182 (44%)]\tLoss: 0.460065\tGrad Norm: 1.262814\tLR: 0.030000\n",
      "Train Epoch: 504 [106496/194182 (54%)]\tLoss: 0.469032\tGrad Norm: 1.226205\tLR: 0.030000\n",
      "Train Epoch: 504 [126976/194182 (65%)]\tLoss: 0.469534\tGrad Norm: 0.786030\tLR: 0.030000\n",
      "Train Epoch: 504 [147456/194182 (75%)]\tLoss: 0.466387\tGrad Norm: 0.867038\tLR: 0.030000\n",
      "Train Epoch: 504 [167936/194182 (85%)]\tLoss: 0.460161\tGrad Norm: 0.919449\tLR: 0.030000\n",
      "Train Epoch: 504 [188416/194182 (96%)]\tLoss: 0.462405\tGrad Norm: 0.760953\tLR: 0.030000\n",
      "Train set: Average loss: 0.4671\n",
      "Test set: Average loss: 0.2457, Average MAE: 0.3509\n",
      "Train Epoch: 505 [4096/194182 (2%)]\tLoss: 0.459800\tGrad Norm: 0.928045\tLR: 0.030000\n",
      "Train Epoch: 505 [24576/194182 (12%)]\tLoss: 0.470508\tGrad Norm: 1.130204\tLR: 0.030000\n",
      "Train Epoch: 505 [45056/194182 (23%)]\tLoss: 0.472334\tGrad Norm: 0.986516\tLR: 0.030000\n",
      "Train Epoch: 505 [65536/194182 (33%)]\tLoss: 0.476028\tGrad Norm: 1.300421\tLR: 0.030000\n",
      "Train Epoch: 505 [86016/194182 (44%)]\tLoss: 0.471842\tGrad Norm: 1.250413\tLR: 0.030000\n",
      "Train Epoch: 505 [106496/194182 (54%)]\tLoss: 0.464686\tGrad Norm: 1.161361\tLR: 0.030000\n",
      "Train Epoch: 505 [126976/194182 (65%)]\tLoss: 0.464551\tGrad Norm: 1.488549\tLR: 0.030000\n",
      "Train Epoch: 505 [147456/194182 (75%)]\tLoss: 0.469359\tGrad Norm: 1.075382\tLR: 0.030000\n",
      "Train Epoch: 505 [167936/194182 (85%)]\tLoss: 0.465814\tGrad Norm: 0.880676\tLR: 0.030000\n",
      "Train Epoch: 505 [188416/194182 (96%)]\tLoss: 0.461097\tGrad Norm: 0.863611\tLR: 0.030000\n",
      "Train set: Average loss: 0.4672\n",
      "Test set: Average loss: 0.2444, Average MAE: 0.3440\n",
      "Epoch 505: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 506 [4096/194182 (2%)]\tLoss: 0.462961\tGrad Norm: 0.898019\tLR: 0.030000\n",
      "Train Epoch: 506 [24576/194182 (12%)]\tLoss: 0.458712\tGrad Norm: 1.021187\tLR: 0.030000\n",
      "Train Epoch: 506 [45056/194182 (23%)]\tLoss: 0.466613\tGrad Norm: 1.132539\tLR: 0.030000\n",
      "Train Epoch: 506 [65536/194182 (33%)]\tLoss: 0.473498\tGrad Norm: 1.322979\tLR: 0.030000\n",
      "Train Epoch: 506 [86016/194182 (44%)]\tLoss: 0.477208\tGrad Norm: 1.305693\tLR: 0.030000\n",
      "Train Epoch: 506 [106496/194182 (54%)]\tLoss: 0.478398\tGrad Norm: 1.320708\tLR: 0.030000\n",
      "Train Epoch: 506 [126976/194182 (65%)]\tLoss: 0.471264\tGrad Norm: 1.175405\tLR: 0.030000\n",
      "Train Epoch: 506 [147456/194182 (75%)]\tLoss: 0.470849\tGrad Norm: 1.249756\tLR: 0.030000\n",
      "Train Epoch: 506 [167936/194182 (85%)]\tLoss: 0.459837\tGrad Norm: 0.858537\tLR: 0.030000\n",
      "Train Epoch: 506 [188416/194182 (96%)]\tLoss: 0.465214\tGrad Norm: 1.147670\tLR: 0.030000\n",
      "Train set: Average loss: 0.4665\n",
      "Test set: Average loss: 0.2475, Average MAE: 0.3503\n",
      "Train Epoch: 507 [4096/194182 (2%)]\tLoss: 0.463856\tGrad Norm: 1.012156\tLR: 0.030000\n",
      "Train Epoch: 507 [24576/194182 (12%)]\tLoss: 0.466994\tGrad Norm: 0.971355\tLR: 0.030000\n",
      "Train Epoch: 507 [45056/194182 (23%)]\tLoss: 0.463989\tGrad Norm: 1.078781\tLR: 0.030000\n",
      "Train Epoch: 507 [65536/194182 (33%)]\tLoss: 0.465301\tGrad Norm: 1.170084\tLR: 0.030000\n",
      "Train Epoch: 507 [86016/194182 (44%)]\tLoss: 0.469532\tGrad Norm: 1.265195\tLR: 0.030000\n",
      "Train Epoch: 507 [106496/194182 (54%)]\tLoss: 0.465503\tGrad Norm: 1.045369\tLR: 0.030000\n",
      "Train Epoch: 507 [126976/194182 (65%)]\tLoss: 0.468918\tGrad Norm: 1.243923\tLR: 0.030000\n",
      "Train Epoch: 507 [147456/194182 (75%)]\tLoss: 0.465898\tGrad Norm: 0.906824\tLR: 0.030000\n",
      "Train Epoch: 507 [167936/194182 (85%)]\tLoss: 0.455066\tGrad Norm: 0.696462\tLR: 0.030000\n",
      "Train Epoch: 507 [188416/194182 (96%)]\tLoss: 0.471167\tGrad Norm: 1.320270\tLR: 0.030000\n",
      "Train set: Average loss: 0.4660\n",
      "Test set: Average loss: 0.2584, Average MAE: 0.3438\n",
      "Train Epoch: 508 [4096/194182 (2%)]\tLoss: 0.476452\tGrad Norm: 2.378746\tLR: 0.030000\n",
      "Train Epoch: 508 [24576/194182 (12%)]\tLoss: 0.472460\tGrad Norm: 1.671026\tLR: 0.030000\n",
      "Train Epoch: 508 [45056/194182 (23%)]\tLoss: 0.462792\tGrad Norm: 1.023785\tLR: 0.030000\n",
      "Train Epoch: 508 [65536/194182 (33%)]\tLoss: 0.463841\tGrad Norm: 1.372780\tLR: 0.030000\n",
      "Train Epoch: 508 [86016/194182 (44%)]\tLoss: 0.470074\tGrad Norm: 1.445012\tLR: 0.030000\n",
      "Train Epoch: 508 [106496/194182 (54%)]\tLoss: 0.453394\tGrad Norm: 0.798554\tLR: 0.030000\n",
      "Train Epoch: 508 [126976/194182 (65%)]\tLoss: 0.466688\tGrad Norm: 0.857609\tLR: 0.030000\n",
      "Train Epoch: 508 [147456/194182 (75%)]\tLoss: 0.467404\tGrad Norm: 1.083247\tLR: 0.030000\n",
      "Train Epoch: 508 [167936/194182 (85%)]\tLoss: 0.461828\tGrad Norm: 0.963745\tLR: 0.030000\n",
      "Train Epoch: 508 [188416/194182 (96%)]\tLoss: 0.466362\tGrad Norm: 1.114259\tLR: 0.030000\n",
      "Train set: Average loss: 0.4670\n",
      "Test set: Average loss: 0.2455, Average MAE: 0.3412\n",
      "Train Epoch: 509 [4096/194182 (2%)]\tLoss: 0.465281\tGrad Norm: 1.012043\tLR: 0.030000\n",
      "Train Epoch: 509 [24576/194182 (12%)]\tLoss: 0.472662\tGrad Norm: 1.163566\tLR: 0.030000\n",
      "Train Epoch: 509 [45056/194182 (23%)]\tLoss: 0.457066\tGrad Norm: 1.275757\tLR: 0.030000\n",
      "Train Epoch: 509 [65536/194182 (33%)]\tLoss: 0.468428\tGrad Norm: 1.059334\tLR: 0.030000\n",
      "Train Epoch: 509 [86016/194182 (44%)]\tLoss: 0.468024\tGrad Norm: 1.178262\tLR: 0.030000\n",
      "Train Epoch: 509 [106496/194182 (54%)]\tLoss: 0.465498\tGrad Norm: 1.147198\tLR: 0.030000\n",
      "Train Epoch: 509 [126976/194182 (65%)]\tLoss: 0.456444\tGrad Norm: 0.987086\tLR: 0.030000\n",
      "Train Epoch: 509 [147456/194182 (75%)]\tLoss: 0.471027\tGrad Norm: 1.381244\tLR: 0.030000\n",
      "Train Epoch: 509 [167936/194182 (85%)]\tLoss: 0.463736\tGrad Norm: 1.239497\tLR: 0.030000\n",
      "Train Epoch: 509 [188416/194182 (96%)]\tLoss: 0.468063\tGrad Norm: 1.255180\tLR: 0.030000\n",
      "Train set: Average loss: 0.4663\n",
      "Test set: Average loss: 0.2475, Average MAE: 0.3373\n",
      "Train Epoch: 510 [4096/194182 (2%)]\tLoss: 0.462962\tGrad Norm: 1.114185\tLR: 0.030000\n",
      "Train Epoch: 510 [24576/194182 (12%)]\tLoss: 0.461551\tGrad Norm: 0.903062\tLR: 0.030000\n",
      "Train Epoch: 510 [45056/194182 (23%)]\tLoss: 0.471220\tGrad Norm: 1.394246\tLR: 0.030000\n",
      "Train Epoch: 510 [65536/194182 (33%)]\tLoss: 0.469393\tGrad Norm: 1.540124\tLR: 0.030000\n",
      "Train Epoch: 510 [86016/194182 (44%)]\tLoss: 0.466630\tGrad Norm: 1.052534\tLR: 0.030000\n",
      "Train Epoch: 510 [106496/194182 (54%)]\tLoss: 0.464866\tGrad Norm: 1.142074\tLR: 0.030000\n",
      "Train Epoch: 510 [126976/194182 (65%)]\tLoss: 0.459974\tGrad Norm: 1.226105\tLR: 0.030000\n",
      "Train Epoch: 510 [147456/194182 (75%)]\tLoss: 0.469442\tGrad Norm: 1.176732\tLR: 0.030000\n",
      "Train Epoch: 510 [167936/194182 (85%)]\tLoss: 0.467957\tGrad Norm: 1.187981\tLR: 0.030000\n",
      "Train Epoch: 510 [188416/194182 (96%)]\tLoss: 0.473436\tGrad Norm: 1.443860\tLR: 0.030000\n",
      "Train set: Average loss: 0.4664\n",
      "Test set: Average loss: 0.2515, Average MAE: 0.3508\n",
      "Epoch 510: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 511 [4096/194182 (2%)]\tLoss: 0.467614\tGrad Norm: 1.247134\tLR: 0.030000\n",
      "Train Epoch: 511 [24576/194182 (12%)]\tLoss: 0.466637\tGrad Norm: 1.115387\tLR: 0.030000\n",
      "Train Epoch: 511 [45056/194182 (23%)]\tLoss: 0.474403\tGrad Norm: 1.175210\tLR: 0.030000\n",
      "Train Epoch: 511 [65536/194182 (33%)]\tLoss: 0.465246\tGrad Norm: 1.025109\tLR: 0.030000\n",
      "Train Epoch: 511 [86016/194182 (44%)]\tLoss: 0.461963\tGrad Norm: 1.152450\tLR: 0.030000\n",
      "Train Epoch: 511 [106496/194182 (54%)]\tLoss: 0.459903\tGrad Norm: 1.219788\tLR: 0.030000\n",
      "Train Epoch: 511 [126976/194182 (65%)]\tLoss: 0.467634\tGrad Norm: 1.072571\tLR: 0.030000\n",
      "Train Epoch: 511 [147456/194182 (75%)]\tLoss: 0.467824\tGrad Norm: 1.328586\tLR: 0.030000\n",
      "Train Epoch: 511 [167936/194182 (85%)]\tLoss: 0.463059\tGrad Norm: 0.976542\tLR: 0.030000\n",
      "Train Epoch: 511 [188416/194182 (96%)]\tLoss: 0.461555\tGrad Norm: 1.201782\tLR: 0.030000\n",
      "Train set: Average loss: 0.4643\n",
      "Test set: Average loss: 0.2500, Average MAE: 0.3394\n",
      "Train Epoch: 512 [4096/194182 (2%)]\tLoss: 0.470290\tGrad Norm: 1.282794\tLR: 0.030000\n",
      "Train Epoch: 512 [24576/194182 (12%)]\tLoss: 0.464815\tGrad Norm: 1.182679\tLR: 0.030000\n",
      "Train Epoch: 512 [45056/194182 (23%)]\tLoss: 0.463695\tGrad Norm: 1.132767\tLR: 0.030000\n",
      "Train Epoch: 512 [65536/194182 (33%)]\tLoss: 0.468906\tGrad Norm: 1.275717\tLR: 0.030000\n",
      "Train Epoch: 512 [86016/194182 (44%)]\tLoss: 0.446282\tGrad Norm: 0.549356\tLR: 0.030000\n",
      "Train Epoch: 512 [106496/194182 (54%)]\tLoss: 0.458988\tGrad Norm: 0.773704\tLR: 0.030000\n",
      "Train Epoch: 512 [126976/194182 (65%)]\tLoss: 0.460260\tGrad Norm: 0.834430\tLR: 0.030000\n",
      "Train Epoch: 512 [147456/194182 (75%)]\tLoss: 0.466976\tGrad Norm: 1.380878\tLR: 0.030000\n",
      "Train Epoch: 512 [167936/194182 (85%)]\tLoss: 0.459110\tGrad Norm: 1.085331\tLR: 0.030000\n",
      "Train Epoch: 512 [188416/194182 (96%)]\tLoss: 0.462890\tGrad Norm: 1.339218\tLR: 0.030000\n",
      "Train set: Average loss: 0.4638\n",
      "Test set: Average loss: 0.2478, Average MAE: 0.3395\n",
      "Train Epoch: 513 [4096/194182 (2%)]\tLoss: 0.468142\tGrad Norm: 1.199885\tLR: 0.030000\n",
      "Train Epoch: 513 [24576/194182 (12%)]\tLoss: 0.467947\tGrad Norm: 0.975438\tLR: 0.030000\n",
      "Train Epoch: 513 [45056/194182 (23%)]\tLoss: 0.467315\tGrad Norm: 1.152401\tLR: 0.030000\n",
      "Train Epoch: 513 [65536/194182 (33%)]\tLoss: 0.453263\tGrad Norm: 1.019471\tLR: 0.030000\n",
      "Train Epoch: 513 [86016/194182 (44%)]\tLoss: 0.459274\tGrad Norm: 1.281722\tLR: 0.030000\n",
      "Train Epoch: 513 [106496/194182 (54%)]\tLoss: 0.466926\tGrad Norm: 1.093402\tLR: 0.030000\n",
      "Train Epoch: 513 [126976/194182 (65%)]\tLoss: 0.467232\tGrad Norm: 1.102258\tLR: 0.030000\n",
      "Train Epoch: 513 [147456/194182 (75%)]\tLoss: 0.466591\tGrad Norm: 1.385457\tLR: 0.030000\n",
      "Train Epoch: 513 [167936/194182 (85%)]\tLoss: 0.465361\tGrad Norm: 1.450669\tLR: 0.030000\n",
      "Train Epoch: 513 [188416/194182 (96%)]\tLoss: 0.475720\tGrad Norm: 1.209793\tLR: 0.030000\n",
      "Train set: Average loss: 0.4643\n",
      "Test set: Average loss: 0.2450, Average MAE: 0.3519\n",
      "Train Epoch: 514 [4096/194182 (2%)]\tLoss: 0.464228\tGrad Norm: 0.882532\tLR: 0.030000\n",
      "Train Epoch: 514 [24576/194182 (12%)]\tLoss: 0.460099\tGrad Norm: 1.086440\tLR: 0.030000\n",
      "Train Epoch: 514 [45056/194182 (23%)]\tLoss: 0.458360\tGrad Norm: 1.025336\tLR: 0.030000\n",
      "Train Epoch: 514 [65536/194182 (33%)]\tLoss: 0.456658\tGrad Norm: 0.954616\tLR: 0.030000\n",
      "Train Epoch: 514 [86016/194182 (44%)]\tLoss: 0.455975\tGrad Norm: 1.118591\tLR: 0.030000\n",
      "Train Epoch: 514 [106496/194182 (54%)]\tLoss: 0.469120\tGrad Norm: 1.264442\tLR: 0.030000\n",
      "Train Epoch: 514 [126976/194182 (65%)]\tLoss: 0.463812\tGrad Norm: 1.466670\tLR: 0.030000\n",
      "Train Epoch: 514 [147456/194182 (75%)]\tLoss: 0.457595\tGrad Norm: 1.048244\tLR: 0.030000\n",
      "Train Epoch: 514 [167936/194182 (85%)]\tLoss: 0.460572\tGrad Norm: 1.097256\tLR: 0.030000\n",
      "Train Epoch: 514 [188416/194182 (96%)]\tLoss: 0.456786\tGrad Norm: 0.708323\tLR: 0.030000\n",
      "Train set: Average loss: 0.4625\n",
      "Test set: Average loss: 0.2452, Average MAE: 0.3513\n",
      "Train Epoch: 515 [4096/194182 (2%)]\tLoss: 0.461180\tGrad Norm: 0.908455\tLR: 0.030000\n",
      "Train Epoch: 515 [24576/194182 (12%)]\tLoss: 0.456489\tGrad Norm: 0.785103\tLR: 0.030000\n",
      "Train Epoch: 515 [45056/194182 (23%)]\tLoss: 0.452792\tGrad Norm: 0.776310\tLR: 0.030000\n",
      "Train Epoch: 515 [65536/194182 (33%)]\tLoss: 0.452592\tGrad Norm: 1.086464\tLR: 0.030000\n",
      "Train Epoch: 515 [86016/194182 (44%)]\tLoss: 0.469914\tGrad Norm: 1.054674\tLR: 0.030000\n",
      "Train Epoch: 515 [106496/194182 (54%)]\tLoss: 0.467934\tGrad Norm: 1.344098\tLR: 0.030000\n",
      "Train Epoch: 515 [126976/194182 (65%)]\tLoss: 0.463547\tGrad Norm: 1.071107\tLR: 0.030000\n",
      "Train Epoch: 515 [147456/194182 (75%)]\tLoss: 0.462468\tGrad Norm: 1.039279\tLR: 0.030000\n",
      "Train Epoch: 515 [167936/194182 (85%)]\tLoss: 0.469121\tGrad Norm: 1.020860\tLR: 0.030000\n",
      "Train Epoch: 515 [188416/194182 (96%)]\tLoss: 0.471784\tGrad Norm: 1.263570\tLR: 0.030000\n",
      "Train set: Average loss: 0.4620\n",
      "Test set: Average loss: 0.2439, Average MAE: 0.3479\n",
      "Epoch 515: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 516 [4096/194182 (2%)]\tLoss: 0.452392\tGrad Norm: 0.742627\tLR: 0.030000\n",
      "Train Epoch: 516 [24576/194182 (12%)]\tLoss: 0.466851\tGrad Norm: 1.417607\tLR: 0.030000\n",
      "Train Epoch: 516 [45056/194182 (23%)]\tLoss: 0.465820\tGrad Norm: 1.257426\tLR: 0.030000\n",
      "Train Epoch: 516 [65536/194182 (33%)]\tLoss: 0.462774\tGrad Norm: 1.126415\tLR: 0.030000\n",
      "Train Epoch: 516 [86016/194182 (44%)]\tLoss: 0.462914\tGrad Norm: 0.859773\tLR: 0.030000\n",
      "Train Epoch: 516 [106496/194182 (54%)]\tLoss: 0.458148\tGrad Norm: 1.146996\tLR: 0.030000\n",
      "Train Epoch: 516 [126976/194182 (65%)]\tLoss: 0.466465\tGrad Norm: 1.522180\tLR: 0.030000\n",
      "Train Epoch: 516 [147456/194182 (75%)]\tLoss: 0.467768\tGrad Norm: 1.428537\tLR: 0.030000\n",
      "Train Epoch: 516 [167936/194182 (85%)]\tLoss: 0.470051\tGrad Norm: 1.473925\tLR: 0.030000\n",
      "Train Epoch: 516 [188416/194182 (96%)]\tLoss: 0.459194\tGrad Norm: 1.268252\tLR: 0.030000\n",
      "Train set: Average loss: 0.4640\n",
      "Test set: Average loss: 0.2457, Average MAE: 0.3360\n",
      "Train Epoch: 517 [4096/194182 (2%)]\tLoss: 0.454098\tGrad Norm: 0.905829\tLR: 0.030000\n",
      "Train Epoch: 517 [24576/194182 (12%)]\tLoss: 0.458013\tGrad Norm: 1.289735\tLR: 0.030000\n",
      "Train Epoch: 517 [45056/194182 (23%)]\tLoss: 0.467499\tGrad Norm: 1.250584\tLR: 0.030000\n",
      "Train Epoch: 517 [65536/194182 (33%)]\tLoss: 0.465218\tGrad Norm: 0.904887\tLR: 0.030000\n",
      "Train Epoch: 517 [86016/194182 (44%)]\tLoss: 0.456864\tGrad Norm: 1.165503\tLR: 0.030000\n",
      "Train Epoch: 517 [106496/194182 (54%)]\tLoss: 0.454226\tGrad Norm: 1.119784\tLR: 0.030000\n",
      "Train Epoch: 517 [126976/194182 (65%)]\tLoss: 0.467720\tGrad Norm: 1.330240\tLR: 0.030000\n",
      "Train Epoch: 517 [147456/194182 (75%)]\tLoss: 0.462066\tGrad Norm: 1.126594\tLR: 0.030000\n",
      "Train Epoch: 517 [167936/194182 (85%)]\tLoss: 0.452996\tGrad Norm: 0.919074\tLR: 0.030000\n",
      "Train Epoch: 517 [188416/194182 (96%)]\tLoss: 0.456695\tGrad Norm: 0.914390\tLR: 0.030000\n",
      "Train set: Average loss: 0.4613\n",
      "Test set: Average loss: 0.2440, Average MAE: 0.3469\n",
      "Train Epoch: 518 [4096/194182 (2%)]\tLoss: 0.457063\tGrad Norm: 0.757001\tLR: 0.030000\n",
      "Train Epoch: 518 [24576/194182 (12%)]\tLoss: 0.455588\tGrad Norm: 0.883286\tLR: 0.030000\n",
      "Train Epoch: 518 [45056/194182 (23%)]\tLoss: 0.464958\tGrad Norm: 1.317641\tLR: 0.030000\n",
      "Train Epoch: 518 [65536/194182 (33%)]\tLoss: 0.467289\tGrad Norm: 1.294375\tLR: 0.030000\n",
      "Train Epoch: 518 [86016/194182 (44%)]\tLoss: 0.466294\tGrad Norm: 1.429091\tLR: 0.030000\n",
      "Train Epoch: 518 [106496/194182 (54%)]\tLoss: 0.469148\tGrad Norm: 1.463231\tLR: 0.030000\n",
      "Train Epoch: 518 [126976/194182 (65%)]\tLoss: 0.462791\tGrad Norm: 1.177214\tLR: 0.030000\n",
      "Train Epoch: 518 [147456/194182 (75%)]\tLoss: 0.458164\tGrad Norm: 1.077174\tLR: 0.030000\n",
      "Train Epoch: 518 [167936/194182 (85%)]\tLoss: 0.461846\tGrad Norm: 1.034587\tLR: 0.030000\n",
      "Train Epoch: 518 [188416/194182 (96%)]\tLoss: 0.464744\tGrad Norm: 1.171407\tLR: 0.030000\n",
      "Train set: Average loss: 0.4625\n",
      "Test set: Average loss: 0.2490, Average MAE: 0.3371\n",
      "Train Epoch: 519 [4096/194182 (2%)]\tLoss: 0.454433\tGrad Norm: 1.304883\tLR: 0.030000\n",
      "Train Epoch: 519 [24576/194182 (12%)]\tLoss: 0.456220\tGrad Norm: 1.092908\tLR: 0.030000\n",
      "Train Epoch: 519 [45056/194182 (23%)]\tLoss: 0.456211\tGrad Norm: 0.649887\tLR: 0.030000\n",
      "Train Epoch: 519 [65536/194182 (33%)]\tLoss: 0.462037\tGrad Norm: 0.919045\tLR: 0.030000\n",
      "Train Epoch: 519 [86016/194182 (44%)]\tLoss: 0.452344\tGrad Norm: 0.817566\tLR: 0.030000\n",
      "Train Epoch: 519 [106496/194182 (54%)]\tLoss: 0.461572\tGrad Norm: 1.184337\tLR: 0.030000\n",
      "Train Epoch: 519 [126976/194182 (65%)]\tLoss: 0.470531\tGrad Norm: 1.290664\tLR: 0.030000\n",
      "Train Epoch: 519 [147456/194182 (75%)]\tLoss: 0.463995\tGrad Norm: 1.164751\tLR: 0.030000\n",
      "Train Epoch: 519 [167936/194182 (85%)]\tLoss: 0.457504\tGrad Norm: 1.223814\tLR: 0.030000\n",
      "Train Epoch: 519 [188416/194182 (96%)]\tLoss: 0.455919\tGrad Norm: 1.166485\tLR: 0.030000\n",
      "Train set: Average loss: 0.4604\n",
      "Test set: Average loss: 0.2497, Average MAE: 0.3401\n",
      "Train Epoch: 520 [4096/194182 (2%)]\tLoss: 0.463081\tGrad Norm: 1.343782\tLR: 0.030000\n",
      "Train Epoch: 520 [24576/194182 (12%)]\tLoss: 0.457152\tGrad Norm: 1.260368\tLR: 0.030000\n",
      "Train Epoch: 520 [45056/194182 (23%)]\tLoss: 0.463482\tGrad Norm: 1.326753\tLR: 0.030000\n",
      "Train Epoch: 520 [65536/194182 (33%)]\tLoss: 0.455922\tGrad Norm: 1.058809\tLR: 0.030000\n",
      "Train Epoch: 520 [86016/194182 (44%)]\tLoss: 0.455325\tGrad Norm: 1.142324\tLR: 0.030000\n",
      "Train Epoch: 520 [106496/194182 (54%)]\tLoss: 0.459993\tGrad Norm: 1.266123\tLR: 0.030000\n",
      "Train Epoch: 520 [126976/194182 (65%)]\tLoss: 0.464223\tGrad Norm: 0.998444\tLR: 0.030000\n",
      "Train Epoch: 520 [147456/194182 (75%)]\tLoss: 0.457323\tGrad Norm: 0.879999\tLR: 0.030000\n",
      "Train Epoch: 520 [167936/194182 (85%)]\tLoss: 0.458428\tGrad Norm: 1.105243\tLR: 0.030000\n",
      "Train Epoch: 520 [188416/194182 (96%)]\tLoss: 0.457165\tGrad Norm: 0.840452\tLR: 0.030000\n",
      "Train set: Average loss: 0.4608\n",
      "Test set: Average loss: 0.2446, Average MAE: 0.3518\n",
      "Epoch 520: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 521 [4096/194182 (2%)]\tLoss: 0.457639\tGrad Norm: 0.810536\tLR: 0.030000\n",
      "Train Epoch: 521 [24576/194182 (12%)]\tLoss: 0.451669\tGrad Norm: 0.793245\tLR: 0.030000\n",
      "Train Epoch: 521 [45056/194182 (23%)]\tLoss: 0.458826\tGrad Norm: 0.951575\tLR: 0.030000\n",
      "Train Epoch: 521 [65536/194182 (33%)]\tLoss: 0.457402\tGrad Norm: 1.143899\tLR: 0.030000\n",
      "Train Epoch: 521 [86016/194182 (44%)]\tLoss: 0.474100\tGrad Norm: 1.395783\tLR: 0.030000\n",
      "Train Epoch: 521 [106496/194182 (54%)]\tLoss: 0.466604\tGrad Norm: 1.443962\tLR: 0.030000\n",
      "Train Epoch: 521 [126976/194182 (65%)]\tLoss: 0.457073\tGrad Norm: 1.123916\tLR: 0.030000\n",
      "Train Epoch: 521 [147456/194182 (75%)]\tLoss: 0.458464\tGrad Norm: 1.064954\tLR: 0.030000\n",
      "Train Epoch: 521 [167936/194182 (85%)]\tLoss: 0.469697\tGrad Norm: 1.284687\tLR: 0.030000\n",
      "Train Epoch: 521 [188416/194182 (96%)]\tLoss: 0.459070\tGrad Norm: 1.190422\tLR: 0.030000\n",
      "Train set: Average loss: 0.4612\n",
      "Test set: Average loss: 0.2466, Average MAE: 0.3572\n",
      "Train Epoch: 522 [4096/194182 (2%)]\tLoss: 0.458671\tGrad Norm: 0.994770\tLR: 0.030000\n",
      "Train Epoch: 522 [24576/194182 (12%)]\tLoss: 0.458615\tGrad Norm: 0.910364\tLR: 0.030000\n",
      "Train Epoch: 522 [45056/194182 (23%)]\tLoss: 0.452963\tGrad Norm: 0.816670\tLR: 0.030000\n",
      "Train Epoch: 522 [65536/194182 (33%)]\tLoss: 0.465609\tGrad Norm: 1.029746\tLR: 0.030000\n",
      "Train Epoch: 522 [86016/194182 (44%)]\tLoss: 0.461619\tGrad Norm: 1.299690\tLR: 0.030000\n",
      "Train Epoch: 522 [106496/194182 (54%)]\tLoss: 0.463701\tGrad Norm: 0.941841\tLR: 0.030000\n",
      "Train Epoch: 522 [126976/194182 (65%)]\tLoss: 0.461310\tGrad Norm: 1.163287\tLR: 0.030000\n",
      "Train Epoch: 522 [147456/194182 (75%)]\tLoss: 0.457355\tGrad Norm: 1.151187\tLR: 0.030000\n",
      "Train Epoch: 522 [167936/194182 (85%)]\tLoss: 0.454710\tGrad Norm: 1.038604\tLR: 0.030000\n",
      "Train Epoch: 522 [188416/194182 (96%)]\tLoss: 0.455501\tGrad Norm: 1.042957\tLR: 0.030000\n",
      "Train set: Average loss: 0.4593\n",
      "Test set: Average loss: 0.2470, Average MAE: 0.3558\n",
      "Train Epoch: 523 [4096/194182 (2%)]\tLoss: 0.458310\tGrad Norm: 1.175479\tLR: 0.030000\n",
      "Train Epoch: 523 [24576/194182 (12%)]\tLoss: 0.474102\tGrad Norm: 1.748866\tLR: 0.030000\n",
      "Train Epoch: 523 [45056/194182 (23%)]\tLoss: 0.456389\tGrad Norm: 0.768141\tLR: 0.030000\n",
      "Train Epoch: 523 [65536/194182 (33%)]\tLoss: 0.465055\tGrad Norm: 0.764185\tLR: 0.030000\n",
      "Train Epoch: 523 [86016/194182 (44%)]\tLoss: 0.461115\tGrad Norm: 1.108975\tLR: 0.030000\n",
      "Train Epoch: 523 [106496/194182 (54%)]\tLoss: 0.468714\tGrad Norm: 1.535207\tLR: 0.030000\n",
      "Train Epoch: 523 [126976/194182 (65%)]\tLoss: 0.462721\tGrad Norm: 1.087721\tLR: 0.030000\n",
      "Train Epoch: 523 [147456/194182 (75%)]\tLoss: 0.459475\tGrad Norm: 1.170474\tLR: 0.030000\n",
      "Train Epoch: 523 [167936/194182 (85%)]\tLoss: 0.452772\tGrad Norm: 0.995500\tLR: 0.030000\n",
      "Train Epoch: 523 [188416/194182 (96%)]\tLoss: 0.456508\tGrad Norm: 1.029213\tLR: 0.030000\n",
      "Train set: Average loss: 0.4593\n",
      "Test set: Average loss: 0.2445, Average MAE: 0.3549\n",
      "Train Epoch: 524 [4096/194182 (2%)]\tLoss: 0.454901\tGrad Norm: 0.861597\tLR: 0.030000\n",
      "Train Epoch: 524 [24576/194182 (12%)]\tLoss: 0.458452\tGrad Norm: 1.034543\tLR: 0.030000\n",
      "Train Epoch: 524 [45056/194182 (23%)]\tLoss: 0.468651\tGrad Norm: 1.228426\tLR: 0.030000\n",
      "Train Epoch: 524 [65536/194182 (33%)]\tLoss: 0.463014\tGrad Norm: 1.206551\tLR: 0.030000\n",
      "Train Epoch: 524 [86016/194182 (44%)]\tLoss: 0.467816\tGrad Norm: 1.301501\tLR: 0.030000\n",
      "Train Epoch: 524 [106496/194182 (54%)]\tLoss: 0.455085\tGrad Norm: 1.029314\tLR: 0.030000\n",
      "Train Epoch: 524 [126976/194182 (65%)]\tLoss: 0.461072\tGrad Norm: 1.098415\tLR: 0.030000\n",
      "Train Epoch: 524 [147456/194182 (75%)]\tLoss: 0.457490\tGrad Norm: 1.197913\tLR: 0.030000\n",
      "Train Epoch: 524 [167936/194182 (85%)]\tLoss: 0.460322\tGrad Norm: 0.925944\tLR: 0.030000\n",
      "Train Epoch: 524 [188416/194182 (96%)]\tLoss: 0.450583\tGrad Norm: 1.097420\tLR: 0.030000\n",
      "Train set: Average loss: 0.4583\n",
      "Test set: Average loss: 0.2442, Average MAE: 0.3498\n",
      "Train Epoch: 525 [4096/194182 (2%)]\tLoss: 0.456697\tGrad Norm: 0.916800\tLR: 0.030000\n",
      "Train Epoch: 525 [24576/194182 (12%)]\tLoss: 0.465514\tGrad Norm: 1.256094\tLR: 0.030000\n",
      "Train Epoch: 525 [45056/194182 (23%)]\tLoss: 0.455787\tGrad Norm: 1.350157\tLR: 0.030000\n",
      "Train Epoch: 525 [65536/194182 (33%)]\tLoss: 0.460251\tGrad Norm: 0.982972\tLR: 0.030000\n",
      "Train Epoch: 525 [86016/194182 (44%)]\tLoss: 0.444522\tGrad Norm: 0.610863\tLR: 0.030000\n",
      "Train Epoch: 525 [106496/194182 (54%)]\tLoss: 0.457928\tGrad Norm: 1.072065\tLR: 0.030000\n",
      "Train Epoch: 525 [126976/194182 (65%)]\tLoss: 0.461977\tGrad Norm: 1.136943\tLR: 0.030000\n",
      "Train Epoch: 525 [147456/194182 (75%)]\tLoss: 0.457513\tGrad Norm: 1.390357\tLR: 0.030000\n",
      "Train Epoch: 525 [167936/194182 (85%)]\tLoss: 0.463926\tGrad Norm: 1.477905\tLR: 0.030000\n",
      "Train Epoch: 525 [188416/194182 (96%)]\tLoss: 0.467046\tGrad Norm: 1.530779\tLR: 0.030000\n",
      "Train set: Average loss: 0.4589\n",
      "Test set: Average loss: 0.2489, Average MAE: 0.3624\n",
      "Epoch 525: Mean reward = 0.049 +/- 0.030\n",
      "Train Epoch: 526 [4096/194182 (2%)]\tLoss: 0.462897\tGrad Norm: 1.260850\tLR: 0.030000\n",
      "Train Epoch: 526 [24576/194182 (12%)]\tLoss: 0.454918\tGrad Norm: 1.018032\tLR: 0.030000\n",
      "Train Epoch: 526 [45056/194182 (23%)]\tLoss: 0.453400\tGrad Norm: 0.710645\tLR: 0.030000\n",
      "Train Epoch: 526 [65536/194182 (33%)]\tLoss: 0.459973\tGrad Norm: 1.132996\tLR: 0.030000\n",
      "Train Epoch: 526 [86016/194182 (44%)]\tLoss: 0.466314\tGrad Norm: 1.337943\tLR: 0.030000\n",
      "Train Epoch: 526 [106496/194182 (54%)]\tLoss: 0.457335\tGrad Norm: 0.988308\tLR: 0.030000\n",
      "Train Epoch: 526 [126976/194182 (65%)]\tLoss: 0.450138\tGrad Norm: 0.976035\tLR: 0.030000\n",
      "Train Epoch: 526 [147456/194182 (75%)]\tLoss: 0.465992\tGrad Norm: 1.243932\tLR: 0.030000\n",
      "Train Epoch: 526 [167936/194182 (85%)]\tLoss: 0.462810\tGrad Norm: 1.183487\tLR: 0.030000\n",
      "Train Epoch: 526 [188416/194182 (96%)]\tLoss: 0.454613\tGrad Norm: 0.876028\tLR: 0.030000\n",
      "Train set: Average loss: 0.4576\n",
      "Test set: Average loss: 0.2446, Average MAE: 0.3434\n",
      "Train Epoch: 527 [4096/194182 (2%)]\tLoss: 0.452414\tGrad Norm: 0.862671\tLR: 0.030000\n",
      "Train Epoch: 527 [24576/194182 (12%)]\tLoss: 0.459770\tGrad Norm: 0.867408\tLR: 0.030000\n",
      "Train Epoch: 527 [45056/194182 (23%)]\tLoss: 0.463242\tGrad Norm: 1.035236\tLR: 0.030000\n",
      "Train Epoch: 527 [65536/194182 (33%)]\tLoss: 0.454571\tGrad Norm: 1.056202\tLR: 0.030000\n",
      "Train Epoch: 527 [86016/194182 (44%)]\tLoss: 0.454093\tGrad Norm: 0.902129\tLR: 0.030000\n",
      "Train Epoch: 527 [106496/194182 (54%)]\tLoss: 0.459702\tGrad Norm: 1.044190\tLR: 0.030000\n",
      "Train Epoch: 527 [126976/194182 (65%)]\tLoss: 0.452424\tGrad Norm: 1.044138\tLR: 0.030000\n",
      "Train Epoch: 527 [147456/194182 (75%)]\tLoss: 0.464557\tGrad Norm: 1.306316\tLR: 0.030000\n",
      "Train Epoch: 527 [167936/194182 (85%)]\tLoss: 0.455901\tGrad Norm: 1.222268\tLR: 0.030000\n",
      "Train Epoch: 527 [188416/194182 (96%)]\tLoss: 0.464212\tGrad Norm: 1.593876\tLR: 0.030000\n",
      "Train set: Average loss: 0.4577\n",
      "Test set: Average loss: 0.2610, Average MAE: 0.3411\n",
      "Train Epoch: 528 [4096/194182 (2%)]\tLoss: 0.464826\tGrad Norm: 1.790687\tLR: 0.030000\n",
      "Train Epoch: 528 [24576/194182 (12%)]\tLoss: 0.460311\tGrad Norm: 1.141594\tLR: 0.030000\n",
      "Train Epoch: 528 [45056/194182 (23%)]\tLoss: 0.458225\tGrad Norm: 0.861936\tLR: 0.030000\n",
      "Train Epoch: 528 [65536/194182 (33%)]\tLoss: 0.457475\tGrad Norm: 0.886118\tLR: 0.030000\n",
      "Train Epoch: 528 [86016/194182 (44%)]\tLoss: 0.453662\tGrad Norm: 0.940270\tLR: 0.030000\n",
      "Train Epoch: 528 [106496/194182 (54%)]\tLoss: 0.460231\tGrad Norm: 1.255937\tLR: 0.030000\n",
      "Train Epoch: 528 [126976/194182 (65%)]\tLoss: 0.462154\tGrad Norm: 1.456333\tLR: 0.030000\n",
      "Train Epoch: 528 [147456/194182 (75%)]\tLoss: 0.462355\tGrad Norm: 1.200297\tLR: 0.030000\n",
      "Train Epoch: 528 [167936/194182 (85%)]\tLoss: 0.460864\tGrad Norm: 1.245353\tLR: 0.030000\n",
      "Train Epoch: 528 [188416/194182 (96%)]\tLoss: 0.454102\tGrad Norm: 1.223444\tLR: 0.030000\n",
      "Train set: Average loss: 0.4582\n",
      "Test set: Average loss: 0.2478, Average MAE: 0.3604\n",
      "Train Epoch: 529 [4096/194182 (2%)]\tLoss: 0.457171\tGrad Norm: 1.385734\tLR: 0.030000\n",
      "Train Epoch: 529 [24576/194182 (12%)]\tLoss: 0.454415\tGrad Norm: 1.255816\tLR: 0.030000\n",
      "Train Epoch: 529 [45056/194182 (23%)]\tLoss: 0.457367\tGrad Norm: 0.942392\tLR: 0.030000\n",
      "Train Epoch: 529 [65536/194182 (33%)]\tLoss: 0.447846\tGrad Norm: 0.539973\tLR: 0.030000\n",
      "Train Epoch: 529 [86016/194182 (44%)]\tLoss: 0.462650\tGrad Norm: 0.876355\tLR: 0.030000\n",
      "Train Epoch: 529 [106496/194182 (54%)]\tLoss: 0.453248\tGrad Norm: 1.229503\tLR: 0.030000\n",
      "Train Epoch: 529 [126976/194182 (65%)]\tLoss: 0.461446\tGrad Norm: 1.250814\tLR: 0.030000\n",
      "Train Epoch: 529 [147456/194182 (75%)]\tLoss: 0.458896\tGrad Norm: 1.343790\tLR: 0.030000\n",
      "Train Epoch: 529 [167936/194182 (85%)]\tLoss: 0.463356\tGrad Norm: 1.277285\tLR: 0.030000\n",
      "Train Epoch: 529 [188416/194182 (96%)]\tLoss: 0.457018\tGrad Norm: 1.020091\tLR: 0.030000\n",
      "Train set: Average loss: 0.4566\n",
      "Test set: Average loss: 0.2502, Average MAE: 0.3454\n",
      "Train Epoch: 530 [4096/194182 (2%)]\tLoss: 0.462937\tGrad Norm: 1.332110\tLR: 0.030000\n",
      "Train Epoch: 530 [24576/194182 (12%)]\tLoss: 0.464687\tGrad Norm: 1.231080\tLR: 0.030000\n",
      "Train Epoch: 530 [45056/194182 (23%)]\tLoss: 0.457831\tGrad Norm: 1.080866\tLR: 0.030000\n",
      "Train Epoch: 530 [65536/194182 (33%)]\tLoss: 0.458605\tGrad Norm: 1.193799\tLR: 0.030000\n",
      "Train Epoch: 530 [86016/194182 (44%)]\tLoss: 0.453603\tGrad Norm: 0.994399\tLR: 0.030000\n",
      "Train Epoch: 530 [106496/194182 (54%)]\tLoss: 0.457020\tGrad Norm: 1.186111\tLR: 0.030000\n",
      "Train Epoch: 530 [126976/194182 (65%)]\tLoss: 0.459155\tGrad Norm: 1.327509\tLR: 0.030000\n",
      "Train Epoch: 530 [147456/194182 (75%)]\tLoss: 0.462103\tGrad Norm: 1.383182\tLR: 0.030000\n",
      "Train Epoch: 530 [167936/194182 (85%)]\tLoss: 0.456623\tGrad Norm: 0.993265\tLR: 0.030000\n",
      "Train Epoch: 530 [188416/194182 (96%)]\tLoss: 0.455255\tGrad Norm: 0.864402\tLR: 0.030000\n",
      "Train set: Average loss: 0.4564\n",
      "Test set: Average loss: 0.2477, Average MAE: 0.3371\n",
      "Epoch 530: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 531 [4096/194182 (2%)]\tLoss: 0.453259\tGrad Norm: 1.252911\tLR: 0.030000\n",
      "Train Epoch: 531 [24576/194182 (12%)]\tLoss: 0.467378\tGrad Norm: 1.675267\tLR: 0.030000\n",
      "Train Epoch: 531 [45056/194182 (23%)]\tLoss: 0.442266\tGrad Norm: 1.029397\tLR: 0.030000\n",
      "Train Epoch: 531 [65536/194182 (33%)]\tLoss: 0.459871\tGrad Norm: 0.673330\tLR: 0.030000\n",
      "Train Epoch: 531 [86016/194182 (44%)]\tLoss: 0.452232\tGrad Norm: 0.738898\tLR: 0.030000\n",
      "Train Epoch: 531 [106496/194182 (54%)]\tLoss: 0.458692\tGrad Norm: 1.070878\tLR: 0.030000\n",
      "Train Epoch: 531 [126976/194182 (65%)]\tLoss: 0.456334\tGrad Norm: 1.204360\tLR: 0.030000\n",
      "Train Epoch: 531 [147456/194182 (75%)]\tLoss: 0.453803\tGrad Norm: 1.125869\tLR: 0.030000\n",
      "Train Epoch: 531 [167936/194182 (85%)]\tLoss: 0.458907\tGrad Norm: 1.282903\tLR: 0.030000\n",
      "Train Epoch: 531 [188416/194182 (96%)]\tLoss: 0.455197\tGrad Norm: 1.287079\tLR: 0.030000\n",
      "Train set: Average loss: 0.4564\n",
      "Test set: Average loss: 0.2531, Average MAE: 0.3367\n",
      "Train Epoch: 532 [4096/194182 (2%)]\tLoss: 0.474172\tGrad Norm: 1.783304\tLR: 0.030000\n",
      "Train Epoch: 532 [24576/194182 (12%)]\tLoss: 0.457817\tGrad Norm: 1.416901\tLR: 0.030000\n",
      "Train Epoch: 532 [45056/194182 (23%)]\tLoss: 0.458918\tGrad Norm: 1.218734\tLR: 0.030000\n",
      "Train Epoch: 532 [65536/194182 (33%)]\tLoss: 0.454298\tGrad Norm: 1.186747\tLR: 0.030000\n",
      "Train Epoch: 532 [86016/194182 (44%)]\tLoss: 0.462349\tGrad Norm: 1.139552\tLR: 0.030000\n",
      "Train Epoch: 532 [106496/194182 (54%)]\tLoss: 0.459317\tGrad Norm: 1.028164\tLR: 0.030000\n",
      "Train Epoch: 532 [126976/194182 (65%)]\tLoss: 0.459917\tGrad Norm: 0.907569\tLR: 0.030000\n",
      "Train Epoch: 532 [147456/194182 (75%)]\tLoss: 0.454705\tGrad Norm: 1.139058\tLR: 0.030000\n",
      "Train Epoch: 532 [167936/194182 (85%)]\tLoss: 0.453305\tGrad Norm: 0.937307\tLR: 0.030000\n",
      "Train Epoch: 532 [188416/194182 (96%)]\tLoss: 0.458357\tGrad Norm: 0.864071\tLR: 0.030000\n",
      "Train set: Average loss: 0.4561\n",
      "Test set: Average loss: 0.2433, Average MAE: 0.3440\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 533 [4096/194182 (2%)]\tLoss: 0.446608\tGrad Norm: 0.856115\tLR: 0.030000\n",
      "Train Epoch: 533 [24576/194182 (12%)]\tLoss: 0.452515\tGrad Norm: 1.060721\tLR: 0.030000\n",
      "Train Epoch: 533 [45056/194182 (23%)]\tLoss: 0.457806\tGrad Norm: 1.498833\tLR: 0.030000\n",
      "Train Epoch: 533 [65536/194182 (33%)]\tLoss: 0.466513\tGrad Norm: 1.480889\tLR: 0.030000\n",
      "Train Epoch: 533 [86016/194182 (44%)]\tLoss: 0.457385\tGrad Norm: 1.280157\tLR: 0.030000\n",
      "Train Epoch: 533 [106496/194182 (54%)]\tLoss: 0.456117\tGrad Norm: 1.101325\tLR: 0.030000\n",
      "Train Epoch: 533 [126976/194182 (65%)]\tLoss: 0.441902\tGrad Norm: 0.744539\tLR: 0.030000\n",
      "Train Epoch: 533 [147456/194182 (75%)]\tLoss: 0.451875\tGrad Norm: 0.778955\tLR: 0.030000\n",
      "Train Epoch: 533 [167936/194182 (85%)]\tLoss: 0.449612\tGrad Norm: 0.967568\tLR: 0.030000\n",
      "Train Epoch: 533 [188416/194182 (96%)]\tLoss: 0.466057\tGrad Norm: 1.395051\tLR: 0.030000\n",
      "Train set: Average loss: 0.4560\n",
      "Test set: Average loss: 0.2518, Average MAE: 0.3648\n",
      "Train Epoch: 534 [4096/194182 (2%)]\tLoss: 0.461529\tGrad Norm: 1.509377\tLR: 0.030000\n",
      "Train Epoch: 534 [24576/194182 (12%)]\tLoss: 0.456609\tGrad Norm: 1.003345\tLR: 0.030000\n",
      "Train Epoch: 534 [45056/194182 (23%)]\tLoss: 0.457351\tGrad Norm: 1.149499\tLR: 0.030000\n",
      "Train Epoch: 534 [65536/194182 (33%)]\tLoss: 0.450450\tGrad Norm: 0.895835\tLR: 0.030000\n",
      "Train Epoch: 534 [86016/194182 (44%)]\tLoss: 0.452030\tGrad Norm: 0.963251\tLR: 0.030000\n",
      "Train Epoch: 534 [106496/194182 (54%)]\tLoss: 0.456504\tGrad Norm: 1.019284\tLR: 0.030000\n",
      "Train Epoch: 534 [126976/194182 (65%)]\tLoss: 0.457299\tGrad Norm: 1.090768\tLR: 0.030000\n",
      "Train Epoch: 534 [147456/194182 (75%)]\tLoss: 0.455044\tGrad Norm: 1.267189\tLR: 0.030000\n",
      "Train Epoch: 534 [167936/194182 (85%)]\tLoss: 0.451781\tGrad Norm: 1.206788\tLR: 0.030000\n",
      "Train Epoch: 534 [188416/194182 (96%)]\tLoss: 0.452602\tGrad Norm: 0.848658\tLR: 0.030000\n",
      "Train set: Average loss: 0.4545\n",
      "Test set: Average loss: 0.2464, Average MAE: 0.3578\n",
      "Train Epoch: 535 [4096/194182 (2%)]\tLoss: 0.460402\tGrad Norm: 0.974573\tLR: 0.030000\n",
      "Train Epoch: 535 [24576/194182 (12%)]\tLoss: 0.464857\tGrad Norm: 1.219908\tLR: 0.030000\n",
      "Train Epoch: 535 [45056/194182 (23%)]\tLoss: 0.460411\tGrad Norm: 1.288255\tLR: 0.030000\n",
      "Train Epoch: 535 [65536/194182 (33%)]\tLoss: 0.460513\tGrad Norm: 1.222064\tLR: 0.030000\n",
      "Train Epoch: 535 [86016/194182 (44%)]\tLoss: 0.452040\tGrad Norm: 1.020058\tLR: 0.030000\n",
      "Train Epoch: 535 [106496/194182 (54%)]\tLoss: 0.459228\tGrad Norm: 1.194020\tLR: 0.030000\n",
      "Train Epoch: 535 [126976/194182 (65%)]\tLoss: 0.457290\tGrad Norm: 0.981957\tLR: 0.030000\n",
      "Train Epoch: 535 [147456/194182 (75%)]\tLoss: 0.453185\tGrad Norm: 0.735202\tLR: 0.030000\n",
      "Train Epoch: 535 [167936/194182 (85%)]\tLoss: 0.452118\tGrad Norm: 0.777579\tLR: 0.030000\n",
      "Train Epoch: 535 [188416/194182 (96%)]\tLoss: 0.457901\tGrad Norm: 1.066686\tLR: 0.030000\n",
      "Train set: Average loss: 0.4535\n",
      "Test set: Average loss: 0.2466, Average MAE: 0.3419\n",
      "Epoch 535: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 536 [4096/194182 (2%)]\tLoss: 0.458578\tGrad Norm: 1.202554\tLR: 0.030000\n",
      "Train Epoch: 536 [24576/194182 (12%)]\tLoss: 0.449978\tGrad Norm: 1.292973\tLR: 0.030000\n",
      "Train Epoch: 536 [45056/194182 (23%)]\tLoss: 0.455454\tGrad Norm: 1.143821\tLR: 0.030000\n",
      "Train Epoch: 536 [65536/194182 (33%)]\tLoss: 0.452071\tGrad Norm: 1.178497\tLR: 0.030000\n",
      "Train Epoch: 536 [86016/194182 (44%)]\tLoss: 0.453747\tGrad Norm: 1.309001\tLR: 0.030000\n",
      "Train Epoch: 536 [106496/194182 (54%)]\tLoss: 0.459025\tGrad Norm: 1.337060\tLR: 0.030000\n",
      "Train Epoch: 536 [126976/194182 (65%)]\tLoss: 0.459798\tGrad Norm: 1.540475\tLR: 0.030000\n",
      "Train Epoch: 536 [147456/194182 (75%)]\tLoss: 0.456127\tGrad Norm: 1.289298\tLR: 0.030000\n",
      "Train Epoch: 536 [167936/194182 (85%)]\tLoss: 0.448581\tGrad Norm: 0.887144\tLR: 0.030000\n",
      "Train Epoch: 536 [188416/194182 (96%)]\tLoss: 0.452638\tGrad Norm: 1.215382\tLR: 0.030000\n",
      "Train set: Average loss: 0.4560\n",
      "Test set: Average loss: 0.2476, Average MAE: 0.3354\n",
      "Train Epoch: 537 [4096/194182 (2%)]\tLoss: 0.450474\tGrad Norm: 1.273734\tLR: 0.030000\n",
      "Train Epoch: 537 [24576/194182 (12%)]\tLoss: 0.459968\tGrad Norm: 0.963908\tLR: 0.030000\n",
      "Train Epoch: 537 [45056/194182 (23%)]\tLoss: 0.451011\tGrad Norm: 0.706158\tLR: 0.030000\n",
      "Train Epoch: 537 [65536/194182 (33%)]\tLoss: 0.450535\tGrad Norm: 0.868465\tLR: 0.030000\n",
      "Train Epoch: 537 [86016/194182 (44%)]\tLoss: 0.455405\tGrad Norm: 1.117807\tLR: 0.030000\n",
      "Train Epoch: 537 [106496/194182 (54%)]\tLoss: 0.457793\tGrad Norm: 1.064688\tLR: 0.030000\n",
      "Train Epoch: 537 [126976/194182 (65%)]\tLoss: 0.448825\tGrad Norm: 1.074681\tLR: 0.030000\n",
      "Train Epoch: 537 [147456/194182 (75%)]\tLoss: 0.453107\tGrad Norm: 0.893717\tLR: 0.030000\n",
      "Train Epoch: 537 [167936/194182 (85%)]\tLoss: 0.446960\tGrad Norm: 1.162255\tLR: 0.030000\n",
      "Train Epoch: 537 [188416/194182 (96%)]\tLoss: 0.462398\tGrad Norm: 1.264772\tLR: 0.030000\n",
      "Train set: Average loss: 0.4517\n",
      "Test set: Average loss: 0.2479, Average MAE: 0.3551\n",
      "Train Epoch: 538 [4096/194182 (2%)]\tLoss: 0.457423\tGrad Norm: 1.173599\tLR: 0.030000\n",
      "Train Epoch: 538 [24576/194182 (12%)]\tLoss: 0.454025\tGrad Norm: 1.223134\tLR: 0.030000\n",
      "Train Epoch: 538 [45056/194182 (23%)]\tLoss: 0.452379\tGrad Norm: 1.129349\tLR: 0.030000\n",
      "Train Epoch: 538 [65536/194182 (33%)]\tLoss: 0.452353\tGrad Norm: 1.037046\tLR: 0.030000\n",
      "Train Epoch: 538 [86016/194182 (44%)]\tLoss: 0.463460\tGrad Norm: 1.154143\tLR: 0.030000\n",
      "Train Epoch: 538 [106496/194182 (54%)]\tLoss: 0.454169\tGrad Norm: 1.062560\tLR: 0.030000\n",
      "Train Epoch: 538 [126976/194182 (65%)]\tLoss: 0.458210\tGrad Norm: 1.586812\tLR: 0.030000\n",
      "Train Epoch: 538 [147456/194182 (75%)]\tLoss: 0.453682\tGrad Norm: 1.111470\tLR: 0.030000\n",
      "Train Epoch: 538 [167936/194182 (85%)]\tLoss: 0.452660\tGrad Norm: 0.896473\tLR: 0.030000\n",
      "Train Epoch: 538 [188416/194182 (96%)]\tLoss: 0.448930\tGrad Norm: 1.148040\tLR: 0.030000\n",
      "Train set: Average loss: 0.4537\n",
      "Test set: Average loss: 0.2466, Average MAE: 0.3429\n",
      "Train Epoch: 539 [4096/194182 (2%)]\tLoss: 0.457359\tGrad Norm: 1.143542\tLR: 0.030000\n",
      "Train Epoch: 539 [24576/194182 (12%)]\tLoss: 0.453239\tGrad Norm: 1.039260\tLR: 0.030000\n",
      "Train Epoch: 539 [45056/194182 (23%)]\tLoss: 0.438711\tGrad Norm: 0.857327\tLR: 0.030000\n",
      "Train Epoch: 539 [65536/194182 (33%)]\tLoss: 0.455073\tGrad Norm: 1.128232\tLR: 0.030000\n",
      "Train Epoch: 539 [86016/194182 (44%)]\tLoss: 0.454477\tGrad Norm: 1.345386\tLR: 0.030000\n",
      "Train Epoch: 539 [106496/194182 (54%)]\tLoss: 0.453641\tGrad Norm: 1.229750\tLR: 0.030000\n",
      "Train Epoch: 539 [126976/194182 (65%)]\tLoss: 0.455779\tGrad Norm: 1.289059\tLR: 0.030000\n",
      "Train Epoch: 539 [147456/194182 (75%)]\tLoss: 0.465341\tGrad Norm: 1.450253\tLR: 0.030000\n",
      "Train Epoch: 539 [167936/194182 (85%)]\tLoss: 0.452117\tGrad Norm: 1.235538\tLR: 0.030000\n",
      "Train Epoch: 539 [188416/194182 (96%)]\tLoss: 0.450099\tGrad Norm: 0.707853\tLR: 0.030000\n",
      "Train set: Average loss: 0.4533\n",
      "Test set: Average loss: 0.2402, Average MAE: 0.3379\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 540 [4096/194182 (2%)]\tLoss: 0.455689\tGrad Norm: 0.589299\tLR: 0.030000\n",
      "Train Epoch: 540 [24576/194182 (12%)]\tLoss: 0.451436\tGrad Norm: 0.734030\tLR: 0.030000\n",
      "Train Epoch: 540 [45056/194182 (23%)]\tLoss: 0.449966\tGrad Norm: 0.915834\tLR: 0.030000\n",
      "Train Epoch: 540 [65536/194182 (33%)]\tLoss: 0.444018\tGrad Norm: 0.732554\tLR: 0.030000\n",
      "Train Epoch: 540 [86016/194182 (44%)]\tLoss: 0.445789\tGrad Norm: 0.997263\tLR: 0.030000\n",
      "Train Epoch: 540 [106496/194182 (54%)]\tLoss: 0.454107\tGrad Norm: 1.004691\tLR: 0.030000\n",
      "Train Epoch: 540 [126976/194182 (65%)]\tLoss: 0.446629\tGrad Norm: 1.064915\tLR: 0.030000\n",
      "Train Epoch: 540 [147456/194182 (75%)]\tLoss: 0.450452\tGrad Norm: 1.324327\tLR: 0.030000\n",
      "Train Epoch: 540 [167936/194182 (85%)]\tLoss: 0.451794\tGrad Norm: 1.186794\tLR: 0.030000\n",
      "Train Epoch: 540 [188416/194182 (96%)]\tLoss: 0.450797\tGrad Norm: 1.300442\tLR: 0.030000\n",
      "Train set: Average loss: 0.4508\n",
      "Test set: Average loss: 0.2474, Average MAE: 0.3366\n",
      "Epoch 540: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 541 [4096/194182 (2%)]\tLoss: 0.455394\tGrad Norm: 1.417312\tLR: 0.030000\n",
      "Train Epoch: 541 [24576/194182 (12%)]\tLoss: 0.452951\tGrad Norm: 1.489699\tLR: 0.030000\n",
      "Train Epoch: 541 [45056/194182 (23%)]\tLoss: 0.464403\tGrad Norm: 1.590873\tLR: 0.030000\n",
      "Train Epoch: 541 [65536/194182 (33%)]\tLoss: 0.452433\tGrad Norm: 1.035367\tLR: 0.030000\n",
      "Train Epoch: 541 [86016/194182 (44%)]\tLoss: 0.465852\tGrad Norm: 1.065885\tLR: 0.030000\n",
      "Train Epoch: 541 [106496/194182 (54%)]\tLoss: 0.444058\tGrad Norm: 0.829345\tLR: 0.030000\n",
      "Train Epoch: 541 [126976/194182 (65%)]\tLoss: 0.444958\tGrad Norm: 0.861311\tLR: 0.030000\n",
      "Train Epoch: 541 [147456/194182 (75%)]\tLoss: 0.453989\tGrad Norm: 1.277419\tLR: 0.030000\n",
      "Train Epoch: 541 [167936/194182 (85%)]\tLoss: 0.447455\tGrad Norm: 1.131232\tLR: 0.030000\n",
      "Train Epoch: 541 [188416/194182 (96%)]\tLoss: 0.447865\tGrad Norm: 1.056091\tLR: 0.030000\n",
      "Train set: Average loss: 0.4535\n",
      "Test set: Average loss: 0.2440, Average MAE: 0.3448\n",
      "Train Epoch: 542 [4096/194182 (2%)]\tLoss: 0.442602\tGrad Norm: 0.986710\tLR: 0.030000\n",
      "Train Epoch: 542 [24576/194182 (12%)]\tLoss: 0.455050\tGrad Norm: 0.957154\tLR: 0.030000\n",
      "Train Epoch: 542 [45056/194182 (23%)]\tLoss: 0.450434\tGrad Norm: 0.720696\tLR: 0.030000\n",
      "Train Epoch: 542 [65536/194182 (33%)]\tLoss: 0.444887\tGrad Norm: 0.824060\tLR: 0.030000\n",
      "Train Epoch: 542 [86016/194182 (44%)]\tLoss: 0.447212\tGrad Norm: 0.917563\tLR: 0.030000\n",
      "Train Epoch: 542 [106496/194182 (54%)]\tLoss: 0.444286\tGrad Norm: 1.091791\tLR: 0.030000\n",
      "Train Epoch: 542 [126976/194182 (65%)]\tLoss: 0.456012\tGrad Norm: 1.149074\tLR: 0.030000\n",
      "Train Epoch: 542 [147456/194182 (75%)]\tLoss: 0.443967\tGrad Norm: 1.080098\tLR: 0.030000\n",
      "Train Epoch: 542 [167936/194182 (85%)]\tLoss: 0.446123\tGrad Norm: 0.769526\tLR: 0.030000\n",
      "Train Epoch: 542 [188416/194182 (96%)]\tLoss: 0.444305\tGrad Norm: 0.987229\tLR: 0.030000\n",
      "Train set: Average loss: 0.4497\n",
      "Test set: Average loss: 0.2472, Average MAE: 0.3590\n",
      "Train Epoch: 543 [4096/194182 (2%)]\tLoss: 0.450118\tGrad Norm: 1.293101\tLR: 0.030000\n",
      "Train Epoch: 543 [24576/194182 (12%)]\tLoss: 0.447902\tGrad Norm: 1.359246\tLR: 0.030000\n",
      "Train Epoch: 543 [45056/194182 (23%)]\tLoss: 0.458360\tGrad Norm: 1.021866\tLR: 0.030000\n",
      "Train Epoch: 543 [65536/194182 (33%)]\tLoss: 0.457357\tGrad Norm: 1.484366\tLR: 0.030000\n",
      "Train Epoch: 543 [86016/194182 (44%)]\tLoss: 0.445517\tGrad Norm: 0.879695\tLR: 0.030000\n",
      "Train Epoch: 543 [106496/194182 (54%)]\tLoss: 0.451695\tGrad Norm: 1.138877\tLR: 0.030000\n",
      "Train Epoch: 543 [126976/194182 (65%)]\tLoss: 0.455620\tGrad Norm: 1.326861\tLR: 0.030000\n",
      "Train Epoch: 543 [147456/194182 (75%)]\tLoss: 0.460803\tGrad Norm: 1.440133\tLR: 0.030000\n",
      "Train Epoch: 543 [167936/194182 (85%)]\tLoss: 0.455312\tGrad Norm: 1.101986\tLR: 0.030000\n",
      "Train Epoch: 543 [188416/194182 (96%)]\tLoss: 0.444109\tGrad Norm: 0.700535\tLR: 0.030000\n",
      "Train set: Average loss: 0.4517\n",
      "Test set: Average loss: 0.2441, Average MAE: 0.3404\n",
      "Train Epoch: 544 [4096/194182 (2%)]\tLoss: 0.453914\tGrad Norm: 1.124258\tLR: 0.030000\n",
      "Train Epoch: 544 [24576/194182 (12%)]\tLoss: 0.461054\tGrad Norm: 1.427200\tLR: 0.030000\n",
      "Train Epoch: 544 [45056/194182 (23%)]\tLoss: 0.454134\tGrad Norm: 1.439747\tLR: 0.030000\n",
      "Train Epoch: 544 [65536/194182 (33%)]\tLoss: 0.442913\tGrad Norm: 1.005812\tLR: 0.030000\n",
      "Train Epoch: 544 [86016/194182 (44%)]\tLoss: 0.447196\tGrad Norm: 0.846016\tLR: 0.030000\n",
      "Train Epoch: 544 [106496/194182 (54%)]\tLoss: 0.439527\tGrad Norm: 0.800377\tLR: 0.030000\n",
      "Train Epoch: 544 [126976/194182 (65%)]\tLoss: 0.445253\tGrad Norm: 1.029149\tLR: 0.030000\n",
      "Train Epoch: 544 [147456/194182 (75%)]\tLoss: 0.441240\tGrad Norm: 1.131238\tLR: 0.030000\n",
      "Train Epoch: 544 [167936/194182 (85%)]\tLoss: 0.461695\tGrad Norm: 1.388993\tLR: 0.030000\n",
      "Train Epoch: 544 [188416/194182 (96%)]\tLoss: 0.451801\tGrad Norm: 1.245378\tLR: 0.030000\n",
      "Train set: Average loss: 0.4516\n",
      "Test set: Average loss: 0.2451, Average MAE: 0.3422\n",
      "Train Epoch: 545 [4096/194182 (2%)]\tLoss: 0.447609\tGrad Norm: 1.029814\tLR: 0.030000\n",
      "Train Epoch: 545 [24576/194182 (12%)]\tLoss: 0.445501\tGrad Norm: 0.798479\tLR: 0.030000\n",
      "Train Epoch: 545 [45056/194182 (23%)]\tLoss: 0.452158\tGrad Norm: 1.478915\tLR: 0.030000\n",
      "Train Epoch: 545 [65536/194182 (33%)]\tLoss: 0.460356\tGrad Norm: 1.505506\tLR: 0.030000\n",
      "Train Epoch: 545 [86016/194182 (44%)]\tLoss: 0.446810\tGrad Norm: 1.277430\tLR: 0.030000\n",
      "Train Epoch: 545 [106496/194182 (54%)]\tLoss: 0.441590\tGrad Norm: 0.954376\tLR: 0.030000\n",
      "Train Epoch: 545 [126976/194182 (65%)]\tLoss: 0.443181\tGrad Norm: 0.904439\tLR: 0.030000\n",
      "Train Epoch: 545 [147456/194182 (75%)]\tLoss: 0.447060\tGrad Norm: 0.741435\tLR: 0.030000\n",
      "Train Epoch: 545 [167936/194182 (85%)]\tLoss: 0.445904\tGrad Norm: 1.083731\tLR: 0.030000\n",
      "Train Epoch: 545 [188416/194182 (96%)]\tLoss: 0.461641\tGrad Norm: 1.449295\tLR: 0.030000\n",
      "Train set: Average loss: 0.4503\n",
      "Test set: Average loss: 0.2483, Average MAE: 0.3372\n",
      "Epoch 545: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 546 [4096/194182 (2%)]\tLoss: 0.451167\tGrad Norm: 1.306958\tLR: 0.030000\n",
      "Train Epoch: 546 [24576/194182 (12%)]\tLoss: 0.445974\tGrad Norm: 1.087145\tLR: 0.030000\n",
      "Train Epoch: 546 [45056/194182 (23%)]\tLoss: 0.451386\tGrad Norm: 1.082293\tLR: 0.030000\n",
      "Train Epoch: 546 [65536/194182 (33%)]\tLoss: 0.444631\tGrad Norm: 1.124104\tLR: 0.030000\n",
      "Train Epoch: 546 [86016/194182 (44%)]\tLoss: 0.449955\tGrad Norm: 0.982724\tLR: 0.030000\n",
      "Train Epoch: 546 [106496/194182 (54%)]\tLoss: 0.455338\tGrad Norm: 0.838830\tLR: 0.030000\n",
      "Train Epoch: 546 [126976/194182 (65%)]\tLoss: 0.451987\tGrad Norm: 0.879158\tLR: 0.030000\n",
      "Train Epoch: 546 [147456/194182 (75%)]\tLoss: 0.450249\tGrad Norm: 1.233547\tLR: 0.030000\n",
      "Train Epoch: 546 [167936/194182 (85%)]\tLoss: 0.451831\tGrad Norm: 1.045207\tLR: 0.030000\n",
      "Train Epoch: 546 [188416/194182 (96%)]\tLoss: 0.440971\tGrad Norm: 0.949540\tLR: 0.030000\n",
      "Train set: Average loss: 0.4490\n",
      "Test set: Average loss: 0.2452, Average MAE: 0.3553\n",
      "Train Epoch: 547 [4096/194182 (2%)]\tLoss: 0.449883\tGrad Norm: 1.005265\tLR: 0.030000\n",
      "Train Epoch: 547 [24576/194182 (12%)]\tLoss: 0.452626\tGrad Norm: 1.255513\tLR: 0.030000\n",
      "Train Epoch: 547 [45056/194182 (23%)]\tLoss: 0.452924\tGrad Norm: 1.316657\tLR: 0.030000\n",
      "Train Epoch: 547 [65536/194182 (33%)]\tLoss: 0.455131\tGrad Norm: 1.427921\tLR: 0.030000\n",
      "Train Epoch: 547 [86016/194182 (44%)]\tLoss: 0.449144\tGrad Norm: 1.573552\tLR: 0.030000\n",
      "Train Epoch: 547 [106496/194182 (54%)]\tLoss: 0.458532\tGrad Norm: 1.552851\tLR: 0.030000\n",
      "Train Epoch: 547 [126976/194182 (65%)]\tLoss: 0.448566\tGrad Norm: 1.085032\tLR: 0.030000\n",
      "Train Epoch: 547 [147456/194182 (75%)]\tLoss: 0.453014\tGrad Norm: 1.138861\tLR: 0.030000\n",
      "Train Epoch: 547 [167936/194182 (85%)]\tLoss: 0.455427\tGrad Norm: 1.332994\tLR: 0.030000\n",
      "Train Epoch: 547 [188416/194182 (96%)]\tLoss: 0.449011\tGrad Norm: 1.101071\tLR: 0.030000\n",
      "Train set: Average loss: 0.4517\n",
      "Test set: Average loss: 0.2484, Average MAE: 0.3595\n",
      "Train Epoch: 548 [4096/194182 (2%)]\tLoss: 0.451043\tGrad Norm: 1.197431\tLR: 0.030000\n",
      "Train Epoch: 548 [24576/194182 (12%)]\tLoss: 0.451557\tGrad Norm: 1.153174\tLR: 0.030000\n",
      "Train Epoch: 548 [45056/194182 (23%)]\tLoss: 0.440519\tGrad Norm: 1.129810\tLR: 0.030000\n",
      "Train Epoch: 548 [65536/194182 (33%)]\tLoss: 0.447237\tGrad Norm: 1.113698\tLR: 0.030000\n",
      "Train Epoch: 548 [86016/194182 (44%)]\tLoss: 0.453509\tGrad Norm: 1.117556\tLR: 0.030000\n",
      "Train Epoch: 548 [106496/194182 (54%)]\tLoss: 0.454262\tGrad Norm: 1.045655\tLR: 0.030000\n",
      "Train Epoch: 548 [126976/194182 (65%)]\tLoss: 0.450593\tGrad Norm: 1.356680\tLR: 0.030000\n",
      "Train Epoch: 548 [147456/194182 (75%)]\tLoss: 0.460479\tGrad Norm: 1.245581\tLR: 0.030000\n",
      "Train Epoch: 548 [167936/194182 (85%)]\tLoss: 0.458752\tGrad Norm: 1.182002\tLR: 0.030000\n",
      "Train Epoch: 548 [188416/194182 (96%)]\tLoss: 0.443859\tGrad Norm: 1.136803\tLR: 0.030000\n",
      "Train set: Average loss: 0.4502\n",
      "Test set: Average loss: 0.2461, Average MAE: 0.3544\n",
      "Train Epoch: 549 [4096/194182 (2%)]\tLoss: 0.450975\tGrad Norm: 1.121532\tLR: 0.030000\n",
      "Train Epoch: 549 [24576/194182 (12%)]\tLoss: 0.446309\tGrad Norm: 0.898068\tLR: 0.030000\n",
      "Train Epoch: 549 [45056/194182 (23%)]\tLoss: 0.452488\tGrad Norm: 1.065068\tLR: 0.030000\n",
      "Train Epoch: 549 [65536/194182 (33%)]\tLoss: 0.438222\tGrad Norm: 1.048019\tLR: 0.030000\n",
      "Train Epoch: 549 [86016/194182 (44%)]\tLoss: 0.454657\tGrad Norm: 1.388862\tLR: 0.030000\n",
      "Train Epoch: 549 [106496/194182 (54%)]\tLoss: 0.447595\tGrad Norm: 1.209129\tLR: 0.030000\n",
      "Train Epoch: 549 [126976/194182 (65%)]\tLoss: 0.443905\tGrad Norm: 1.178809\tLR: 0.030000\n",
      "Train Epoch: 549 [147456/194182 (75%)]\tLoss: 0.447367\tGrad Norm: 1.057638\tLR: 0.030000\n",
      "Train Epoch: 549 [167936/194182 (85%)]\tLoss: 0.453947\tGrad Norm: 1.023311\tLR: 0.030000\n",
      "Train Epoch: 549 [188416/194182 (96%)]\tLoss: 0.444368\tGrad Norm: 0.820839\tLR: 0.030000\n",
      "Train set: Average loss: 0.4491\n",
      "Test set: Average loss: 0.2439, Average MAE: 0.3543\n",
      "Train Epoch: 550 [4096/194182 (2%)]\tLoss: 0.450519\tGrad Norm: 0.962688\tLR: 0.030000\n",
      "Train Epoch: 550 [24576/194182 (12%)]\tLoss: 0.448667\tGrad Norm: 1.177184\tLR: 0.030000\n",
      "Train Epoch: 550 [45056/194182 (23%)]\tLoss: 0.449902\tGrad Norm: 1.079537\tLR: 0.030000\n",
      "Train Epoch: 550 [65536/194182 (33%)]\tLoss: 0.445281\tGrad Norm: 0.954243\tLR: 0.030000\n",
      "Train Epoch: 550 [86016/194182 (44%)]\tLoss: 0.447807\tGrad Norm: 1.098946\tLR: 0.030000\n",
      "Train Epoch: 550 [106496/194182 (54%)]\tLoss: 0.442389\tGrad Norm: 1.149205\tLR: 0.030000\n",
      "Train Epoch: 550 [126976/194182 (65%)]\tLoss: 0.447348\tGrad Norm: 1.173175\tLR: 0.030000\n",
      "Train Epoch: 550 [147456/194182 (75%)]\tLoss: 0.448756\tGrad Norm: 1.219028\tLR: 0.030000\n",
      "Train Epoch: 550 [167936/194182 (85%)]\tLoss: 0.448444\tGrad Norm: 1.245033\tLR: 0.030000\n",
      "Train Epoch: 550 [188416/194182 (96%)]\tLoss: 0.439998\tGrad Norm: 1.101298\tLR: 0.030000\n",
      "Train set: Average loss: 0.4488\n",
      "Test set: Average loss: 0.2485, Average MAE: 0.3511\n",
      "Epoch 550: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 551 [4096/194182 (2%)]\tLoss: 0.454225\tGrad Norm: 1.304768\tLR: 0.030000\n",
      "Train Epoch: 551 [24576/194182 (12%)]\tLoss: 0.453296\tGrad Norm: 1.177775\tLR: 0.030000\n",
      "Train Epoch: 551 [45056/194182 (23%)]\tLoss: 0.448814\tGrad Norm: 1.244370\tLR: 0.030000\n",
      "Train Epoch: 551 [65536/194182 (33%)]\tLoss: 0.440272\tGrad Norm: 1.243284\tLR: 0.030000\n",
      "Train Epoch: 551 [86016/194182 (44%)]\tLoss: 0.447487\tGrad Norm: 1.135911\tLR: 0.030000\n",
      "Train Epoch: 551 [106496/194182 (54%)]\tLoss: 0.445966\tGrad Norm: 0.927651\tLR: 0.030000\n",
      "Train Epoch: 551 [126976/194182 (65%)]\tLoss: 0.445223\tGrad Norm: 1.009601\tLR: 0.030000\n",
      "Train Epoch: 551 [147456/194182 (75%)]\tLoss: 0.451872\tGrad Norm: 1.284806\tLR: 0.030000\n",
      "Train Epoch: 551 [167936/194182 (85%)]\tLoss: 0.458596\tGrad Norm: 1.127055\tLR: 0.030000\n",
      "Train Epoch: 551 [188416/194182 (96%)]\tLoss: 0.448938\tGrad Norm: 1.430602\tLR: 0.030000\n",
      "Train set: Average loss: 0.4485\n",
      "Test set: Average loss: 0.2531, Average MAE: 0.3353\n",
      "Train Epoch: 552 [4096/194182 (2%)]\tLoss: 0.457966\tGrad Norm: 1.890962\tLR: 0.030000\n",
      "Train Epoch: 552 [24576/194182 (12%)]\tLoss: 0.452912\tGrad Norm: 0.966427\tLR: 0.030000\n",
      "Train Epoch: 552 [45056/194182 (23%)]\tLoss: 0.441564\tGrad Norm: 0.810268\tLR: 0.030000\n",
      "Train Epoch: 552 [65536/194182 (33%)]\tLoss: 0.454570\tGrad Norm: 1.230881\tLR: 0.030000\n",
      "Train Epoch: 552 [86016/194182 (44%)]\tLoss: 0.441582\tGrad Norm: 1.118349\tLR: 0.030000\n",
      "Train Epoch: 552 [106496/194182 (54%)]\tLoss: 0.454624\tGrad Norm: 1.131715\tLR: 0.030000\n",
      "Train Epoch: 552 [126976/194182 (65%)]\tLoss: 0.445710\tGrad Norm: 0.950717\tLR: 0.030000\n",
      "Train Epoch: 552 [147456/194182 (75%)]\tLoss: 0.435029\tGrad Norm: 0.538078\tLR: 0.030000\n",
      "Train Epoch: 552 [167936/194182 (85%)]\tLoss: 0.451193\tGrad Norm: 0.765374\tLR: 0.030000\n",
      "Train Epoch: 552 [188416/194182 (96%)]\tLoss: 0.436524\tGrad Norm: 0.579788\tLR: 0.030000\n",
      "Train set: Average loss: 0.4461\n",
      "Test set: Average loss: 0.2408, Average MAE: 0.3450\n",
      "Train Epoch: 553 [4096/194182 (2%)]\tLoss: 0.435407\tGrad Norm: 0.889844\tLR: 0.030000\n",
      "Train Epoch: 553 [24576/194182 (12%)]\tLoss: 0.441756\tGrad Norm: 1.175916\tLR: 0.030000\n",
      "Train Epoch: 553 [45056/194182 (23%)]\tLoss: 0.450883\tGrad Norm: 1.486146\tLR: 0.030000\n",
      "Train Epoch: 553 [65536/194182 (33%)]\tLoss: 0.444225\tGrad Norm: 0.900801\tLR: 0.030000\n",
      "Train Epoch: 553 [86016/194182 (44%)]\tLoss: 0.445520\tGrad Norm: 0.703193\tLR: 0.030000\n",
      "Train Epoch: 553 [106496/194182 (54%)]\tLoss: 0.442750\tGrad Norm: 1.374259\tLR: 0.030000\n",
      "Train Epoch: 553 [126976/194182 (65%)]\tLoss: 0.452349\tGrad Norm: 1.257111\tLR: 0.030000\n",
      "Train Epoch: 553 [147456/194182 (75%)]\tLoss: 0.453769\tGrad Norm: 1.241184\tLR: 0.030000\n",
      "Train Epoch: 553 [167936/194182 (85%)]\tLoss: 0.446084\tGrad Norm: 1.110422\tLR: 0.030000\n",
      "Train Epoch: 553 [188416/194182 (96%)]\tLoss: 0.447951\tGrad Norm: 1.085925\tLR: 0.030000\n",
      "Train set: Average loss: 0.4473\n",
      "Test set: Average loss: 0.2451, Average MAE: 0.3568\n",
      "Train Epoch: 554 [4096/194182 (2%)]\tLoss: 0.444867\tGrad Norm: 1.088094\tLR: 0.030000\n",
      "Train Epoch: 554 [24576/194182 (12%)]\tLoss: 0.449925\tGrad Norm: 1.322139\tLR: 0.030000\n",
      "Train Epoch: 554 [45056/194182 (23%)]\tLoss: 0.442561\tGrad Norm: 1.051173\tLR: 0.030000\n",
      "Train Epoch: 554 [65536/194182 (33%)]\tLoss: 0.442355\tGrad Norm: 0.803154\tLR: 0.030000\n",
      "Train Epoch: 554 [86016/194182 (44%)]\tLoss: 0.448142\tGrad Norm: 1.225510\tLR: 0.030000\n",
      "Train Epoch: 554 [106496/194182 (54%)]\tLoss: 0.452300\tGrad Norm: 1.107526\tLR: 0.030000\n",
      "Train Epoch: 554 [126976/194182 (65%)]\tLoss: 0.448139\tGrad Norm: 1.356249\tLR: 0.030000\n",
      "Train Epoch: 554 [147456/194182 (75%)]\tLoss: 0.446126\tGrad Norm: 1.250944\tLR: 0.030000\n",
      "Train Epoch: 554 [167936/194182 (85%)]\tLoss: 0.447243\tGrad Norm: 1.133131\tLR: 0.030000\n",
      "Train Epoch: 554 [188416/194182 (96%)]\tLoss: 0.444252\tGrad Norm: 1.170931\tLR: 0.030000\n",
      "Train set: Average loss: 0.4468\n",
      "Test set: Average loss: 0.2419, Average MAE: 0.3385\n",
      "Train Epoch: 555 [4096/194182 (2%)]\tLoss: 0.442239\tGrad Norm: 0.975063\tLR: 0.030000\n",
      "Train Epoch: 555 [24576/194182 (12%)]\tLoss: 0.451477\tGrad Norm: 1.277244\tLR: 0.030000\n",
      "Train Epoch: 555 [45056/194182 (23%)]\tLoss: 0.454556\tGrad Norm: 1.244811\tLR: 0.030000\n",
      "Train Epoch: 555 [65536/194182 (33%)]\tLoss: 0.446635\tGrad Norm: 1.010824\tLR: 0.030000\n",
      "Train Epoch: 555 [86016/194182 (44%)]\tLoss: 0.448462\tGrad Norm: 1.274504\tLR: 0.030000\n",
      "Train Epoch: 555 [106496/194182 (54%)]\tLoss: 0.446123\tGrad Norm: 1.157722\tLR: 0.030000\n",
      "Train Epoch: 555 [126976/194182 (65%)]\tLoss: 0.451082\tGrad Norm: 1.725959\tLR: 0.030000\n",
      "Train Epoch: 555 [147456/194182 (75%)]\tLoss: 0.446036\tGrad Norm: 1.260618\tLR: 0.030000\n",
      "Train Epoch: 555 [167936/194182 (85%)]\tLoss: 0.437068\tGrad Norm: 0.700326\tLR: 0.030000\n",
      "Train Epoch: 555 [188416/194182 (96%)]\tLoss: 0.441891\tGrad Norm: 0.792398\tLR: 0.030000\n",
      "Train set: Average loss: 0.4464\n",
      "Test set: Average loss: 0.2428, Average MAE: 0.3411\n",
      "Epoch 555: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 556 [4096/194182 (2%)]\tLoss: 0.447436\tGrad Norm: 1.025905\tLR: 0.030000\n",
      "Train Epoch: 556 [24576/194182 (12%)]\tLoss: 0.442082\tGrad Norm: 1.161072\tLR: 0.030000\n",
      "Train Epoch: 556 [45056/194182 (23%)]\tLoss: 0.446386\tGrad Norm: 1.123674\tLR: 0.030000\n",
      "Train Epoch: 556 [65536/194182 (33%)]\tLoss: 0.442913\tGrad Norm: 1.058685\tLR: 0.030000\n",
      "Train Epoch: 556 [86016/194182 (44%)]\tLoss: 0.444889\tGrad Norm: 1.264091\tLR: 0.030000\n",
      "Train Epoch: 556 [106496/194182 (54%)]\tLoss: 0.442650\tGrad Norm: 1.013866\tLR: 0.030000\n",
      "Train Epoch: 556 [126976/194182 (65%)]\tLoss: 0.452714\tGrad Norm: 1.246885\tLR: 0.030000\n",
      "Train Epoch: 556 [147456/194182 (75%)]\tLoss: 0.450134\tGrad Norm: 0.976736\tLR: 0.030000\n",
      "Train Epoch: 556 [167936/194182 (85%)]\tLoss: 0.439141\tGrad Norm: 1.006971\tLR: 0.030000\n",
      "Train Epoch: 556 [188416/194182 (96%)]\tLoss: 0.442738\tGrad Norm: 1.242058\tLR: 0.030000\n",
      "Train set: Average loss: 0.4458\n",
      "Test set: Average loss: 0.2459, Average MAE: 0.3599\n",
      "Train Epoch: 557 [4096/194182 (2%)]\tLoss: 0.437201\tGrad Norm: 1.370317\tLR: 0.030000\n",
      "Train Epoch: 557 [24576/194182 (12%)]\tLoss: 0.450922\tGrad Norm: 1.347203\tLR: 0.030000\n",
      "Train Epoch: 557 [45056/194182 (23%)]\tLoss: 0.444501\tGrad Norm: 1.300150\tLR: 0.030000\n",
      "Train Epoch: 557 [65536/194182 (33%)]\tLoss: 0.446871\tGrad Norm: 1.181730\tLR: 0.030000\n",
      "Train Epoch: 557 [86016/194182 (44%)]\tLoss: 0.436583\tGrad Norm: 1.206998\tLR: 0.030000\n",
      "Train Epoch: 557 [106496/194182 (54%)]\tLoss: 0.446408\tGrad Norm: 1.049560\tLR: 0.030000\n",
      "Train Epoch: 557 [126976/194182 (65%)]\tLoss: 0.461247\tGrad Norm: 1.708085\tLR: 0.030000\n",
      "Train Epoch: 557 [147456/194182 (75%)]\tLoss: 0.440599\tGrad Norm: 1.025736\tLR: 0.030000\n",
      "Train Epoch: 557 [167936/194182 (85%)]\tLoss: 0.448926\tGrad Norm: 1.139841\tLR: 0.030000\n",
      "Train Epoch: 557 [188416/194182 (96%)]\tLoss: 0.453928\tGrad Norm: 0.982439\tLR: 0.030000\n",
      "Train set: Average loss: 0.4464\n",
      "Test set: Average loss: 0.2526, Average MAE: 0.3432\n",
      "Train Epoch: 558 [4096/194182 (2%)]\tLoss: 0.454654\tGrad Norm: 2.073090\tLR: 0.030000\n",
      "Train Epoch: 558 [24576/194182 (12%)]\tLoss: 0.443113\tGrad Norm: 1.091627\tLR: 0.030000\n",
      "Train Epoch: 558 [45056/194182 (23%)]\tLoss: 0.440422\tGrad Norm: 1.036934\tLR: 0.030000\n",
      "Train Epoch: 558 [65536/194182 (33%)]\tLoss: 0.450499\tGrad Norm: 1.225877\tLR: 0.030000\n",
      "Train Epoch: 558 [86016/194182 (44%)]\tLoss: 0.447109\tGrad Norm: 1.301366\tLR: 0.030000\n",
      "Train Epoch: 558 [106496/194182 (54%)]\tLoss: 0.448058\tGrad Norm: 1.169509\tLR: 0.030000\n",
      "Train Epoch: 558 [126976/194182 (65%)]\tLoss: 0.444192\tGrad Norm: 1.001281\tLR: 0.030000\n",
      "Train Epoch: 558 [147456/194182 (75%)]\tLoss: 0.451529\tGrad Norm: 1.515457\tLR: 0.030000\n",
      "Train Epoch: 558 [167936/194182 (85%)]\tLoss: 0.449932\tGrad Norm: 1.461293\tLR: 0.030000\n",
      "Train Epoch: 558 [188416/194182 (96%)]\tLoss: 0.443598\tGrad Norm: 0.929028\tLR: 0.030000\n",
      "Train set: Average loss: 0.4469\n",
      "Test set: Average loss: 0.2435, Average MAE: 0.3508\n",
      "Train Epoch: 559 [4096/194182 (2%)]\tLoss: 0.444372\tGrad Norm: 0.927014\tLR: 0.030000\n",
      "Train Epoch: 559 [24576/194182 (12%)]\tLoss: 0.441693\tGrad Norm: 0.898787\tLR: 0.030000\n",
      "Train Epoch: 559 [45056/194182 (23%)]\tLoss: 0.439724\tGrad Norm: 1.126408\tLR: 0.030000\n",
      "Train Epoch: 559 [65536/194182 (33%)]\tLoss: 0.439432\tGrad Norm: 0.923367\tLR: 0.030000\n",
      "Train Epoch: 559 [86016/194182 (44%)]\tLoss: 0.438668\tGrad Norm: 1.168169\tLR: 0.030000\n",
      "Train Epoch: 559 [106496/194182 (54%)]\tLoss: 0.445121\tGrad Norm: 1.060580\tLR: 0.030000\n",
      "Train Epoch: 559 [126976/194182 (65%)]\tLoss: 0.429624\tGrad Norm: 0.999018\tLR: 0.030000\n",
      "Train Epoch: 559 [147456/194182 (75%)]\tLoss: 0.438522\tGrad Norm: 0.986339\tLR: 0.030000\n",
      "Train Epoch: 559 [167936/194182 (85%)]\tLoss: 0.449272\tGrad Norm: 1.133754\tLR: 0.030000\n",
      "Train Epoch: 559 [188416/194182 (96%)]\tLoss: 0.447158\tGrad Norm: 0.904148\tLR: 0.030000\n",
      "Train set: Average loss: 0.4429\n",
      "Test set: Average loss: 0.2409, Average MAE: 0.3452\n",
      "Train Epoch: 560 [4096/194182 (2%)]\tLoss: 0.439432\tGrad Norm: 0.632555\tLR: 0.030000\n",
      "Train Epoch: 560 [24576/194182 (12%)]\tLoss: 0.440977\tGrad Norm: 0.961225\tLR: 0.030000\n",
      "Train Epoch: 560 [45056/194182 (23%)]\tLoss: 0.451002\tGrad Norm: 1.337111\tLR: 0.030000\n",
      "Train Epoch: 560 [65536/194182 (33%)]\tLoss: 0.446842\tGrad Norm: 1.128946\tLR: 0.030000\n",
      "Train Epoch: 560 [86016/194182 (44%)]\tLoss: 0.448084\tGrad Norm: 1.180576\tLR: 0.030000\n",
      "Train Epoch: 560 [106496/194182 (54%)]\tLoss: 0.445805\tGrad Norm: 1.501502\tLR: 0.030000\n",
      "Train Epoch: 560 [126976/194182 (65%)]\tLoss: 0.443472\tGrad Norm: 1.279985\tLR: 0.030000\n",
      "Train Epoch: 560 [147456/194182 (75%)]\tLoss: 0.443740\tGrad Norm: 1.290522\tLR: 0.030000\n",
      "Train Epoch: 560 [167936/194182 (85%)]\tLoss: 0.440149\tGrad Norm: 1.002331\tLR: 0.030000\n",
      "Train Epoch: 560 [188416/194182 (96%)]\tLoss: 0.437383\tGrad Norm: 1.112950\tLR: 0.030000\n",
      "Train set: Average loss: 0.4445\n",
      "Test set: Average loss: 0.2436, Average MAE: 0.3403\n",
      "Epoch 560: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 561 [4096/194182 (2%)]\tLoss: 0.448081\tGrad Norm: 1.050692\tLR: 0.030000\n",
      "Train Epoch: 561 [24576/194182 (12%)]\tLoss: 0.444625\tGrad Norm: 1.478538\tLR: 0.030000\n",
      "Train Epoch: 561 [45056/194182 (23%)]\tLoss: 0.445104\tGrad Norm: 1.153021\tLR: 0.030000\n",
      "Train Epoch: 561 [65536/194182 (33%)]\tLoss: 0.442360\tGrad Norm: 0.829405\tLR: 0.030000\n",
      "Train Epoch: 561 [86016/194182 (44%)]\tLoss: 0.441043\tGrad Norm: 1.018041\tLR: 0.030000\n",
      "Train Epoch: 561 [106496/194182 (54%)]\tLoss: 0.461327\tGrad Norm: 1.411514\tLR: 0.030000\n",
      "Train Epoch: 561 [126976/194182 (65%)]\tLoss: 0.446946\tGrad Norm: 1.085787\tLR: 0.030000\n",
      "Train Epoch: 561 [147456/194182 (75%)]\tLoss: 0.451937\tGrad Norm: 1.508631\tLR: 0.030000\n",
      "Train Epoch: 561 [167936/194182 (85%)]\tLoss: 0.437635\tGrad Norm: 1.145775\tLR: 0.030000\n",
      "Train Epoch: 561 [188416/194182 (96%)]\tLoss: 0.449728\tGrad Norm: 0.854097\tLR: 0.030000\n",
      "Train set: Average loss: 0.4438\n",
      "Test set: Average loss: 0.2433, Average MAE: 0.3536\n",
      "Train Epoch: 562 [4096/194182 (2%)]\tLoss: 0.440919\tGrad Norm: 1.003410\tLR: 0.030000\n",
      "Train Epoch: 562 [24576/194182 (12%)]\tLoss: 0.443770\tGrad Norm: 1.106574\tLR: 0.030000\n",
      "Train Epoch: 562 [45056/194182 (23%)]\tLoss: 0.434437\tGrad Norm: 0.959535\tLR: 0.030000\n",
      "Train Epoch: 562 [65536/194182 (33%)]\tLoss: 0.441604\tGrad Norm: 1.131111\tLR: 0.030000\n",
      "Train Epoch: 562 [86016/194182 (44%)]\tLoss: 0.446197\tGrad Norm: 0.965715\tLR: 0.030000\n",
      "Train Epoch: 562 [106496/194182 (54%)]\tLoss: 0.437793\tGrad Norm: 0.831994\tLR: 0.030000\n",
      "Train Epoch: 562 [126976/194182 (65%)]\tLoss: 0.435232\tGrad Norm: 1.001312\tLR: 0.030000\n",
      "Train Epoch: 562 [147456/194182 (75%)]\tLoss: 0.454377\tGrad Norm: 1.540528\tLR: 0.030000\n",
      "Train Epoch: 562 [167936/194182 (85%)]\tLoss: 0.441989\tGrad Norm: 1.239675\tLR: 0.030000\n",
      "Train Epoch: 562 [188416/194182 (96%)]\tLoss: 0.444283\tGrad Norm: 1.181748\tLR: 0.030000\n",
      "Train set: Average loss: 0.4431\n",
      "Test set: Average loss: 0.2466, Average MAE: 0.3498\n",
      "Train Epoch: 563 [4096/194182 (2%)]\tLoss: 0.441104\tGrad Norm: 1.241718\tLR: 0.030000\n",
      "Train Epoch: 563 [24576/194182 (12%)]\tLoss: 0.451563\tGrad Norm: 1.263765\tLR: 0.030000\n",
      "Train Epoch: 563 [45056/194182 (23%)]\tLoss: 0.442757\tGrad Norm: 1.049045\tLR: 0.030000\n",
      "Train Epoch: 563 [65536/194182 (33%)]\tLoss: 0.441255\tGrad Norm: 1.076205\tLR: 0.030000\n",
      "Train Epoch: 563 [86016/194182 (44%)]\tLoss: 0.443042\tGrad Norm: 1.136757\tLR: 0.030000\n",
      "Train Epoch: 563 [106496/194182 (54%)]\tLoss: 0.442431\tGrad Norm: 1.319849\tLR: 0.030000\n",
      "Train Epoch: 563 [126976/194182 (65%)]\tLoss: 0.445711\tGrad Norm: 1.597898\tLR: 0.030000\n",
      "Train Epoch: 563 [147456/194182 (75%)]\tLoss: 0.446774\tGrad Norm: 1.508576\tLR: 0.030000\n",
      "Train Epoch: 563 [167936/194182 (85%)]\tLoss: 0.434908\tGrad Norm: 0.838460\tLR: 0.030000\n",
      "Train Epoch: 563 [188416/194182 (96%)]\tLoss: 0.435087\tGrad Norm: 0.924695\tLR: 0.030000\n",
      "Train set: Average loss: 0.4444\n",
      "Test set: Average loss: 0.2455, Average MAE: 0.3384\n",
      "Train Epoch: 564 [4096/194182 (2%)]\tLoss: 0.446827\tGrad Norm: 1.134917\tLR: 0.030000\n",
      "Train Epoch: 564 [24576/194182 (12%)]\tLoss: 0.437317\tGrad Norm: 1.210226\tLR: 0.030000\n",
      "Train Epoch: 564 [45056/194182 (23%)]\tLoss: 0.443709\tGrad Norm: 1.091907\tLR: 0.030000\n",
      "Train Epoch: 564 [65536/194182 (33%)]\tLoss: 0.435548\tGrad Norm: 0.884663\tLR: 0.030000\n",
      "Train Epoch: 564 [86016/194182 (44%)]\tLoss: 0.440839\tGrad Norm: 1.018465\tLR: 0.030000\n",
      "Train Epoch: 564 [106496/194182 (54%)]\tLoss: 0.448304\tGrad Norm: 1.380785\tLR: 0.030000\n",
      "Train Epoch: 564 [126976/194182 (65%)]\tLoss: 0.433444\tGrad Norm: 1.124851\tLR: 0.030000\n",
      "Train Epoch: 564 [147456/194182 (75%)]\tLoss: 0.441849\tGrad Norm: 0.896759\tLR: 0.030000\n",
      "Train Epoch: 564 [167936/194182 (85%)]\tLoss: 0.432652\tGrad Norm: 1.088916\tLR: 0.030000\n",
      "Train Epoch: 564 [188416/194182 (96%)]\tLoss: 0.441895\tGrad Norm: 1.080590\tLR: 0.030000\n",
      "Train set: Average loss: 0.4428\n",
      "Test set: Average loss: 0.2468, Average MAE: 0.3556\n",
      "Train Epoch: 565 [4096/194182 (2%)]\tLoss: 0.441472\tGrad Norm: 1.100488\tLR: 0.030000\n",
      "Train Epoch: 565 [24576/194182 (12%)]\tLoss: 0.464241\tGrad Norm: 1.665628\tLR: 0.030000\n",
      "Train Epoch: 565 [45056/194182 (23%)]\tLoss: 0.447791\tGrad Norm: 1.377151\tLR: 0.030000\n",
      "Train Epoch: 565 [65536/194182 (33%)]\tLoss: 0.446285\tGrad Norm: 1.398507\tLR: 0.030000\n",
      "Train Epoch: 565 [86016/194182 (44%)]\tLoss: 0.441410\tGrad Norm: 1.155154\tLR: 0.030000\n",
      "Train Epoch: 565 [106496/194182 (54%)]\tLoss: 0.442352\tGrad Norm: 1.240705\tLR: 0.030000\n",
      "Train Epoch: 565 [126976/194182 (65%)]\tLoss: 0.447540\tGrad Norm: 1.097859\tLR: 0.030000\n",
      "Train Epoch: 565 [147456/194182 (75%)]\tLoss: 0.440322\tGrad Norm: 1.449767\tLR: 0.030000\n",
      "Train Epoch: 565 [167936/194182 (85%)]\tLoss: 0.440847\tGrad Norm: 0.928075\tLR: 0.030000\n",
      "Train Epoch: 565 [188416/194182 (96%)]\tLoss: 0.457171\tGrad Norm: 1.253531\tLR: 0.030000\n",
      "Train set: Average loss: 0.4443\n",
      "Test set: Average loss: 0.2468, Average MAE: 0.3489\n",
      "Epoch 565: Mean reward = 0.049 +/- 0.042\n",
      "Train Epoch: 566 [4096/194182 (2%)]\tLoss: 0.450874\tGrad Norm: 1.264172\tLR: 0.030000\n",
      "Train Epoch: 566 [24576/194182 (12%)]\tLoss: 0.444817\tGrad Norm: 1.143880\tLR: 0.030000\n",
      "Train Epoch: 566 [45056/194182 (23%)]\tLoss: 0.437483\tGrad Norm: 1.135940\tLR: 0.030000\n",
      "Train Epoch: 566 [65536/194182 (33%)]\tLoss: 0.432126\tGrad Norm: 0.845132\tLR: 0.030000\n",
      "Train Epoch: 566 [86016/194182 (44%)]\tLoss: 0.447192\tGrad Norm: 1.126225\tLR: 0.030000\n",
      "Train Epoch: 566 [106496/194182 (54%)]\tLoss: 0.440027\tGrad Norm: 0.844304\tLR: 0.030000\n",
      "Train Epoch: 566 [126976/194182 (65%)]\tLoss: 0.435039\tGrad Norm: 1.014217\tLR: 0.030000\n",
      "Train Epoch: 566 [147456/194182 (75%)]\tLoss: 0.437013\tGrad Norm: 0.690381\tLR: 0.030000\n",
      "Train Epoch: 566 [167936/194182 (85%)]\tLoss: 0.444799\tGrad Norm: 1.058785\tLR: 0.030000\n",
      "Train Epoch: 566 [188416/194182 (96%)]\tLoss: 0.442963\tGrad Norm: 0.934151\tLR: 0.030000\n",
      "Train set: Average loss: 0.4407\n",
      "Test set: Average loss: 0.2417, Average MAE: 0.3369\n",
      "Train Epoch: 567 [4096/194182 (2%)]\tLoss: 0.435838\tGrad Norm: 0.894085\tLR: 0.030000\n",
      "Train Epoch: 567 [24576/194182 (12%)]\tLoss: 0.435709\tGrad Norm: 0.968139\tLR: 0.030000\n",
      "Train Epoch: 567 [45056/194182 (23%)]\tLoss: 0.439604\tGrad Norm: 1.111160\tLR: 0.030000\n",
      "Train Epoch: 567 [65536/194182 (33%)]\tLoss: 0.446583\tGrad Norm: 1.388204\tLR: 0.030000\n",
      "Train Epoch: 567 [86016/194182 (44%)]\tLoss: 0.438872\tGrad Norm: 1.188627\tLR: 0.030000\n",
      "Train Epoch: 567 [106496/194182 (54%)]\tLoss: 0.437787\tGrad Norm: 1.081794\tLR: 0.030000\n",
      "Train Epoch: 567 [126976/194182 (65%)]\tLoss: 0.431726\tGrad Norm: 0.792582\tLR: 0.030000\n",
      "Train Epoch: 567 [147456/194182 (75%)]\tLoss: 0.430496\tGrad Norm: 0.793014\tLR: 0.030000\n",
      "Train Epoch: 567 [167936/194182 (85%)]\tLoss: 0.440244\tGrad Norm: 0.937250\tLR: 0.030000\n",
      "Train Epoch: 567 [188416/194182 (96%)]\tLoss: 0.443890\tGrad Norm: 1.312767\tLR: 0.030000\n",
      "Train set: Average loss: 0.4412\n",
      "Test set: Average loss: 0.2457, Average MAE: 0.3428\n",
      "Train Epoch: 568 [4096/194182 (2%)]\tLoss: 0.445559\tGrad Norm: 1.191399\tLR: 0.030000\n",
      "Train Epoch: 568 [24576/194182 (12%)]\tLoss: 0.439844\tGrad Norm: 1.310873\tLR: 0.030000\n",
      "Train Epoch: 568 [45056/194182 (23%)]\tLoss: 0.438632\tGrad Norm: 0.962448\tLR: 0.030000\n",
      "Train Epoch: 568 [65536/194182 (33%)]\tLoss: 0.441902\tGrad Norm: 0.971110\tLR: 0.030000\n",
      "Train Epoch: 568 [86016/194182 (44%)]\tLoss: 0.436642\tGrad Norm: 1.184177\tLR: 0.030000\n",
      "Train Epoch: 568 [106496/194182 (54%)]\tLoss: 0.451600\tGrad Norm: 1.182763\tLR: 0.030000\n",
      "Train Epoch: 568 [126976/194182 (65%)]\tLoss: 0.446269\tGrad Norm: 1.159974\tLR: 0.030000\n",
      "Train Epoch: 568 [147456/194182 (75%)]\tLoss: 0.436173\tGrad Norm: 1.228775\tLR: 0.030000\n",
      "Train Epoch: 568 [167936/194182 (85%)]\tLoss: 0.440749\tGrad Norm: 1.148021\tLR: 0.030000\n",
      "Train Epoch: 568 [188416/194182 (96%)]\tLoss: 0.446576\tGrad Norm: 1.431297\tLR: 0.030000\n",
      "Train set: Average loss: 0.4421\n",
      "Test set: Average loss: 0.2522, Average MAE: 0.3692\n",
      "Train Epoch: 569 [4096/194182 (2%)]\tLoss: 0.448850\tGrad Norm: 1.572624\tLR: 0.030000\n",
      "Train Epoch: 569 [24576/194182 (12%)]\tLoss: 0.444141\tGrad Norm: 1.326256\tLR: 0.030000\n",
      "Train Epoch: 569 [45056/194182 (23%)]\tLoss: 0.438356\tGrad Norm: 1.286110\tLR: 0.030000\n",
      "Train Epoch: 569 [65536/194182 (33%)]\tLoss: 0.443390\tGrad Norm: 1.128240\tLR: 0.030000\n",
      "Train Epoch: 569 [86016/194182 (44%)]\tLoss: 0.440237\tGrad Norm: 1.126338\tLR: 0.030000\n",
      "Train Epoch: 569 [106496/194182 (54%)]\tLoss: 0.452243\tGrad Norm: 1.188947\tLR: 0.030000\n",
      "Train Epoch: 569 [126976/194182 (65%)]\tLoss: 0.436070\tGrad Norm: 1.140275\tLR: 0.030000\n",
      "Train Epoch: 569 [147456/194182 (75%)]\tLoss: 0.436057\tGrad Norm: 0.937936\tLR: 0.030000\n",
      "Train Epoch: 569 [167936/194182 (85%)]\tLoss: 0.438074\tGrad Norm: 0.650324\tLR: 0.030000\n",
      "Train Epoch: 569 [188416/194182 (96%)]\tLoss: 0.436803\tGrad Norm: 1.058456\tLR: 0.030000\n",
      "Train set: Average loss: 0.4416\n",
      "Test set: Average loss: 0.2483, Average MAE: 0.3480\n",
      "Train Epoch: 570 [4096/194182 (2%)]\tLoss: 0.443362\tGrad Norm: 1.294926\tLR: 0.030000\n",
      "Train Epoch: 570 [24576/194182 (12%)]\tLoss: 0.443665\tGrad Norm: 1.463910\tLR: 0.030000\n",
      "Train Epoch: 570 [45056/194182 (23%)]\tLoss: 0.428228\tGrad Norm: 1.039956\tLR: 0.030000\n",
      "Train Epoch: 570 [65536/194182 (33%)]\tLoss: 0.447455\tGrad Norm: 1.318697\tLR: 0.030000\n",
      "Train Epoch: 570 [86016/194182 (44%)]\tLoss: 0.450276\tGrad Norm: 1.217843\tLR: 0.030000\n",
      "Train Epoch: 570 [106496/194182 (54%)]\tLoss: 0.449937\tGrad Norm: 1.576999\tLR: 0.030000\n",
      "Train Epoch: 570 [126976/194182 (65%)]\tLoss: 0.437042\tGrad Norm: 1.030544\tLR: 0.030000\n",
      "Train Epoch: 570 [147456/194182 (75%)]\tLoss: 0.446081\tGrad Norm: 1.045028\tLR: 0.030000\n",
      "Train Epoch: 570 [167936/194182 (85%)]\tLoss: 0.435178\tGrad Norm: 0.811837\tLR: 0.030000\n",
      "Train Epoch: 570 [188416/194182 (96%)]\tLoss: 0.425108\tGrad Norm: 0.887997\tLR: 0.030000\n",
      "Train set: Average loss: 0.4410\n",
      "Test set: Average loss: 0.2429, Average MAE: 0.3347\n",
      "Epoch 570: Mean reward = 0.042 +/- 0.014\n",
      "Train Epoch: 571 [4096/194182 (2%)]\tLoss: 0.440254\tGrad Norm: 1.028815\tLR: 0.030000\n",
      "Train Epoch: 571 [24576/194182 (12%)]\tLoss: 0.439545\tGrad Norm: 1.140434\tLR: 0.030000\n",
      "Train Epoch: 571 [45056/194182 (23%)]\tLoss: 0.442996\tGrad Norm: 1.246874\tLR: 0.030000\n",
      "Train Epoch: 571 [65536/194182 (33%)]\tLoss: 0.432754\tGrad Norm: 1.094302\tLR: 0.030000\n",
      "Train Epoch: 571 [86016/194182 (44%)]\tLoss: 0.441449\tGrad Norm: 0.778869\tLR: 0.030000\n",
      "Train Epoch: 571 [106496/194182 (54%)]\tLoss: 0.446340\tGrad Norm: 1.240309\tLR: 0.030000\n",
      "Train Epoch: 571 [126976/194182 (65%)]\tLoss: 0.438344\tGrad Norm: 1.266847\tLR: 0.030000\n",
      "Train Epoch: 571 [147456/194182 (75%)]\tLoss: 0.436464\tGrad Norm: 1.527309\tLR: 0.030000\n",
      "Train Epoch: 571 [167936/194182 (85%)]\tLoss: 0.454442\tGrad Norm: 1.330811\tLR: 0.030000\n",
      "Train Epoch: 571 [188416/194182 (96%)]\tLoss: 0.447728\tGrad Norm: 1.360355\tLR: 0.030000\n",
      "Train set: Average loss: 0.4411\n",
      "Test set: Average loss: 0.2461, Average MAE: 0.3413\n",
      "Train Epoch: 572 [4096/194182 (2%)]\tLoss: 0.436913\tGrad Norm: 1.086215\tLR: 0.030000\n",
      "Train Epoch: 572 [24576/194182 (12%)]\tLoss: 0.458228\tGrad Norm: 1.018453\tLR: 0.030000\n",
      "Train Epoch: 572 [45056/194182 (23%)]\tLoss: 0.442750\tGrad Norm: 1.253016\tLR: 0.030000\n",
      "Train Epoch: 572 [65536/194182 (33%)]\tLoss: 0.444111\tGrad Norm: 1.150540\tLR: 0.030000\n",
      "Train Epoch: 572 [86016/194182 (44%)]\tLoss: 0.447931\tGrad Norm: 1.418937\tLR: 0.030000\n",
      "Train Epoch: 572 [106496/194182 (54%)]\tLoss: 0.448843\tGrad Norm: 1.612093\tLR: 0.030000\n",
      "Train Epoch: 572 [126976/194182 (65%)]\tLoss: 0.443979\tGrad Norm: 1.276761\tLR: 0.030000\n",
      "Train Epoch: 572 [147456/194182 (75%)]\tLoss: 0.434023\tGrad Norm: 0.881669\tLR: 0.030000\n",
      "Train Epoch: 572 [167936/194182 (85%)]\tLoss: 0.441117\tGrad Norm: 1.160870\tLR: 0.030000\n",
      "Train Epoch: 572 [188416/194182 (96%)]\tLoss: 0.429728\tGrad Norm: 1.023646\tLR: 0.030000\n",
      "Train set: Average loss: 0.4401\n",
      "Test set: Average loss: 0.2433, Average MAE: 0.3358\n",
      "Train Epoch: 573 [4096/194182 (2%)]\tLoss: 0.446283\tGrad Norm: 1.083096\tLR: 0.030000\n",
      "Train Epoch: 573 [24576/194182 (12%)]\tLoss: 0.436940\tGrad Norm: 1.352181\tLR: 0.030000\n",
      "Train Epoch: 573 [45056/194182 (23%)]\tLoss: 0.443050\tGrad Norm: 1.327268\tLR: 0.030000\n",
      "Train Epoch: 573 [65536/194182 (33%)]\tLoss: 0.438426\tGrad Norm: 1.012511\tLR: 0.030000\n",
      "Train Epoch: 573 [86016/194182 (44%)]\tLoss: 0.430693\tGrad Norm: 1.074983\tLR: 0.030000\n",
      "Train Epoch: 573 [106496/194182 (54%)]\tLoss: 0.443172\tGrad Norm: 1.212970\tLR: 0.030000\n",
      "Train Epoch: 573 [126976/194182 (65%)]\tLoss: 0.441286\tGrad Norm: 1.109948\tLR: 0.030000\n",
      "Train Epoch: 573 [147456/194182 (75%)]\tLoss: 0.447660\tGrad Norm: 1.372793\tLR: 0.030000\n",
      "Train Epoch: 573 [167936/194182 (85%)]\tLoss: 0.434148\tGrad Norm: 1.196750\tLR: 0.030000\n",
      "Train Epoch: 573 [188416/194182 (96%)]\tLoss: 0.436465\tGrad Norm: 0.874906\tLR: 0.030000\n",
      "Train set: Average loss: 0.4403\n",
      "Test set: Average loss: 0.2440, Average MAE: 0.3462\n",
      "Train Epoch: 574 [4096/194182 (2%)]\tLoss: 0.432471\tGrad Norm: 1.031754\tLR: 0.030000\n",
      "Train Epoch: 574 [24576/194182 (12%)]\tLoss: 0.430072\tGrad Norm: 1.055119\tLR: 0.030000\n",
      "Train Epoch: 574 [45056/194182 (23%)]\tLoss: 0.447309\tGrad Norm: 1.248648\tLR: 0.030000\n",
      "Train Epoch: 574 [65536/194182 (33%)]\tLoss: 0.438005\tGrad Norm: 0.881342\tLR: 0.030000\n",
      "Train Epoch: 574 [86016/194182 (44%)]\tLoss: 0.429293\tGrad Norm: 0.818902\tLR: 0.030000\n",
      "Train Epoch: 574 [106496/194182 (54%)]\tLoss: 0.436214\tGrad Norm: 1.046378\tLR: 0.030000\n",
      "Train Epoch: 574 [126976/194182 (65%)]\tLoss: 0.438289\tGrad Norm: 0.923868\tLR: 0.030000\n",
      "Train Epoch: 574 [147456/194182 (75%)]\tLoss: 0.438041\tGrad Norm: 0.967780\tLR: 0.030000\n",
      "Train Epoch: 574 [167936/194182 (85%)]\tLoss: 0.438342\tGrad Norm: 1.613536\tLR: 0.030000\n",
      "Train Epoch: 574 [188416/194182 (96%)]\tLoss: 0.443812\tGrad Norm: 1.144687\tLR: 0.030000\n",
      "Train set: Average loss: 0.4381\n",
      "Test set: Average loss: 0.2423, Average MAE: 0.3338\n",
      "Train Epoch: 575 [4096/194182 (2%)]\tLoss: 0.434761\tGrad Norm: 1.059263\tLR: 0.030000\n",
      "Train Epoch: 575 [24576/194182 (12%)]\tLoss: 0.453157\tGrad Norm: 1.716107\tLR: 0.030000\n",
      "Train Epoch: 575 [45056/194182 (23%)]\tLoss: 0.444578\tGrad Norm: 1.194910\tLR: 0.030000\n",
      "Train Epoch: 575 [65536/194182 (33%)]\tLoss: 0.440027\tGrad Norm: 0.904229\tLR: 0.030000\n",
      "Train Epoch: 575 [86016/194182 (44%)]\tLoss: 0.441393\tGrad Norm: 1.181839\tLR: 0.030000\n",
      "Train Epoch: 575 [106496/194182 (54%)]\tLoss: 0.433813\tGrad Norm: 1.080418\tLR: 0.030000\n",
      "Train Epoch: 575 [126976/194182 (65%)]\tLoss: 0.444096\tGrad Norm: 1.147681\tLR: 0.030000\n",
      "Train Epoch: 575 [147456/194182 (75%)]\tLoss: 0.440324\tGrad Norm: 1.288381\tLR: 0.030000\n",
      "Train Epoch: 575 [167936/194182 (85%)]\tLoss: 0.439971\tGrad Norm: 1.118583\tLR: 0.030000\n",
      "Train Epoch: 575 [188416/194182 (96%)]\tLoss: 0.431432\tGrad Norm: 0.918743\tLR: 0.030000\n",
      "Train set: Average loss: 0.4395\n",
      "Test set: Average loss: 0.2429, Average MAE: 0.3388\n",
      "Epoch 575: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 576 [4096/194182 (2%)]\tLoss: 0.445336\tGrad Norm: 1.067799\tLR: 0.030000\n",
      "Train Epoch: 576 [24576/194182 (12%)]\tLoss: 0.448250\tGrad Norm: 1.234964\tLR: 0.030000\n",
      "Train Epoch: 576 [45056/194182 (23%)]\tLoss: 0.437353\tGrad Norm: 1.196643\tLR: 0.030000\n",
      "Train Epoch: 576 [65536/194182 (33%)]\tLoss: 0.432261\tGrad Norm: 1.010148\tLR: 0.030000\n",
      "Train Epoch: 576 [86016/194182 (44%)]\tLoss: 0.433528\tGrad Norm: 0.773880\tLR: 0.030000\n",
      "Train Epoch: 576 [106496/194182 (54%)]\tLoss: 0.439047\tGrad Norm: 0.912500\tLR: 0.030000\n",
      "Train Epoch: 576 [126976/194182 (65%)]\tLoss: 0.434512\tGrad Norm: 0.955598\tLR: 0.030000\n",
      "Train Epoch: 576 [147456/194182 (75%)]\tLoss: 0.436402\tGrad Norm: 0.967025\tLR: 0.030000\n",
      "Train Epoch: 576 [167936/194182 (85%)]\tLoss: 0.437351\tGrad Norm: 1.494567\tLR: 0.030000\n",
      "Train Epoch: 576 [188416/194182 (96%)]\tLoss: 0.435954\tGrad Norm: 1.206606\tLR: 0.030000\n",
      "Train set: Average loss: 0.4377\n",
      "Test set: Average loss: 0.2466, Average MAE: 0.3596\n",
      "Train Epoch: 577 [4096/194182 (2%)]\tLoss: 0.439784\tGrad Norm: 1.379329\tLR: 0.030000\n",
      "Train Epoch: 577 [24576/194182 (12%)]\tLoss: 0.439093\tGrad Norm: 1.244107\tLR: 0.030000\n",
      "Train Epoch: 577 [45056/194182 (23%)]\tLoss: 0.433091\tGrad Norm: 1.166597\tLR: 0.030000\n",
      "Train Epoch: 577 [65536/194182 (33%)]\tLoss: 0.439215\tGrad Norm: 1.432903\tLR: 0.030000\n",
      "Train Epoch: 577 [86016/194182 (44%)]\tLoss: 0.440394\tGrad Norm: 1.406918\tLR: 0.030000\n",
      "Train Epoch: 577 [106496/194182 (54%)]\tLoss: 0.431514\tGrad Norm: 0.958313\tLR: 0.030000\n",
      "Train Epoch: 577 [126976/194182 (65%)]\tLoss: 0.435398\tGrad Norm: 0.640363\tLR: 0.030000\n",
      "Train Epoch: 577 [147456/194182 (75%)]\tLoss: 0.425299\tGrad Norm: 0.554142\tLR: 0.030000\n",
      "Train Epoch: 577 [167936/194182 (85%)]\tLoss: 0.438049\tGrad Norm: 0.767435\tLR: 0.030000\n",
      "Train Epoch: 577 [188416/194182 (96%)]\tLoss: 0.430033\tGrad Norm: 1.108576\tLR: 0.030000\n",
      "Train set: Average loss: 0.4369\n",
      "Test set: Average loss: 0.2434, Average MAE: 0.3359\n",
      "Train Epoch: 578 [4096/194182 (2%)]\tLoss: 0.434418\tGrad Norm: 1.105615\tLR: 0.030000\n",
      "Train Epoch: 578 [24576/194182 (12%)]\tLoss: 0.439075\tGrad Norm: 0.982383\tLR: 0.030000\n",
      "Train Epoch: 578 [45056/194182 (23%)]\tLoss: 0.432409\tGrad Norm: 1.050285\tLR: 0.030000\n",
      "Train Epoch: 578 [65536/194182 (33%)]\tLoss: 0.438842\tGrad Norm: 1.122253\tLR: 0.030000\n",
      "Train Epoch: 578 [86016/194182 (44%)]\tLoss: 0.441912\tGrad Norm: 1.547237\tLR: 0.030000\n",
      "Train Epoch: 578 [106496/194182 (54%)]\tLoss: 0.439661\tGrad Norm: 1.269299\tLR: 0.030000\n",
      "Train Epoch: 578 [126976/194182 (65%)]\tLoss: 0.437078\tGrad Norm: 1.082437\tLR: 0.030000\n",
      "Train Epoch: 578 [147456/194182 (75%)]\tLoss: 0.433889\tGrad Norm: 0.999896\tLR: 0.030000\n",
      "Train Epoch: 578 [167936/194182 (85%)]\tLoss: 0.437238\tGrad Norm: 0.822455\tLR: 0.030000\n",
      "Train Epoch: 578 [188416/194182 (96%)]\tLoss: 0.427361\tGrad Norm: 0.927596\tLR: 0.030000\n",
      "Train set: Average loss: 0.4368\n",
      "Test set: Average loss: 0.2422, Average MAE: 0.3451\n",
      "Train Epoch: 579 [4096/194182 (2%)]\tLoss: 0.439712\tGrad Norm: 0.989059\tLR: 0.030000\n",
      "Train Epoch: 579 [24576/194182 (12%)]\tLoss: 0.438104\tGrad Norm: 1.098166\tLR: 0.030000\n",
      "Train Epoch: 579 [45056/194182 (23%)]\tLoss: 0.442002\tGrad Norm: 1.343013\tLR: 0.030000\n",
      "Train Epoch: 579 [65536/194182 (33%)]\tLoss: 0.444132\tGrad Norm: 1.525803\tLR: 0.030000\n",
      "Train Epoch: 579 [86016/194182 (44%)]\tLoss: 0.439812\tGrad Norm: 1.339040\tLR: 0.030000\n",
      "Train Epoch: 579 [106496/194182 (54%)]\tLoss: 0.433467\tGrad Norm: 0.690759\tLR: 0.030000\n",
      "Train Epoch: 579 [126976/194182 (65%)]\tLoss: 0.434721\tGrad Norm: 0.859049\tLR: 0.030000\n",
      "Train Epoch: 579 [147456/194182 (75%)]\tLoss: 0.435509\tGrad Norm: 1.161226\tLR: 0.030000\n",
      "Train Epoch: 579 [167936/194182 (85%)]\tLoss: 0.431221\tGrad Norm: 1.203591\tLR: 0.030000\n",
      "Train Epoch: 579 [188416/194182 (96%)]\tLoss: 0.438785\tGrad Norm: 1.180922\tLR: 0.030000\n",
      "Train set: Average loss: 0.4378\n",
      "Test set: Average loss: 0.2463, Average MAE: 0.3377\n",
      "Train Epoch: 580 [4096/194182 (2%)]\tLoss: 0.441741\tGrad Norm: 1.320953\tLR: 0.030000\n",
      "Train Epoch: 580 [24576/194182 (12%)]\tLoss: 0.441217\tGrad Norm: 1.095674\tLR: 0.030000\n",
      "Train Epoch: 580 [45056/194182 (23%)]\tLoss: 0.431383\tGrad Norm: 1.013009\tLR: 0.030000\n",
      "Train Epoch: 580 [65536/194182 (33%)]\tLoss: 0.432544\tGrad Norm: 1.014105\tLR: 0.030000\n",
      "Train Epoch: 580 [86016/194182 (44%)]\tLoss: 0.435267\tGrad Norm: 0.792139\tLR: 0.030000\n",
      "Train Epoch: 580 [106496/194182 (54%)]\tLoss: 0.431272\tGrad Norm: 1.214143\tLR: 0.030000\n",
      "Train Epoch: 580 [126976/194182 (65%)]\tLoss: 0.445815\tGrad Norm: 1.504310\tLR: 0.030000\n",
      "Train Epoch: 580 [147456/194182 (75%)]\tLoss: 0.438276\tGrad Norm: 1.440284\tLR: 0.030000\n",
      "Train Epoch: 580 [167936/194182 (85%)]\tLoss: 0.438935\tGrad Norm: 1.075686\tLR: 0.030000\n",
      "Train Epoch: 580 [188416/194182 (96%)]\tLoss: 0.433416\tGrad Norm: 0.913090\tLR: 0.030000\n",
      "Train set: Average loss: 0.4363\n",
      "Test set: Average loss: 0.2409, Average MAE: 0.3422\n",
      "Epoch 580: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 581 [4096/194182 (2%)]\tLoss: 0.433767\tGrad Norm: 0.818824\tLR: 0.030000\n",
      "Train Epoch: 581 [24576/194182 (12%)]\tLoss: 0.428995\tGrad Norm: 0.861851\tLR: 0.030000\n",
      "Train Epoch: 581 [45056/194182 (23%)]\tLoss: 0.435075\tGrad Norm: 1.095318\tLR: 0.030000\n",
      "Train Epoch: 581 [65536/194182 (33%)]\tLoss: 0.432302\tGrad Norm: 1.212009\tLR: 0.030000\n",
      "Train Epoch: 581 [86016/194182 (44%)]\tLoss: 0.436990\tGrad Norm: 1.486391\tLR: 0.030000\n",
      "Train Epoch: 581 [106496/194182 (54%)]\tLoss: 0.437213\tGrad Norm: 1.435042\tLR: 0.030000\n",
      "Train Epoch: 581 [126976/194182 (65%)]\tLoss: 0.441673\tGrad Norm: 1.185554\tLR: 0.030000\n",
      "Train Epoch: 581 [147456/194182 (75%)]\tLoss: 0.445569\tGrad Norm: 1.611582\tLR: 0.030000\n",
      "Train Epoch: 581 [167936/194182 (85%)]\tLoss: 0.439386\tGrad Norm: 1.177424\tLR: 0.030000\n",
      "Train Epoch: 581 [188416/194182 (96%)]\tLoss: 0.436004\tGrad Norm: 1.241623\tLR: 0.030000\n",
      "Train set: Average loss: 0.4374\n",
      "Test set: Average loss: 0.2470, Average MAE: 0.3385\n",
      "Train Epoch: 582 [4096/194182 (2%)]\tLoss: 0.441025\tGrad Norm: 1.267761\tLR: 0.030000\n",
      "Train Epoch: 582 [24576/194182 (12%)]\tLoss: 0.442972\tGrad Norm: 1.499633\tLR: 0.030000\n",
      "Train Epoch: 582 [45056/194182 (23%)]\tLoss: 0.427602\tGrad Norm: 0.767516\tLR: 0.030000\n",
      "Train Epoch: 582 [65536/194182 (33%)]\tLoss: 0.427340\tGrad Norm: 0.805583\tLR: 0.030000\n",
      "Train Epoch: 582 [86016/194182 (44%)]\tLoss: 0.429958\tGrad Norm: 0.920016\tLR: 0.030000\n",
      "Train Epoch: 582 [106496/194182 (54%)]\tLoss: 0.441618\tGrad Norm: 1.001864\tLR: 0.030000\n",
      "Train Epoch: 582 [126976/194182 (65%)]\tLoss: 0.430251\tGrad Norm: 0.962469\tLR: 0.030000\n",
      "Train Epoch: 582 [147456/194182 (75%)]\tLoss: 0.430369\tGrad Norm: 1.278465\tLR: 0.030000\n",
      "Train Epoch: 582 [167936/194182 (85%)]\tLoss: 0.435610\tGrad Norm: 1.123597\tLR: 0.030000\n",
      "Train Epoch: 582 [188416/194182 (96%)]\tLoss: 0.437863\tGrad Norm: 1.268855\tLR: 0.030000\n",
      "Train set: Average loss: 0.4352\n",
      "Test set: Average loss: 0.2478, Average MAE: 0.3354\n",
      "Train Epoch: 583 [4096/194182 (2%)]\tLoss: 0.441255\tGrad Norm: 1.434822\tLR: 0.030000\n",
      "Train Epoch: 583 [24576/194182 (12%)]\tLoss: 0.437930\tGrad Norm: 1.276873\tLR: 0.030000\n",
      "Train Epoch: 583 [45056/194182 (23%)]\tLoss: 0.438217\tGrad Norm: 1.118939\tLR: 0.030000\n",
      "Train Epoch: 583 [65536/194182 (33%)]\tLoss: 0.428114\tGrad Norm: 0.929912\tLR: 0.030000\n",
      "Train Epoch: 583 [86016/194182 (44%)]\tLoss: 0.431951\tGrad Norm: 1.083830\tLR: 0.030000\n",
      "Train Epoch: 583 [106496/194182 (54%)]\tLoss: 0.431960\tGrad Norm: 1.027268\tLR: 0.030000\n",
      "Train Epoch: 583 [126976/194182 (65%)]\tLoss: 0.441723\tGrad Norm: 1.318857\tLR: 0.030000\n",
      "Train Epoch: 583 [147456/194182 (75%)]\tLoss: 0.439637\tGrad Norm: 1.489786\tLR: 0.030000\n",
      "Train Epoch: 583 [167936/194182 (85%)]\tLoss: 0.442231\tGrad Norm: 1.349034\tLR: 0.030000\n",
      "Train Epoch: 583 [188416/194182 (96%)]\tLoss: 0.438087\tGrad Norm: 1.329262\tLR: 0.030000\n",
      "Train set: Average loss: 0.4371\n",
      "Test set: Average loss: 0.2494, Average MAE: 0.3607\n",
      "Train Epoch: 584 [4096/194182 (2%)]\tLoss: 0.444374\tGrad Norm: 1.282217\tLR: 0.030000\n",
      "Train Epoch: 584 [24576/194182 (12%)]\tLoss: 0.424968\tGrad Norm: 0.928837\tLR: 0.030000\n",
      "Train Epoch: 584 [45056/194182 (23%)]\tLoss: 0.428628\tGrad Norm: 0.910327\tLR: 0.030000\n",
      "Train Epoch: 584 [65536/194182 (33%)]\tLoss: 0.439270\tGrad Norm: 1.083160\tLR: 0.030000\n",
      "Train Epoch: 584 [86016/194182 (44%)]\tLoss: 0.438528\tGrad Norm: 1.227616\tLR: 0.030000\n",
      "Train Epoch: 584 [106496/194182 (54%)]\tLoss: 0.434599\tGrad Norm: 1.286952\tLR: 0.030000\n",
      "Train Epoch: 584 [126976/194182 (65%)]\tLoss: 0.439433\tGrad Norm: 1.665771\tLR: 0.030000\n",
      "Train Epoch: 584 [147456/194182 (75%)]\tLoss: 0.439444\tGrad Norm: 1.460739\tLR: 0.030000\n",
      "Train Epoch: 584 [167936/194182 (85%)]\tLoss: 0.432408\tGrad Norm: 1.125153\tLR: 0.030000\n",
      "Train Epoch: 584 [188416/194182 (96%)]\tLoss: 0.446758\tGrad Norm: 1.431994\tLR: 0.030000\n",
      "Train set: Average loss: 0.4364\n",
      "Test set: Average loss: 0.2462, Average MAE: 0.3545\n",
      "Train Epoch: 585 [4096/194182 (2%)]\tLoss: 0.435636\tGrad Norm: 1.194742\tLR: 0.030000\n",
      "Train Epoch: 585 [24576/194182 (12%)]\tLoss: 0.431629\tGrad Norm: 0.844280\tLR: 0.030000\n",
      "Train Epoch: 585 [45056/194182 (23%)]\tLoss: 0.436401\tGrad Norm: 1.386705\tLR: 0.030000\n",
      "Train Epoch: 585 [65536/194182 (33%)]\tLoss: 0.431394\tGrad Norm: 1.215408\tLR: 0.030000\n",
      "Train Epoch: 585 [86016/194182 (44%)]\tLoss: 0.435927\tGrad Norm: 0.805375\tLR: 0.030000\n",
      "Train Epoch: 585 [106496/194182 (54%)]\tLoss: 0.429987\tGrad Norm: 0.977909\tLR: 0.030000\n",
      "Train Epoch: 585 [126976/194182 (65%)]\tLoss: 0.435305\tGrad Norm: 0.772773\tLR: 0.030000\n",
      "Train Epoch: 585 [147456/194182 (75%)]\tLoss: 0.431829\tGrad Norm: 0.886262\tLR: 0.030000\n",
      "Train Epoch: 585 [167936/194182 (85%)]\tLoss: 0.434400\tGrad Norm: 0.800658\tLR: 0.030000\n",
      "Train Epoch: 585 [188416/194182 (96%)]\tLoss: 0.436172\tGrad Norm: 0.945592\tLR: 0.030000\n",
      "Train set: Average loss: 0.4336\n",
      "Test set: Average loss: 0.2459, Average MAE: 0.3455\n",
      "Epoch 585: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 586 [4096/194182 (2%)]\tLoss: 0.428265\tGrad Norm: 1.155589\tLR: 0.030000\n",
      "Train Epoch: 586 [24576/194182 (12%)]\tLoss: 0.435947\tGrad Norm: 1.091242\tLR: 0.030000\n",
      "Train Epoch: 586 [45056/194182 (23%)]\tLoss: 0.430804\tGrad Norm: 0.831829\tLR: 0.030000\n",
      "Train Epoch: 586 [65536/194182 (33%)]\tLoss: 0.429595\tGrad Norm: 0.954159\tLR: 0.030000\n",
      "Train Epoch: 586 [86016/194182 (44%)]\tLoss: 0.427550\tGrad Norm: 1.179887\tLR: 0.030000\n",
      "Train Epoch: 586 [106496/194182 (54%)]\tLoss: 0.447021\tGrad Norm: 1.731065\tLR: 0.030000\n",
      "Train Epoch: 586 [126976/194182 (65%)]\tLoss: 0.440475\tGrad Norm: 1.535012\tLR: 0.030000\n",
      "Train Epoch: 586 [147456/194182 (75%)]\tLoss: 0.442447\tGrad Norm: 1.331149\tLR: 0.030000\n",
      "Train Epoch: 586 [167936/194182 (85%)]\tLoss: 0.443337\tGrad Norm: 1.094028\tLR: 0.030000\n",
      "Train Epoch: 586 [188416/194182 (96%)]\tLoss: 0.445483\tGrad Norm: 1.176014\tLR: 0.030000\n",
      "Train set: Average loss: 0.4351\n",
      "Test set: Average loss: 0.2466, Average MAE: 0.3542\n",
      "Train Epoch: 587 [4096/194182 (2%)]\tLoss: 0.439046\tGrad Norm: 1.178101\tLR: 0.030000\n",
      "Train Epoch: 587 [24576/194182 (12%)]\tLoss: 0.424588\tGrad Norm: 0.802343\tLR: 0.030000\n",
      "Train Epoch: 587 [45056/194182 (23%)]\tLoss: 0.425004\tGrad Norm: 0.986087\tLR: 0.030000\n",
      "Train Epoch: 587 [65536/194182 (33%)]\tLoss: 0.438069\tGrad Norm: 1.241975\tLR: 0.030000\n",
      "Train Epoch: 587 [86016/194182 (44%)]\tLoss: 0.433463\tGrad Norm: 1.267936\tLR: 0.030000\n",
      "Train Epoch: 587 [106496/194182 (54%)]\tLoss: 0.439150\tGrad Norm: 0.972333\tLR: 0.030000\n",
      "Train Epoch: 587 [126976/194182 (65%)]\tLoss: 0.446443\tGrad Norm: 1.238976\tLR: 0.030000\n",
      "Train Epoch: 587 [147456/194182 (75%)]\tLoss: 0.436953\tGrad Norm: 1.247325\tLR: 0.030000\n",
      "Train Epoch: 587 [167936/194182 (85%)]\tLoss: 0.443695\tGrad Norm: 1.533459\tLR: 0.030000\n",
      "Train Epoch: 587 [188416/194182 (96%)]\tLoss: 0.436168\tGrad Norm: 1.071541\tLR: 0.030000\n",
      "Train set: Average loss: 0.4340\n",
      "Test set: Average loss: 0.2436, Average MAE: 0.3343\n",
      "Train Epoch: 588 [4096/194182 (2%)]\tLoss: 0.431939\tGrad Norm: 1.155219\tLR: 0.030000\n",
      "Train Epoch: 588 [24576/194182 (12%)]\tLoss: 0.437362\tGrad Norm: 1.451492\tLR: 0.030000\n",
      "Train Epoch: 588 [45056/194182 (23%)]\tLoss: 0.436547\tGrad Norm: 1.276263\tLR: 0.030000\n",
      "Train Epoch: 588 [65536/194182 (33%)]\tLoss: 0.434177\tGrad Norm: 1.307985\tLR: 0.030000\n",
      "Train Epoch: 588 [86016/194182 (44%)]\tLoss: 0.436445\tGrad Norm: 1.467648\tLR: 0.030000\n",
      "Train Epoch: 588 [106496/194182 (54%)]\tLoss: 0.440743\tGrad Norm: 1.294765\tLR: 0.030000\n",
      "Train Epoch: 588 [126976/194182 (65%)]\tLoss: 0.434837\tGrad Norm: 1.171573\tLR: 0.030000\n",
      "Train Epoch: 588 [147456/194182 (75%)]\tLoss: 0.441180\tGrad Norm: 1.456546\tLR: 0.030000\n",
      "Train Epoch: 588 [167936/194182 (85%)]\tLoss: 0.429367\tGrad Norm: 1.069643\tLR: 0.030000\n",
      "Train Epoch: 588 [188416/194182 (96%)]\tLoss: 0.430504\tGrad Norm: 1.307802\tLR: 0.030000\n",
      "Train set: Average loss: 0.4356\n",
      "Test set: Average loss: 0.2507, Average MAE: 0.3521\n",
      "Train Epoch: 589 [4096/194182 (2%)]\tLoss: 0.445148\tGrad Norm: 1.487739\tLR: 0.030000\n",
      "Train Epoch: 589 [24576/194182 (12%)]\tLoss: 0.431863\tGrad Norm: 1.015553\tLR: 0.030000\n",
      "Train Epoch: 589 [45056/194182 (23%)]\tLoss: 0.437375\tGrad Norm: 1.284029\tLR: 0.030000\n",
      "Train Epoch: 589 [65536/194182 (33%)]\tLoss: 0.423802\tGrad Norm: 0.900296\tLR: 0.030000\n",
      "Train Epoch: 589 [86016/194182 (44%)]\tLoss: 0.429124\tGrad Norm: 0.722153\tLR: 0.030000\n",
      "Train Epoch: 589 [106496/194182 (54%)]\tLoss: 0.427263\tGrad Norm: 1.033161\tLR: 0.030000\n",
      "Train Epoch: 589 [126976/194182 (65%)]\tLoss: 0.425740\tGrad Norm: 0.904309\tLR: 0.030000\n",
      "Train Epoch: 589 [147456/194182 (75%)]\tLoss: 0.434688\tGrad Norm: 1.032971\tLR: 0.030000\n",
      "Train Epoch: 589 [167936/194182 (85%)]\tLoss: 0.433828\tGrad Norm: 1.416522\tLR: 0.030000\n",
      "Train Epoch: 589 [188416/194182 (96%)]\tLoss: 0.436937\tGrad Norm: 1.275309\tLR: 0.030000\n",
      "Train set: Average loss: 0.4334\n",
      "Test set: Average loss: 0.2532, Average MAE: 0.3685\n",
      "Train Epoch: 590 [4096/194182 (2%)]\tLoss: 0.439943\tGrad Norm: 1.612070\tLR: 0.030000\n",
      "Train Epoch: 590 [24576/194182 (12%)]\tLoss: 0.434954\tGrad Norm: 1.190559\tLR: 0.030000\n",
      "Train Epoch: 590 [45056/194182 (23%)]\tLoss: 0.430743\tGrad Norm: 0.856687\tLR: 0.030000\n",
      "Train Epoch: 590 [65536/194182 (33%)]\tLoss: 0.433274\tGrad Norm: 1.205598\tLR: 0.030000\n",
      "Train Epoch: 590 [86016/194182 (44%)]\tLoss: 0.432603\tGrad Norm: 1.144287\tLR: 0.030000\n",
      "Train Epoch: 590 [106496/194182 (54%)]\tLoss: 0.441773\tGrad Norm: 1.422404\tLR: 0.030000\n",
      "Train Epoch: 590 [126976/194182 (65%)]\tLoss: 0.435127\tGrad Norm: 1.105592\tLR: 0.030000\n",
      "Train Epoch: 590 [147456/194182 (75%)]\tLoss: 0.432937\tGrad Norm: 1.336851\tLR: 0.030000\n",
      "Train Epoch: 590 [167936/194182 (85%)]\tLoss: 0.432166\tGrad Norm: 1.117678\tLR: 0.030000\n",
      "Train Epoch: 590 [188416/194182 (96%)]\tLoss: 0.434299\tGrad Norm: 1.234304\tLR: 0.030000\n",
      "Train set: Average loss: 0.4340\n",
      "Test set: Average loss: 0.2467, Average MAE: 0.3548\n",
      "Epoch 590: Mean reward = 0.063 +/- 0.048\n",
      "Train Epoch: 591 [4096/194182 (2%)]\tLoss: 0.427183\tGrad Norm: 1.268120\tLR: 0.030000\n",
      "Train Epoch: 591 [24576/194182 (12%)]\tLoss: 0.444256\tGrad Norm: 1.120631\tLR: 0.030000\n",
      "Train Epoch: 591 [45056/194182 (23%)]\tLoss: 0.431766\tGrad Norm: 1.247090\tLR: 0.030000\n",
      "Train Epoch: 591 [65536/194182 (33%)]\tLoss: 0.429478\tGrad Norm: 1.339032\tLR: 0.030000\n",
      "Train Epoch: 591 [86016/194182 (44%)]\tLoss: 0.430176\tGrad Norm: 0.827534\tLR: 0.030000\n",
      "Train Epoch: 591 [106496/194182 (54%)]\tLoss: 0.434015\tGrad Norm: 0.792269\tLR: 0.030000\n",
      "Train Epoch: 591 [126976/194182 (65%)]\tLoss: 0.432612\tGrad Norm: 1.105956\tLR: 0.030000\n",
      "Train Epoch: 591 [147456/194182 (75%)]\tLoss: 0.435598\tGrad Norm: 1.204941\tLR: 0.030000\n",
      "Train Epoch: 591 [167936/194182 (85%)]\tLoss: 0.428799\tGrad Norm: 1.017907\tLR: 0.030000\n",
      "Train Epoch: 591 [188416/194182 (96%)]\tLoss: 0.439304\tGrad Norm: 1.238568\tLR: 0.030000\n",
      "Train set: Average loss: 0.4322\n",
      "Test set: Average loss: 0.2504, Average MAE: 0.3448\n",
      "Train Epoch: 592 [4096/194182 (2%)]\tLoss: 0.441039\tGrad Norm: 1.527049\tLR: 0.030000\n",
      "Train Epoch: 592 [24576/194182 (12%)]\tLoss: 0.427477\tGrad Norm: 1.404555\tLR: 0.030000\n",
      "Train Epoch: 592 [45056/194182 (23%)]\tLoss: 0.433039\tGrad Norm: 1.143197\tLR: 0.030000\n",
      "Train Epoch: 592 [65536/194182 (33%)]\tLoss: 0.421820\tGrad Norm: 1.098551\tLR: 0.030000\n",
      "Train Epoch: 592 [86016/194182 (44%)]\tLoss: 0.435609\tGrad Norm: 1.022611\tLR: 0.030000\n",
      "Train Epoch: 592 [106496/194182 (54%)]\tLoss: 0.429976\tGrad Norm: 0.847306\tLR: 0.030000\n",
      "Train Epoch: 592 [126976/194182 (65%)]\tLoss: 0.436655\tGrad Norm: 1.061924\tLR: 0.030000\n",
      "Train Epoch: 592 [147456/194182 (75%)]\tLoss: 0.435031\tGrad Norm: 1.093664\tLR: 0.030000\n",
      "Train Epoch: 592 [167936/194182 (85%)]\tLoss: 0.441311\tGrad Norm: 1.355166\tLR: 0.030000\n",
      "Train Epoch: 592 [188416/194182 (96%)]\tLoss: 0.437842\tGrad Norm: 1.118107\tLR: 0.030000\n",
      "Train set: Average loss: 0.4332\n",
      "Test set: Average loss: 0.2448, Average MAE: 0.3336\n",
      "Train Epoch: 593 [4096/194182 (2%)]\tLoss: 0.426324\tGrad Norm: 1.183360\tLR: 0.030000\n",
      "Train Epoch: 593 [24576/194182 (12%)]\tLoss: 0.433648\tGrad Norm: 1.187026\tLR: 0.030000\n",
      "Train Epoch: 593 [45056/194182 (23%)]\tLoss: 0.427694\tGrad Norm: 1.336047\tLR: 0.030000\n",
      "Train Epoch: 593 [65536/194182 (33%)]\tLoss: 0.437137\tGrad Norm: 1.520255\tLR: 0.030000\n",
      "Train Epoch: 593 [86016/194182 (44%)]\tLoss: 0.431298\tGrad Norm: 1.087131\tLR: 0.030000\n",
      "Train Epoch: 593 [106496/194182 (54%)]\tLoss: 0.437469\tGrad Norm: 1.278589\tLR: 0.030000\n",
      "Train Epoch: 593 [126976/194182 (65%)]\tLoss: 0.428318\tGrad Norm: 1.111812\tLR: 0.030000\n",
      "Train Epoch: 593 [147456/194182 (75%)]\tLoss: 0.430698\tGrad Norm: 1.276724\tLR: 0.030000\n",
      "Train Epoch: 593 [167936/194182 (85%)]\tLoss: 0.429131\tGrad Norm: 0.840655\tLR: 0.030000\n",
      "Train Epoch: 593 [188416/194182 (96%)]\tLoss: 0.433700\tGrad Norm: 1.081456\tLR: 0.030000\n",
      "Train set: Average loss: 0.4326\n",
      "Test set: Average loss: 0.2442, Average MAE: 0.3391\n",
      "Train Epoch: 594 [4096/194182 (2%)]\tLoss: 0.430125\tGrad Norm: 1.157763\tLR: 0.030000\n",
      "Train Epoch: 594 [24576/194182 (12%)]\tLoss: 0.431044\tGrad Norm: 1.140516\tLR: 0.030000\n",
      "Train Epoch: 594 [45056/194182 (23%)]\tLoss: 0.434051\tGrad Norm: 1.157959\tLR: 0.030000\n",
      "Train Epoch: 594 [65536/194182 (33%)]\tLoss: 0.436679\tGrad Norm: 1.275733\tLR: 0.030000\n",
      "Train Epoch: 594 [86016/194182 (44%)]\tLoss: 0.427514\tGrad Norm: 1.390676\tLR: 0.030000\n",
      "Train Epoch: 594 [106496/194182 (54%)]\tLoss: 0.433885\tGrad Norm: 1.673062\tLR: 0.030000\n",
      "Train Epoch: 594 [126976/194182 (65%)]\tLoss: 0.435532\tGrad Norm: 1.116508\tLR: 0.030000\n",
      "Train Epoch: 594 [147456/194182 (75%)]\tLoss: 0.435978\tGrad Norm: 1.063413\tLR: 0.030000\n",
      "Train Epoch: 594 [167936/194182 (85%)]\tLoss: 0.438453\tGrad Norm: 1.262508\tLR: 0.030000\n",
      "Train Epoch: 594 [188416/194182 (96%)]\tLoss: 0.437777\tGrad Norm: 1.515982\tLR: 0.030000\n",
      "Train set: Average loss: 0.4339\n",
      "Test set: Average loss: 0.2469, Average MAE: 0.3411\n",
      "Train Epoch: 595 [4096/194182 (2%)]\tLoss: 0.435865\tGrad Norm: 1.259082\tLR: 0.030000\n",
      "Train Epoch: 595 [24576/194182 (12%)]\tLoss: 0.428549\tGrad Norm: 1.003786\tLR: 0.030000\n",
      "Train Epoch: 595 [45056/194182 (23%)]\tLoss: 0.439450\tGrad Norm: 1.292389\tLR: 0.030000\n",
      "Train Epoch: 595 [65536/194182 (33%)]\tLoss: 0.429275\tGrad Norm: 1.017468\tLR: 0.030000\n",
      "Train Epoch: 595 [86016/194182 (44%)]\tLoss: 0.430088\tGrad Norm: 0.943955\tLR: 0.030000\n",
      "Train Epoch: 595 [106496/194182 (54%)]\tLoss: 0.433159\tGrad Norm: 1.089116\tLR: 0.030000\n",
      "Train Epoch: 595 [126976/194182 (65%)]\tLoss: 0.427671\tGrad Norm: 1.105075\tLR: 0.030000\n",
      "Train Epoch: 595 [147456/194182 (75%)]\tLoss: 0.433486\tGrad Norm: 0.913087\tLR: 0.030000\n",
      "Train Epoch: 595 [167936/194182 (85%)]\tLoss: 0.437184\tGrad Norm: 1.016032\tLR: 0.030000\n",
      "Train Epoch: 595 [188416/194182 (96%)]\tLoss: 0.429795\tGrad Norm: 1.123613\tLR: 0.030000\n",
      "Train set: Average loss: 0.4303\n",
      "Test set: Average loss: 0.2445, Average MAE: 0.3478\n",
      "Epoch 595: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 596 [4096/194182 (2%)]\tLoss: 0.424957\tGrad Norm: 1.104334\tLR: 0.030000\n",
      "Train Epoch: 596 [24576/194182 (12%)]\tLoss: 0.427758\tGrad Norm: 1.073272\tLR: 0.030000\n",
      "Train Epoch: 596 [45056/194182 (23%)]\tLoss: 0.436982\tGrad Norm: 1.189760\tLR: 0.030000\n",
      "Train Epoch: 596 [65536/194182 (33%)]\tLoss: 0.430897\tGrad Norm: 1.312598\tLR: 0.030000\n",
      "Train Epoch: 596 [86016/194182 (44%)]\tLoss: 0.425127\tGrad Norm: 1.201244\tLR: 0.030000\n",
      "Train Epoch: 596 [106496/194182 (54%)]\tLoss: 0.426317\tGrad Norm: 1.095183\tLR: 0.030000\n",
      "Train Epoch: 596 [126976/194182 (65%)]\tLoss: 0.425612\tGrad Norm: 0.970478\tLR: 0.030000\n",
      "Train Epoch: 596 [147456/194182 (75%)]\tLoss: 0.437575\tGrad Norm: 1.301664\tLR: 0.030000\n",
      "Train Epoch: 596 [167936/194182 (85%)]\tLoss: 0.428677\tGrad Norm: 0.808279\tLR: 0.030000\n",
      "Train Epoch: 596 [188416/194182 (96%)]\tLoss: 0.430411\tGrad Norm: 1.044849\tLR: 0.030000\n",
      "Train set: Average loss: 0.4310\n",
      "Test set: Average loss: 0.2481, Average MAE: 0.3620\n",
      "Train Epoch: 597 [4096/194182 (2%)]\tLoss: 0.433474\tGrad Norm: 1.364371\tLR: 0.030000\n",
      "Train Epoch: 597 [24576/194182 (12%)]\tLoss: 0.433923\tGrad Norm: 1.120082\tLR: 0.030000\n",
      "Train Epoch: 597 [45056/194182 (23%)]\tLoss: 0.429247\tGrad Norm: 1.154018\tLR: 0.030000\n",
      "Train Epoch: 597 [65536/194182 (33%)]\tLoss: 0.430925\tGrad Norm: 0.870253\tLR: 0.030000\n",
      "Train Epoch: 597 [86016/194182 (44%)]\tLoss: 0.420813\tGrad Norm: 0.710157\tLR: 0.030000\n",
      "Train Epoch: 597 [106496/194182 (54%)]\tLoss: 0.428387\tGrad Norm: 1.063903\tLR: 0.030000\n",
      "Train Epoch: 597 [126976/194182 (65%)]\tLoss: 0.434886\tGrad Norm: 1.537412\tLR: 0.030000\n",
      "Train Epoch: 597 [147456/194182 (75%)]\tLoss: 0.438937\tGrad Norm: 1.435891\tLR: 0.030000\n",
      "Train Epoch: 597 [167936/194182 (85%)]\tLoss: 0.430684\tGrad Norm: 1.345768\tLR: 0.030000\n",
      "Train Epoch: 597 [188416/194182 (96%)]\tLoss: 0.428365\tGrad Norm: 1.150210\tLR: 0.030000\n",
      "Train set: Average loss: 0.4310\n",
      "Test set: Average loss: 0.2443, Average MAE: 0.3547\n",
      "Train Epoch: 598 [4096/194182 (2%)]\tLoss: 0.429113\tGrad Norm: 1.053145\tLR: 0.030000\n",
      "Train Epoch: 598 [24576/194182 (12%)]\tLoss: 0.428240\tGrad Norm: 1.389486\tLR: 0.030000\n",
      "Train Epoch: 598 [45056/194182 (23%)]\tLoss: 0.434161\tGrad Norm: 1.137064\tLR: 0.030000\n",
      "Train Epoch: 598 [65536/194182 (33%)]\tLoss: 0.421747\tGrad Norm: 0.994937\tLR: 0.030000\n",
      "Train Epoch: 598 [86016/194182 (44%)]\tLoss: 0.423937\tGrad Norm: 0.994364\tLR: 0.030000\n",
      "Train Epoch: 598 [106496/194182 (54%)]\tLoss: 0.420203\tGrad Norm: 0.837399\tLR: 0.030000\n",
      "Train Epoch: 598 [126976/194182 (65%)]\tLoss: 0.422045\tGrad Norm: 0.618125\tLR: 0.030000\n",
      "Train Epoch: 598 [147456/194182 (75%)]\tLoss: 0.424805\tGrad Norm: 0.668889\tLR: 0.030000\n",
      "Train Epoch: 598 [167936/194182 (85%)]\tLoss: 0.432158\tGrad Norm: 1.092850\tLR: 0.030000\n",
      "Train Epoch: 598 [188416/194182 (96%)]\tLoss: 0.424707\tGrad Norm: 1.026143\tLR: 0.030000\n",
      "Train set: Average loss: 0.4284\n",
      "Test set: Average loss: 0.2536, Average MAE: 0.3681\n",
      "Train Epoch: 599 [4096/194182 (2%)]\tLoss: 0.441770\tGrad Norm: 1.614219\tLR: 0.030000\n",
      "Train Epoch: 599 [24576/194182 (12%)]\tLoss: 0.430864\tGrad Norm: 1.280296\tLR: 0.030000\n",
      "Train Epoch: 599 [45056/194182 (23%)]\tLoss: 0.424633\tGrad Norm: 0.965401\tLR: 0.030000\n",
      "Train Epoch: 599 [65536/194182 (33%)]\tLoss: 0.417708\tGrad Norm: 0.605628\tLR: 0.030000\n",
      "Train Epoch: 599 [86016/194182 (44%)]\tLoss: 0.418646\tGrad Norm: 0.908563\tLR: 0.030000\n",
      "Train Epoch: 599 [106496/194182 (54%)]\tLoss: 0.421570\tGrad Norm: 0.928744\tLR: 0.030000\n",
      "Train Epoch: 599 [126976/194182 (65%)]\tLoss: 0.432013\tGrad Norm: 1.079157\tLR: 0.030000\n",
      "Train Epoch: 599 [147456/194182 (75%)]\tLoss: 0.442674\tGrad Norm: 1.359232\tLR: 0.030000\n",
      "Train Epoch: 599 [167936/194182 (85%)]\tLoss: 0.435285\tGrad Norm: 1.551054\tLR: 0.030000\n",
      "Train Epoch: 599 [188416/194182 (96%)]\tLoss: 0.427976\tGrad Norm: 1.060595\tLR: 0.030000\n",
      "Train set: Average loss: 0.4301\n",
      "Test set: Average loss: 0.2436, Average MAE: 0.3360\n",
      "Train Epoch: 600 [4096/194182 (2%)]\tLoss: 0.430645\tGrad Norm: 1.202580\tLR: 0.030000\n",
      "Train Epoch: 600 [24576/194182 (12%)]\tLoss: 0.431752\tGrad Norm: 1.180401\tLR: 0.030000\n",
      "Train Epoch: 600 [45056/194182 (23%)]\tLoss: 0.426612\tGrad Norm: 1.128916\tLR: 0.030000\n",
      "Train Epoch: 600 [65536/194182 (33%)]\tLoss: 0.425797\tGrad Norm: 1.217438\tLR: 0.030000\n",
      "Train Epoch: 600 [86016/194182 (44%)]\tLoss: 0.434866\tGrad Norm: 1.431641\tLR: 0.030000\n",
      "Train Epoch: 600 [106496/194182 (54%)]\tLoss: 0.422495\tGrad Norm: 1.171420\tLR: 0.030000\n",
      "Train Epoch: 600 [126976/194182 (65%)]\tLoss: 0.425346\tGrad Norm: 0.995234\tLR: 0.030000\n",
      "Train Epoch: 600 [147456/194182 (75%)]\tLoss: 0.431463\tGrad Norm: 1.051818\tLR: 0.030000\n",
      "Train Epoch: 600 [167936/194182 (85%)]\tLoss: 0.432717\tGrad Norm: 1.340762\tLR: 0.030000\n",
      "Train Epoch: 600 [188416/194182 (96%)]\tLoss: 0.432157\tGrad Norm: 1.390723\tLR: 0.030000\n",
      "Train set: Average loss: 0.4301\n",
      "Test set: Average loss: 0.2460, Average MAE: 0.3596\n",
      "Epoch 600: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 601 [4096/194182 (2%)]\tLoss: 0.428450\tGrad Norm: 1.137174\tLR: 0.030000\n",
      "Train Epoch: 601 [24576/194182 (12%)]\tLoss: 0.420469\tGrad Norm: 0.822144\tLR: 0.030000\n",
      "Train Epoch: 601 [45056/194182 (23%)]\tLoss: 0.422837\tGrad Norm: 0.763368\tLR: 0.030000\n",
      "Train Epoch: 601 [65536/194182 (33%)]\tLoss: 0.420529\tGrad Norm: 0.990776\tLR: 0.030000\n",
      "Train Epoch: 601 [86016/194182 (44%)]\tLoss: 0.434441\tGrad Norm: 1.281344\tLR: 0.030000\n",
      "Train Epoch: 601 [106496/194182 (54%)]\tLoss: 0.441014\tGrad Norm: 1.690087\tLR: 0.030000\n",
      "Train Epoch: 601 [126976/194182 (65%)]\tLoss: 0.424688\tGrad Norm: 0.941380\tLR: 0.030000\n",
      "Train Epoch: 601 [147456/194182 (75%)]\tLoss: 0.425961\tGrad Norm: 1.042877\tLR: 0.030000\n",
      "Train Epoch: 601 [167936/194182 (85%)]\tLoss: 0.429339\tGrad Norm: 1.349558\tLR: 0.030000\n",
      "Train Epoch: 601 [188416/194182 (96%)]\tLoss: 0.428433\tGrad Norm: 1.423473\tLR: 0.030000\n",
      "Train set: Average loss: 0.4286\n",
      "Test set: Average loss: 0.2484, Average MAE: 0.3614\n",
      "Train Epoch: 602 [4096/194182 (2%)]\tLoss: 0.432143\tGrad Norm: 1.342370\tLR: 0.030000\n",
      "Train Epoch: 602 [24576/194182 (12%)]\tLoss: 0.435839\tGrad Norm: 1.597964\tLR: 0.030000\n",
      "Train Epoch: 602 [45056/194182 (23%)]\tLoss: 0.431212\tGrad Norm: 1.174592\tLR: 0.030000\n",
      "Train Epoch: 602 [65536/194182 (33%)]\tLoss: 0.420585\tGrad Norm: 0.793331\tLR: 0.030000\n",
      "Train Epoch: 602 [86016/194182 (44%)]\tLoss: 0.430897\tGrad Norm: 1.008560\tLR: 0.030000\n",
      "Train Epoch: 602 [106496/194182 (54%)]\tLoss: 0.434536\tGrad Norm: 1.309150\tLR: 0.030000\n",
      "Train Epoch: 602 [126976/194182 (65%)]\tLoss: 0.429918\tGrad Norm: 1.141773\tLR: 0.030000\n",
      "Train Epoch: 602 [147456/194182 (75%)]\tLoss: 0.432737\tGrad Norm: 0.985658\tLR: 0.030000\n",
      "Train Epoch: 602 [167936/194182 (85%)]\tLoss: 0.436337\tGrad Norm: 1.297000\tLR: 0.030000\n",
      "Train Epoch: 602 [188416/194182 (96%)]\tLoss: 0.422819\tGrad Norm: 0.798182\tLR: 0.030000\n",
      "Train set: Average loss: 0.4292\n",
      "Test set: Average loss: 0.2454, Average MAE: 0.3582\n",
      "Train Epoch: 603 [4096/194182 (2%)]\tLoss: 0.423046\tGrad Norm: 1.302378\tLR: 0.030000\n",
      "Train Epoch: 603 [24576/194182 (12%)]\tLoss: 0.426028\tGrad Norm: 1.073099\tLR: 0.030000\n",
      "Train Epoch: 603 [45056/194182 (23%)]\tLoss: 0.437157\tGrad Norm: 0.820428\tLR: 0.030000\n",
      "Train Epoch: 603 [65536/194182 (33%)]\tLoss: 0.421225\tGrad Norm: 0.906687\tLR: 0.030000\n",
      "Train Epoch: 603 [86016/194182 (44%)]\tLoss: 0.420225\tGrad Norm: 1.005484\tLR: 0.030000\n",
      "Train Epoch: 603 [106496/194182 (54%)]\tLoss: 0.431707\tGrad Norm: 1.335847\tLR: 0.030000\n",
      "Train Epoch: 603 [126976/194182 (65%)]\tLoss: 0.428390\tGrad Norm: 1.242884\tLR: 0.030000\n",
      "Train Epoch: 603 [147456/194182 (75%)]\tLoss: 0.423499\tGrad Norm: 0.847016\tLR: 0.030000\n",
      "Train Epoch: 603 [167936/194182 (85%)]\tLoss: 0.423893\tGrad Norm: 0.870649\tLR: 0.030000\n",
      "Train Epoch: 603 [188416/194182 (96%)]\tLoss: 0.428855\tGrad Norm: 1.190948\tLR: 0.030000\n",
      "Train set: Average loss: 0.4269\n",
      "Test set: Average loss: 0.2485, Average MAE: 0.3580\n",
      "Train Epoch: 604 [4096/194182 (2%)]\tLoss: 0.427972\tGrad Norm: 1.462796\tLR: 0.030000\n",
      "Train Epoch: 604 [24576/194182 (12%)]\tLoss: 0.434058\tGrad Norm: 1.280998\tLR: 0.030000\n",
      "Train Epoch: 604 [45056/194182 (23%)]\tLoss: 0.430542\tGrad Norm: 1.483715\tLR: 0.030000\n",
      "Train Epoch: 604 [65536/194182 (33%)]\tLoss: 0.422850\tGrad Norm: 0.758083\tLR: 0.030000\n",
      "Train Epoch: 604 [86016/194182 (44%)]\tLoss: 0.426633\tGrad Norm: 1.165798\tLR: 0.030000\n",
      "Train Epoch: 604 [106496/194182 (54%)]\tLoss: 0.441518\tGrad Norm: 1.534984\tLR: 0.030000\n",
      "Train Epoch: 604 [126976/194182 (65%)]\tLoss: 0.436687\tGrad Norm: 1.245001\tLR: 0.030000\n",
      "Train Epoch: 604 [147456/194182 (75%)]\tLoss: 0.424804\tGrad Norm: 1.205230\tLR: 0.030000\n",
      "Train Epoch: 604 [167936/194182 (85%)]\tLoss: 0.425728\tGrad Norm: 1.095531\tLR: 0.030000\n",
      "Train Epoch: 604 [188416/194182 (96%)]\tLoss: 0.425703\tGrad Norm: 0.907702\tLR: 0.030000\n",
      "Train set: Average loss: 0.4291\n",
      "Test set: Average loss: 0.2487, Average MAE: 0.3599\n",
      "Train Epoch: 605 [4096/194182 (2%)]\tLoss: 0.428294\tGrad Norm: 1.389473\tLR: 0.030000\n",
      "Train Epoch: 605 [24576/194182 (12%)]\tLoss: 0.435670\tGrad Norm: 1.312198\tLR: 0.030000\n",
      "Train Epoch: 605 [45056/194182 (23%)]\tLoss: 0.426169\tGrad Norm: 1.072858\tLR: 0.030000\n",
      "Train Epoch: 605 [65536/194182 (33%)]\tLoss: 0.410218\tGrad Norm: 0.984829\tLR: 0.030000\n",
      "Train Epoch: 605 [86016/194182 (44%)]\tLoss: 0.431482\tGrad Norm: 1.236598\tLR: 0.030000\n",
      "Train Epoch: 605 [106496/194182 (54%)]\tLoss: 0.423720\tGrad Norm: 1.366056\tLR: 0.030000\n",
      "Train Epoch: 605 [126976/194182 (65%)]\tLoss: 0.425739\tGrad Norm: 1.265301\tLR: 0.030000\n",
      "Train Epoch: 605 [147456/194182 (75%)]\tLoss: 0.423504\tGrad Norm: 1.006762\tLR: 0.030000\n",
      "Train Epoch: 605 [167936/194182 (85%)]\tLoss: 0.425224\tGrad Norm: 1.042748\tLR: 0.030000\n",
      "Train Epoch: 605 [188416/194182 (96%)]\tLoss: 0.436639\tGrad Norm: 1.052044\tLR: 0.030000\n",
      "Train set: Average loss: 0.4274\n",
      "Test set: Average loss: 0.2410, Average MAE: 0.3477\n",
      "Epoch 605: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 606 [4096/194182 (2%)]\tLoss: 0.426589\tGrad Norm: 0.885546\tLR: 0.030000\n",
      "Train Epoch: 606 [24576/194182 (12%)]\tLoss: 0.434390\tGrad Norm: 1.177569\tLR: 0.030000\n",
      "Train Epoch: 606 [45056/194182 (23%)]\tLoss: 0.427825\tGrad Norm: 0.937082\tLR: 0.030000\n",
      "Train Epoch: 606 [65536/194182 (33%)]\tLoss: 0.443759\tGrad Norm: 1.570633\tLR: 0.030000\n",
      "Train Epoch: 606 [86016/194182 (44%)]\tLoss: 0.429546\tGrad Norm: 1.340983\tLR: 0.030000\n",
      "Train Epoch: 606 [106496/194182 (54%)]\tLoss: 0.435140\tGrad Norm: 1.319414\tLR: 0.030000\n",
      "Train Epoch: 606 [126976/194182 (65%)]\tLoss: 0.430656\tGrad Norm: 0.973366\tLR: 0.030000\n",
      "Train Epoch: 606 [147456/194182 (75%)]\tLoss: 0.418486\tGrad Norm: 1.239570\tLR: 0.030000\n",
      "Train Epoch: 606 [167936/194182 (85%)]\tLoss: 0.428522\tGrad Norm: 1.214000\tLR: 0.030000\n",
      "Train Epoch: 606 [188416/194182 (96%)]\tLoss: 0.436679\tGrad Norm: 1.741056\tLR: 0.030000\n",
      "Train set: Average loss: 0.4288\n",
      "Test set: Average loss: 0.2524, Average MAE: 0.3682\n",
      "Train Epoch: 607 [4096/194182 (2%)]\tLoss: 0.434877\tGrad Norm: 1.640521\tLR: 0.030000\n",
      "Train Epoch: 607 [24576/194182 (12%)]\tLoss: 0.430206\tGrad Norm: 1.469243\tLR: 0.030000\n",
      "Train Epoch: 607 [45056/194182 (23%)]\tLoss: 0.429011\tGrad Norm: 1.150683\tLR: 0.030000\n",
      "Train Epoch: 607 [65536/194182 (33%)]\tLoss: 0.441351\tGrad Norm: 1.848111\tLR: 0.030000\n",
      "Train Epoch: 607 [86016/194182 (44%)]\tLoss: 0.431088\tGrad Norm: 1.323796\tLR: 0.030000\n",
      "Train Epoch: 607 [106496/194182 (54%)]\tLoss: 0.420325\tGrad Norm: 1.060092\tLR: 0.030000\n",
      "Train Epoch: 607 [126976/194182 (65%)]\tLoss: 0.435754\tGrad Norm: 0.947685\tLR: 0.030000\n",
      "Train Epoch: 607 [147456/194182 (75%)]\tLoss: 0.419750\tGrad Norm: 0.942589\tLR: 0.030000\n",
      "Train Epoch: 607 [167936/194182 (85%)]\tLoss: 0.426752\tGrad Norm: 0.709866\tLR: 0.030000\n",
      "Train Epoch: 607 [188416/194182 (96%)]\tLoss: 0.424308\tGrad Norm: 0.789126\tLR: 0.030000\n",
      "Train set: Average loss: 0.4281\n",
      "Test set: Average loss: 0.2410, Average MAE: 0.3426\n",
      "Train Epoch: 608 [4096/194182 (2%)]\tLoss: 0.423428\tGrad Norm: 0.932297\tLR: 0.030000\n",
      "Train Epoch: 608 [24576/194182 (12%)]\tLoss: 0.424537\tGrad Norm: 0.850755\tLR: 0.030000\n",
      "Train Epoch: 608 [45056/194182 (23%)]\tLoss: 0.428378\tGrad Norm: 0.887586\tLR: 0.030000\n",
      "Train Epoch: 608 [65536/194182 (33%)]\tLoss: 0.425509\tGrad Norm: 1.090565\tLR: 0.030000\n",
      "Train Epoch: 608 [86016/194182 (44%)]\tLoss: 0.423653\tGrad Norm: 0.979048\tLR: 0.030000\n",
      "Train Epoch: 608 [106496/194182 (54%)]\tLoss: 0.431104\tGrad Norm: 1.213490\tLR: 0.030000\n",
      "Train Epoch: 608 [126976/194182 (65%)]\tLoss: 0.434820\tGrad Norm: 1.073433\tLR: 0.030000\n",
      "Train Epoch: 608 [147456/194182 (75%)]\tLoss: 0.433645\tGrad Norm: 1.500917\tLR: 0.030000\n",
      "Train Epoch: 608 [167936/194182 (85%)]\tLoss: 0.433717\tGrad Norm: 1.404187\tLR: 0.030000\n",
      "Train Epoch: 608 [188416/194182 (96%)]\tLoss: 0.435521\tGrad Norm: 1.418621\tLR: 0.030000\n",
      "Train set: Average loss: 0.4266\n",
      "Test set: Average loss: 0.2452, Average MAE: 0.3559\n",
      "Train Epoch: 609 [4096/194182 (2%)]\tLoss: 0.433196\tGrad Norm: 1.196233\tLR: 0.030000\n",
      "Train Epoch: 609 [24576/194182 (12%)]\tLoss: 0.420424\tGrad Norm: 1.109837\tLR: 0.030000\n",
      "Train Epoch: 609 [45056/194182 (23%)]\tLoss: 0.421264\tGrad Norm: 0.709643\tLR: 0.030000\n",
      "Train Epoch: 609 [65536/194182 (33%)]\tLoss: 0.423054\tGrad Norm: 0.812966\tLR: 0.030000\n",
      "Train Epoch: 609 [86016/194182 (44%)]\tLoss: 0.429323\tGrad Norm: 1.224971\tLR: 0.030000\n",
      "Train Epoch: 609 [106496/194182 (54%)]\tLoss: 0.431269\tGrad Norm: 1.202727\tLR: 0.030000\n",
      "Train Epoch: 609 [126976/194182 (65%)]\tLoss: 0.426322\tGrad Norm: 1.188279\tLR: 0.030000\n",
      "Train Epoch: 609 [147456/194182 (75%)]\tLoss: 0.427983\tGrad Norm: 1.399790\tLR: 0.030000\n",
      "Train Epoch: 609 [167936/194182 (85%)]\tLoss: 0.422555\tGrad Norm: 1.020067\tLR: 0.030000\n",
      "Train Epoch: 609 [188416/194182 (96%)]\tLoss: 0.422361\tGrad Norm: 1.124922\tLR: 0.030000\n",
      "Train set: Average loss: 0.4257\n",
      "Test set: Average loss: 0.2520, Average MAE: 0.3704\n",
      "Train Epoch: 610 [4096/194182 (2%)]\tLoss: 0.428891\tGrad Norm: 1.609346\tLR: 0.030000\n",
      "Train Epoch: 610 [24576/194182 (12%)]\tLoss: 0.427965\tGrad Norm: 1.020051\tLR: 0.030000\n",
      "Train Epoch: 610 [45056/194182 (23%)]\tLoss: 0.420084\tGrad Norm: 1.014031\tLR: 0.030000\n",
      "Train Epoch: 610 [65536/194182 (33%)]\tLoss: 0.423498\tGrad Norm: 1.076617\tLR: 0.030000\n",
      "Train Epoch: 610 [86016/194182 (44%)]\tLoss: 0.426383\tGrad Norm: 0.907753\tLR: 0.030000\n",
      "Train Epoch: 610 [106496/194182 (54%)]\tLoss: 0.429017\tGrad Norm: 0.937803\tLR: 0.030000\n",
      "Train Epoch: 610 [126976/194182 (65%)]\tLoss: 0.423594\tGrad Norm: 1.090467\tLR: 0.030000\n",
      "Train Epoch: 610 [147456/194182 (75%)]\tLoss: 0.425096\tGrad Norm: 0.729644\tLR: 0.030000\n",
      "Train Epoch: 610 [167936/194182 (85%)]\tLoss: 0.428939\tGrad Norm: 1.355502\tLR: 0.030000\n",
      "Train Epoch: 610 [188416/194182 (96%)]\tLoss: 0.425695\tGrad Norm: 1.202152\tLR: 0.030000\n",
      "Train set: Average loss: 0.4243\n",
      "Test set: Average loss: 0.2439, Average MAE: 0.3329\n",
      "Epoch 610: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 611 [4096/194182 (2%)]\tLoss: 0.423647\tGrad Norm: 1.179374\tLR: 0.030000\n",
      "Train Epoch: 611 [24576/194182 (12%)]\tLoss: 0.425623\tGrad Norm: 1.135805\tLR: 0.030000\n",
      "Train Epoch: 611 [45056/194182 (23%)]\tLoss: 0.421828\tGrad Norm: 0.927334\tLR: 0.030000\n",
      "Train Epoch: 611 [65536/194182 (33%)]\tLoss: 0.430516\tGrad Norm: 1.202516\tLR: 0.030000\n",
      "Train Epoch: 611 [86016/194182 (44%)]\tLoss: 0.427556\tGrad Norm: 1.500898\tLR: 0.030000\n",
      "Train Epoch: 611 [106496/194182 (54%)]\tLoss: 0.431272\tGrad Norm: 1.283827\tLR: 0.030000\n",
      "Train Epoch: 611 [126976/194182 (65%)]\tLoss: 0.422146\tGrad Norm: 1.232457\tLR: 0.030000\n",
      "Train Epoch: 611 [147456/194182 (75%)]\tLoss: 0.431180\tGrad Norm: 1.049383\tLR: 0.030000\n",
      "Train Epoch: 611 [167936/194182 (85%)]\tLoss: 0.424611\tGrad Norm: 0.825162\tLR: 0.030000\n",
      "Train Epoch: 611 [188416/194182 (96%)]\tLoss: 0.424334\tGrad Norm: 1.362037\tLR: 0.030000\n",
      "Train set: Average loss: 0.4260\n",
      "Test set: Average loss: 0.2430, Average MAE: 0.3392\n",
      "Train Epoch: 612 [4096/194182 (2%)]\tLoss: 0.424788\tGrad Norm: 1.260839\tLR: 0.030000\n",
      "Train Epoch: 612 [24576/194182 (12%)]\tLoss: 0.418518\tGrad Norm: 1.235243\tLR: 0.030000\n",
      "Train Epoch: 612 [45056/194182 (23%)]\tLoss: 0.429748\tGrad Norm: 1.403011\tLR: 0.030000\n",
      "Train Epoch: 612 [65536/194182 (33%)]\tLoss: 0.431258\tGrad Norm: 1.370824\tLR: 0.030000\n",
      "Train Epoch: 612 [86016/194182 (44%)]\tLoss: 0.425779\tGrad Norm: 1.583865\tLR: 0.030000\n",
      "Train Epoch: 612 [106496/194182 (54%)]\tLoss: 0.421758\tGrad Norm: 1.034736\tLR: 0.030000\n",
      "Train Epoch: 612 [126976/194182 (65%)]\tLoss: 0.419562\tGrad Norm: 1.050246\tLR: 0.030000\n",
      "Train Epoch: 612 [147456/194182 (75%)]\tLoss: 0.428254\tGrad Norm: 1.054467\tLR: 0.030000\n",
      "Train Epoch: 612 [167936/194182 (85%)]\tLoss: 0.425454\tGrad Norm: 1.107101\tLR: 0.030000\n",
      "Train Epoch: 612 [188416/194182 (96%)]\tLoss: 0.423521\tGrad Norm: 1.106184\tLR: 0.030000\n",
      "Train set: Average loss: 0.4258\n",
      "Test set: Average loss: 0.2402, Average MAE: 0.3398\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 613 [4096/194182 (2%)]\tLoss: 0.422827\tGrad Norm: 0.864582\tLR: 0.030000\n",
      "Train Epoch: 613 [24576/194182 (12%)]\tLoss: 0.423687\tGrad Norm: 1.199036\tLR: 0.030000\n",
      "Train Epoch: 613 [45056/194182 (23%)]\tLoss: 0.424567\tGrad Norm: 1.253618\tLR: 0.030000\n",
      "Train Epoch: 613 [65536/194182 (33%)]\tLoss: 0.428166\tGrad Norm: 0.935130\tLR: 0.030000\n",
      "Train Epoch: 613 [86016/194182 (44%)]\tLoss: 0.411587\tGrad Norm: 0.757933\tLR: 0.030000\n",
      "Train Epoch: 613 [106496/194182 (54%)]\tLoss: 0.427130\tGrad Norm: 1.208265\tLR: 0.030000\n",
      "Train Epoch: 613 [126976/194182 (65%)]\tLoss: 0.427551\tGrad Norm: 1.061933\tLR: 0.030000\n",
      "Train Epoch: 613 [147456/194182 (75%)]\tLoss: 0.421670\tGrad Norm: 1.034918\tLR: 0.030000\n",
      "Train Epoch: 613 [167936/194182 (85%)]\tLoss: 0.430941\tGrad Norm: 1.289546\tLR: 0.030000\n",
      "Train Epoch: 613 [188416/194182 (96%)]\tLoss: 0.429735\tGrad Norm: 1.296135\tLR: 0.030000\n",
      "Train set: Average loss: 0.4244\n",
      "Test set: Average loss: 0.2470, Average MAE: 0.3554\n",
      "Train Epoch: 614 [4096/194182 (2%)]\tLoss: 0.426625\tGrad Norm: 1.255407\tLR: 0.030000\n",
      "Train Epoch: 614 [24576/194182 (12%)]\tLoss: 0.436657\tGrad Norm: 1.342300\tLR: 0.030000\n",
      "Train Epoch: 614 [45056/194182 (23%)]\tLoss: 0.423383\tGrad Norm: 1.107989\tLR: 0.030000\n",
      "Train Epoch: 614 [65536/194182 (33%)]\tLoss: 0.425956\tGrad Norm: 1.146468\tLR: 0.030000\n",
      "Train Epoch: 614 [86016/194182 (44%)]\tLoss: 0.425828\tGrad Norm: 1.438982\tLR: 0.030000\n",
      "Train Epoch: 614 [106496/194182 (54%)]\tLoss: 0.420246\tGrad Norm: 0.977253\tLR: 0.030000\n",
      "Train Epoch: 614 [126976/194182 (65%)]\tLoss: 0.413447\tGrad Norm: 0.971216\tLR: 0.030000\n",
      "Train Epoch: 614 [147456/194182 (75%)]\tLoss: 0.430310\tGrad Norm: 1.478877\tLR: 0.030000\n",
      "Train Epoch: 614 [167936/194182 (85%)]\tLoss: 0.419208\tGrad Norm: 1.119922\tLR: 0.030000\n",
      "Train Epoch: 614 [188416/194182 (96%)]\tLoss: 0.432922\tGrad Norm: 1.483767\tLR: 0.030000\n",
      "Train set: Average loss: 0.4247\n",
      "Test set: Average loss: 0.2420, Average MAE: 0.3512\n",
      "Train Epoch: 615 [4096/194182 (2%)]\tLoss: 0.419138\tGrad Norm: 0.973605\tLR: 0.030000\n",
      "Train Epoch: 615 [24576/194182 (12%)]\tLoss: 0.423417\tGrad Norm: 0.749644\tLR: 0.030000\n",
      "Train Epoch: 615 [45056/194182 (23%)]\tLoss: 0.418241\tGrad Norm: 0.896409\tLR: 0.030000\n",
      "Train Epoch: 615 [65536/194182 (33%)]\tLoss: 0.420279\tGrad Norm: 1.285122\tLR: 0.030000\n",
      "Train Epoch: 615 [86016/194182 (44%)]\tLoss: 0.427625\tGrad Norm: 1.413488\tLR: 0.030000\n",
      "Train Epoch: 615 [106496/194182 (54%)]\tLoss: 0.427061\tGrad Norm: 1.307068\tLR: 0.030000\n",
      "Train Epoch: 615 [126976/194182 (65%)]\tLoss: 0.417857\tGrad Norm: 1.213786\tLR: 0.030000\n",
      "Train Epoch: 615 [147456/194182 (75%)]\tLoss: 0.424622\tGrad Norm: 1.136642\tLR: 0.030000\n",
      "Train Epoch: 615 [167936/194182 (85%)]\tLoss: 0.424604\tGrad Norm: 1.405429\tLR: 0.030000\n",
      "Train Epoch: 615 [188416/194182 (96%)]\tLoss: 0.428933\tGrad Norm: 1.071303\tLR: 0.030000\n",
      "Train set: Average loss: 0.4245\n",
      "Test set: Average loss: 0.2451, Average MAE: 0.3413\n",
      "Epoch 615: Mean reward = 0.050 +/- 0.046\n",
      "Train Epoch: 616 [4096/194182 (2%)]\tLoss: 0.423553\tGrad Norm: 1.229474\tLR: 0.030000\n",
      "Train Epoch: 616 [24576/194182 (12%)]\tLoss: 0.427278\tGrad Norm: 1.224852\tLR: 0.030000\n",
      "Train Epoch: 616 [45056/194182 (23%)]\tLoss: 0.418134\tGrad Norm: 1.142435\tLR: 0.030000\n",
      "Train Epoch: 616 [65536/194182 (33%)]\tLoss: 0.420947\tGrad Norm: 0.835019\tLR: 0.030000\n",
      "Train Epoch: 616 [86016/194182 (44%)]\tLoss: 0.413236\tGrad Norm: 0.967342\tLR: 0.030000\n",
      "Train Epoch: 616 [106496/194182 (54%)]\tLoss: 0.429060\tGrad Norm: 1.031079\tLR: 0.030000\n",
      "Train Epoch: 616 [126976/194182 (65%)]\tLoss: 0.426124\tGrad Norm: 1.164510\tLR: 0.030000\n",
      "Train Epoch: 616 [147456/194182 (75%)]\tLoss: 0.428074\tGrad Norm: 1.276480\tLR: 0.030000\n",
      "Train Epoch: 616 [167936/194182 (85%)]\tLoss: 0.428426\tGrad Norm: 1.362940\tLR: 0.030000\n",
      "Train Epoch: 616 [188416/194182 (96%)]\tLoss: 0.429172\tGrad Norm: 1.438165\tLR: 0.030000\n",
      "Train set: Average loss: 0.4239\n",
      "Test set: Average loss: 0.2475, Average MAE: 0.3572\n",
      "Train Epoch: 617 [4096/194182 (2%)]\tLoss: 0.432406\tGrad Norm: 1.404051\tLR: 0.030000\n",
      "Train Epoch: 617 [24576/194182 (12%)]\tLoss: 0.426298\tGrad Norm: 1.059621\tLR: 0.030000\n",
      "Train Epoch: 617 [45056/194182 (23%)]\tLoss: 0.419371\tGrad Norm: 0.789664\tLR: 0.030000\n",
      "Train Epoch: 617 [65536/194182 (33%)]\tLoss: 0.422541\tGrad Norm: 1.034139\tLR: 0.030000\n",
      "Train Epoch: 617 [86016/194182 (44%)]\tLoss: 0.422438\tGrad Norm: 1.119840\tLR: 0.030000\n",
      "Train Epoch: 617 [106496/194182 (54%)]\tLoss: 0.418720\tGrad Norm: 1.313973\tLR: 0.030000\n",
      "Train Epoch: 617 [126976/194182 (65%)]\tLoss: 0.419541\tGrad Norm: 1.155301\tLR: 0.030000\n",
      "Train Epoch: 617 [147456/194182 (75%)]\tLoss: 0.426570\tGrad Norm: 1.176785\tLR: 0.030000\n",
      "Train Epoch: 617 [167936/194182 (85%)]\tLoss: 0.424394\tGrad Norm: 1.297235\tLR: 0.030000\n",
      "Train Epoch: 617 [188416/194182 (96%)]\tLoss: 0.431029\tGrad Norm: 1.469881\tLR: 0.030000\n",
      "Train set: Average loss: 0.4238\n",
      "Test set: Average loss: 0.2527, Average MAE: 0.3695\n",
      "Train Epoch: 618 [4096/194182 (2%)]\tLoss: 0.421621\tGrad Norm: 1.519017\tLR: 0.030000\n",
      "Train Epoch: 618 [24576/194182 (12%)]\tLoss: 0.421366\tGrad Norm: 1.049213\tLR: 0.030000\n",
      "Train Epoch: 618 [45056/194182 (23%)]\tLoss: 0.423573\tGrad Norm: 1.146758\tLR: 0.030000\n",
      "Train Epoch: 618 [65536/194182 (33%)]\tLoss: 0.420596\tGrad Norm: 0.670999\tLR: 0.030000\n",
      "Train Epoch: 618 [86016/194182 (44%)]\tLoss: 0.425952\tGrad Norm: 1.039612\tLR: 0.030000\n",
      "Train Epoch: 618 [106496/194182 (54%)]\tLoss: 0.428217\tGrad Norm: 1.185202\tLR: 0.030000\n",
      "Train Epoch: 618 [126976/194182 (65%)]\tLoss: 0.418201\tGrad Norm: 1.154462\tLR: 0.030000\n",
      "Train Epoch: 618 [147456/194182 (75%)]\tLoss: 0.420310\tGrad Norm: 0.987904\tLR: 0.030000\n",
      "Train Epoch: 618 [167936/194182 (85%)]\tLoss: 0.420284\tGrad Norm: 1.238255\tLR: 0.030000\n",
      "Train Epoch: 618 [188416/194182 (96%)]\tLoss: 0.425145\tGrad Norm: 1.399738\tLR: 0.030000\n",
      "Train set: Average loss: 0.4224\n",
      "Test set: Average loss: 0.2455, Average MAE: 0.3332\n",
      "Train Epoch: 619 [4096/194182 (2%)]\tLoss: 0.423820\tGrad Norm: 1.338062\tLR: 0.030000\n",
      "Train Epoch: 619 [24576/194182 (12%)]\tLoss: 0.430235\tGrad Norm: 1.129615\tLR: 0.030000\n",
      "Train Epoch: 619 [45056/194182 (23%)]\tLoss: 0.429447\tGrad Norm: 1.378249\tLR: 0.030000\n",
      "Train Epoch: 619 [65536/194182 (33%)]\tLoss: 0.423374\tGrad Norm: 0.965507\tLR: 0.030000\n",
      "Train Epoch: 619 [86016/194182 (44%)]\tLoss: 0.413007\tGrad Norm: 0.660735\tLR: 0.030000\n",
      "Train Epoch: 619 [106496/194182 (54%)]\tLoss: 0.418030\tGrad Norm: 0.862978\tLR: 0.030000\n",
      "Train Epoch: 619 [126976/194182 (65%)]\tLoss: 0.420744\tGrad Norm: 1.168729\tLR: 0.030000\n",
      "Train Epoch: 619 [147456/194182 (75%)]\tLoss: 0.420815\tGrad Norm: 1.167307\tLR: 0.030000\n",
      "Train Epoch: 619 [167936/194182 (85%)]\tLoss: 0.427522\tGrad Norm: 1.509714\tLR: 0.030000\n",
      "Train Epoch: 619 [188416/194182 (96%)]\tLoss: 0.426622\tGrad Norm: 1.160808\tLR: 0.030000\n",
      "Train set: Average loss: 0.4233\n",
      "Test set: Average loss: 0.2476, Average MAE: 0.3529\n",
      "Train Epoch: 620 [4096/194182 (2%)]\tLoss: 0.425805\tGrad Norm: 1.316631\tLR: 0.030000\n",
      "Train Epoch: 620 [24576/194182 (12%)]\tLoss: 0.423783\tGrad Norm: 1.330399\tLR: 0.030000\n",
      "Train Epoch: 620 [45056/194182 (23%)]\tLoss: 0.430080\tGrad Norm: 1.665203\tLR: 0.030000\n",
      "Train Epoch: 620 [65536/194182 (33%)]\tLoss: 0.414957\tGrad Norm: 1.376537\tLR: 0.030000\n",
      "Train Epoch: 620 [86016/194182 (44%)]\tLoss: 0.418907\tGrad Norm: 1.032383\tLR: 0.030000\n",
      "Train Epoch: 620 [106496/194182 (54%)]\tLoss: 0.420340\tGrad Norm: 1.225278\tLR: 0.030000\n",
      "Train Epoch: 620 [126976/194182 (65%)]\tLoss: 0.424821\tGrad Norm: 1.487495\tLR: 0.030000\n",
      "Train Epoch: 620 [147456/194182 (75%)]\tLoss: 0.426103\tGrad Norm: 1.412515\tLR: 0.030000\n",
      "Train Epoch: 620 [167936/194182 (85%)]\tLoss: 0.424107\tGrad Norm: 1.595807\tLR: 0.030000\n",
      "Train Epoch: 620 [188416/194182 (96%)]\tLoss: 0.420405\tGrad Norm: 1.097049\tLR: 0.030000\n",
      "Train set: Average loss: 0.4250\n",
      "Test set: Average loss: 0.2443, Average MAE: 0.3485\n",
      "Epoch 620: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 621 [4096/194182 (2%)]\tLoss: 0.427231\tGrad Norm: 1.192974\tLR: 0.030000\n",
      "Train Epoch: 621 [24576/194182 (12%)]\tLoss: 0.426469\tGrad Norm: 1.250494\tLR: 0.030000\n",
      "Train Epoch: 621 [45056/194182 (23%)]\tLoss: 0.421824\tGrad Norm: 1.105098\tLR: 0.030000\n",
      "Train Epoch: 621 [65536/194182 (33%)]\tLoss: 0.422455\tGrad Norm: 1.228331\tLR: 0.030000\n",
      "Train Epoch: 621 [86016/194182 (44%)]\tLoss: 0.418932\tGrad Norm: 0.858980\tLR: 0.030000\n",
      "Train Epoch: 621 [106496/194182 (54%)]\tLoss: 0.419187\tGrad Norm: 1.141598\tLR: 0.030000\n",
      "Train Epoch: 621 [126976/194182 (65%)]\tLoss: 0.424147\tGrad Norm: 1.115673\tLR: 0.030000\n",
      "Train Epoch: 621 [147456/194182 (75%)]\tLoss: 0.419843\tGrad Norm: 1.015498\tLR: 0.030000\n",
      "Train Epoch: 621 [167936/194182 (85%)]\tLoss: 0.423304\tGrad Norm: 0.901351\tLR: 0.030000\n",
      "Train Epoch: 621 [188416/194182 (96%)]\tLoss: 0.419052\tGrad Norm: 0.976861\tLR: 0.030000\n",
      "Train set: Average loss: 0.4213\n",
      "Test set: Average loss: 0.2426, Average MAE: 0.3455\n",
      "Train Epoch: 622 [4096/194182 (2%)]\tLoss: 0.422080\tGrad Norm: 1.082217\tLR: 0.030000\n",
      "Train Epoch: 622 [24576/194182 (12%)]\tLoss: 0.424036\tGrad Norm: 1.276340\tLR: 0.030000\n",
      "Train Epoch: 622 [45056/194182 (23%)]\tLoss: 0.416877\tGrad Norm: 1.176222\tLR: 0.030000\n",
      "Train Epoch: 622 [65536/194182 (33%)]\tLoss: 0.415387\tGrad Norm: 1.104791\tLR: 0.030000\n",
      "Train Epoch: 622 [86016/194182 (44%)]\tLoss: 0.419971\tGrad Norm: 0.923653\tLR: 0.030000\n",
      "Train Epoch: 622 [106496/194182 (54%)]\tLoss: 0.424128\tGrad Norm: 0.865051\tLR: 0.030000\n",
      "Train Epoch: 622 [126976/194182 (65%)]\tLoss: 0.415224\tGrad Norm: 0.725060\tLR: 0.030000\n",
      "Train Epoch: 622 [147456/194182 (75%)]\tLoss: 0.421311\tGrad Norm: 1.236657\tLR: 0.030000\n",
      "Train Epoch: 622 [167936/194182 (85%)]\tLoss: 0.417813\tGrad Norm: 1.067072\tLR: 0.030000\n",
      "Train Epoch: 622 [188416/194182 (96%)]\tLoss: 0.426479\tGrad Norm: 1.069519\tLR: 0.030000\n",
      "Train set: Average loss: 0.4204\n",
      "Test set: Average loss: 0.2463, Average MAE: 0.3559\n",
      "Train Epoch: 623 [4096/194182 (2%)]\tLoss: 0.431437\tGrad Norm: 1.214929\tLR: 0.030000\n",
      "Train Epoch: 623 [24576/194182 (12%)]\tLoss: 0.421128\tGrad Norm: 1.126353\tLR: 0.030000\n",
      "Train Epoch: 623 [45056/194182 (23%)]\tLoss: 0.410220\tGrad Norm: 1.074571\tLR: 0.030000\n",
      "Train Epoch: 623 [65536/194182 (33%)]\tLoss: 0.426788\tGrad Norm: 1.680173\tLR: 0.030000\n",
      "Train Epoch: 623 [86016/194182 (44%)]\tLoss: 0.419343\tGrad Norm: 1.393277\tLR: 0.030000\n",
      "Train Epoch: 623 [106496/194182 (54%)]\tLoss: 0.431345\tGrad Norm: 1.482083\tLR: 0.030000\n",
      "Train Epoch: 623 [126976/194182 (65%)]\tLoss: 0.422759\tGrad Norm: 1.279299\tLR: 0.030000\n",
      "Train Epoch: 623 [147456/194182 (75%)]\tLoss: 0.424943\tGrad Norm: 0.997483\tLR: 0.030000\n",
      "Train Epoch: 623 [167936/194182 (85%)]\tLoss: 0.418166\tGrad Norm: 0.875194\tLR: 0.030000\n",
      "Train Epoch: 623 [188416/194182 (96%)]\tLoss: 0.417723\tGrad Norm: 0.968476\tLR: 0.030000\n",
      "Train set: Average loss: 0.4220\n",
      "Test set: Average loss: 0.2457, Average MAE: 0.3541\n",
      "Train Epoch: 624 [4096/194182 (2%)]\tLoss: 0.422995\tGrad Norm: 1.162660\tLR: 0.030000\n",
      "Train Epoch: 624 [24576/194182 (12%)]\tLoss: 0.412780\tGrad Norm: 1.022699\tLR: 0.030000\n",
      "Train Epoch: 624 [45056/194182 (23%)]\tLoss: 0.422476\tGrad Norm: 1.028683\tLR: 0.030000\n",
      "Train Epoch: 624 [65536/194182 (33%)]\tLoss: 0.425527\tGrad Norm: 1.049328\tLR: 0.030000\n",
      "Train Epoch: 624 [86016/194182 (44%)]\tLoss: 0.415795\tGrad Norm: 1.189918\tLR: 0.030000\n",
      "Train Epoch: 624 [106496/194182 (54%)]\tLoss: 0.423473\tGrad Norm: 1.527867\tLR: 0.030000\n",
      "Train Epoch: 624 [126976/194182 (65%)]\tLoss: 0.417558\tGrad Norm: 1.161597\tLR: 0.030000\n",
      "Train Epoch: 624 [147456/194182 (75%)]\tLoss: 0.416722\tGrad Norm: 1.047519\tLR: 0.030000\n",
      "Train Epoch: 624 [167936/194182 (85%)]\tLoss: 0.426435\tGrad Norm: 1.111069\tLR: 0.030000\n",
      "Train Epoch: 624 [188416/194182 (96%)]\tLoss: 0.431603\tGrad Norm: 1.249516\tLR: 0.030000\n",
      "Train set: Average loss: 0.4213\n",
      "Test set: Average loss: 0.2437, Average MAE: 0.3387\n",
      "Train Epoch: 625 [4096/194182 (2%)]\tLoss: 0.418432\tGrad Norm: 1.133469\tLR: 0.030000\n",
      "Train Epoch: 625 [24576/194182 (12%)]\tLoss: 0.414487\tGrad Norm: 0.845644\tLR: 0.030000\n",
      "Train Epoch: 625 [45056/194182 (23%)]\tLoss: 0.416293\tGrad Norm: 0.955339\tLR: 0.030000\n",
      "Train Epoch: 625 [65536/194182 (33%)]\tLoss: 0.407824\tGrad Norm: 0.763773\tLR: 0.030000\n",
      "Train Epoch: 625 [86016/194182 (44%)]\tLoss: 0.422291\tGrad Norm: 1.068905\tLR: 0.030000\n",
      "Train Epoch: 625 [106496/194182 (54%)]\tLoss: 0.422021\tGrad Norm: 1.243919\tLR: 0.030000\n",
      "Train Epoch: 625 [126976/194182 (65%)]\tLoss: 0.419445\tGrad Norm: 1.590068\tLR: 0.030000\n",
      "Train Epoch: 625 [147456/194182 (75%)]\tLoss: 0.423914\tGrad Norm: 1.098977\tLR: 0.030000\n",
      "Train Epoch: 625 [167936/194182 (85%)]\tLoss: 0.427523\tGrad Norm: 1.237319\tLR: 0.030000\n",
      "Train Epoch: 625 [188416/194182 (96%)]\tLoss: 0.417215\tGrad Norm: 1.148122\tLR: 0.030000\n",
      "Train set: Average loss: 0.4196\n",
      "Test set: Average loss: 0.2422, Average MAE: 0.3485\n",
      "Epoch 625: Mean reward = 0.047 +/- 0.039\n",
      "Train Epoch: 626 [4096/194182 (2%)]\tLoss: 0.419874\tGrad Norm: 1.153113\tLR: 0.030000\n",
      "Train Epoch: 626 [24576/194182 (12%)]\tLoss: 0.430696\tGrad Norm: 1.149047\tLR: 0.030000\n",
      "Train Epoch: 626 [45056/194182 (23%)]\tLoss: 0.410861\tGrad Norm: 1.014254\tLR: 0.030000\n",
      "Train Epoch: 626 [65536/194182 (33%)]\tLoss: 0.422881\tGrad Norm: 1.180522\tLR: 0.030000\n",
      "Train Epoch: 626 [86016/194182 (44%)]\tLoss: 0.416779\tGrad Norm: 1.351587\tLR: 0.030000\n",
      "Train Epoch: 626 [106496/194182 (54%)]\tLoss: 0.414084\tGrad Norm: 0.992661\tLR: 0.030000\n",
      "Train Epoch: 626 [126976/194182 (65%)]\tLoss: 0.417687\tGrad Norm: 1.208678\tLR: 0.030000\n",
      "Train Epoch: 626 [147456/194182 (75%)]\tLoss: 0.419196\tGrad Norm: 1.131906\tLR: 0.030000\n",
      "Train Epoch: 626 [167936/194182 (85%)]\tLoss: 0.418597\tGrad Norm: 1.109116\tLR: 0.030000\n",
      "Train Epoch: 626 [188416/194182 (96%)]\tLoss: 0.420660\tGrad Norm: 1.126985\tLR: 0.030000\n",
      "Train set: Average loss: 0.4201\n",
      "Test set: Average loss: 0.2461, Average MAE: 0.3571\n",
      "Train Epoch: 627 [4096/194182 (2%)]\tLoss: 0.420860\tGrad Norm: 1.470067\tLR: 0.030000\n",
      "Train Epoch: 627 [24576/194182 (12%)]\tLoss: 0.424624\tGrad Norm: 1.280097\tLR: 0.030000\n",
      "Train Epoch: 627 [45056/194182 (23%)]\tLoss: 0.420210\tGrad Norm: 0.783391\tLR: 0.030000\n",
      "Train Epoch: 627 [65536/194182 (33%)]\tLoss: 0.421164\tGrad Norm: 1.199984\tLR: 0.030000\n",
      "Train Epoch: 627 [86016/194182 (44%)]\tLoss: 0.420304\tGrad Norm: 1.435560\tLR: 0.030000\n",
      "Train Epoch: 627 [106496/194182 (54%)]\tLoss: 0.425412\tGrad Norm: 1.149253\tLR: 0.030000\n",
      "Train Epoch: 627 [126976/194182 (65%)]\tLoss: 0.422422\tGrad Norm: 1.252070\tLR: 0.030000\n",
      "Train Epoch: 627 [147456/194182 (75%)]\tLoss: 0.422002\tGrad Norm: 0.862651\tLR: 0.030000\n",
      "Train Epoch: 627 [167936/194182 (85%)]\tLoss: 0.409345\tGrad Norm: 0.708583\tLR: 0.030000\n",
      "Train Epoch: 627 [188416/194182 (96%)]\tLoss: 0.415660\tGrad Norm: 1.203062\tLR: 0.030000\n",
      "Train set: Average loss: 0.4191\n",
      "Test set: Average loss: 0.2445, Average MAE: 0.3342\n",
      "Train Epoch: 628 [4096/194182 (2%)]\tLoss: 0.418203\tGrad Norm: 1.367866\tLR: 0.030000\n",
      "Train Epoch: 628 [24576/194182 (12%)]\tLoss: 0.422273\tGrad Norm: 1.060759\tLR: 0.030000\n",
      "Train Epoch: 628 [45056/194182 (23%)]\tLoss: 0.417021\tGrad Norm: 1.295138\tLR: 0.030000\n",
      "Train Epoch: 628 [65536/194182 (33%)]\tLoss: 0.413278\tGrad Norm: 1.243475\tLR: 0.030000\n",
      "Train Epoch: 628 [86016/194182 (44%)]\tLoss: 0.417665\tGrad Norm: 1.425966\tLR: 0.030000\n",
      "Train Epoch: 628 [106496/194182 (54%)]\tLoss: 0.425034\tGrad Norm: 1.455562\tLR: 0.030000\n",
      "Train Epoch: 628 [126976/194182 (65%)]\tLoss: 0.420008\tGrad Norm: 1.207853\tLR: 0.030000\n",
      "Train Epoch: 628 [147456/194182 (75%)]\tLoss: 0.414150\tGrad Norm: 1.101028\tLR: 0.030000\n",
      "Train Epoch: 628 [167936/194182 (85%)]\tLoss: 0.430050\tGrad Norm: 1.351738\tLR: 0.030000\n",
      "Train Epoch: 628 [188416/194182 (96%)]\tLoss: 0.418158\tGrad Norm: 1.176057\tLR: 0.030000\n",
      "Train set: Average loss: 0.4206\n",
      "Test set: Average loss: 0.2504, Average MAE: 0.3390\n",
      "Train Epoch: 629 [4096/194182 (2%)]\tLoss: 0.430727\tGrad Norm: 2.087835\tLR: 0.030000\n",
      "Train Epoch: 629 [24576/194182 (12%)]\tLoss: 0.420121\tGrad Norm: 1.345526\tLR: 0.030000\n",
      "Train Epoch: 629 [45056/194182 (23%)]\tLoss: 0.421901\tGrad Norm: 0.834749\tLR: 0.030000\n",
      "Train Epoch: 629 [65536/194182 (33%)]\tLoss: 0.418770\tGrad Norm: 0.691930\tLR: 0.030000\n",
      "Train Epoch: 629 [86016/194182 (44%)]\tLoss: 0.416266\tGrad Norm: 1.061490\tLR: 0.030000\n",
      "Train Epoch: 629 [106496/194182 (54%)]\tLoss: 0.424148\tGrad Norm: 1.300937\tLR: 0.030000\n",
      "Train Epoch: 629 [126976/194182 (65%)]\tLoss: 0.425586\tGrad Norm: 1.309663\tLR: 0.030000\n",
      "Train Epoch: 629 [147456/194182 (75%)]\tLoss: 0.425151\tGrad Norm: 1.383949\tLR: 0.030000\n",
      "Train Epoch: 629 [167936/194182 (85%)]\tLoss: 0.421791\tGrad Norm: 1.321231\tLR: 0.030000\n",
      "Train Epoch: 629 [188416/194182 (96%)]\tLoss: 0.411415\tGrad Norm: 1.197340\tLR: 0.030000\n",
      "Train set: Average loss: 0.4205\n",
      "Test set: Average loss: 0.2454, Average MAE: 0.3538\n",
      "Train Epoch: 630 [4096/194182 (2%)]\tLoss: 0.426574\tGrad Norm: 1.208900\tLR: 0.030000\n",
      "Train Epoch: 630 [24576/194182 (12%)]\tLoss: 0.420139\tGrad Norm: 0.892986\tLR: 0.030000\n",
      "Train Epoch: 630 [45056/194182 (23%)]\tLoss: 0.416107\tGrad Norm: 1.188843\tLR: 0.030000\n",
      "Train Epoch: 630 [65536/194182 (33%)]\tLoss: 0.417009\tGrad Norm: 1.211744\tLR: 0.030000\n",
      "Train Epoch: 630 [86016/194182 (44%)]\tLoss: 0.407365\tGrad Norm: 0.995374\tLR: 0.030000\n",
      "Train Epoch: 630 [106496/194182 (54%)]\tLoss: 0.423380\tGrad Norm: 1.204342\tLR: 0.030000\n",
      "Train Epoch: 630 [126976/194182 (65%)]\tLoss: 0.419799\tGrad Norm: 1.052260\tLR: 0.030000\n",
      "Train Epoch: 630 [147456/194182 (75%)]\tLoss: 0.415117\tGrad Norm: 1.467888\tLR: 0.030000\n",
      "Train Epoch: 630 [167936/194182 (85%)]\tLoss: 0.424013\tGrad Norm: 1.127465\tLR: 0.030000\n",
      "Train Epoch: 630 [188416/194182 (96%)]\tLoss: 0.423121\tGrad Norm: 1.185389\tLR: 0.030000\n",
      "Train set: Average loss: 0.4184\n",
      "Test set: Average loss: 0.2455, Average MAE: 0.3586\n",
      "Epoch 630: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 631 [4096/194182 (2%)]\tLoss: 0.428111\tGrad Norm: 1.108930\tLR: 0.030000\n",
      "Train Epoch: 631 [24576/194182 (12%)]\tLoss: 0.410850\tGrad Norm: 0.999153\tLR: 0.030000\n",
      "Train Epoch: 631 [45056/194182 (23%)]\tLoss: 0.412348\tGrad Norm: 1.239141\tLR: 0.030000\n",
      "Train Epoch: 631 [65536/194182 (33%)]\tLoss: 0.420234\tGrad Norm: 1.105539\tLR: 0.030000\n",
      "Train Epoch: 631 [86016/194182 (44%)]\tLoss: 0.411808\tGrad Norm: 0.794456\tLR: 0.030000\n",
      "Train Epoch: 631 [106496/194182 (54%)]\tLoss: 0.419429\tGrad Norm: 0.944034\tLR: 0.030000\n",
      "Train Epoch: 631 [126976/194182 (65%)]\tLoss: 0.422137\tGrad Norm: 1.257849\tLR: 0.030000\n",
      "Train Epoch: 631 [147456/194182 (75%)]\tLoss: 0.420435\tGrad Norm: 1.478477\tLR: 0.030000\n",
      "Train Epoch: 631 [167936/194182 (85%)]\tLoss: 0.408680\tGrad Norm: 1.152137\tLR: 0.030000\n",
      "Train Epoch: 631 [188416/194182 (96%)]\tLoss: 0.414640\tGrad Norm: 0.992486\tLR: 0.030000\n",
      "Train set: Average loss: 0.4171\n",
      "Test set: Average loss: 0.2415, Average MAE: 0.3388\n",
      "Train Epoch: 632 [4096/194182 (2%)]\tLoss: 0.417631\tGrad Norm: 0.884769\tLR: 0.030000\n",
      "Train Epoch: 632 [24576/194182 (12%)]\tLoss: 0.414300\tGrad Norm: 1.126529\tLR: 0.030000\n",
      "Train Epoch: 632 [45056/194182 (23%)]\tLoss: 0.421060\tGrad Norm: 1.174609\tLR: 0.030000\n",
      "Train Epoch: 632 [65536/194182 (33%)]\tLoss: 0.418634\tGrad Norm: 1.000058\tLR: 0.030000\n",
      "Train Epoch: 632 [86016/194182 (44%)]\tLoss: 0.428730\tGrad Norm: 1.303332\tLR: 0.030000\n",
      "Train Epoch: 632 [106496/194182 (54%)]\tLoss: 0.420830\tGrad Norm: 1.137787\tLR: 0.030000\n",
      "Train Epoch: 632 [126976/194182 (65%)]\tLoss: 0.417929\tGrad Norm: 0.856958\tLR: 0.030000\n",
      "Train Epoch: 632 [147456/194182 (75%)]\tLoss: 0.424135\tGrad Norm: 1.183659\tLR: 0.030000\n",
      "Train Epoch: 632 [167936/194182 (85%)]\tLoss: 0.419894\tGrad Norm: 1.227156\tLR: 0.030000\n",
      "Train Epoch: 632 [188416/194182 (96%)]\tLoss: 0.424925\tGrad Norm: 1.585192\tLR: 0.030000\n",
      "Train set: Average loss: 0.4181\n",
      "Test set: Average loss: 0.2476, Average MAE: 0.3340\n",
      "Train Epoch: 633 [4096/194182 (2%)]\tLoss: 0.423783\tGrad Norm: 1.496835\tLR: 0.030000\n",
      "Train Epoch: 633 [24576/194182 (12%)]\tLoss: 0.422620\tGrad Norm: 1.294990\tLR: 0.030000\n",
      "Train Epoch: 633 [45056/194182 (23%)]\tLoss: 0.419247\tGrad Norm: 1.360279\tLR: 0.030000\n",
      "Train Epoch: 633 [65536/194182 (33%)]\tLoss: 0.413840\tGrad Norm: 0.990299\tLR: 0.030000\n",
      "Train Epoch: 633 [86016/194182 (44%)]\tLoss: 0.415438\tGrad Norm: 1.152060\tLR: 0.030000\n",
      "Train Epoch: 633 [106496/194182 (54%)]\tLoss: 0.417085\tGrad Norm: 1.043443\tLR: 0.030000\n",
      "Train Epoch: 633 [126976/194182 (65%)]\tLoss: 0.417186\tGrad Norm: 0.864699\tLR: 0.030000\n",
      "Train Epoch: 633 [147456/194182 (75%)]\tLoss: 0.412841\tGrad Norm: 1.069199\tLR: 0.030000\n",
      "Train Epoch: 633 [167936/194182 (85%)]\tLoss: 0.426618\tGrad Norm: 1.351303\tLR: 0.030000\n",
      "Train Epoch: 633 [188416/194182 (96%)]\tLoss: 0.418305\tGrad Norm: 0.990604\tLR: 0.030000\n",
      "Train set: Average loss: 0.4178\n",
      "Test set: Average loss: 0.2406, Average MAE: 0.3468\n",
      "Train Epoch: 634 [4096/194182 (2%)]\tLoss: 0.419557\tGrad Norm: 0.926515\tLR: 0.030000\n",
      "Train Epoch: 634 [24576/194182 (12%)]\tLoss: 0.416891\tGrad Norm: 0.908372\tLR: 0.030000\n",
      "Train Epoch: 634 [45056/194182 (23%)]\tLoss: 0.419073\tGrad Norm: 1.214356\tLR: 0.030000\n",
      "Train Epoch: 634 [65536/194182 (33%)]\tLoss: 0.426164\tGrad Norm: 1.536669\tLR: 0.030000\n",
      "Train Epoch: 634 [86016/194182 (44%)]\tLoss: 0.418079\tGrad Norm: 1.504336\tLR: 0.030000\n",
      "Train Epoch: 634 [106496/194182 (54%)]\tLoss: 0.420124\tGrad Norm: 1.153290\tLR: 0.030000\n",
      "Train Epoch: 634 [126976/194182 (65%)]\tLoss: 0.416970\tGrad Norm: 1.086442\tLR: 0.030000\n",
      "Train Epoch: 634 [147456/194182 (75%)]\tLoss: 0.417583\tGrad Norm: 1.630419\tLR: 0.030000\n",
      "Train Epoch: 634 [167936/194182 (85%)]\tLoss: 0.425703\tGrad Norm: 1.552677\tLR: 0.030000\n",
      "Train Epoch: 634 [188416/194182 (96%)]\tLoss: 0.426703\tGrad Norm: 1.369316\tLR: 0.030000\n",
      "Train set: Average loss: 0.4195\n",
      "Test set: Average loss: 0.2524, Average MAE: 0.3653\n",
      "Train Epoch: 635 [4096/194182 (2%)]\tLoss: 0.430657\tGrad Norm: 1.592641\tLR: 0.030000\n",
      "Train Epoch: 635 [24576/194182 (12%)]\tLoss: 0.417268\tGrad Norm: 1.152698\tLR: 0.030000\n",
      "Train Epoch: 635 [45056/194182 (23%)]\tLoss: 0.414231\tGrad Norm: 1.411290\tLR: 0.030000\n",
      "Train Epoch: 635 [65536/194182 (33%)]\tLoss: 0.416066\tGrad Norm: 1.150737\tLR: 0.030000\n",
      "Train Epoch: 635 [86016/194182 (44%)]\tLoss: 0.418273\tGrad Norm: 0.915817\tLR: 0.030000\n",
      "Train Epoch: 635 [106496/194182 (54%)]\tLoss: 0.411157\tGrad Norm: 0.871721\tLR: 0.030000\n",
      "Train Epoch: 635 [126976/194182 (65%)]\tLoss: 0.415951\tGrad Norm: 1.000691\tLR: 0.030000\n",
      "Train Epoch: 635 [147456/194182 (75%)]\tLoss: 0.421139\tGrad Norm: 1.153333\tLR: 0.030000\n",
      "Train Epoch: 635 [167936/194182 (85%)]\tLoss: 0.415188\tGrad Norm: 1.241323\tLR: 0.030000\n",
      "Train Epoch: 635 [188416/194182 (96%)]\tLoss: 0.417188\tGrad Norm: 1.201995\tLR: 0.030000\n",
      "Train set: Average loss: 0.4170\n",
      "Test set: Average loss: 0.2487, Average MAE: 0.3581\n",
      "Epoch 635: Mean reward = 0.061 +/- 0.056\n",
      "Train Epoch: 636 [4096/194182 (2%)]\tLoss: 0.418865\tGrad Norm: 1.434601\tLR: 0.030000\n",
      "Train Epoch: 636 [24576/194182 (12%)]\tLoss: 0.419716\tGrad Norm: 1.134153\tLR: 0.030000\n",
      "Train Epoch: 636 [45056/194182 (23%)]\tLoss: 0.414956\tGrad Norm: 1.005533\tLR: 0.030000\n",
      "Train Epoch: 636 [65536/194182 (33%)]\tLoss: 0.420196\tGrad Norm: 1.220927\tLR: 0.030000\n",
      "Train Epoch: 636 [86016/194182 (44%)]\tLoss: 0.412917\tGrad Norm: 1.492510\tLR: 0.030000\n",
      "Train Epoch: 636 [106496/194182 (54%)]\tLoss: 0.419051\tGrad Norm: 1.413354\tLR: 0.030000\n",
      "Train Epoch: 636 [126976/194182 (65%)]\tLoss: 0.421273\tGrad Norm: 1.447112\tLR: 0.030000\n",
      "Train Epoch: 636 [147456/194182 (75%)]\tLoss: 0.418551\tGrad Norm: 1.430740\tLR: 0.030000\n",
      "Train Epoch: 636 [167936/194182 (85%)]\tLoss: 0.414646\tGrad Norm: 1.282665\tLR: 0.030000\n",
      "Train Epoch: 636 [188416/194182 (96%)]\tLoss: 0.419066\tGrad Norm: 1.007494\tLR: 0.030000\n",
      "Train set: Average loss: 0.4182\n",
      "Test set: Average loss: 0.2387, Average MAE: 0.3350\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 637 [4096/194182 (2%)]\tLoss: 0.422789\tGrad Norm: 0.850161\tLR: 0.030000\n",
      "Train Epoch: 637 [24576/194182 (12%)]\tLoss: 0.413365\tGrad Norm: 1.202060\tLR: 0.030000\n",
      "Train Epoch: 637 [45056/194182 (23%)]\tLoss: 0.415490\tGrad Norm: 1.207665\tLR: 0.030000\n",
      "Train Epoch: 637 [65536/194182 (33%)]\tLoss: 0.419230\tGrad Norm: 0.921105\tLR: 0.030000\n",
      "Train Epoch: 637 [86016/194182 (44%)]\tLoss: 0.413613\tGrad Norm: 1.013619\tLR: 0.030000\n",
      "Train Epoch: 637 [106496/194182 (54%)]\tLoss: 0.419979\tGrad Norm: 1.254203\tLR: 0.030000\n",
      "Train Epoch: 637 [126976/194182 (65%)]\tLoss: 0.415584\tGrad Norm: 1.079260\tLR: 0.030000\n",
      "Train Epoch: 637 [147456/194182 (75%)]\tLoss: 0.417988\tGrad Norm: 1.124568\tLR: 0.030000\n",
      "Train Epoch: 637 [167936/194182 (85%)]\tLoss: 0.417832\tGrad Norm: 1.294732\tLR: 0.030000\n",
      "Train Epoch: 637 [188416/194182 (96%)]\tLoss: 0.419766\tGrad Norm: 1.181431\tLR: 0.030000\n",
      "Train set: Average loss: 0.4159\n",
      "Test set: Average loss: 0.2508, Average MAE: 0.3384\n",
      "Train Epoch: 638 [4096/194182 (2%)]\tLoss: 0.424191\tGrad Norm: 1.611052\tLR: 0.030000\n",
      "Train Epoch: 638 [24576/194182 (12%)]\tLoss: 0.414935\tGrad Norm: 1.126430\tLR: 0.030000\n",
      "Train Epoch: 638 [45056/194182 (23%)]\tLoss: 0.412826\tGrad Norm: 1.398293\tLR: 0.030000\n",
      "Train Epoch: 638 [65536/194182 (33%)]\tLoss: 0.408388\tGrad Norm: 1.194065\tLR: 0.030000\n",
      "Train Epoch: 638 [86016/194182 (44%)]\tLoss: 0.423471\tGrad Norm: 1.344373\tLR: 0.030000\n",
      "Train Epoch: 638 [106496/194182 (54%)]\tLoss: 0.408952\tGrad Norm: 0.992529\tLR: 0.030000\n",
      "Train Epoch: 638 [126976/194182 (65%)]\tLoss: 0.415308\tGrad Norm: 1.033586\tLR: 0.030000\n",
      "Train Epoch: 638 [147456/194182 (75%)]\tLoss: 0.420531\tGrad Norm: 1.265890\tLR: 0.030000\n",
      "Train Epoch: 638 [167936/194182 (85%)]\tLoss: 0.415482\tGrad Norm: 1.494985\tLR: 0.030000\n",
      "Train Epoch: 638 [188416/194182 (96%)]\tLoss: 0.425952\tGrad Norm: 1.579926\tLR: 0.030000\n",
      "Train set: Average loss: 0.4186\n",
      "Test set: Average loss: 0.2473, Average MAE: 0.3613\n",
      "Train Epoch: 639 [4096/194182 (2%)]\tLoss: 0.419927\tGrad Norm: 1.256653\tLR: 0.030000\n",
      "Train Epoch: 639 [24576/194182 (12%)]\tLoss: 0.411558\tGrad Norm: 0.998107\tLR: 0.030000\n",
      "Train Epoch: 639 [45056/194182 (23%)]\tLoss: 0.422824\tGrad Norm: 1.066675\tLR: 0.030000\n",
      "Train Epoch: 639 [65536/194182 (33%)]\tLoss: 0.423330\tGrad Norm: 1.051763\tLR: 0.030000\n",
      "Train Epoch: 639 [86016/194182 (44%)]\tLoss: 0.415053\tGrad Norm: 0.958717\tLR: 0.030000\n",
      "Train Epoch: 639 [106496/194182 (54%)]\tLoss: 0.409099\tGrad Norm: 1.190068\tLR: 0.030000\n",
      "Train Epoch: 639 [126976/194182 (65%)]\tLoss: 0.410557\tGrad Norm: 1.240633\tLR: 0.030000\n",
      "Train Epoch: 639 [147456/194182 (75%)]\tLoss: 0.410204\tGrad Norm: 1.243725\tLR: 0.030000\n",
      "Train Epoch: 639 [167936/194182 (85%)]\tLoss: 0.419849\tGrad Norm: 1.205869\tLR: 0.030000\n",
      "Train Epoch: 639 [188416/194182 (96%)]\tLoss: 0.420946\tGrad Norm: 1.395628\tLR: 0.030000\n",
      "Train set: Average loss: 0.4159\n",
      "Test set: Average loss: 0.2419, Average MAE: 0.3349\n",
      "Train Epoch: 640 [4096/194182 (2%)]\tLoss: 0.404991\tGrad Norm: 0.976380\tLR: 0.030000\n",
      "Train Epoch: 640 [24576/194182 (12%)]\tLoss: 0.414111\tGrad Norm: 0.886542\tLR: 0.030000\n",
      "Train Epoch: 640 [45056/194182 (23%)]\tLoss: 0.416246\tGrad Norm: 0.883873\tLR: 0.030000\n",
      "Train Epoch: 640 [65536/194182 (33%)]\tLoss: 0.414078\tGrad Norm: 1.134141\tLR: 0.030000\n",
      "Train Epoch: 640 [86016/194182 (44%)]\tLoss: 0.426670\tGrad Norm: 1.664559\tLR: 0.030000\n",
      "Train Epoch: 640 [106496/194182 (54%)]\tLoss: 0.418428\tGrad Norm: 1.144287\tLR: 0.030000\n",
      "Train Epoch: 640 [126976/194182 (65%)]\tLoss: 0.423096\tGrad Norm: 1.285591\tLR: 0.030000\n",
      "Train Epoch: 640 [147456/194182 (75%)]\tLoss: 0.404177\tGrad Norm: 0.914639\tLR: 0.030000\n",
      "Train Epoch: 640 [167936/194182 (85%)]\tLoss: 0.414973\tGrad Norm: 1.231530\tLR: 0.030000\n",
      "Train Epoch: 640 [188416/194182 (96%)]\tLoss: 0.419648\tGrad Norm: 1.278226\tLR: 0.030000\n",
      "Train set: Average loss: 0.4151\n",
      "Test set: Average loss: 0.2433, Average MAE: 0.3387\n",
      "Epoch 640: Mean reward = 0.049 +/- 0.043\n",
      "Train Epoch: 641 [4096/194182 (2%)]\tLoss: 0.411395\tGrad Norm: 1.117417\tLR: 0.030000\n",
      "Train Epoch: 641 [24576/194182 (12%)]\tLoss: 0.415091\tGrad Norm: 1.061704\tLR: 0.030000\n",
      "Train Epoch: 641 [45056/194182 (23%)]\tLoss: 0.411587\tGrad Norm: 1.324211\tLR: 0.030000\n",
      "Train Epoch: 641 [65536/194182 (33%)]\tLoss: 0.409996\tGrad Norm: 1.077330\tLR: 0.030000\n",
      "Train Epoch: 641 [86016/194182 (44%)]\tLoss: 0.417984\tGrad Norm: 1.605787\tLR: 0.030000\n",
      "Train Epoch: 641 [106496/194182 (54%)]\tLoss: 0.425315\tGrad Norm: 1.540970\tLR: 0.030000\n",
      "Train Epoch: 641 [126976/194182 (65%)]\tLoss: 0.412132\tGrad Norm: 1.035380\tLR: 0.030000\n",
      "Train Epoch: 641 [147456/194182 (75%)]\tLoss: 0.406596\tGrad Norm: 0.965839\tLR: 0.030000\n",
      "Train Epoch: 641 [167936/194182 (85%)]\tLoss: 0.414733\tGrad Norm: 1.061967\tLR: 0.030000\n",
      "Train Epoch: 641 [188416/194182 (96%)]\tLoss: 0.408637\tGrad Norm: 0.974838\tLR: 0.030000\n",
      "Train set: Average loss: 0.4156\n",
      "Test set: Average loss: 0.2421, Average MAE: 0.3500\n",
      "Train Epoch: 642 [4096/194182 (2%)]\tLoss: 0.411306\tGrad Norm: 0.980503\tLR: 0.030000\n",
      "Train Epoch: 642 [24576/194182 (12%)]\tLoss: 0.414479\tGrad Norm: 0.782077\tLR: 0.030000\n",
      "Train Epoch: 642 [45056/194182 (23%)]\tLoss: 0.419234\tGrad Norm: 0.797666\tLR: 0.030000\n",
      "Train Epoch: 642 [65536/194182 (33%)]\tLoss: 0.416284\tGrad Norm: 0.976909\tLR: 0.030000\n",
      "Train Epoch: 642 [86016/194182 (44%)]\tLoss: 0.415451\tGrad Norm: 1.353362\tLR: 0.030000\n",
      "Train Epoch: 642 [106496/194182 (54%)]\tLoss: 0.405937\tGrad Norm: 1.228408\tLR: 0.030000\n",
      "Train Epoch: 642 [126976/194182 (65%)]\tLoss: 0.413490\tGrad Norm: 1.043714\tLR: 0.030000\n",
      "Train Epoch: 642 [147456/194182 (75%)]\tLoss: 0.424590\tGrad Norm: 1.778191\tLR: 0.030000\n",
      "Train Epoch: 642 [167936/194182 (85%)]\tLoss: 0.414483\tGrad Norm: 1.296677\tLR: 0.030000\n",
      "Train Epoch: 642 [188416/194182 (96%)]\tLoss: 0.411745\tGrad Norm: 1.239852\tLR: 0.030000\n",
      "Train set: Average loss: 0.4148\n",
      "Test set: Average loss: 0.2413, Average MAE: 0.3518\n",
      "Train Epoch: 643 [4096/194182 (2%)]\tLoss: 0.408344\tGrad Norm: 1.012767\tLR: 0.030000\n",
      "Train Epoch: 643 [24576/194182 (12%)]\tLoss: 0.412250\tGrad Norm: 1.214732\tLR: 0.030000\n",
      "Train Epoch: 643 [45056/194182 (23%)]\tLoss: 0.409124\tGrad Norm: 1.109524\tLR: 0.030000\n",
      "Train Epoch: 643 [65536/194182 (33%)]\tLoss: 0.416861\tGrad Norm: 1.080207\tLR: 0.030000\n",
      "Train Epoch: 643 [86016/194182 (44%)]\tLoss: 0.406990\tGrad Norm: 0.841179\tLR: 0.030000\n",
      "Train Epoch: 643 [106496/194182 (54%)]\tLoss: 0.421210\tGrad Norm: 1.541642\tLR: 0.030000\n",
      "Train Epoch: 643 [126976/194182 (65%)]\tLoss: 0.421602\tGrad Norm: 1.367682\tLR: 0.030000\n",
      "Train Epoch: 643 [147456/194182 (75%)]\tLoss: 0.412432\tGrad Norm: 1.262302\tLR: 0.030000\n",
      "Train Epoch: 643 [167936/194182 (85%)]\tLoss: 0.418517\tGrad Norm: 1.177328\tLR: 0.030000\n",
      "Train Epoch: 643 [188416/194182 (96%)]\tLoss: 0.412262\tGrad Norm: 0.741954\tLR: 0.030000\n",
      "Train set: Average loss: 0.4148\n",
      "Test set: Average loss: 0.2421, Average MAE: 0.3485\n",
      "Train Epoch: 644 [4096/194182 (2%)]\tLoss: 0.408761\tGrad Norm: 0.988022\tLR: 0.030000\n",
      "Train Epoch: 644 [24576/194182 (12%)]\tLoss: 0.408492\tGrad Norm: 0.864538\tLR: 0.030000\n",
      "Train Epoch: 644 [45056/194182 (23%)]\tLoss: 0.410403\tGrad Norm: 1.035712\tLR: 0.030000\n",
      "Train Epoch: 644 [65536/194182 (33%)]\tLoss: 0.418645\tGrad Norm: 1.354315\tLR: 0.030000\n",
      "Train Epoch: 644 [86016/194182 (44%)]\tLoss: 0.412924\tGrad Norm: 1.118159\tLR: 0.030000\n",
      "Train Epoch: 644 [106496/194182 (54%)]\tLoss: 0.416409\tGrad Norm: 1.152612\tLR: 0.030000\n",
      "Train Epoch: 644 [126976/194182 (65%)]\tLoss: 0.418928\tGrad Norm: 1.383279\tLR: 0.030000\n",
      "Train Epoch: 644 [147456/194182 (75%)]\tLoss: 0.410683\tGrad Norm: 1.375972\tLR: 0.030000\n",
      "Train Epoch: 644 [167936/194182 (85%)]\tLoss: 0.412704\tGrad Norm: 1.177468\tLR: 0.030000\n",
      "Train Epoch: 644 [188416/194182 (96%)]\tLoss: 0.418917\tGrad Norm: 1.238176\tLR: 0.030000\n",
      "Train set: Average loss: 0.4146\n",
      "Test set: Average loss: 0.2405, Average MAE: 0.3338\n",
      "Train Epoch: 645 [4096/194182 (2%)]\tLoss: 0.409708\tGrad Norm: 0.994746\tLR: 0.030000\n",
      "Train Epoch: 645 [24576/194182 (12%)]\tLoss: 0.408011\tGrad Norm: 0.752129\tLR: 0.030000\n",
      "Train Epoch: 645 [45056/194182 (23%)]\tLoss: 0.413354\tGrad Norm: 1.107718\tLR: 0.030000\n",
      "Train Epoch: 645 [65536/194182 (33%)]\tLoss: 0.409181\tGrad Norm: 0.979138\tLR: 0.030000\n",
      "Train Epoch: 645 [86016/194182 (44%)]\tLoss: 0.417440\tGrad Norm: 1.120232\tLR: 0.030000\n",
      "Train Epoch: 645 [106496/194182 (54%)]\tLoss: 0.422549\tGrad Norm: 1.264825\tLR: 0.030000\n",
      "Train Epoch: 645 [126976/194182 (65%)]\tLoss: 0.415259\tGrad Norm: 1.306130\tLR: 0.030000\n",
      "Train Epoch: 645 [147456/194182 (75%)]\tLoss: 0.408274\tGrad Norm: 1.260360\tLR: 0.030000\n",
      "Train Epoch: 645 [167936/194182 (85%)]\tLoss: 0.422334\tGrad Norm: 1.145381\tLR: 0.030000\n",
      "Train Epoch: 645 [188416/194182 (96%)]\tLoss: 0.413523\tGrad Norm: 1.048530\tLR: 0.030000\n",
      "Train set: Average loss: 0.4130\n",
      "Test set: Average loss: 0.2467, Average MAE: 0.3501\n",
      "Epoch 645: Mean reward = 0.050 +/- 0.045\n",
      "Train Epoch: 646 [4096/194182 (2%)]\tLoss: 0.412974\tGrad Norm: 1.237658\tLR: 0.030000\n",
      "Train Epoch: 646 [24576/194182 (12%)]\tLoss: 0.424180\tGrad Norm: 1.641824\tLR: 0.030000\n",
      "Train Epoch: 646 [45056/194182 (23%)]\tLoss: 0.421018\tGrad Norm: 1.325920\tLR: 0.030000\n",
      "Train Epoch: 646 [65536/194182 (33%)]\tLoss: 0.407928\tGrad Norm: 1.066140\tLR: 0.030000\n",
      "Train Epoch: 646 [86016/194182 (44%)]\tLoss: 0.415509\tGrad Norm: 1.062353\tLR: 0.030000\n",
      "Train Epoch: 646 [106496/194182 (54%)]\tLoss: 0.412191\tGrad Norm: 0.834552\tLR: 0.030000\n",
      "Train Epoch: 646 [126976/194182 (65%)]\tLoss: 0.410959\tGrad Norm: 0.968466\tLR: 0.030000\n",
      "Train Epoch: 646 [147456/194182 (75%)]\tLoss: 0.427656\tGrad Norm: 1.507612\tLR: 0.030000\n",
      "Train Epoch: 646 [167936/194182 (85%)]\tLoss: 0.415513\tGrad Norm: 1.100892\tLR: 0.030000\n",
      "Train Epoch: 646 [188416/194182 (96%)]\tLoss: 0.416030\tGrad Norm: 1.293907\tLR: 0.030000\n",
      "Train set: Average loss: 0.4140\n",
      "Test set: Average loss: 0.2437, Average MAE: 0.3579\n",
      "Train Epoch: 647 [4096/194182 (2%)]\tLoss: 0.414709\tGrad Norm: 1.109502\tLR: 0.030000\n",
      "Train Epoch: 647 [24576/194182 (12%)]\tLoss: 0.424554\tGrad Norm: 1.507038\tLR: 0.030000\n",
      "Train Epoch: 647 [45056/194182 (23%)]\tLoss: 0.417169\tGrad Norm: 1.492686\tLR: 0.030000\n",
      "Train Epoch: 647 [65536/194182 (33%)]\tLoss: 0.412685\tGrad Norm: 1.146507\tLR: 0.030000\n",
      "Train Epoch: 647 [86016/194182 (44%)]\tLoss: 0.407396\tGrad Norm: 0.985334\tLR: 0.030000\n",
      "Train Epoch: 647 [106496/194182 (54%)]\tLoss: 0.415518\tGrad Norm: 0.861705\tLR: 0.030000\n",
      "Train Epoch: 647 [126976/194182 (65%)]\tLoss: 0.415477\tGrad Norm: 1.010872\tLR: 0.030000\n",
      "Train Epoch: 647 [147456/194182 (75%)]\tLoss: 0.416272\tGrad Norm: 1.324014\tLR: 0.030000\n",
      "Train Epoch: 647 [167936/194182 (85%)]\tLoss: 0.412489\tGrad Norm: 0.996291\tLR: 0.030000\n",
      "Train Epoch: 647 [188416/194182 (96%)]\tLoss: 0.414655\tGrad Norm: 1.056041\tLR: 0.030000\n",
      "Train set: Average loss: 0.4128\n",
      "Test set: Average loss: 0.2452, Average MAE: 0.3407\n",
      "Train Epoch: 648 [4096/194182 (2%)]\tLoss: 0.412619\tGrad Norm: 1.215392\tLR: 0.030000\n",
      "Train Epoch: 648 [24576/194182 (12%)]\tLoss: 0.420298\tGrad Norm: 1.314522\tLR: 0.030000\n",
      "Train Epoch: 648 [45056/194182 (23%)]\tLoss: 0.420469\tGrad Norm: 1.536325\tLR: 0.030000\n",
      "Train Epoch: 648 [65536/194182 (33%)]\tLoss: 0.407247\tGrad Norm: 1.165914\tLR: 0.030000\n",
      "Train Epoch: 648 [86016/194182 (44%)]\tLoss: 0.421432\tGrad Norm: 1.532085\tLR: 0.030000\n",
      "Train Epoch: 648 [106496/194182 (54%)]\tLoss: 0.409985\tGrad Norm: 1.203429\tLR: 0.030000\n",
      "Train Epoch: 648 [126976/194182 (65%)]\tLoss: 0.428759\tGrad Norm: 1.698994\tLR: 0.030000\n",
      "Train Epoch: 648 [147456/194182 (75%)]\tLoss: 0.411174\tGrad Norm: 1.478995\tLR: 0.030000\n",
      "Train Epoch: 648 [167936/194182 (85%)]\tLoss: 0.414467\tGrad Norm: 1.186926\tLR: 0.030000\n",
      "Train Epoch: 648 [188416/194182 (96%)]\tLoss: 0.415164\tGrad Norm: 1.236425\tLR: 0.030000\n",
      "Train set: Average loss: 0.4161\n",
      "Test set: Average loss: 0.2425, Average MAE: 0.3372\n",
      "Train Epoch: 649 [4096/194182 (2%)]\tLoss: 0.413187\tGrad Norm: 1.323690\tLR: 0.030000\n",
      "Train Epoch: 649 [24576/194182 (12%)]\tLoss: 0.413223\tGrad Norm: 1.294614\tLR: 0.030000\n",
      "Train Epoch: 649 [45056/194182 (23%)]\tLoss: 0.405603\tGrad Norm: 1.038073\tLR: 0.030000\n",
      "Train Epoch: 649 [65536/194182 (33%)]\tLoss: 0.414415\tGrad Norm: 0.911644\tLR: 0.030000\n",
      "Train Epoch: 649 [86016/194182 (44%)]\tLoss: 0.415068\tGrad Norm: 1.171137\tLR: 0.030000\n",
      "Train Epoch: 649 [106496/194182 (54%)]\tLoss: 0.412119\tGrad Norm: 1.223248\tLR: 0.030000\n",
      "Train Epoch: 649 [126976/194182 (65%)]\tLoss: 0.410610\tGrad Norm: 1.254062\tLR: 0.030000\n",
      "Train Epoch: 649 [147456/194182 (75%)]\tLoss: 0.409851\tGrad Norm: 1.174585\tLR: 0.030000\n",
      "Train Epoch: 649 [167936/194182 (85%)]\tLoss: 0.420396\tGrad Norm: 1.406341\tLR: 0.030000\n",
      "Train Epoch: 649 [188416/194182 (96%)]\tLoss: 0.413037\tGrad Norm: 1.065523\tLR: 0.030000\n",
      "Train set: Average loss: 0.4128\n",
      "Test set: Average loss: 0.2433, Average MAE: 0.3469\n",
      "Train Epoch: 650 [4096/194182 (2%)]\tLoss: 0.411009\tGrad Norm: 1.148770\tLR: 0.030000\n",
      "Train Epoch: 650 [24576/194182 (12%)]\tLoss: 0.404327\tGrad Norm: 1.082153\tLR: 0.030000\n",
      "Train Epoch: 650 [45056/194182 (23%)]\tLoss: 0.411293\tGrad Norm: 1.139645\tLR: 0.030000\n",
      "Train Epoch: 650 [65536/194182 (33%)]\tLoss: 0.413685\tGrad Norm: 0.784386\tLR: 0.030000\n",
      "Train Epoch: 650 [86016/194182 (44%)]\tLoss: 0.408374\tGrad Norm: 1.047258\tLR: 0.030000\n",
      "Train Epoch: 650 [106496/194182 (54%)]\tLoss: 0.412107\tGrad Norm: 1.373140\tLR: 0.030000\n",
      "Train Epoch: 650 [126976/194182 (65%)]\tLoss: 0.410064\tGrad Norm: 1.275136\tLR: 0.030000\n",
      "Train Epoch: 650 [147456/194182 (75%)]\tLoss: 0.417493\tGrad Norm: 1.453906\tLR: 0.030000\n",
      "Train Epoch: 650 [167936/194182 (85%)]\tLoss: 0.423034\tGrad Norm: 1.630911\tLR: 0.030000\n",
      "Train Epoch: 650 [188416/194182 (96%)]\tLoss: 0.416642\tGrad Norm: 1.032721\tLR: 0.030000\n",
      "Train set: Average loss: 0.4124\n",
      "Test set: Average loss: 0.2398, Average MAE: 0.3403\n",
      "Epoch 650: Mean reward = 0.046 +/- 0.001\n",
      "Train Epoch: 651 [4096/194182 (2%)]\tLoss: 0.397120\tGrad Norm: 0.872778\tLR: 0.030000\n",
      "Train Epoch: 651 [24576/194182 (12%)]\tLoss: 0.414479\tGrad Norm: 1.141487\tLR: 0.030000\n",
      "Train Epoch: 651 [45056/194182 (23%)]\tLoss: 0.410678\tGrad Norm: 0.845064\tLR: 0.030000\n",
      "Train Epoch: 651 [65536/194182 (33%)]\tLoss: 0.402013\tGrad Norm: 0.920781\tLR: 0.030000\n",
      "Train Epoch: 651 [86016/194182 (44%)]\tLoss: 0.409492\tGrad Norm: 1.283883\tLR: 0.030000\n",
      "Train Epoch: 651 [106496/194182 (54%)]\tLoss: 0.409360\tGrad Norm: 0.681153\tLR: 0.030000\n",
      "Train Epoch: 651 [126976/194182 (65%)]\tLoss: 0.404913\tGrad Norm: 0.974765\tLR: 0.030000\n",
      "Train Epoch: 651 [147456/194182 (75%)]\tLoss: 0.412450\tGrad Norm: 1.094289\tLR: 0.030000\n",
      "Train Epoch: 651 [167936/194182 (85%)]\tLoss: 0.414099\tGrad Norm: 0.987088\tLR: 0.030000\n",
      "Train Epoch: 651 [188416/194182 (96%)]\tLoss: 0.414544\tGrad Norm: 1.397180\tLR: 0.030000\n",
      "Train set: Average loss: 0.4099\n",
      "Test set: Average loss: 0.2443, Average MAE: 0.3512\n",
      "Train Epoch: 652 [4096/194182 (2%)]\tLoss: 0.411678\tGrad Norm: 1.208716\tLR: 0.030000\n",
      "Train Epoch: 652 [24576/194182 (12%)]\tLoss: 0.415751\tGrad Norm: 1.160075\tLR: 0.030000\n",
      "Train Epoch: 652 [45056/194182 (23%)]\tLoss: 0.408988\tGrad Norm: 1.242660\tLR: 0.030000\n",
      "Train Epoch: 652 [65536/194182 (33%)]\tLoss: 0.410376\tGrad Norm: 1.175014\tLR: 0.030000\n",
      "Train Epoch: 652 [86016/194182 (44%)]\tLoss: 0.421019\tGrad Norm: 1.649043\tLR: 0.030000\n",
      "Train Epoch: 652 [106496/194182 (54%)]\tLoss: 0.420605\tGrad Norm: 1.610356\tLR: 0.030000\n",
      "Train Epoch: 652 [126976/194182 (65%)]\tLoss: 0.415060\tGrad Norm: 1.609308\tLR: 0.030000\n",
      "Train Epoch: 652 [147456/194182 (75%)]\tLoss: 0.423869\tGrad Norm: 1.752932\tLR: 0.030000\n",
      "Train Epoch: 652 [167936/194182 (85%)]\tLoss: 0.407433\tGrad Norm: 1.276482\tLR: 0.030000\n",
      "Train Epoch: 652 [188416/194182 (96%)]\tLoss: 0.402485\tGrad Norm: 0.955099\tLR: 0.030000\n",
      "Train set: Average loss: 0.4148\n",
      "Test set: Average loss: 0.2395, Average MAE: 0.3464\n",
      "Train Epoch: 653 [4096/194182 (2%)]\tLoss: 0.404968\tGrad Norm: 0.990188\tLR: 0.030000\n",
      "Train Epoch: 653 [24576/194182 (12%)]\tLoss: 0.399228\tGrad Norm: 0.810438\tLR: 0.030000\n",
      "Train Epoch: 653 [45056/194182 (23%)]\tLoss: 0.401882\tGrad Norm: 0.755396\tLR: 0.030000\n",
      "Train Epoch: 653 [65536/194182 (33%)]\tLoss: 0.401042\tGrad Norm: 0.761936\tLR: 0.030000\n",
      "Train Epoch: 653 [86016/194182 (44%)]\tLoss: 0.411417\tGrad Norm: 1.011322\tLR: 0.030000\n",
      "Train Epoch: 653 [106496/194182 (54%)]\tLoss: 0.411164\tGrad Norm: 1.143548\tLR: 0.030000\n",
      "Train Epoch: 653 [126976/194182 (65%)]\tLoss: 0.410886\tGrad Norm: 1.206214\tLR: 0.030000\n",
      "Train Epoch: 653 [147456/194182 (75%)]\tLoss: 0.408125\tGrad Norm: 1.057327\tLR: 0.030000\n",
      "Train Epoch: 653 [167936/194182 (85%)]\tLoss: 0.414851\tGrad Norm: 1.163714\tLR: 0.030000\n",
      "Train Epoch: 653 [188416/194182 (96%)]\tLoss: 0.411174\tGrad Norm: 1.103804\tLR: 0.030000\n",
      "Train set: Average loss: 0.4091\n",
      "Test set: Average loss: 0.2402, Average MAE: 0.3364\n",
      "Train Epoch: 654 [4096/194182 (2%)]\tLoss: 0.407242\tGrad Norm: 0.974191\tLR: 0.030000\n",
      "Train Epoch: 654 [24576/194182 (12%)]\tLoss: 0.413240\tGrad Norm: 1.290796\tLR: 0.030000\n",
      "Train Epoch: 654 [45056/194182 (23%)]\tLoss: 0.419544\tGrad Norm: 1.373705\tLR: 0.030000\n",
      "Train Epoch: 654 [65536/194182 (33%)]\tLoss: 0.406726\tGrad Norm: 1.666976\tLR: 0.030000\n",
      "Train Epoch: 654 [86016/194182 (44%)]\tLoss: 0.407103\tGrad Norm: 0.983019\tLR: 0.030000\n",
      "Train Epoch: 654 [106496/194182 (54%)]\tLoss: 0.403792\tGrad Norm: 0.843246\tLR: 0.030000\n",
      "Train Epoch: 654 [126976/194182 (65%)]\tLoss: 0.408801\tGrad Norm: 0.962694\tLR: 0.030000\n",
      "Train Epoch: 654 [147456/194182 (75%)]\tLoss: 0.408918\tGrad Norm: 1.057339\tLR: 0.030000\n",
      "Train Epoch: 654 [167936/194182 (85%)]\tLoss: 0.411435\tGrad Norm: 1.222775\tLR: 0.030000\n",
      "Train Epoch: 654 [188416/194182 (96%)]\tLoss: 0.414536\tGrad Norm: 1.259638\tLR: 0.030000\n",
      "Train set: Average loss: 0.4107\n",
      "Test set: Average loss: 0.2424, Average MAE: 0.3485\n",
      "Train Epoch: 655 [4096/194182 (2%)]\tLoss: 0.403083\tGrad Norm: 1.062836\tLR: 0.030000\n",
      "Train Epoch: 655 [24576/194182 (12%)]\tLoss: 0.403743\tGrad Norm: 0.986772\tLR: 0.030000\n",
      "Train Epoch: 655 [45056/194182 (23%)]\tLoss: 0.412685\tGrad Norm: 1.275633\tLR: 0.030000\n",
      "Train Epoch: 655 [65536/194182 (33%)]\tLoss: 0.406393\tGrad Norm: 1.014300\tLR: 0.030000\n",
      "Train Epoch: 655 [86016/194182 (44%)]\tLoss: 0.409499\tGrad Norm: 1.210053\tLR: 0.030000\n",
      "Train Epoch: 655 [106496/194182 (54%)]\tLoss: 0.418692\tGrad Norm: 1.455651\tLR: 0.030000\n",
      "Train Epoch: 655 [126976/194182 (65%)]\tLoss: 0.418981\tGrad Norm: 1.452114\tLR: 0.030000\n",
      "Train Epoch: 655 [147456/194182 (75%)]\tLoss: 0.418874\tGrad Norm: 1.220577\tLR: 0.030000\n",
      "Train Epoch: 655 [167936/194182 (85%)]\tLoss: 0.419719\tGrad Norm: 1.549724\tLR: 0.030000\n",
      "Train Epoch: 655 [188416/194182 (96%)]\tLoss: 0.416653\tGrad Norm: 1.380796\tLR: 0.030000\n",
      "Train set: Average loss: 0.4121\n",
      "Test set: Average loss: 0.2458, Average MAE: 0.3520\n",
      "Epoch 655: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 656 [4096/194182 (2%)]\tLoss: 0.414535\tGrad Norm: 1.324961\tLR: 0.030000\n",
      "Train Epoch: 656 [24576/194182 (12%)]\tLoss: 0.408537\tGrad Norm: 1.451674\tLR: 0.030000\n",
      "Train Epoch: 656 [45056/194182 (23%)]\tLoss: 0.412439\tGrad Norm: 1.383906\tLR: 0.030000\n",
      "Train Epoch: 656 [65536/194182 (33%)]\tLoss: 0.407764\tGrad Norm: 1.108900\tLR: 0.030000\n",
      "Train Epoch: 656 [86016/194182 (44%)]\tLoss: 0.404381\tGrad Norm: 0.794951\tLR: 0.030000\n",
      "Train Epoch: 656 [106496/194182 (54%)]\tLoss: 0.410285\tGrad Norm: 1.006005\tLR: 0.030000\n",
      "Train Epoch: 656 [126976/194182 (65%)]\tLoss: 0.415079\tGrad Norm: 1.385732\tLR: 0.030000\n",
      "Train Epoch: 656 [147456/194182 (75%)]\tLoss: 0.417103\tGrad Norm: 1.249837\tLR: 0.030000\n",
      "Train Epoch: 656 [167936/194182 (85%)]\tLoss: 0.413487\tGrad Norm: 1.209510\tLR: 0.030000\n",
      "Train Epoch: 656 [188416/194182 (96%)]\tLoss: 0.408581\tGrad Norm: 1.181205\tLR: 0.030000\n",
      "Train set: Average loss: 0.4103\n",
      "Test set: Average loss: 0.2435, Average MAE: 0.3537\n",
      "Train Epoch: 657 [4096/194182 (2%)]\tLoss: 0.414589\tGrad Norm: 1.070894\tLR: 0.030000\n",
      "Train Epoch: 657 [24576/194182 (12%)]\tLoss: 0.405104\tGrad Norm: 1.010744\tLR: 0.030000\n",
      "Train Epoch: 657 [45056/194182 (23%)]\tLoss: 0.415338\tGrad Norm: 1.238060\tLR: 0.030000\n",
      "Train Epoch: 657 [65536/194182 (33%)]\tLoss: 0.415929\tGrad Norm: 1.720926\tLR: 0.030000\n",
      "Train Epoch: 657 [86016/194182 (44%)]\tLoss: 0.418342\tGrad Norm: 1.488749\tLR: 0.030000\n",
      "Train Epoch: 657 [106496/194182 (54%)]\tLoss: 0.405945\tGrad Norm: 0.927432\tLR: 0.030000\n",
      "Train Epoch: 657 [126976/194182 (65%)]\tLoss: 0.400968\tGrad Norm: 0.963932\tLR: 0.030000\n",
      "Train Epoch: 657 [147456/194182 (75%)]\tLoss: 0.406836\tGrad Norm: 1.278306\tLR: 0.030000\n",
      "Train Epoch: 657 [167936/194182 (85%)]\tLoss: 0.405410\tGrad Norm: 1.070403\tLR: 0.030000\n",
      "Train Epoch: 657 [188416/194182 (96%)]\tLoss: 0.416725\tGrad Norm: 1.607747\tLR: 0.030000\n",
      "Train set: Average loss: 0.4106\n",
      "Test set: Average loss: 0.2488, Average MAE: 0.3625\n",
      "Train Epoch: 658 [4096/194182 (2%)]\tLoss: 0.415636\tGrad Norm: 1.532637\tLR: 0.030000\n",
      "Train Epoch: 658 [24576/194182 (12%)]\tLoss: 0.412424\tGrad Norm: 1.200842\tLR: 0.030000\n",
      "Train Epoch: 658 [45056/194182 (23%)]\tLoss: 0.413365\tGrad Norm: 1.126946\tLR: 0.030000\n",
      "Train Epoch: 658 [65536/194182 (33%)]\tLoss: 0.410810\tGrad Norm: 0.963586\tLR: 0.030000\n",
      "Train Epoch: 658 [86016/194182 (44%)]\tLoss: 0.414443\tGrad Norm: 1.137845\tLR: 0.030000\n",
      "Train Epoch: 658 [106496/194182 (54%)]\tLoss: 0.410045\tGrad Norm: 1.284888\tLR: 0.030000\n",
      "Train Epoch: 658 [126976/194182 (65%)]\tLoss: 0.409326\tGrad Norm: 1.139057\tLR: 0.030000\n",
      "Train Epoch: 658 [147456/194182 (75%)]\tLoss: 0.414192\tGrad Norm: 1.292674\tLR: 0.030000\n",
      "Train Epoch: 658 [167936/194182 (85%)]\tLoss: 0.412232\tGrad Norm: 0.939844\tLR: 0.030000\n",
      "Train Epoch: 658 [188416/194182 (96%)]\tLoss: 0.397513\tGrad Norm: 0.985151\tLR: 0.030000\n",
      "Train set: Average loss: 0.4094\n",
      "Test set: Average loss: 0.2423, Average MAE: 0.3506\n",
      "Train Epoch: 659 [4096/194182 (2%)]\tLoss: 0.401826\tGrad Norm: 1.014202\tLR: 0.030000\n",
      "Train Epoch: 659 [24576/194182 (12%)]\tLoss: 0.404692\tGrad Norm: 1.227218\tLR: 0.030000\n",
      "Train Epoch: 659 [45056/194182 (23%)]\tLoss: 0.406664\tGrad Norm: 0.983408\tLR: 0.030000\n",
      "Train Epoch: 659 [65536/194182 (33%)]\tLoss: 0.407809\tGrad Norm: 1.128942\tLR: 0.030000\n",
      "Train Epoch: 659 [86016/194182 (44%)]\tLoss: 0.417305\tGrad Norm: 1.064059\tLR: 0.030000\n",
      "Train Epoch: 659 [106496/194182 (54%)]\tLoss: 0.416695\tGrad Norm: 1.258139\tLR: 0.030000\n",
      "Train Epoch: 659 [126976/194182 (65%)]\tLoss: 0.409417\tGrad Norm: 1.267906\tLR: 0.030000\n",
      "Train Epoch: 659 [147456/194182 (75%)]\tLoss: 0.416530\tGrad Norm: 1.609619\tLR: 0.030000\n",
      "Train Epoch: 659 [167936/194182 (85%)]\tLoss: 0.412749\tGrad Norm: 1.384360\tLR: 0.030000\n",
      "Train Epoch: 659 [188416/194182 (96%)]\tLoss: 0.412327\tGrad Norm: 1.159910\tLR: 0.030000\n",
      "Train set: Average loss: 0.4102\n",
      "Test set: Average loss: 0.2437, Average MAE: 0.3577\n",
      "Train Epoch: 660 [4096/194182 (2%)]\tLoss: 0.409759\tGrad Norm: 1.227941\tLR: 0.030000\n",
      "Train Epoch: 660 [24576/194182 (12%)]\tLoss: 0.407395\tGrad Norm: 0.999926\tLR: 0.030000\n",
      "Train Epoch: 660 [45056/194182 (23%)]\tLoss: 0.408082\tGrad Norm: 1.381080\tLR: 0.030000\n",
      "Train Epoch: 660 [65536/194182 (33%)]\tLoss: 0.408429\tGrad Norm: 1.163568\tLR: 0.030000\n",
      "Train Epoch: 660 [86016/194182 (44%)]\tLoss: 0.404626\tGrad Norm: 0.816481\tLR: 0.030000\n",
      "Train Epoch: 660 [106496/194182 (54%)]\tLoss: 0.404817\tGrad Norm: 0.852788\tLR: 0.030000\n",
      "Train Epoch: 660 [126976/194182 (65%)]\tLoss: 0.399458\tGrad Norm: 1.147677\tLR: 0.030000\n",
      "Train Epoch: 660 [147456/194182 (75%)]\tLoss: 0.411611\tGrad Norm: 1.367641\tLR: 0.030000\n",
      "Train Epoch: 660 [167936/194182 (85%)]\tLoss: 0.414461\tGrad Norm: 1.484836\tLR: 0.030000\n",
      "Train Epoch: 660 [188416/194182 (96%)]\tLoss: 0.414248\tGrad Norm: 1.368056\tLR: 0.030000\n",
      "Train set: Average loss: 0.4092\n",
      "Test set: Average loss: 0.2467, Average MAE: 0.3588\n",
      "Epoch 660: Mean reward = 0.059 +/- 0.040\n",
      "Train Epoch: 661 [4096/194182 (2%)]\tLoss: 0.409525\tGrad Norm: 1.325375\tLR: 0.030000\n",
      "Train Epoch: 661 [24576/194182 (12%)]\tLoss: 0.404231\tGrad Norm: 0.706040\tLR: 0.030000\n",
      "Train Epoch: 661 [45056/194182 (23%)]\tLoss: 0.402275\tGrad Norm: 0.553447\tLR: 0.030000\n",
      "Train Epoch: 661 [65536/194182 (33%)]\tLoss: 0.400466\tGrad Norm: 0.888252\tLR: 0.030000\n",
      "Train Epoch: 661 [86016/194182 (44%)]\tLoss: 0.415155\tGrad Norm: 1.215771\tLR: 0.030000\n",
      "Train Epoch: 661 [106496/194182 (54%)]\tLoss: 0.410157\tGrad Norm: 1.063560\tLR: 0.030000\n",
      "Train Epoch: 661 [126976/194182 (65%)]\tLoss: 0.413154\tGrad Norm: 1.164889\tLR: 0.030000\n",
      "Train Epoch: 661 [147456/194182 (75%)]\tLoss: 0.397468\tGrad Norm: 1.409241\tLR: 0.030000\n",
      "Train Epoch: 661 [167936/194182 (85%)]\tLoss: 0.414184\tGrad Norm: 1.583804\tLR: 0.030000\n",
      "Train Epoch: 661 [188416/194182 (96%)]\tLoss: 0.409403\tGrad Norm: 1.104645\tLR: 0.030000\n",
      "Train set: Average loss: 0.4079\n",
      "Test set: Average loss: 0.2428, Average MAE: 0.3511\n",
      "Train Epoch: 662 [4096/194182 (2%)]\tLoss: 0.407465\tGrad Norm: 1.074130\tLR: 0.030000\n",
      "Train Epoch: 662 [24576/194182 (12%)]\tLoss: 0.415993\tGrad Norm: 1.358790\tLR: 0.030000\n",
      "Train Epoch: 662 [45056/194182 (23%)]\tLoss: 0.403709\tGrad Norm: 1.197126\tLR: 0.030000\n",
      "Train Epoch: 662 [65536/194182 (33%)]\tLoss: 0.411339\tGrad Norm: 1.359134\tLR: 0.030000\n",
      "Train Epoch: 662 [86016/194182 (44%)]\tLoss: 0.413814\tGrad Norm: 1.318869\tLR: 0.030000\n",
      "Train Epoch: 662 [106496/194182 (54%)]\tLoss: 0.410834\tGrad Norm: 1.315885\tLR: 0.030000\n",
      "Train Epoch: 662 [126976/194182 (65%)]\tLoss: 0.400063\tGrad Norm: 1.200417\tLR: 0.030000\n",
      "Train Epoch: 662 [147456/194182 (75%)]\tLoss: 0.403302\tGrad Norm: 0.915081\tLR: 0.030000\n",
      "Train Epoch: 662 [167936/194182 (85%)]\tLoss: 0.394571\tGrad Norm: 0.945023\tLR: 0.030000\n",
      "Train Epoch: 662 [188416/194182 (96%)]\tLoss: 0.415467\tGrad Norm: 1.080520\tLR: 0.030000\n",
      "Train set: Average loss: 0.4085\n",
      "Test set: Average loss: 0.2407, Average MAE: 0.3493\n",
      "Train Epoch: 663 [4096/194182 (2%)]\tLoss: 0.403698\tGrad Norm: 0.936329\tLR: 0.030000\n",
      "Train Epoch: 663 [24576/194182 (12%)]\tLoss: 0.391534\tGrad Norm: 0.984587\tLR: 0.030000\n",
      "Train Epoch: 663 [45056/194182 (23%)]\tLoss: 0.409748\tGrad Norm: 0.814401\tLR: 0.030000\n",
      "Train Epoch: 663 [65536/194182 (33%)]\tLoss: 0.412333\tGrad Norm: 1.081189\tLR: 0.030000\n",
      "Train Epoch: 663 [86016/194182 (44%)]\tLoss: 0.418961\tGrad Norm: 1.507831\tLR: 0.030000\n",
      "Train Epoch: 663 [106496/194182 (54%)]\tLoss: 0.412390\tGrad Norm: 1.192832\tLR: 0.030000\n",
      "Train Epoch: 663 [126976/194182 (65%)]\tLoss: 0.404132\tGrad Norm: 1.084531\tLR: 0.030000\n",
      "Train Epoch: 663 [147456/194182 (75%)]\tLoss: 0.399744\tGrad Norm: 1.278929\tLR: 0.030000\n",
      "Train Epoch: 663 [167936/194182 (85%)]\tLoss: 0.408819\tGrad Norm: 1.308200\tLR: 0.030000\n",
      "Train Epoch: 663 [188416/194182 (96%)]\tLoss: 0.406793\tGrad Norm: 1.108383\tLR: 0.030000\n",
      "Train set: Average loss: 0.4070\n",
      "Test set: Average loss: 0.2423, Average MAE: 0.3308\n",
      "Train Epoch: 664 [4096/194182 (2%)]\tLoss: 0.408445\tGrad Norm: 1.290220\tLR: 0.030000\n",
      "Train Epoch: 664 [24576/194182 (12%)]\tLoss: 0.410704\tGrad Norm: 1.146914\tLR: 0.030000\n",
      "Train Epoch: 664 [45056/194182 (23%)]\tLoss: 0.404630\tGrad Norm: 1.217559\tLR: 0.030000\n",
      "Train Epoch: 664 [65536/194182 (33%)]\tLoss: 0.406737\tGrad Norm: 1.247261\tLR: 0.030000\n",
      "Train Epoch: 664 [86016/194182 (44%)]\tLoss: 0.412077\tGrad Norm: 1.381892\tLR: 0.030000\n",
      "Train Epoch: 664 [106496/194182 (54%)]\tLoss: 0.412503\tGrad Norm: 1.592387\tLR: 0.030000\n",
      "Train Epoch: 664 [126976/194182 (65%)]\tLoss: 0.414718\tGrad Norm: 1.350844\tLR: 0.030000\n",
      "Train Epoch: 664 [147456/194182 (75%)]\tLoss: 0.407603\tGrad Norm: 1.047186\tLR: 0.030000\n",
      "Train Epoch: 664 [167936/194182 (85%)]\tLoss: 0.411431\tGrad Norm: 1.110602\tLR: 0.030000\n",
      "Train Epoch: 664 [188416/194182 (96%)]\tLoss: 0.412950\tGrad Norm: 1.167029\tLR: 0.030000\n",
      "Train set: Average loss: 0.4085\n",
      "Test set: Average loss: 0.2413, Average MAE: 0.3380\n",
      "Train Epoch: 665 [4096/194182 (2%)]\tLoss: 0.412940\tGrad Norm: 1.134496\tLR: 0.030000\n",
      "Train Epoch: 665 [24576/194182 (12%)]\tLoss: 0.413248\tGrad Norm: 1.198040\tLR: 0.030000\n",
      "Train Epoch: 665 [45056/194182 (23%)]\tLoss: 0.412693\tGrad Norm: 1.214067\tLR: 0.030000\n",
      "Train Epoch: 665 [65536/194182 (33%)]\tLoss: 0.414130\tGrad Norm: 0.962384\tLR: 0.030000\n",
      "Train Epoch: 665 [86016/194182 (44%)]\tLoss: 0.404018\tGrad Norm: 1.243691\tLR: 0.030000\n",
      "Train Epoch: 665 [106496/194182 (54%)]\tLoss: 0.398455\tGrad Norm: 1.233944\tLR: 0.030000\n",
      "Train Epoch: 665 [126976/194182 (65%)]\tLoss: 0.404262\tGrad Norm: 1.245634\tLR: 0.030000\n",
      "Train Epoch: 665 [147456/194182 (75%)]\tLoss: 0.401926\tGrad Norm: 0.888597\tLR: 0.030000\n",
      "Train Epoch: 665 [167936/194182 (85%)]\tLoss: 0.406937\tGrad Norm: 1.256950\tLR: 0.030000\n",
      "Train Epoch: 665 [188416/194182 (96%)]\tLoss: 0.410684\tGrad Norm: 1.257359\tLR: 0.030000\n",
      "Train set: Average loss: 0.4066\n",
      "Test set: Average loss: 0.2514, Average MAE: 0.3402\n",
      "Epoch 665: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 666 [4096/194182 (2%)]\tLoss: 0.418270\tGrad Norm: 2.343347\tLR: 0.030000\n",
      "Train Epoch: 666 [24576/194182 (12%)]\tLoss: 0.404043\tGrad Norm: 1.238940\tLR: 0.030000\n",
      "Train Epoch: 666 [45056/194182 (23%)]\tLoss: 0.408242\tGrad Norm: 1.108563\tLR: 0.030000\n",
      "Train Epoch: 666 [65536/194182 (33%)]\tLoss: 0.417258\tGrad Norm: 1.244537\tLR: 0.030000\n",
      "Train Epoch: 666 [86016/194182 (44%)]\tLoss: 0.406963\tGrad Norm: 1.283307\tLR: 0.030000\n",
      "Train Epoch: 666 [106496/194182 (54%)]\tLoss: 0.408241\tGrad Norm: 1.247234\tLR: 0.030000\n",
      "Train Epoch: 666 [126976/194182 (65%)]\tLoss: 0.420783\tGrad Norm: 1.704804\tLR: 0.030000\n",
      "Train Epoch: 666 [147456/194182 (75%)]\tLoss: 0.403690\tGrad Norm: 1.183635\tLR: 0.030000\n",
      "Train Epoch: 666 [167936/194182 (85%)]\tLoss: 0.400724\tGrad Norm: 1.119103\tLR: 0.030000\n",
      "Train Epoch: 666 [188416/194182 (96%)]\tLoss: 0.409223\tGrad Norm: 1.163864\tLR: 0.030000\n",
      "Train set: Average loss: 0.4085\n",
      "Test set: Average loss: 0.2419, Average MAE: 0.3367\n",
      "Train Epoch: 667 [4096/194182 (2%)]\tLoss: 0.403200\tGrad Norm: 1.171605\tLR: 0.030000\n",
      "Train Epoch: 667 [24576/194182 (12%)]\tLoss: 0.414326\tGrad Norm: 1.348101\tLR: 0.030000\n",
      "Train Epoch: 667 [45056/194182 (23%)]\tLoss: 0.412471\tGrad Norm: 1.348045\tLR: 0.030000\n",
      "Train Epoch: 667 [65536/194182 (33%)]\tLoss: 0.412876\tGrad Norm: 1.579073\tLR: 0.030000\n",
      "Train Epoch: 667 [86016/194182 (44%)]\tLoss: 0.414005\tGrad Norm: 1.329036\tLR: 0.030000\n",
      "Train Epoch: 667 [106496/194182 (54%)]\tLoss: 0.408954\tGrad Norm: 1.137986\tLR: 0.030000\n",
      "Train Epoch: 667 [126976/194182 (65%)]\tLoss: 0.411635\tGrad Norm: 1.375336\tLR: 0.030000\n",
      "Train Epoch: 667 [147456/194182 (75%)]\tLoss: 0.408041\tGrad Norm: 1.350587\tLR: 0.030000\n",
      "Train Epoch: 667 [167936/194182 (85%)]\tLoss: 0.398568\tGrad Norm: 0.998073\tLR: 0.030000\n",
      "Train Epoch: 667 [188416/194182 (96%)]\tLoss: 0.412068\tGrad Norm: 0.878766\tLR: 0.030000\n",
      "Train set: Average loss: 0.4081\n",
      "Test set: Average loss: 0.2434, Average MAE: 0.3429\n",
      "Train Epoch: 668 [4096/194182 (2%)]\tLoss: 0.403492\tGrad Norm: 1.175293\tLR: 0.030000\n",
      "Train Epoch: 668 [24576/194182 (12%)]\tLoss: 0.406804\tGrad Norm: 1.074980\tLR: 0.030000\n",
      "Train Epoch: 668 [45056/194182 (23%)]\tLoss: 0.400074\tGrad Norm: 0.742293\tLR: 0.030000\n",
      "Train Epoch: 668 [65536/194182 (33%)]\tLoss: 0.405610\tGrad Norm: 1.243477\tLR: 0.030000\n",
      "Train Epoch: 668 [86016/194182 (44%)]\tLoss: 0.401591\tGrad Norm: 1.025082\tLR: 0.030000\n",
      "Train Epoch: 668 [106496/194182 (54%)]\tLoss: 0.406776\tGrad Norm: 0.970612\tLR: 0.030000\n",
      "Train Epoch: 668 [126976/194182 (65%)]\tLoss: 0.396580\tGrad Norm: 0.862088\tLR: 0.030000\n",
      "Train Epoch: 668 [147456/194182 (75%)]\tLoss: 0.408824\tGrad Norm: 1.382687\tLR: 0.030000\n",
      "Train Epoch: 668 [167936/194182 (85%)]\tLoss: 0.417914\tGrad Norm: 1.392313\tLR: 0.030000\n",
      "Train Epoch: 668 [188416/194182 (96%)]\tLoss: 0.402533\tGrad Norm: 1.081956\tLR: 0.030000\n",
      "Train set: Average loss: 0.4052\n",
      "Test set: Average loss: 0.2394, Average MAE: 0.3360\n",
      "Train Epoch: 669 [4096/194182 (2%)]\tLoss: 0.410260\tGrad Norm: 1.037503\tLR: 0.030000\n",
      "Train Epoch: 669 [24576/194182 (12%)]\tLoss: 0.408753\tGrad Norm: 1.611833\tLR: 0.030000\n",
      "Train Epoch: 669 [45056/194182 (23%)]\tLoss: 0.411863\tGrad Norm: 1.435775\tLR: 0.030000\n",
      "Train Epoch: 669 [65536/194182 (33%)]\tLoss: 0.413655\tGrad Norm: 1.538119\tLR: 0.030000\n",
      "Train Epoch: 669 [86016/194182 (44%)]\tLoss: 0.398754\tGrad Norm: 1.264011\tLR: 0.030000\n",
      "Train Epoch: 669 [106496/194182 (54%)]\tLoss: 0.395536\tGrad Norm: 1.176486\tLR: 0.030000\n",
      "Train Epoch: 669 [126976/194182 (65%)]\tLoss: 0.406532\tGrad Norm: 1.232604\tLR: 0.030000\n",
      "Train Epoch: 669 [147456/194182 (75%)]\tLoss: 0.402808\tGrad Norm: 1.097468\tLR: 0.030000\n",
      "Train Epoch: 669 [167936/194182 (85%)]\tLoss: 0.403362\tGrad Norm: 0.798168\tLR: 0.030000\n",
      "Train Epoch: 669 [188416/194182 (96%)]\tLoss: 0.411894\tGrad Norm: 1.131464\tLR: 0.030000\n",
      "Train set: Average loss: 0.4071\n",
      "Test set: Average loss: 0.2420, Average MAE: 0.3422\n",
      "Train Epoch: 670 [4096/194182 (2%)]\tLoss: 0.406795\tGrad Norm: 1.070200\tLR: 0.030000\n",
      "Train Epoch: 670 [24576/194182 (12%)]\tLoss: 0.398502\tGrad Norm: 0.865387\tLR: 0.030000\n",
      "Train Epoch: 670 [45056/194182 (23%)]\tLoss: 0.398733\tGrad Norm: 1.315365\tLR: 0.030000\n",
      "Train Epoch: 670 [65536/194182 (33%)]\tLoss: 0.407611\tGrad Norm: 1.227273\tLR: 0.030000\n",
      "Train Epoch: 670 [86016/194182 (44%)]\tLoss: 0.405533\tGrad Norm: 1.414819\tLR: 0.030000\n",
      "Train Epoch: 670 [106496/194182 (54%)]\tLoss: 0.401488\tGrad Norm: 1.107833\tLR: 0.030000\n",
      "Train Epoch: 670 [126976/194182 (65%)]\tLoss: 0.400503\tGrad Norm: 0.931839\tLR: 0.030000\n",
      "Train Epoch: 670 [147456/194182 (75%)]\tLoss: 0.396980\tGrad Norm: 1.026418\tLR: 0.030000\n",
      "Train Epoch: 670 [167936/194182 (85%)]\tLoss: 0.404857\tGrad Norm: 1.369028\tLR: 0.030000\n",
      "Train Epoch: 670 [188416/194182 (96%)]\tLoss: 0.404820\tGrad Norm: 1.240906\tLR: 0.030000\n",
      "Train set: Average loss: 0.4058\n",
      "Test set: Average loss: 0.2466, Average MAE: 0.3573\n",
      "Epoch 670: Mean reward = 0.048 +/- 0.026\n",
      "Train Epoch: 671 [4096/194182 (2%)]\tLoss: 0.405177\tGrad Norm: 1.368532\tLR: 0.030000\n",
      "Train Epoch: 671 [24576/194182 (12%)]\tLoss: 0.396376\tGrad Norm: 0.770820\tLR: 0.030000\n",
      "Train Epoch: 671 [45056/194182 (23%)]\tLoss: 0.403906\tGrad Norm: 1.267912\tLR: 0.030000\n",
      "Train Epoch: 671 [65536/194182 (33%)]\tLoss: 0.399020\tGrad Norm: 1.210406\tLR: 0.030000\n",
      "Train Epoch: 671 [86016/194182 (44%)]\tLoss: 0.398319\tGrad Norm: 1.249583\tLR: 0.030000\n",
      "Train Epoch: 671 [106496/194182 (54%)]\tLoss: 0.399411\tGrad Norm: 1.310455\tLR: 0.030000\n",
      "Train Epoch: 671 [126976/194182 (65%)]\tLoss: 0.409797\tGrad Norm: 1.159975\tLR: 0.030000\n",
      "Train Epoch: 671 [147456/194182 (75%)]\tLoss: 0.401748\tGrad Norm: 0.948376\tLR: 0.030000\n",
      "Train Epoch: 671 [167936/194182 (85%)]\tLoss: 0.404600\tGrad Norm: 1.060279\tLR: 0.030000\n",
      "Train Epoch: 671 [188416/194182 (96%)]\tLoss: 0.399433\tGrad Norm: 1.013133\tLR: 0.030000\n",
      "Train set: Average loss: 0.4050\n",
      "Test set: Average loss: 0.2439, Average MAE: 0.3450\n",
      "Train Epoch: 672 [4096/194182 (2%)]\tLoss: 0.408905\tGrad Norm: 1.303410\tLR: 0.030000\n",
      "Train Epoch: 672 [24576/194182 (12%)]\tLoss: 0.404455\tGrad Norm: 1.128945\tLR: 0.030000\n",
      "Train Epoch: 672 [45056/194182 (23%)]\tLoss: 0.398284\tGrad Norm: 1.018235\tLR: 0.030000\n",
      "Train Epoch: 672 [65536/194182 (33%)]\tLoss: 0.405507\tGrad Norm: 0.983716\tLR: 0.030000\n",
      "Train Epoch: 672 [86016/194182 (44%)]\tLoss: 0.401027\tGrad Norm: 0.945957\tLR: 0.030000\n",
      "Train Epoch: 672 [106496/194182 (54%)]\tLoss: 0.399586\tGrad Norm: 1.130336\tLR: 0.030000\n",
      "Train Epoch: 672 [126976/194182 (65%)]\tLoss: 0.398551\tGrad Norm: 0.918679\tLR: 0.030000\n",
      "Train Epoch: 672 [147456/194182 (75%)]\tLoss: 0.396878\tGrad Norm: 1.001455\tLR: 0.030000\n",
      "Train Epoch: 672 [167936/194182 (85%)]\tLoss: 0.403495\tGrad Norm: 1.116545\tLR: 0.030000\n",
      "Train Epoch: 672 [188416/194182 (96%)]\tLoss: 0.406262\tGrad Norm: 1.369218\tLR: 0.030000\n",
      "Train set: Average loss: 0.4041\n",
      "Test set: Average loss: 0.2506, Average MAE: 0.3340\n",
      "Train Epoch: 673 [4096/194182 (2%)]\tLoss: 0.420950\tGrad Norm: 1.867500\tLR: 0.030000\n",
      "Train Epoch: 673 [24576/194182 (12%)]\tLoss: 0.408549\tGrad Norm: 1.412719\tLR: 0.030000\n",
      "Train Epoch: 673 [45056/194182 (23%)]\tLoss: 0.397735\tGrad Norm: 1.111692\tLR: 0.030000\n",
      "Train Epoch: 673 [65536/194182 (33%)]\tLoss: 0.405717\tGrad Norm: 1.278770\tLR: 0.030000\n",
      "Train Epoch: 673 [86016/194182 (44%)]\tLoss: 0.414112\tGrad Norm: 1.208778\tLR: 0.030000\n",
      "Train Epoch: 673 [106496/194182 (54%)]\tLoss: 0.410736\tGrad Norm: 1.317185\tLR: 0.030000\n",
      "Train Epoch: 673 [126976/194182 (65%)]\tLoss: 0.399519\tGrad Norm: 1.415059\tLR: 0.030000\n",
      "Train Epoch: 673 [147456/194182 (75%)]\tLoss: 0.410446\tGrad Norm: 1.111349\tLR: 0.030000\n",
      "Train Epoch: 673 [167936/194182 (85%)]\tLoss: 0.405774\tGrad Norm: 1.105558\tLR: 0.030000\n",
      "Train Epoch: 673 [188416/194182 (96%)]\tLoss: 0.395962\tGrad Norm: 1.034303\tLR: 0.030000\n",
      "Train set: Average loss: 0.4061\n",
      "Test set: Average loss: 0.2400, Average MAE: 0.3366\n",
      "Train Epoch: 674 [4096/194182 (2%)]\tLoss: 0.403236\tGrad Norm: 1.020093\tLR: 0.030000\n",
      "Train Epoch: 674 [24576/194182 (12%)]\tLoss: 0.396302\tGrad Norm: 0.886115\tLR: 0.030000\n",
      "Train Epoch: 674 [45056/194182 (23%)]\tLoss: 0.408017\tGrad Norm: 1.121379\tLR: 0.030000\n",
      "Train Epoch: 674 [65536/194182 (33%)]\tLoss: 0.407426\tGrad Norm: 1.254206\tLR: 0.030000\n",
      "Train Epoch: 674 [86016/194182 (44%)]\tLoss: 0.398702\tGrad Norm: 1.328270\tLR: 0.030000\n",
      "Train Epoch: 674 [106496/194182 (54%)]\tLoss: 0.403443\tGrad Norm: 0.911838\tLR: 0.030000\n",
      "Train Epoch: 674 [126976/194182 (65%)]\tLoss: 0.399338\tGrad Norm: 1.406354\tLR: 0.030000\n",
      "Train Epoch: 674 [147456/194182 (75%)]\tLoss: 0.407152\tGrad Norm: 1.291232\tLR: 0.030000\n",
      "Train Epoch: 674 [167936/194182 (85%)]\tLoss: 0.401833\tGrad Norm: 1.032701\tLR: 0.030000\n",
      "Train Epoch: 674 [188416/194182 (96%)]\tLoss: 0.392605\tGrad Norm: 1.080394\tLR: 0.030000\n",
      "Train set: Average loss: 0.4039\n",
      "Test set: Average loss: 0.2458, Average MAE: 0.3329\n",
      "Train Epoch: 675 [4096/194182 (2%)]\tLoss: 0.415314\tGrad Norm: 1.752090\tLR: 0.030000\n",
      "Train Epoch: 675 [24576/194182 (12%)]\tLoss: 0.400262\tGrad Norm: 1.377321\tLR: 0.030000\n",
      "Train Epoch: 675 [45056/194182 (23%)]\tLoss: 0.397666\tGrad Norm: 0.744173\tLR: 0.030000\n",
      "Train Epoch: 675 [65536/194182 (33%)]\tLoss: 0.399446\tGrad Norm: 0.827836\tLR: 0.030000\n",
      "Train Epoch: 675 [86016/194182 (44%)]\tLoss: 0.400450\tGrad Norm: 1.195558\tLR: 0.030000\n",
      "Train Epoch: 675 [106496/194182 (54%)]\tLoss: 0.407931\tGrad Norm: 1.112599\tLR: 0.030000\n",
      "Train Epoch: 675 [126976/194182 (65%)]\tLoss: 0.399065\tGrad Norm: 0.853523\tLR: 0.030000\n",
      "Train Epoch: 675 [147456/194182 (75%)]\tLoss: 0.395584\tGrad Norm: 0.768576\tLR: 0.030000\n",
      "Train Epoch: 675 [167936/194182 (85%)]\tLoss: 0.403634\tGrad Norm: 1.006705\tLR: 0.030000\n",
      "Train Epoch: 675 [188416/194182 (96%)]\tLoss: 0.403212\tGrad Norm: 1.389760\tLR: 0.030000\n",
      "Train set: Average loss: 0.4026\n",
      "Test set: Average loss: 0.2477, Average MAE: 0.3465\n",
      "Epoch 675: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 676 [4096/194182 (2%)]\tLoss: 0.405835\tGrad Norm: 1.475456\tLR: 0.030000\n",
      "Train Epoch: 676 [24576/194182 (12%)]\tLoss: 0.405700\tGrad Norm: 1.171516\tLR: 0.030000\n",
      "Train Epoch: 676 [45056/194182 (23%)]\tLoss: 0.397712\tGrad Norm: 0.991694\tLR: 0.030000\n",
      "Train Epoch: 676 [65536/194182 (33%)]\tLoss: 0.399929\tGrad Norm: 1.336543\tLR: 0.030000\n",
      "Train Epoch: 676 [86016/194182 (44%)]\tLoss: 0.417048\tGrad Norm: 1.743411\tLR: 0.030000\n",
      "Train Epoch: 676 [106496/194182 (54%)]\tLoss: 0.407567\tGrad Norm: 1.394429\tLR: 0.030000\n",
      "Train Epoch: 676 [126976/194182 (65%)]\tLoss: 0.401416\tGrad Norm: 1.167672\tLR: 0.030000\n",
      "Train Epoch: 676 [147456/194182 (75%)]\tLoss: 0.406375\tGrad Norm: 1.194583\tLR: 0.030000\n",
      "Train Epoch: 676 [167936/194182 (85%)]\tLoss: 0.406110\tGrad Norm: 1.604737\tLR: 0.030000\n",
      "Train Epoch: 676 [188416/194182 (96%)]\tLoss: 0.403302\tGrad Norm: 1.209254\tLR: 0.030000\n",
      "Train set: Average loss: 0.4055\n",
      "Test set: Average loss: 0.2497, Average MAE: 0.3612\n",
      "Train Epoch: 677 [4096/194182 (2%)]\tLoss: 0.398333\tGrad Norm: 1.546750\tLR: 0.030000\n",
      "Train Epoch: 677 [24576/194182 (12%)]\tLoss: 0.402520\tGrad Norm: 1.449959\tLR: 0.030000\n",
      "Train Epoch: 677 [45056/194182 (23%)]\tLoss: 0.404516\tGrad Norm: 1.330170\tLR: 0.030000\n",
      "Train Epoch: 677 [65536/194182 (33%)]\tLoss: 0.400592\tGrad Norm: 1.171708\tLR: 0.030000\n",
      "Train Epoch: 677 [86016/194182 (44%)]\tLoss: 0.394509\tGrad Norm: 0.876252\tLR: 0.030000\n",
      "Train Epoch: 677 [106496/194182 (54%)]\tLoss: 0.392216\tGrad Norm: 1.023683\tLR: 0.030000\n",
      "Train Epoch: 677 [126976/194182 (65%)]\tLoss: 0.403644\tGrad Norm: 0.935912\tLR: 0.030000\n",
      "Train Epoch: 677 [147456/194182 (75%)]\tLoss: 0.395581\tGrad Norm: 1.028395\tLR: 0.030000\n",
      "Train Epoch: 677 [167936/194182 (85%)]\tLoss: 0.397363\tGrad Norm: 0.903353\tLR: 0.030000\n",
      "Train Epoch: 677 [188416/194182 (96%)]\tLoss: 0.402775\tGrad Norm: 0.928774\tLR: 0.030000\n",
      "Train set: Average loss: 0.4024\n",
      "Test set: Average loss: 0.2422, Average MAE: 0.3496\n",
      "Train Epoch: 678 [4096/194182 (2%)]\tLoss: 0.401555\tGrad Norm: 1.132749\tLR: 0.030000\n",
      "Train Epoch: 678 [24576/194182 (12%)]\tLoss: 0.406938\tGrad Norm: 1.148925\tLR: 0.030000\n",
      "Train Epoch: 678 [45056/194182 (23%)]\tLoss: 0.408806\tGrad Norm: 1.492006\tLR: 0.030000\n",
      "Train Epoch: 678 [65536/194182 (33%)]\tLoss: 0.402478\tGrad Norm: 1.117576\tLR: 0.030000\n",
      "Train Epoch: 678 [86016/194182 (44%)]\tLoss: 0.399232\tGrad Norm: 0.922340\tLR: 0.030000\n",
      "Train Epoch: 678 [106496/194182 (54%)]\tLoss: 0.405160\tGrad Norm: 1.333491\tLR: 0.030000\n",
      "Train Epoch: 678 [126976/194182 (65%)]\tLoss: 0.402479\tGrad Norm: 1.337435\tLR: 0.030000\n",
      "Train Epoch: 678 [147456/194182 (75%)]\tLoss: 0.404179\tGrad Norm: 1.546046\tLR: 0.030000\n",
      "Train Epoch: 678 [167936/194182 (85%)]\tLoss: 0.411788\tGrad Norm: 1.648744\tLR: 0.030000\n",
      "Train Epoch: 678 [188416/194182 (96%)]\tLoss: 0.395557\tGrad Norm: 1.165833\tLR: 0.030000\n",
      "Train set: Average loss: 0.4047\n",
      "Test set: Average loss: 0.2460, Average MAE: 0.3561\n",
      "Train Epoch: 679 [4096/194182 (2%)]\tLoss: 0.408358\tGrad Norm: 1.309398\tLR: 0.030000\n",
      "Train Epoch: 679 [24576/194182 (12%)]\tLoss: 0.402051\tGrad Norm: 1.368408\tLR: 0.030000\n",
      "Train Epoch: 679 [45056/194182 (23%)]\tLoss: 0.412726\tGrad Norm: 1.522127\tLR: 0.030000\n",
      "Train Epoch: 679 [65536/194182 (33%)]\tLoss: 0.411382\tGrad Norm: 1.467765\tLR: 0.030000\n",
      "Train Epoch: 679 [86016/194182 (44%)]\tLoss: 0.401365\tGrad Norm: 1.221612\tLR: 0.030000\n",
      "Train Epoch: 679 [106496/194182 (54%)]\tLoss: 0.406063\tGrad Norm: 1.283964\tLR: 0.030000\n",
      "Train Epoch: 679 [126976/194182 (65%)]\tLoss: 0.405876\tGrad Norm: 1.263554\tLR: 0.030000\n",
      "Train Epoch: 679 [147456/194182 (75%)]\tLoss: 0.401121\tGrad Norm: 1.219583\tLR: 0.030000\n",
      "Train Epoch: 679 [167936/194182 (85%)]\tLoss: 0.409602\tGrad Norm: 1.415431\tLR: 0.030000\n",
      "Train Epoch: 679 [188416/194182 (96%)]\tLoss: 0.407729\tGrad Norm: 1.066048\tLR: 0.030000\n",
      "Train set: Average loss: 0.4048\n",
      "Test set: Average loss: 0.2428, Average MAE: 0.3431\n",
      "Train Epoch: 680 [4096/194182 (2%)]\tLoss: 0.398081\tGrad Norm: 1.161909\tLR: 0.030000\n",
      "Train Epoch: 680 [24576/194182 (12%)]\tLoss: 0.395462\tGrad Norm: 1.194750\tLR: 0.030000\n",
      "Train Epoch: 680 [45056/194182 (23%)]\tLoss: 0.402089\tGrad Norm: 1.240306\tLR: 0.030000\n",
      "Train Epoch: 680 [65536/194182 (33%)]\tLoss: 0.403779\tGrad Norm: 1.485048\tLR: 0.030000\n",
      "Train Epoch: 680 [86016/194182 (44%)]\tLoss: 0.404343\tGrad Norm: 1.317641\tLR: 0.030000\n",
      "Train Epoch: 680 [106496/194182 (54%)]\tLoss: 0.400638\tGrad Norm: 1.301363\tLR: 0.030000\n",
      "Train Epoch: 680 [126976/194182 (65%)]\tLoss: 0.401770\tGrad Norm: 1.075376\tLR: 0.030000\n",
      "Train Epoch: 680 [147456/194182 (75%)]\tLoss: 0.414541\tGrad Norm: 1.437011\tLR: 0.030000\n",
      "Train Epoch: 680 [167936/194182 (85%)]\tLoss: 0.404405\tGrad Norm: 1.281107\tLR: 0.030000\n",
      "Train Epoch: 680 [188416/194182 (96%)]\tLoss: 0.404572\tGrad Norm: 1.291072\tLR: 0.030000\n",
      "Train set: Average loss: 0.4044\n",
      "Test set: Average loss: 0.2451, Average MAE: 0.3438\n",
      "Epoch 680: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 681 [4096/194182 (2%)]\tLoss: 0.407204\tGrad Norm: 1.368401\tLR: 0.030000\n",
      "Train Epoch: 681 [24576/194182 (12%)]\tLoss: 0.398731\tGrad Norm: 0.960699\tLR: 0.030000\n",
      "Train Epoch: 681 [45056/194182 (23%)]\tLoss: 0.397019\tGrad Norm: 0.942258\tLR: 0.030000\n",
      "Train Epoch: 681 [65536/194182 (33%)]\tLoss: 0.403810\tGrad Norm: 1.401155\tLR: 0.030000\n",
      "Train Epoch: 681 [86016/194182 (44%)]\tLoss: 0.398534\tGrad Norm: 0.992412\tLR: 0.030000\n",
      "Train Epoch: 681 [106496/194182 (54%)]\tLoss: 0.404500\tGrad Norm: 1.234767\tLR: 0.030000\n",
      "Train Epoch: 681 [126976/194182 (65%)]\tLoss: 0.387934\tGrad Norm: 0.861084\tLR: 0.030000\n",
      "Train Epoch: 681 [147456/194182 (75%)]\tLoss: 0.394372\tGrad Norm: 0.905361\tLR: 0.030000\n",
      "Train Epoch: 681 [167936/194182 (85%)]\tLoss: 0.402707\tGrad Norm: 0.844077\tLR: 0.030000\n",
      "Train Epoch: 681 [188416/194182 (96%)]\tLoss: 0.405228\tGrad Norm: 1.109357\tLR: 0.030000\n",
      "Train set: Average loss: 0.4005\n",
      "Test set: Average loss: 0.2442, Average MAE: 0.3550\n",
      "Train Epoch: 682 [4096/194182 (2%)]\tLoss: 0.397496\tGrad Norm: 1.269410\tLR: 0.030000\n",
      "Train Epoch: 682 [24576/194182 (12%)]\tLoss: 0.405002\tGrad Norm: 1.297203\tLR: 0.030000\n",
      "Train Epoch: 682 [45056/194182 (23%)]\tLoss: 0.400846\tGrad Norm: 1.242564\tLR: 0.030000\n",
      "Train Epoch: 682 [65536/194182 (33%)]\tLoss: 0.405384\tGrad Norm: 1.406840\tLR: 0.030000\n",
      "Train Epoch: 682 [86016/194182 (44%)]\tLoss: 0.408214\tGrad Norm: 1.527524\tLR: 0.030000\n",
      "Train Epoch: 682 [106496/194182 (54%)]\tLoss: 0.399933\tGrad Norm: 1.347233\tLR: 0.030000\n",
      "Train Epoch: 682 [126976/194182 (65%)]\tLoss: 0.399196\tGrad Norm: 0.933316\tLR: 0.030000\n",
      "Train Epoch: 682 [147456/194182 (75%)]\tLoss: 0.400086\tGrad Norm: 1.029621\tLR: 0.030000\n",
      "Train Epoch: 682 [167936/194182 (85%)]\tLoss: 0.392203\tGrad Norm: 0.691750\tLR: 0.030000\n",
      "Train Epoch: 682 [188416/194182 (96%)]\tLoss: 0.400319\tGrad Norm: 0.976302\tLR: 0.030000\n",
      "Train set: Average loss: 0.4021\n",
      "Test set: Average loss: 0.2403, Average MAE: 0.3507\n",
      "Train Epoch: 683 [4096/194182 (2%)]\tLoss: 0.408003\tGrad Norm: 1.085300\tLR: 0.030000\n",
      "Train Epoch: 683 [24576/194182 (12%)]\tLoss: 0.393259\tGrad Norm: 0.757742\tLR: 0.030000\n",
      "Train Epoch: 683 [45056/194182 (23%)]\tLoss: 0.400202\tGrad Norm: 0.976958\tLR: 0.030000\n",
      "Train Epoch: 683 [65536/194182 (33%)]\tLoss: 0.397741\tGrad Norm: 1.021572\tLR: 0.030000\n",
      "Train Epoch: 683 [86016/194182 (44%)]\tLoss: 0.412660\tGrad Norm: 1.340173\tLR: 0.030000\n",
      "Train Epoch: 683 [106496/194182 (54%)]\tLoss: 0.404066\tGrad Norm: 1.400616\tLR: 0.030000\n",
      "Train Epoch: 683 [126976/194182 (65%)]\tLoss: 0.391506\tGrad Norm: 1.015309\tLR: 0.030000\n",
      "Train Epoch: 683 [147456/194182 (75%)]\tLoss: 0.410661\tGrad Norm: 1.233733\tLR: 0.030000\n",
      "Train Epoch: 683 [167936/194182 (85%)]\tLoss: 0.398034\tGrad Norm: 1.361158\tLR: 0.030000\n",
      "Train Epoch: 683 [188416/194182 (96%)]\tLoss: 0.404471\tGrad Norm: 1.293632\tLR: 0.030000\n",
      "Train set: Average loss: 0.4011\n",
      "Test set: Average loss: 0.2475, Average MAE: 0.3329\n",
      "Train Epoch: 684 [4096/194182 (2%)]\tLoss: 0.400437\tGrad Norm: 1.421898\tLR: 0.030000\n",
      "Train Epoch: 684 [24576/194182 (12%)]\tLoss: 0.401837\tGrad Norm: 1.118921\tLR: 0.030000\n",
      "Train Epoch: 684 [45056/194182 (23%)]\tLoss: 0.404706\tGrad Norm: 1.554550\tLR: 0.030000\n",
      "Train Epoch: 684 [65536/194182 (33%)]\tLoss: 0.406099\tGrad Norm: 1.445883\tLR: 0.030000\n",
      "Train Epoch: 684 [86016/194182 (44%)]\tLoss: 0.402158\tGrad Norm: 1.308963\tLR: 0.030000\n",
      "Train Epoch: 684 [106496/194182 (54%)]\tLoss: 0.402983\tGrad Norm: 1.165147\tLR: 0.030000\n",
      "Train Epoch: 684 [126976/194182 (65%)]\tLoss: 0.398514\tGrad Norm: 1.056084\tLR: 0.030000\n",
      "Train Epoch: 684 [147456/194182 (75%)]\tLoss: 0.396279\tGrad Norm: 0.829428\tLR: 0.030000\n",
      "Train Epoch: 684 [167936/194182 (85%)]\tLoss: 0.408563\tGrad Norm: 1.292944\tLR: 0.030000\n",
      "Train Epoch: 684 [188416/194182 (96%)]\tLoss: 0.413877\tGrad Norm: 1.167564\tLR: 0.030000\n",
      "Train set: Average loss: 0.4023\n",
      "Test set: Average loss: 0.2437, Average MAE: 0.3445\n",
      "Train Epoch: 685 [4096/194182 (2%)]\tLoss: 0.396966\tGrad Norm: 1.266603\tLR: 0.030000\n",
      "Train Epoch: 685 [24576/194182 (12%)]\tLoss: 0.401992\tGrad Norm: 1.142307\tLR: 0.030000\n",
      "Train Epoch: 685 [45056/194182 (23%)]\tLoss: 0.399794\tGrad Norm: 1.171533\tLR: 0.030000\n",
      "Train Epoch: 685 [65536/194182 (33%)]\tLoss: 0.400136\tGrad Norm: 0.943353\tLR: 0.030000\n",
      "Train Epoch: 685 [86016/194182 (44%)]\tLoss: 0.401549\tGrad Norm: 1.118307\tLR: 0.030000\n",
      "Train Epoch: 685 [106496/194182 (54%)]\tLoss: 0.403264\tGrad Norm: 1.193953\tLR: 0.030000\n",
      "Train Epoch: 685 [126976/194182 (65%)]\tLoss: 0.396145\tGrad Norm: 1.141434\tLR: 0.030000\n",
      "Train Epoch: 685 [147456/194182 (75%)]\tLoss: 0.398245\tGrad Norm: 1.342790\tLR: 0.030000\n",
      "Train Epoch: 685 [167936/194182 (85%)]\tLoss: 0.398042\tGrad Norm: 1.531967\tLR: 0.030000\n",
      "Train Epoch: 685 [188416/194182 (96%)]\tLoss: 0.412332\tGrad Norm: 1.602428\tLR: 0.030000\n",
      "Train set: Average loss: 0.4020\n",
      "Test set: Average loss: 0.2459, Average MAE: 0.3588\n",
      "Epoch 685: Mean reward = 0.042 +/- 0.014\n",
      "Train Epoch: 686 [4096/194182 (2%)]\tLoss: 0.407748\tGrad Norm: 1.279453\tLR: 0.030000\n",
      "Train Epoch: 686 [24576/194182 (12%)]\tLoss: 0.402171\tGrad Norm: 1.239847\tLR: 0.030000\n",
      "Train Epoch: 686 [45056/194182 (23%)]\tLoss: 0.394583\tGrad Norm: 1.065940\tLR: 0.030000\n",
      "Train Epoch: 686 [65536/194182 (33%)]\tLoss: 0.410159\tGrad Norm: 1.289740\tLR: 0.030000\n",
      "Train Epoch: 686 [86016/194182 (44%)]\tLoss: 0.407813\tGrad Norm: 1.229362\tLR: 0.030000\n",
      "Train Epoch: 686 [106496/194182 (54%)]\tLoss: 0.403543\tGrad Norm: 1.246791\tLR: 0.030000\n",
      "Train Epoch: 686 [126976/194182 (65%)]\tLoss: 0.407215\tGrad Norm: 1.415149\tLR: 0.030000\n",
      "Train Epoch: 686 [147456/194182 (75%)]\tLoss: 0.406732\tGrad Norm: 1.187644\tLR: 0.030000\n",
      "Train Epoch: 686 [167936/194182 (85%)]\tLoss: 0.399819\tGrad Norm: 1.149669\tLR: 0.030000\n",
      "Train Epoch: 686 [188416/194182 (96%)]\tLoss: 0.394226\tGrad Norm: 1.007927\tLR: 0.030000\n",
      "Train set: Average loss: 0.4016\n",
      "Test set: Average loss: 0.2402, Average MAE: 0.3409\n",
      "Train Epoch: 687 [4096/194182 (2%)]\tLoss: 0.391268\tGrad Norm: 1.091073\tLR: 0.030000\n",
      "Train Epoch: 687 [24576/194182 (12%)]\tLoss: 0.396032\tGrad Norm: 0.919963\tLR: 0.030000\n",
      "Train Epoch: 687 [45056/194182 (23%)]\tLoss: 0.402529\tGrad Norm: 1.109288\tLR: 0.030000\n",
      "Train Epoch: 687 [65536/194182 (33%)]\tLoss: 0.395398\tGrad Norm: 1.269943\tLR: 0.030000\n",
      "Train Epoch: 687 [86016/194182 (44%)]\tLoss: 0.399662\tGrad Norm: 1.203398\tLR: 0.030000\n",
      "Train Epoch: 687 [106496/194182 (54%)]\tLoss: 0.392761\tGrad Norm: 1.051553\tLR: 0.030000\n",
      "Train Epoch: 687 [126976/194182 (65%)]\tLoss: 0.398774\tGrad Norm: 0.813071\tLR: 0.030000\n",
      "Train Epoch: 687 [147456/194182 (75%)]\tLoss: 0.393365\tGrad Norm: 0.735504\tLR: 0.030000\n",
      "Train Epoch: 687 [167936/194182 (85%)]\tLoss: 0.401510\tGrad Norm: 0.724600\tLR: 0.030000\n",
      "Train Epoch: 687 [188416/194182 (96%)]\tLoss: 0.398196\tGrad Norm: 1.055774\tLR: 0.030000\n",
      "Train set: Average loss: 0.3981\n",
      "Test set: Average loss: 0.2465, Average MAE: 0.3522\n",
      "Train Epoch: 688 [4096/194182 (2%)]\tLoss: 0.409906\tGrad Norm: 1.336087\tLR: 0.030000\n",
      "Train Epoch: 688 [24576/194182 (12%)]\tLoss: 0.411012\tGrad Norm: 1.414320\tLR: 0.030000\n",
      "Train Epoch: 688 [45056/194182 (23%)]\tLoss: 0.405967\tGrad Norm: 1.388310\tLR: 0.030000\n",
      "Train Epoch: 688 [65536/194182 (33%)]\tLoss: 0.406325\tGrad Norm: 1.390208\tLR: 0.030000\n",
      "Train Epoch: 688 [86016/194182 (44%)]\tLoss: 0.407398\tGrad Norm: 1.386443\tLR: 0.030000\n",
      "Train Epoch: 688 [106496/194182 (54%)]\tLoss: 0.408124\tGrad Norm: 1.734076\tLR: 0.030000\n",
      "Train Epoch: 688 [126976/194182 (65%)]\tLoss: 0.396669\tGrad Norm: 1.489898\tLR: 0.030000\n",
      "Train Epoch: 688 [147456/194182 (75%)]\tLoss: 0.390766\tGrad Norm: 1.000250\tLR: 0.030000\n",
      "Train Epoch: 688 [167936/194182 (85%)]\tLoss: 0.394836\tGrad Norm: 1.203777\tLR: 0.030000\n",
      "Train Epoch: 688 [188416/194182 (96%)]\tLoss: 0.398091\tGrad Norm: 1.000059\tLR: 0.030000\n",
      "Train set: Average loss: 0.4018\n",
      "Test set: Average loss: 0.2396, Average MAE: 0.3492\n",
      "Train Epoch: 689 [4096/194182 (2%)]\tLoss: 0.398629\tGrad Norm: 0.897681\tLR: 0.030000\n",
      "Train Epoch: 689 [24576/194182 (12%)]\tLoss: 0.394890\tGrad Norm: 0.923912\tLR: 0.030000\n",
      "Train Epoch: 689 [45056/194182 (23%)]\tLoss: 0.392331\tGrad Norm: 0.853311\tLR: 0.030000\n",
      "Train Epoch: 689 [65536/194182 (33%)]\tLoss: 0.409992\tGrad Norm: 1.306183\tLR: 0.030000\n",
      "Train Epoch: 689 [86016/194182 (44%)]\tLoss: 0.395604\tGrad Norm: 1.388708\tLR: 0.030000\n",
      "Train Epoch: 689 [106496/194182 (54%)]\tLoss: 0.415516\tGrad Norm: 1.305411\tLR: 0.030000\n",
      "Train Epoch: 689 [126976/194182 (65%)]\tLoss: 0.396300\tGrad Norm: 1.195009\tLR: 0.030000\n",
      "Train Epoch: 689 [147456/194182 (75%)]\tLoss: 0.398590\tGrad Norm: 1.055434\tLR: 0.030000\n",
      "Train Epoch: 689 [167936/194182 (85%)]\tLoss: 0.406301\tGrad Norm: 1.412927\tLR: 0.030000\n",
      "Train Epoch: 689 [188416/194182 (96%)]\tLoss: 0.397644\tGrad Norm: 1.161222\tLR: 0.030000\n",
      "Train set: Average loss: 0.3988\n",
      "Test set: Average loss: 0.2393, Average MAE: 0.3434\n",
      "Train Epoch: 690 [4096/194182 (2%)]\tLoss: 0.396735\tGrad Norm: 0.927265\tLR: 0.030000\n",
      "Train Epoch: 690 [24576/194182 (12%)]\tLoss: 0.403009\tGrad Norm: 1.089618\tLR: 0.030000\n",
      "Train Epoch: 690 [45056/194182 (23%)]\tLoss: 0.393473\tGrad Norm: 1.102710\tLR: 0.030000\n",
      "Train Epoch: 690 [65536/194182 (33%)]\tLoss: 0.405766\tGrad Norm: 1.222982\tLR: 0.030000\n",
      "Train Epoch: 690 [86016/194182 (44%)]\tLoss: 0.403905\tGrad Norm: 1.266745\tLR: 0.030000\n",
      "Train Epoch: 690 [106496/194182 (54%)]\tLoss: 0.404948\tGrad Norm: 1.438779\tLR: 0.030000\n",
      "Train Epoch: 690 [126976/194182 (65%)]\tLoss: 0.402322\tGrad Norm: 1.504082\tLR: 0.030000\n",
      "Train Epoch: 690 [147456/194182 (75%)]\tLoss: 0.403558\tGrad Norm: 1.270853\tLR: 0.030000\n",
      "Train Epoch: 690 [167936/194182 (85%)]\tLoss: 0.407900\tGrad Norm: 1.377453\tLR: 0.030000\n",
      "Train Epoch: 690 [188416/194182 (96%)]\tLoss: 0.394135\tGrad Norm: 1.041974\tLR: 0.030000\n",
      "Train set: Average loss: 0.4005\n",
      "Test set: Average loss: 0.2415, Average MAE: 0.3516\n",
      "Epoch 690: Mean reward = 0.046 +/- 0.001\n",
      "Train Epoch: 691 [4096/194182 (2%)]\tLoss: 0.396932\tGrad Norm: 1.087527\tLR: 0.030000\n",
      "Train Epoch: 691 [24576/194182 (12%)]\tLoss: 0.401927\tGrad Norm: 1.212109\tLR: 0.030000\n",
      "Train Epoch: 691 [45056/194182 (23%)]\tLoss: 0.407960\tGrad Norm: 0.877884\tLR: 0.030000\n",
      "Train Epoch: 691 [65536/194182 (33%)]\tLoss: 0.395392\tGrad Norm: 0.992030\tLR: 0.030000\n",
      "Train Epoch: 691 [86016/194182 (44%)]\tLoss: 0.399936\tGrad Norm: 1.167398\tLR: 0.030000\n",
      "Train Epoch: 691 [106496/194182 (54%)]\tLoss: 0.397992\tGrad Norm: 1.055369\tLR: 0.030000\n",
      "Train Epoch: 691 [126976/194182 (65%)]\tLoss: 0.403408\tGrad Norm: 1.271002\tLR: 0.030000\n",
      "Train Epoch: 691 [147456/194182 (75%)]\tLoss: 0.407226\tGrad Norm: 1.525702\tLR: 0.030000\n",
      "Train Epoch: 691 [167936/194182 (85%)]\tLoss: 0.399581\tGrad Norm: 1.005984\tLR: 0.030000\n",
      "Train Epoch: 691 [188416/194182 (96%)]\tLoss: 0.416116\tGrad Norm: 1.556225\tLR: 0.030000\n",
      "Train set: Average loss: 0.3999\n",
      "Test set: Average loss: 0.2557, Average MAE: 0.3703\n",
      "Train Epoch: 692 [4096/194182 (2%)]\tLoss: 0.412726\tGrad Norm: 1.755134\tLR: 0.030000\n",
      "Train Epoch: 692 [24576/194182 (12%)]\tLoss: 0.396244\tGrad Norm: 1.375998\tLR: 0.030000\n",
      "Train Epoch: 692 [45056/194182 (23%)]\tLoss: 0.406710\tGrad Norm: 1.472466\tLR: 0.030000\n",
      "Train Epoch: 692 [65536/194182 (33%)]\tLoss: 0.399949\tGrad Norm: 1.244600\tLR: 0.030000\n",
      "Train Epoch: 692 [86016/194182 (44%)]\tLoss: 0.397968\tGrad Norm: 1.088660\tLR: 0.030000\n",
      "Train Epoch: 692 [106496/194182 (54%)]\tLoss: 0.402310\tGrad Norm: 0.999586\tLR: 0.030000\n",
      "Train Epoch: 692 [126976/194182 (65%)]\tLoss: 0.393459\tGrad Norm: 1.029570\tLR: 0.030000\n",
      "Train Epoch: 692 [147456/194182 (75%)]\tLoss: 0.396197\tGrad Norm: 1.003689\tLR: 0.030000\n",
      "Train Epoch: 692 [167936/194182 (85%)]\tLoss: 0.394978\tGrad Norm: 1.217425\tLR: 0.030000\n",
      "Train Epoch: 692 [188416/194182 (96%)]\tLoss: 0.398854\tGrad Norm: 1.504028\tLR: 0.030000\n",
      "Train set: Average loss: 0.3995\n",
      "Test set: Average loss: 0.2484, Average MAE: 0.3469\n",
      "Train Epoch: 693 [4096/194182 (2%)]\tLoss: 0.396685\tGrad Norm: 1.468067\tLR: 0.030000\n",
      "Train Epoch: 693 [24576/194182 (12%)]\tLoss: 0.402120\tGrad Norm: 1.409388\tLR: 0.030000\n",
      "Train Epoch: 693 [45056/194182 (23%)]\tLoss: 0.396033\tGrad Norm: 0.928187\tLR: 0.030000\n",
      "Train Epoch: 693 [65536/194182 (33%)]\tLoss: 0.399146\tGrad Norm: 1.021793\tLR: 0.030000\n",
      "Train Epoch: 693 [86016/194182 (44%)]\tLoss: 0.397321\tGrad Norm: 0.884845\tLR: 0.030000\n",
      "Train Epoch: 693 [106496/194182 (54%)]\tLoss: 0.398308\tGrad Norm: 1.328082\tLR: 0.030000\n",
      "Train Epoch: 693 [126976/194182 (65%)]\tLoss: 0.399260\tGrad Norm: 1.344176\tLR: 0.030000\n",
      "Train Epoch: 693 [147456/194182 (75%)]\tLoss: 0.400093\tGrad Norm: 0.969467\tLR: 0.030000\n",
      "Train Epoch: 693 [167936/194182 (85%)]\tLoss: 0.401174\tGrad Norm: 1.184457\tLR: 0.030000\n",
      "Train Epoch: 693 [188416/194182 (96%)]\tLoss: 0.400227\tGrad Norm: 1.302473\tLR: 0.030000\n",
      "Train set: Average loss: 0.3988\n",
      "Test set: Average loss: 0.2453, Average MAE: 0.3314\n",
      "Train Epoch: 694 [4096/194182 (2%)]\tLoss: 0.398446\tGrad Norm: 1.380740\tLR: 0.030000\n",
      "Train Epoch: 694 [24576/194182 (12%)]\tLoss: 0.406481\tGrad Norm: 1.260123\tLR: 0.030000\n",
      "Train Epoch: 694 [45056/194182 (23%)]\tLoss: 0.400797\tGrad Norm: 1.571558\tLR: 0.030000\n",
      "Train Epoch: 694 [65536/194182 (33%)]\tLoss: 0.397290\tGrad Norm: 1.406750\tLR: 0.030000\n",
      "Train Epoch: 694 [86016/194182 (44%)]\tLoss: 0.405635\tGrad Norm: 1.112106\tLR: 0.030000\n",
      "Train Epoch: 694 [106496/194182 (54%)]\tLoss: 0.399138\tGrad Norm: 1.098793\tLR: 0.030000\n",
      "Train Epoch: 694 [126976/194182 (65%)]\tLoss: 0.399461\tGrad Norm: 1.202200\tLR: 0.030000\n",
      "Train Epoch: 694 [147456/194182 (75%)]\tLoss: 0.394426\tGrad Norm: 1.194144\tLR: 0.030000\n",
      "Train Epoch: 694 [167936/194182 (85%)]\tLoss: 0.405434\tGrad Norm: 1.434653\tLR: 0.030000\n",
      "Train Epoch: 694 [188416/194182 (96%)]\tLoss: 0.395517\tGrad Norm: 1.273725\tLR: 0.030000\n",
      "Train set: Average loss: 0.3996\n",
      "Test set: Average loss: 0.2430, Average MAE: 0.3447\n",
      "Train Epoch: 695 [4096/194182 (2%)]\tLoss: 0.393944\tGrad Norm: 1.149454\tLR: 0.030000\n",
      "Train Epoch: 695 [24576/194182 (12%)]\tLoss: 0.395277\tGrad Norm: 1.026286\tLR: 0.030000\n",
      "Train Epoch: 695 [45056/194182 (23%)]\tLoss: 0.406298\tGrad Norm: 1.286547\tLR: 0.030000\n",
      "Train Epoch: 695 [65536/194182 (33%)]\tLoss: 0.395339\tGrad Norm: 1.170070\tLR: 0.030000\n",
      "Train Epoch: 695 [86016/194182 (44%)]\tLoss: 0.400185\tGrad Norm: 1.041481\tLR: 0.030000\n",
      "Train Epoch: 695 [106496/194182 (54%)]\tLoss: 0.398829\tGrad Norm: 1.120525\tLR: 0.030000\n",
      "Train Epoch: 695 [126976/194182 (65%)]\tLoss: 0.397124\tGrad Norm: 1.191145\tLR: 0.030000\n",
      "Train Epoch: 695 [147456/194182 (75%)]\tLoss: 0.397169\tGrad Norm: 1.107348\tLR: 0.030000\n",
      "Train Epoch: 695 [167936/194182 (85%)]\tLoss: 0.393351\tGrad Norm: 0.965430\tLR: 0.030000\n",
      "Train Epoch: 695 [188416/194182 (96%)]\tLoss: 0.402559\tGrad Norm: 1.301631\tLR: 0.030000\n",
      "Train set: Average loss: 0.3971\n",
      "Test set: Average loss: 0.2422, Average MAE: 0.3429\n",
      "Epoch 695: Mean reward = 0.059 +/- 0.055\n",
      "Train Epoch: 696 [4096/194182 (2%)]\tLoss: 0.400795\tGrad Norm: 1.068302\tLR: 0.030000\n",
      "Train Epoch: 696 [24576/194182 (12%)]\tLoss: 0.399001\tGrad Norm: 1.533943\tLR: 0.030000\n",
      "Train Epoch: 696 [45056/194182 (23%)]\tLoss: 0.395783\tGrad Norm: 1.297638\tLR: 0.030000\n",
      "Train Epoch: 696 [65536/194182 (33%)]\tLoss: 0.394597\tGrad Norm: 0.773838\tLR: 0.030000\n",
      "Train Epoch: 696 [86016/194182 (44%)]\tLoss: 0.398014\tGrad Norm: 1.051674\tLR: 0.030000\n",
      "Train Epoch: 696 [106496/194182 (54%)]\tLoss: 0.406110\tGrad Norm: 1.289897\tLR: 0.030000\n",
      "Train Epoch: 696 [126976/194182 (65%)]\tLoss: 0.390423\tGrad Norm: 1.039132\tLR: 0.030000\n",
      "Train Epoch: 696 [147456/194182 (75%)]\tLoss: 0.398364\tGrad Norm: 1.394332\tLR: 0.030000\n",
      "Train Epoch: 696 [167936/194182 (85%)]\tLoss: 0.401715\tGrad Norm: 1.177817\tLR: 0.030000\n",
      "Train Epoch: 696 [188416/194182 (96%)]\tLoss: 0.399161\tGrad Norm: 1.216101\tLR: 0.030000\n",
      "Train set: Average loss: 0.3978\n",
      "Test set: Average loss: 0.2377, Average MAE: 0.3335\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 697 [4096/194182 (2%)]\tLoss: 0.387857\tGrad Norm: 0.732159\tLR: 0.030000\n",
      "Train Epoch: 697 [24576/194182 (12%)]\tLoss: 0.399586\tGrad Norm: 1.231200\tLR: 0.030000\n",
      "Train Epoch: 697 [45056/194182 (23%)]\tLoss: 0.406352\tGrad Norm: 1.649705\tLR: 0.030000\n",
      "Train Epoch: 697 [65536/194182 (33%)]\tLoss: 0.397347\tGrad Norm: 1.461821\tLR: 0.030000\n",
      "Train Epoch: 697 [86016/194182 (44%)]\tLoss: 0.390029\tGrad Norm: 1.217878\tLR: 0.030000\n",
      "Train Epoch: 697 [106496/194182 (54%)]\tLoss: 0.404353\tGrad Norm: 1.364189\tLR: 0.030000\n",
      "Train Epoch: 697 [126976/194182 (65%)]\tLoss: 0.397939\tGrad Norm: 1.062706\tLR: 0.030000\n",
      "Train Epoch: 697 [147456/194182 (75%)]\tLoss: 0.396964\tGrad Norm: 1.442262\tLR: 0.030000\n",
      "Train Epoch: 697 [167936/194182 (85%)]\tLoss: 0.399400\tGrad Norm: 1.467096\tLR: 0.030000\n",
      "Train Epoch: 697 [188416/194182 (96%)]\tLoss: 0.397795\tGrad Norm: 0.949483\tLR: 0.030000\n",
      "Train set: Average loss: 0.3994\n",
      "Test set: Average loss: 0.2421, Average MAE: 0.3408\n",
      "Train Epoch: 698 [4096/194182 (2%)]\tLoss: 0.398199\tGrad Norm: 1.212045\tLR: 0.030000\n",
      "Train Epoch: 698 [24576/194182 (12%)]\tLoss: 0.401150\tGrad Norm: 1.230113\tLR: 0.030000\n",
      "Train Epoch: 698 [45056/194182 (23%)]\tLoss: 0.390680\tGrad Norm: 1.176174\tLR: 0.030000\n",
      "Train Epoch: 698 [65536/194182 (33%)]\tLoss: 0.393746\tGrad Norm: 1.065925\tLR: 0.030000\n",
      "Train Epoch: 698 [86016/194182 (44%)]\tLoss: 0.389021\tGrad Norm: 0.931179\tLR: 0.030000\n",
      "Train Epoch: 698 [106496/194182 (54%)]\tLoss: 0.396878\tGrad Norm: 1.031782\tLR: 0.030000\n",
      "Train Epoch: 698 [126976/194182 (65%)]\tLoss: 0.409914\tGrad Norm: 1.660315\tLR: 0.030000\n",
      "Train Epoch: 698 [147456/194182 (75%)]\tLoss: 0.390501\tGrad Norm: 1.147178\tLR: 0.030000\n",
      "Train Epoch: 698 [167936/194182 (85%)]\tLoss: 0.399057\tGrad Norm: 1.146980\tLR: 0.030000\n",
      "Train Epoch: 698 [188416/194182 (96%)]\tLoss: 0.390497\tGrad Norm: 1.071477\tLR: 0.030000\n",
      "Train set: Average loss: 0.3974\n",
      "Test set: Average loss: 0.2446, Average MAE: 0.3556\n",
      "Train Epoch: 699 [4096/194182 (2%)]\tLoss: 0.387569\tGrad Norm: 1.266984\tLR: 0.030000\n",
      "Train Epoch: 699 [24576/194182 (12%)]\tLoss: 0.396913\tGrad Norm: 1.320554\tLR: 0.030000\n",
      "Train Epoch: 699 [45056/194182 (23%)]\tLoss: 0.395254\tGrad Norm: 1.043640\tLR: 0.030000\n",
      "Train Epoch: 699 [65536/194182 (33%)]\tLoss: 0.404108\tGrad Norm: 1.299069\tLR: 0.030000\n",
      "Train Epoch: 699 [86016/194182 (44%)]\tLoss: 0.400742\tGrad Norm: 1.553278\tLR: 0.030000\n",
      "Train Epoch: 699 [106496/194182 (54%)]\tLoss: 0.398968\tGrad Norm: 1.220369\tLR: 0.030000\n",
      "Train Epoch: 699 [126976/194182 (65%)]\tLoss: 0.394660\tGrad Norm: 1.092582\tLR: 0.030000\n",
      "Train Epoch: 699 [147456/194182 (75%)]\tLoss: 0.384953\tGrad Norm: 1.008008\tLR: 0.030000\n",
      "Train Epoch: 699 [167936/194182 (85%)]\tLoss: 0.399902\tGrad Norm: 1.131695\tLR: 0.030000\n",
      "Train Epoch: 699 [188416/194182 (96%)]\tLoss: 0.406729\tGrad Norm: 1.221434\tLR: 0.030000\n",
      "Train set: Average loss: 0.3974\n",
      "Test set: Average loss: 0.2454, Average MAE: 0.3493\n",
      "Train Epoch: 700 [4096/194182 (2%)]\tLoss: 0.396173\tGrad Norm: 1.370825\tLR: 0.030000\n",
      "Train Epoch: 700 [24576/194182 (12%)]\tLoss: 0.397872\tGrad Norm: 1.210825\tLR: 0.030000\n",
      "Train Epoch: 700 [45056/194182 (23%)]\tLoss: 0.394365\tGrad Norm: 1.170767\tLR: 0.030000\n",
      "Train Epoch: 700 [65536/194182 (33%)]\tLoss: 0.399137\tGrad Norm: 1.354066\tLR: 0.030000\n",
      "Train Epoch: 700 [86016/194182 (44%)]\tLoss: 0.395607\tGrad Norm: 1.130005\tLR: 0.030000\n",
      "Train Epoch: 700 [106496/194182 (54%)]\tLoss: 0.389089\tGrad Norm: 1.116329\tLR: 0.030000\n",
      "Train Epoch: 700 [126976/194182 (65%)]\tLoss: 0.392317\tGrad Norm: 1.226589\tLR: 0.030000\n",
      "Train Epoch: 700 [147456/194182 (75%)]\tLoss: 0.403093\tGrad Norm: 1.148560\tLR: 0.030000\n",
      "Train Epoch: 700 [167936/194182 (85%)]\tLoss: 0.399944\tGrad Norm: 0.953832\tLR: 0.030000\n",
      "Train Epoch: 700 [188416/194182 (96%)]\tLoss: 0.393478\tGrad Norm: 1.238893\tLR: 0.030000\n",
      "Train set: Average loss: 0.3959\n",
      "Test set: Average loss: 0.2414, Average MAE: 0.3364\n",
      "Epoch 700: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 701 [4096/194182 (2%)]\tLoss: 0.397544\tGrad Norm: 1.303857\tLR: 0.030000\n",
      "Train Epoch: 701 [24576/194182 (12%)]\tLoss: 0.402547\tGrad Norm: 1.558905\tLR: 0.030000\n",
      "Train Epoch: 701 [45056/194182 (23%)]\tLoss: 0.388908\tGrad Norm: 1.200328\tLR: 0.030000\n",
      "Train Epoch: 701 [65536/194182 (33%)]\tLoss: 0.390196\tGrad Norm: 0.979998\tLR: 0.030000\n",
      "Train Epoch: 701 [86016/194182 (44%)]\tLoss: 0.404295\tGrad Norm: 1.489558\tLR: 0.030000\n",
      "Train Epoch: 701 [106496/194182 (54%)]\tLoss: 0.400545\tGrad Norm: 1.230077\tLR: 0.030000\n",
      "Train Epoch: 701 [126976/194182 (65%)]\tLoss: 0.398365\tGrad Norm: 0.921834\tLR: 0.030000\n",
      "Train Epoch: 701 [147456/194182 (75%)]\tLoss: 0.393487\tGrad Norm: 1.102195\tLR: 0.030000\n",
      "Train Epoch: 701 [167936/194182 (85%)]\tLoss: 0.395862\tGrad Norm: 1.256554\tLR: 0.030000\n",
      "Train Epoch: 701 [188416/194182 (96%)]\tLoss: 0.407705\tGrad Norm: 1.467628\tLR: 0.030000\n",
      "Train set: Average loss: 0.3964\n",
      "Test set: Average loss: 0.2458, Average MAE: 0.3508\n",
      "Train Epoch: 702 [4096/194182 (2%)]\tLoss: 0.399715\tGrad Norm: 1.340724\tLR: 0.030000\n",
      "Train Epoch: 702 [24576/194182 (12%)]\tLoss: 0.390269\tGrad Norm: 0.977288\tLR: 0.030000\n",
      "Train Epoch: 702 [45056/194182 (23%)]\tLoss: 0.393204\tGrad Norm: 1.084128\tLR: 0.030000\n",
      "Train Epoch: 702 [65536/194182 (33%)]\tLoss: 0.394319\tGrad Norm: 1.096868\tLR: 0.030000\n",
      "Train Epoch: 702 [86016/194182 (44%)]\tLoss: 0.393457\tGrad Norm: 1.138141\tLR: 0.030000\n",
      "Train Epoch: 702 [106496/194182 (54%)]\tLoss: 0.391849\tGrad Norm: 1.226026\tLR: 0.030000\n",
      "Train Epoch: 702 [126976/194182 (65%)]\tLoss: 0.390299\tGrad Norm: 1.082389\tLR: 0.030000\n",
      "Train Epoch: 702 [147456/194182 (75%)]\tLoss: 0.405906\tGrad Norm: 1.556961\tLR: 0.030000\n",
      "Train Epoch: 702 [167936/194182 (85%)]\tLoss: 0.404989\tGrad Norm: 1.584458\tLR: 0.030000\n",
      "Train Epoch: 702 [188416/194182 (96%)]\tLoss: 0.396486\tGrad Norm: 1.315294\tLR: 0.030000\n",
      "Train set: Average loss: 0.3964\n",
      "Test set: Average loss: 0.2388, Average MAE: 0.3348\n",
      "Train Epoch: 703 [4096/194182 (2%)]\tLoss: 0.391886\tGrad Norm: 0.838017\tLR: 0.030000\n",
      "Train Epoch: 703 [24576/194182 (12%)]\tLoss: 0.397363\tGrad Norm: 0.746731\tLR: 0.030000\n",
      "Train Epoch: 703 [45056/194182 (23%)]\tLoss: 0.390994\tGrad Norm: 0.973064\tLR: 0.030000\n",
      "Train Epoch: 703 [65536/194182 (33%)]\tLoss: 0.393804\tGrad Norm: 1.045586\tLR: 0.030000\n",
      "Train Epoch: 703 [86016/194182 (44%)]\tLoss: 0.393292\tGrad Norm: 1.436906\tLR: 0.030000\n",
      "Train Epoch: 703 [106496/194182 (54%)]\tLoss: 0.385367\tGrad Norm: 1.133210\tLR: 0.030000\n",
      "Train Epoch: 703 [126976/194182 (65%)]\tLoss: 0.395485\tGrad Norm: 1.065081\tLR: 0.030000\n",
      "Train Epoch: 703 [147456/194182 (75%)]\tLoss: 0.393472\tGrad Norm: 1.054211\tLR: 0.030000\n",
      "Train Epoch: 703 [167936/194182 (85%)]\tLoss: 0.389699\tGrad Norm: 0.951967\tLR: 0.030000\n",
      "Train Epoch: 703 [188416/194182 (96%)]\tLoss: 0.396044\tGrad Norm: 1.317464\tLR: 0.030000\n",
      "Train set: Average loss: 0.3933\n",
      "Test set: Average loss: 0.2434, Average MAE: 0.3565\n",
      "Train Epoch: 704 [4096/194182 (2%)]\tLoss: 0.390511\tGrad Norm: 1.407705\tLR: 0.030000\n",
      "Train Epoch: 704 [24576/194182 (12%)]\tLoss: 0.396657\tGrad Norm: 1.492749\tLR: 0.030000\n",
      "Train Epoch: 704 [45056/194182 (23%)]\tLoss: 0.398541\tGrad Norm: 1.268240\tLR: 0.030000\n",
      "Train Epoch: 704 [65536/194182 (33%)]\tLoss: 0.389282\tGrad Norm: 1.056555\tLR: 0.030000\n",
      "Train Epoch: 704 [86016/194182 (44%)]\tLoss: 0.396637\tGrad Norm: 0.992381\tLR: 0.030000\n",
      "Train Epoch: 704 [106496/194182 (54%)]\tLoss: 0.397464\tGrad Norm: 1.159517\tLR: 0.030000\n",
      "Train Epoch: 704 [126976/194182 (65%)]\tLoss: 0.399594\tGrad Norm: 1.427259\tLR: 0.030000\n",
      "Train Epoch: 704 [147456/194182 (75%)]\tLoss: 0.404461\tGrad Norm: 1.261638\tLR: 0.030000\n",
      "Train Epoch: 704 [167936/194182 (85%)]\tLoss: 0.393642\tGrad Norm: 1.116792\tLR: 0.030000\n",
      "Train Epoch: 704 [188416/194182 (96%)]\tLoss: 0.400544\tGrad Norm: 1.379885\tLR: 0.030000\n",
      "Train set: Average loss: 0.3960\n",
      "Test set: Average loss: 0.2497, Average MAE: 0.3630\n",
      "Train Epoch: 705 [4096/194182 (2%)]\tLoss: 0.394722\tGrad Norm: 1.596708\tLR: 0.030000\n",
      "Train Epoch: 705 [24576/194182 (12%)]\tLoss: 0.408220\tGrad Norm: 1.906383\tLR: 0.030000\n",
      "Train Epoch: 705 [45056/194182 (23%)]\tLoss: 0.399564\tGrad Norm: 1.474659\tLR: 0.030000\n",
      "Train Epoch: 705 [65536/194182 (33%)]\tLoss: 0.400603\tGrad Norm: 1.389806\tLR: 0.030000\n",
      "Train Epoch: 705 [86016/194182 (44%)]\tLoss: 0.398039\tGrad Norm: 1.167821\tLR: 0.030000\n",
      "Train Epoch: 705 [106496/194182 (54%)]\tLoss: 0.396443\tGrad Norm: 1.274375\tLR: 0.030000\n",
      "Train Epoch: 705 [126976/194182 (65%)]\tLoss: 0.384947\tGrad Norm: 1.076665\tLR: 0.030000\n",
      "Train Epoch: 705 [147456/194182 (75%)]\tLoss: 0.385850\tGrad Norm: 1.125992\tLR: 0.030000\n",
      "Train Epoch: 705 [167936/194182 (85%)]\tLoss: 0.393975\tGrad Norm: 1.282512\tLR: 0.030000\n",
      "Train Epoch: 705 [188416/194182 (96%)]\tLoss: 0.390799\tGrad Norm: 1.041817\tLR: 0.030000\n",
      "Train set: Average loss: 0.3974\n",
      "Test set: Average loss: 0.2434, Average MAE: 0.3420\n",
      "Epoch 705: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 706 [4096/194182 (2%)]\tLoss: 0.395384\tGrad Norm: 1.330753\tLR: 0.030000\n",
      "Train Epoch: 706 [24576/194182 (12%)]\tLoss: 0.393024\tGrad Norm: 1.219288\tLR: 0.030000\n",
      "Train Epoch: 706 [45056/194182 (23%)]\tLoss: 0.389486\tGrad Norm: 1.276051\tLR: 0.030000\n",
      "Train Epoch: 706 [65536/194182 (33%)]\tLoss: 0.400617\tGrad Norm: 1.230692\tLR: 0.030000\n",
      "Train Epoch: 706 [86016/194182 (44%)]\tLoss: 0.397354\tGrad Norm: 1.230348\tLR: 0.030000\n",
      "Train Epoch: 706 [106496/194182 (54%)]\tLoss: 0.396338\tGrad Norm: 1.333902\tLR: 0.030000\n",
      "Train Epoch: 706 [126976/194182 (65%)]\tLoss: 0.392653\tGrad Norm: 1.435075\tLR: 0.030000\n",
      "Train Epoch: 706 [147456/194182 (75%)]\tLoss: 0.396196\tGrad Norm: 1.731725\tLR: 0.030000\n",
      "Train Epoch: 706 [167936/194182 (85%)]\tLoss: 0.398478\tGrad Norm: 1.229142\tLR: 0.030000\n",
      "Train Epoch: 706 [188416/194182 (96%)]\tLoss: 0.395305\tGrad Norm: 1.141451\tLR: 0.030000\n",
      "Train set: Average loss: 0.3963\n",
      "Test set: Average loss: 0.2401, Average MAE: 0.3492\n",
      "Train Epoch: 707 [4096/194182 (2%)]\tLoss: 0.384629\tGrad Norm: 0.919239\tLR: 0.030000\n",
      "Train Epoch: 707 [24576/194182 (12%)]\tLoss: 0.400668\tGrad Norm: 1.186238\tLR: 0.030000\n",
      "Train Epoch: 707 [45056/194182 (23%)]\tLoss: 0.394225\tGrad Norm: 1.054260\tLR: 0.030000\n",
      "Train Epoch: 707 [65536/194182 (33%)]\tLoss: 0.389856\tGrad Norm: 0.813140\tLR: 0.030000\n",
      "Train Epoch: 707 [86016/194182 (44%)]\tLoss: 0.386674\tGrad Norm: 0.778260\tLR: 0.030000\n",
      "Train Epoch: 707 [106496/194182 (54%)]\tLoss: 0.396842\tGrad Norm: 1.236346\tLR: 0.030000\n",
      "Train Epoch: 707 [126976/194182 (65%)]\tLoss: 0.388328\tGrad Norm: 1.175696\tLR: 0.030000\n",
      "Train Epoch: 707 [147456/194182 (75%)]\tLoss: 0.396701\tGrad Norm: 1.372121\tLR: 0.030000\n",
      "Train Epoch: 707 [167936/194182 (85%)]\tLoss: 0.407686\tGrad Norm: 1.525729\tLR: 0.030000\n",
      "Train Epoch: 707 [188416/194182 (96%)]\tLoss: 0.407148\tGrad Norm: 1.384188\tLR: 0.030000\n",
      "Train set: Average loss: 0.3940\n",
      "Test set: Average loss: 0.2381, Average MAE: 0.3298\n",
      "Train Epoch: 708 [4096/194182 (2%)]\tLoss: 0.382913\tGrad Norm: 0.842738\tLR: 0.030000\n",
      "Train Epoch: 708 [24576/194182 (12%)]\tLoss: 0.389187\tGrad Norm: 0.548007\tLR: 0.030000\n",
      "Train Epoch: 708 [45056/194182 (23%)]\tLoss: 0.394686\tGrad Norm: 1.014227\tLR: 0.030000\n",
      "Train Epoch: 708 [65536/194182 (33%)]\tLoss: 0.401355\tGrad Norm: 1.496367\tLR: 0.030000\n",
      "Train Epoch: 708 [86016/194182 (44%)]\tLoss: 0.394065\tGrad Norm: 1.466918\tLR: 0.030000\n",
      "Train Epoch: 708 [106496/194182 (54%)]\tLoss: 0.398927\tGrad Norm: 1.040518\tLR: 0.030000\n",
      "Train Epoch: 708 [126976/194182 (65%)]\tLoss: 0.383453\tGrad Norm: 0.803906\tLR: 0.030000\n",
      "Train Epoch: 708 [147456/194182 (75%)]\tLoss: 0.384464\tGrad Norm: 0.849492\tLR: 0.030000\n",
      "Train Epoch: 708 [167936/194182 (85%)]\tLoss: 0.389866\tGrad Norm: 1.301419\tLR: 0.030000\n",
      "Train Epoch: 708 [188416/194182 (96%)]\tLoss: 0.391864\tGrad Norm: 1.293888\tLR: 0.030000\n",
      "Train set: Average loss: 0.3934\n",
      "Test set: Average loss: 0.2520, Average MAE: 0.3653\n",
      "Train Epoch: 709 [4096/194182 (2%)]\tLoss: 0.398849\tGrad Norm: 1.661238\tLR: 0.030000\n",
      "Train Epoch: 709 [24576/194182 (12%)]\tLoss: 0.399174\tGrad Norm: 1.374206\tLR: 0.030000\n",
      "Train Epoch: 709 [45056/194182 (23%)]\tLoss: 0.391947\tGrad Norm: 1.106236\tLR: 0.030000\n",
      "Train Epoch: 709 [65536/194182 (33%)]\tLoss: 0.383390\tGrad Norm: 0.907111\tLR: 0.030000\n",
      "Train Epoch: 709 [86016/194182 (44%)]\tLoss: 0.381164\tGrad Norm: 0.930414\tLR: 0.030000\n",
      "Train Epoch: 709 [106496/194182 (54%)]\tLoss: 0.394283\tGrad Norm: 1.157269\tLR: 0.030000\n",
      "Train Epoch: 709 [126976/194182 (65%)]\tLoss: 0.399720\tGrad Norm: 1.382255\tLR: 0.030000\n",
      "Train Epoch: 709 [147456/194182 (75%)]\tLoss: 0.391339\tGrad Norm: 1.258807\tLR: 0.030000\n",
      "Train Epoch: 709 [167936/194182 (85%)]\tLoss: 0.389263\tGrad Norm: 1.061096\tLR: 0.030000\n",
      "Train Epoch: 709 [188416/194182 (96%)]\tLoss: 0.389290\tGrad Norm: 0.731546\tLR: 0.030000\n",
      "Train set: Average loss: 0.3929\n",
      "Test set: Average loss: 0.2361, Average MAE: 0.3360\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 710 [4096/194182 (2%)]\tLoss: 0.384225\tGrad Norm: 0.655924\tLR: 0.030000\n",
      "Train Epoch: 710 [24576/194182 (12%)]\tLoss: 0.395422\tGrad Norm: 1.102575\tLR: 0.030000\n",
      "Train Epoch: 710 [45056/194182 (23%)]\tLoss: 0.400290\tGrad Norm: 1.594717\tLR: 0.030000\n",
      "Train Epoch: 710 [65536/194182 (33%)]\tLoss: 0.401772\tGrad Norm: 1.386949\tLR: 0.030000\n",
      "Train Epoch: 710 [86016/194182 (44%)]\tLoss: 0.397202\tGrad Norm: 1.426727\tLR: 0.030000\n",
      "Train Epoch: 710 [106496/194182 (54%)]\tLoss: 0.397285\tGrad Norm: 1.260492\tLR: 0.030000\n",
      "Train Epoch: 710 [126976/194182 (65%)]\tLoss: 0.394180\tGrad Norm: 1.071834\tLR: 0.030000\n",
      "Train Epoch: 710 [147456/194182 (75%)]\tLoss: 0.396375\tGrad Norm: 1.226919\tLR: 0.030000\n",
      "Train Epoch: 710 [167936/194182 (85%)]\tLoss: 0.387217\tGrad Norm: 1.327300\tLR: 0.030000\n",
      "Train Epoch: 710 [188416/194182 (96%)]\tLoss: 0.400283\tGrad Norm: 1.300986\tLR: 0.030000\n",
      "Train set: Average loss: 0.3949\n",
      "Test set: Average loss: 0.2468, Average MAE: 0.3583\n",
      "Epoch 710: Mean reward = 0.051 +/- 0.035\n",
      "Train Epoch: 711 [4096/194182 (2%)]\tLoss: 0.399307\tGrad Norm: 1.502908\tLR: 0.030000\n",
      "Train Epoch: 711 [24576/194182 (12%)]\tLoss: 0.398039\tGrad Norm: 1.129466\tLR: 0.030000\n",
      "Train Epoch: 711 [45056/194182 (23%)]\tLoss: 0.398332\tGrad Norm: 1.108573\tLR: 0.030000\n",
      "Train Epoch: 711 [65536/194182 (33%)]\tLoss: 0.397193\tGrad Norm: 1.273745\tLR: 0.030000\n",
      "Train Epoch: 711 [86016/194182 (44%)]\tLoss: 0.391928\tGrad Norm: 1.091030\tLR: 0.030000\n",
      "Train Epoch: 711 [106496/194182 (54%)]\tLoss: 0.396400\tGrad Norm: 1.400584\tLR: 0.030000\n",
      "Train Epoch: 711 [126976/194182 (65%)]\tLoss: 0.385636\tGrad Norm: 1.083960\tLR: 0.030000\n",
      "Train Epoch: 711 [147456/194182 (75%)]\tLoss: 0.396714\tGrad Norm: 1.118097\tLR: 0.030000\n",
      "Train Epoch: 711 [167936/194182 (85%)]\tLoss: 0.394229\tGrad Norm: 1.085460\tLR: 0.030000\n",
      "Train Epoch: 711 [188416/194182 (96%)]\tLoss: 0.391257\tGrad Norm: 1.091311\tLR: 0.030000\n",
      "Train set: Average loss: 0.3933\n",
      "Test set: Average loss: 0.2439, Average MAE: 0.3498\n",
      "Train Epoch: 712 [4096/194182 (2%)]\tLoss: 0.387110\tGrad Norm: 1.208178\tLR: 0.030000\n",
      "Train Epoch: 712 [24576/194182 (12%)]\tLoss: 0.395493\tGrad Norm: 1.067921\tLR: 0.030000\n",
      "Train Epoch: 712 [45056/194182 (23%)]\tLoss: 0.395620\tGrad Norm: 1.140833\tLR: 0.030000\n",
      "Train Epoch: 712 [65536/194182 (33%)]\tLoss: 0.396742\tGrad Norm: 1.140911\tLR: 0.030000\n",
      "Train Epoch: 712 [86016/194182 (44%)]\tLoss: 0.392163\tGrad Norm: 1.452634\tLR: 0.030000\n",
      "Train Epoch: 712 [106496/194182 (54%)]\tLoss: 0.402995\tGrad Norm: 1.551412\tLR: 0.030000\n",
      "Train Epoch: 712 [126976/194182 (65%)]\tLoss: 0.393220\tGrad Norm: 1.087615\tLR: 0.030000\n",
      "Train Epoch: 712 [147456/194182 (75%)]\tLoss: 0.389677\tGrad Norm: 1.343956\tLR: 0.030000\n",
      "Train Epoch: 712 [167936/194182 (85%)]\tLoss: 0.392532\tGrad Norm: 1.278568\tLR: 0.030000\n",
      "Train Epoch: 712 [188416/194182 (96%)]\tLoss: 0.392530\tGrad Norm: 1.231120\tLR: 0.030000\n",
      "Train set: Average loss: 0.3935\n",
      "Test set: Average loss: 0.2506, Average MAE: 0.3643\n",
      "Train Epoch: 713 [4096/194182 (2%)]\tLoss: 0.399046\tGrad Norm: 1.799582\tLR: 0.030000\n",
      "Train Epoch: 713 [24576/194182 (12%)]\tLoss: 0.398630\tGrad Norm: 1.441576\tLR: 0.030000\n",
      "Train Epoch: 713 [45056/194182 (23%)]\tLoss: 0.394377\tGrad Norm: 1.230642\tLR: 0.030000\n",
      "Train Epoch: 713 [65536/194182 (33%)]\tLoss: 0.386270\tGrad Norm: 1.062273\tLR: 0.030000\n",
      "Train Epoch: 713 [86016/194182 (44%)]\tLoss: 0.389222\tGrad Norm: 0.861439\tLR: 0.030000\n",
      "Train Epoch: 713 [106496/194182 (54%)]\tLoss: 0.382977\tGrad Norm: 0.727511\tLR: 0.030000\n",
      "Train Epoch: 713 [126976/194182 (65%)]\tLoss: 0.384012\tGrad Norm: 0.777129\tLR: 0.030000\n",
      "Train Epoch: 713 [147456/194182 (75%)]\tLoss: 0.398193\tGrad Norm: 1.254080\tLR: 0.030000\n",
      "Train Epoch: 713 [167936/194182 (85%)]\tLoss: 0.390366\tGrad Norm: 1.335460\tLR: 0.030000\n",
      "Train Epoch: 713 [188416/194182 (96%)]\tLoss: 0.388208\tGrad Norm: 1.267984\tLR: 0.030000\n",
      "Train set: Average loss: 0.3919\n",
      "Test set: Average loss: 0.2453, Average MAE: 0.3461\n",
      "Train Epoch: 714 [4096/194182 (2%)]\tLoss: 0.391350\tGrad Norm: 1.543765\tLR: 0.030000\n",
      "Train Epoch: 714 [24576/194182 (12%)]\tLoss: 0.395034\tGrad Norm: 1.101432\tLR: 0.030000\n",
      "Train Epoch: 714 [45056/194182 (23%)]\tLoss: 0.398772\tGrad Norm: 1.235337\tLR: 0.030000\n",
      "Train Epoch: 714 [65536/194182 (33%)]\tLoss: 0.393055\tGrad Norm: 1.196494\tLR: 0.030000\n",
      "Train Epoch: 714 [86016/194182 (44%)]\tLoss: 0.395493\tGrad Norm: 1.294404\tLR: 0.030000\n",
      "Train Epoch: 714 [106496/194182 (54%)]\tLoss: 0.390236\tGrad Norm: 1.010530\tLR: 0.030000\n",
      "Train Epoch: 714 [126976/194182 (65%)]\tLoss: 0.386109\tGrad Norm: 1.071880\tLR: 0.030000\n",
      "Train Epoch: 714 [147456/194182 (75%)]\tLoss: 0.396794\tGrad Norm: 1.433401\tLR: 0.030000\n",
      "Train Epoch: 714 [167936/194182 (85%)]\tLoss: 0.397142\tGrad Norm: 1.564754\tLR: 0.030000\n",
      "Train Epoch: 714 [188416/194182 (96%)]\tLoss: 0.391040\tGrad Norm: 1.169324\tLR: 0.030000\n",
      "Train set: Average loss: 0.3930\n",
      "Test set: Average loss: 0.2449, Average MAE: 0.3552\n",
      "Train Epoch: 715 [4096/194182 (2%)]\tLoss: 0.388676\tGrad Norm: 1.350748\tLR: 0.030000\n",
      "Train Epoch: 715 [24576/194182 (12%)]\tLoss: 0.394814\tGrad Norm: 1.165303\tLR: 0.030000\n",
      "Train Epoch: 715 [45056/194182 (23%)]\tLoss: 0.393483\tGrad Norm: 1.172951\tLR: 0.030000\n",
      "Train Epoch: 715 [65536/194182 (33%)]\tLoss: 0.382176\tGrad Norm: 0.980005\tLR: 0.030000\n",
      "Train Epoch: 715 [86016/194182 (44%)]\tLoss: 0.391028\tGrad Norm: 1.060647\tLR: 0.030000\n",
      "Train Epoch: 715 [106496/194182 (54%)]\tLoss: 0.402664\tGrad Norm: 1.447172\tLR: 0.030000\n",
      "Train Epoch: 715 [126976/194182 (65%)]\tLoss: 0.398261\tGrad Norm: 1.394092\tLR: 0.030000\n",
      "Train Epoch: 715 [147456/194182 (75%)]\tLoss: 0.383052\tGrad Norm: 1.174868\tLR: 0.030000\n",
      "Train Epoch: 715 [167936/194182 (85%)]\tLoss: 0.392995\tGrad Norm: 1.211208\tLR: 0.030000\n",
      "Train Epoch: 715 [188416/194182 (96%)]\tLoss: 0.398711\tGrad Norm: 1.263371\tLR: 0.030000\n",
      "Train set: Average loss: 0.3926\n",
      "Test set: Average loss: 0.2374, Average MAE: 0.3463\n",
      "Epoch 715: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 716 [4096/194182 (2%)]\tLoss: 0.383364\tGrad Norm: 0.779555\tLR: 0.030000\n",
      "Train Epoch: 716 [24576/194182 (12%)]\tLoss: 0.390796\tGrad Norm: 1.125150\tLR: 0.030000\n",
      "Train Epoch: 716 [45056/194182 (23%)]\tLoss: 0.396918\tGrad Norm: 1.400754\tLR: 0.030000\n",
      "Train Epoch: 716 [65536/194182 (33%)]\tLoss: 0.391232\tGrad Norm: 1.470276\tLR: 0.030000\n",
      "Train Epoch: 716 [86016/194182 (44%)]\tLoss: 0.394557\tGrad Norm: 1.168236\tLR: 0.030000\n",
      "Train Epoch: 716 [106496/194182 (54%)]\tLoss: 0.399906\tGrad Norm: 1.234305\tLR: 0.030000\n",
      "Train Epoch: 716 [126976/194182 (65%)]\tLoss: 0.391815\tGrad Norm: 1.098468\tLR: 0.030000\n",
      "Train Epoch: 716 [147456/194182 (75%)]\tLoss: 0.397591\tGrad Norm: 1.275296\tLR: 0.030000\n",
      "Train Epoch: 716 [167936/194182 (85%)]\tLoss: 0.381279\tGrad Norm: 1.121012\tLR: 0.030000\n",
      "Train Epoch: 716 [188416/194182 (96%)]\tLoss: 0.402611\tGrad Norm: 1.429693\tLR: 0.030000\n",
      "Train set: Average loss: 0.3919\n",
      "Test set: Average loss: 0.2462, Average MAE: 0.3620\n",
      "Train Epoch: 717 [4096/194182 (2%)]\tLoss: 0.393881\tGrad Norm: 1.520203\tLR: 0.030000\n",
      "Train Epoch: 717 [24576/194182 (12%)]\tLoss: 0.401492\tGrad Norm: 1.319762\tLR: 0.030000\n",
      "Train Epoch: 717 [45056/194182 (23%)]\tLoss: 0.387649\tGrad Norm: 0.909198\tLR: 0.030000\n",
      "Train Epoch: 717 [65536/194182 (33%)]\tLoss: 0.387898\tGrad Norm: 1.042199\tLR: 0.030000\n",
      "Train Epoch: 717 [86016/194182 (44%)]\tLoss: 0.390167\tGrad Norm: 1.042598\tLR: 0.030000\n",
      "Train Epoch: 717 [106496/194182 (54%)]\tLoss: 0.384410\tGrad Norm: 1.149113\tLR: 0.030000\n",
      "Train Epoch: 717 [126976/194182 (65%)]\tLoss: 0.384229\tGrad Norm: 1.292613\tLR: 0.030000\n",
      "Train Epoch: 717 [147456/194182 (75%)]\tLoss: 0.380950\tGrad Norm: 0.972226\tLR: 0.030000\n",
      "Train Epoch: 717 [167936/194182 (85%)]\tLoss: 0.393807\tGrad Norm: 1.236701\tLR: 0.030000\n",
      "Train Epoch: 717 [188416/194182 (96%)]\tLoss: 0.386765\tGrad Norm: 1.371571\tLR: 0.030000\n",
      "Train set: Average loss: 0.3902\n",
      "Test set: Average loss: 0.2396, Average MAE: 0.3327\n",
      "Train Epoch: 718 [4096/194182 (2%)]\tLoss: 0.389214\tGrad Norm: 1.035229\tLR: 0.030000\n",
      "Train Epoch: 718 [24576/194182 (12%)]\tLoss: 0.384624\tGrad Norm: 1.181283\tLR: 0.030000\n",
      "Train Epoch: 718 [45056/194182 (23%)]\tLoss: 0.395185\tGrad Norm: 1.348644\tLR: 0.030000\n",
      "Train Epoch: 718 [65536/194182 (33%)]\tLoss: 0.398596\tGrad Norm: 1.329887\tLR: 0.030000\n",
      "Train Epoch: 718 [86016/194182 (44%)]\tLoss: 0.396234\tGrad Norm: 1.357815\tLR: 0.030000\n",
      "Train Epoch: 718 [106496/194182 (54%)]\tLoss: 0.393872\tGrad Norm: 1.562501\tLR: 0.030000\n",
      "Train Epoch: 718 [126976/194182 (65%)]\tLoss: 0.396045\tGrad Norm: 1.479871\tLR: 0.030000\n",
      "Train Epoch: 718 [147456/194182 (75%)]\tLoss: 0.388412\tGrad Norm: 1.236485\tLR: 0.030000\n",
      "Train Epoch: 718 [167936/194182 (85%)]\tLoss: 0.387805\tGrad Norm: 0.961951\tLR: 0.030000\n",
      "Train Epoch: 718 [188416/194182 (96%)]\tLoss: 0.396082\tGrad Norm: 1.263804\tLR: 0.030000\n",
      "Train set: Average loss: 0.3919\n",
      "Test set: Average loss: 0.2414, Average MAE: 0.3518\n",
      "Train Epoch: 719 [4096/194182 (2%)]\tLoss: 0.382759\tGrad Norm: 1.104908\tLR: 0.030000\n",
      "Train Epoch: 719 [24576/194182 (12%)]\tLoss: 0.384603\tGrad Norm: 1.050907\tLR: 0.030000\n",
      "Train Epoch: 719 [45056/194182 (23%)]\tLoss: 0.393442\tGrad Norm: 1.236083\tLR: 0.030000\n",
      "Train Epoch: 719 [65536/194182 (33%)]\tLoss: 0.387471\tGrad Norm: 1.165952\tLR: 0.030000\n",
      "Train Epoch: 719 [86016/194182 (44%)]\tLoss: 0.385389\tGrad Norm: 1.214248\tLR: 0.030000\n",
      "Train Epoch: 719 [106496/194182 (54%)]\tLoss: 0.395873\tGrad Norm: 1.178741\tLR: 0.030000\n",
      "Train Epoch: 719 [126976/194182 (65%)]\tLoss: 0.393011\tGrad Norm: 1.308995\tLR: 0.030000\n",
      "Train Epoch: 719 [147456/194182 (75%)]\tLoss: 0.382023\tGrad Norm: 1.068800\tLR: 0.030000\n",
      "Train Epoch: 719 [167936/194182 (85%)]\tLoss: 0.396836\tGrad Norm: 1.319493\tLR: 0.030000\n",
      "Train Epoch: 719 [188416/194182 (96%)]\tLoss: 0.390121\tGrad Norm: 1.463219\tLR: 0.030000\n",
      "Train set: Average loss: 0.3913\n",
      "Test set: Average loss: 0.2417, Average MAE: 0.3353\n",
      "Train Epoch: 720 [4096/194182 (2%)]\tLoss: 0.394651\tGrad Norm: 1.416545\tLR: 0.030000\n",
      "Train Epoch: 720 [24576/194182 (12%)]\tLoss: 0.388494\tGrad Norm: 1.088436\tLR: 0.030000\n",
      "Train Epoch: 720 [45056/194182 (23%)]\tLoss: 0.384662\tGrad Norm: 0.779523\tLR: 0.030000\n",
      "Train Epoch: 720 [65536/194182 (33%)]\tLoss: 0.384532\tGrad Norm: 0.829680\tLR: 0.030000\n",
      "Train Epoch: 720 [86016/194182 (44%)]\tLoss: 0.378982\tGrad Norm: 1.043113\tLR: 0.030000\n",
      "Train Epoch: 720 [106496/194182 (54%)]\tLoss: 0.393227\tGrad Norm: 1.225154\tLR: 0.030000\n",
      "Train Epoch: 720 [126976/194182 (65%)]\tLoss: 0.390757\tGrad Norm: 1.221162\tLR: 0.030000\n",
      "Train Epoch: 720 [147456/194182 (75%)]\tLoss: 0.385794\tGrad Norm: 1.066468\tLR: 0.030000\n",
      "Train Epoch: 720 [167936/194182 (85%)]\tLoss: 0.393038\tGrad Norm: 1.307131\tLR: 0.030000\n",
      "Train Epoch: 720 [188416/194182 (96%)]\tLoss: 0.399554\tGrad Norm: 1.587564\tLR: 0.030000\n",
      "Train set: Average loss: 0.3903\n",
      "Test set: Average loss: 0.2529, Average MAE: 0.3668\n",
      "Epoch 720: Mean reward = 0.042 +/- 0.014\n",
      "Train Epoch: 721 [4096/194182 (2%)]\tLoss: 0.399841\tGrad Norm: 1.752744\tLR: 0.030000\n",
      "Train Epoch: 721 [24576/194182 (12%)]\tLoss: 0.393902\tGrad Norm: 1.455134\tLR: 0.030000\n",
      "Train Epoch: 721 [45056/194182 (23%)]\tLoss: 0.395365\tGrad Norm: 1.416137\tLR: 0.030000\n",
      "Train Epoch: 721 [65536/194182 (33%)]\tLoss: 0.389999\tGrad Norm: 1.222370\tLR: 0.030000\n",
      "Train Epoch: 721 [86016/194182 (44%)]\tLoss: 0.385260\tGrad Norm: 0.956893\tLR: 0.030000\n",
      "Train Epoch: 721 [106496/194182 (54%)]\tLoss: 0.384438\tGrad Norm: 1.109527\tLR: 0.030000\n",
      "Train Epoch: 721 [126976/194182 (65%)]\tLoss: 0.384107\tGrad Norm: 1.323196\tLR: 0.030000\n",
      "Train Epoch: 721 [147456/194182 (75%)]\tLoss: 0.384294\tGrad Norm: 1.167743\tLR: 0.030000\n",
      "Train Epoch: 721 [167936/194182 (85%)]\tLoss: 0.377150\tGrad Norm: 1.097491\tLR: 0.030000\n",
      "Train Epoch: 721 [188416/194182 (96%)]\tLoss: 0.384807\tGrad Norm: 1.030528\tLR: 0.030000\n",
      "Train set: Average loss: 0.3910\n",
      "Test set: Average loss: 0.2403, Average MAE: 0.3505\n",
      "Train Epoch: 722 [4096/194182 (2%)]\tLoss: 0.388220\tGrad Norm: 1.095731\tLR: 0.030000\n",
      "Train Epoch: 722 [24576/194182 (12%)]\tLoss: 0.382489\tGrad Norm: 0.954956\tLR: 0.030000\n",
      "Train Epoch: 722 [45056/194182 (23%)]\tLoss: 0.387926\tGrad Norm: 0.926501\tLR: 0.030000\n",
      "Train Epoch: 722 [65536/194182 (33%)]\tLoss: 0.388699\tGrad Norm: 1.017664\tLR: 0.030000\n",
      "Train Epoch: 722 [86016/194182 (44%)]\tLoss: 0.387371\tGrad Norm: 0.990285\tLR: 0.030000\n",
      "Train Epoch: 722 [106496/194182 (54%)]\tLoss: 0.391056\tGrad Norm: 1.071122\tLR: 0.030000\n",
      "Train Epoch: 722 [126976/194182 (65%)]\tLoss: 0.392343\tGrad Norm: 1.113085\tLR: 0.030000\n",
      "Train Epoch: 722 [147456/194182 (75%)]\tLoss: 0.393905\tGrad Norm: 1.416134\tLR: 0.030000\n",
      "Train Epoch: 722 [167936/194182 (85%)]\tLoss: 0.393445\tGrad Norm: 1.575004\tLR: 0.030000\n",
      "Train Epoch: 722 [188416/194182 (96%)]\tLoss: 0.386758\tGrad Norm: 1.160196\tLR: 0.030000\n",
      "Train set: Average loss: 0.3888\n",
      "Test set: Average loss: 0.2383, Average MAE: 0.3412\n",
      "Train Epoch: 723 [4096/194182 (2%)]\tLoss: 0.380252\tGrad Norm: 0.844185\tLR: 0.030000\n",
      "Train Epoch: 723 [24576/194182 (12%)]\tLoss: 0.392641\tGrad Norm: 1.247892\tLR: 0.030000\n",
      "Train Epoch: 723 [45056/194182 (23%)]\tLoss: 0.388438\tGrad Norm: 0.901633\tLR: 0.030000\n",
      "Train Epoch: 723 [65536/194182 (33%)]\tLoss: 0.389652\tGrad Norm: 1.133777\tLR: 0.030000\n",
      "Train Epoch: 723 [86016/194182 (44%)]\tLoss: 0.401899\tGrad Norm: 1.641307\tLR: 0.030000\n",
      "Train Epoch: 723 [106496/194182 (54%)]\tLoss: 0.389610\tGrad Norm: 1.722550\tLR: 0.030000\n",
      "Train Epoch: 723 [126976/194182 (65%)]\tLoss: 0.392952\tGrad Norm: 1.215723\tLR: 0.030000\n",
      "Train Epoch: 723 [147456/194182 (75%)]\tLoss: 0.385915\tGrad Norm: 1.277088\tLR: 0.030000\n",
      "Train Epoch: 723 [167936/194182 (85%)]\tLoss: 0.384604\tGrad Norm: 0.908048\tLR: 0.030000\n",
      "Train Epoch: 723 [188416/194182 (96%)]\tLoss: 0.380352\tGrad Norm: 1.061654\tLR: 0.030000\n",
      "Train set: Average loss: 0.3897\n",
      "Test set: Average loss: 0.2413, Average MAE: 0.3332\n",
      "Train Epoch: 724 [4096/194182 (2%)]\tLoss: 0.379957\tGrad Norm: 1.112727\tLR: 0.030000\n",
      "Train Epoch: 724 [24576/194182 (12%)]\tLoss: 0.393775\tGrad Norm: 1.486527\tLR: 0.030000\n",
      "Train Epoch: 724 [45056/194182 (23%)]\tLoss: 0.391236\tGrad Norm: 1.290889\tLR: 0.030000\n",
      "Train Epoch: 724 [65536/194182 (33%)]\tLoss: 0.384215\tGrad Norm: 1.243226\tLR: 0.030000\n",
      "Train Epoch: 724 [86016/194182 (44%)]\tLoss: 0.382808\tGrad Norm: 1.048221\tLR: 0.030000\n",
      "Train Epoch: 724 [106496/194182 (54%)]\tLoss: 0.387344\tGrad Norm: 1.176431\tLR: 0.030000\n",
      "Train Epoch: 724 [126976/194182 (65%)]\tLoss: 0.403569\tGrad Norm: 1.733789\tLR: 0.030000\n",
      "Train Epoch: 724 [147456/194182 (75%)]\tLoss: 0.387165\tGrad Norm: 1.168464\tLR: 0.030000\n",
      "Train Epoch: 724 [167936/194182 (85%)]\tLoss: 0.395109\tGrad Norm: 1.247304\tLR: 0.030000\n",
      "Train Epoch: 724 [188416/194182 (96%)]\tLoss: 0.394639\tGrad Norm: 1.294780\tLR: 0.030000\n",
      "Train set: Average loss: 0.3907\n",
      "Test set: Average loss: 0.2405, Average MAE: 0.3299\n",
      "Train Epoch: 725 [4096/194182 (2%)]\tLoss: 0.393322\tGrad Norm: 1.253411\tLR: 0.030000\n",
      "Train Epoch: 725 [24576/194182 (12%)]\tLoss: 0.404871\tGrad Norm: 1.346607\tLR: 0.030000\n",
      "Train Epoch: 725 [45056/194182 (23%)]\tLoss: 0.391694\tGrad Norm: 1.270090\tLR: 0.030000\n",
      "Train Epoch: 725 [65536/194182 (33%)]\tLoss: 0.391175\tGrad Norm: 1.686162\tLR: 0.030000\n",
      "Train Epoch: 725 [86016/194182 (44%)]\tLoss: 0.391791\tGrad Norm: 1.392398\tLR: 0.030000\n",
      "Train Epoch: 725 [106496/194182 (54%)]\tLoss: 0.385923\tGrad Norm: 1.158219\tLR: 0.030000\n",
      "Train Epoch: 725 [126976/194182 (65%)]\tLoss: 0.392456\tGrad Norm: 1.217142\tLR: 0.030000\n",
      "Train Epoch: 725 [147456/194182 (75%)]\tLoss: 0.392745\tGrad Norm: 1.171440\tLR: 0.030000\n",
      "Train Epoch: 725 [167936/194182 (85%)]\tLoss: 0.379929\tGrad Norm: 1.062394\tLR: 0.030000\n",
      "Train Epoch: 725 [188416/194182 (96%)]\tLoss: 0.383244\tGrad Norm: 0.919022\tLR: 0.030000\n",
      "Train set: Average loss: 0.3899\n",
      "Test set: Average loss: 0.2415, Average MAE: 0.3385\n",
      "Epoch 725: Mean reward = 0.037 +/- 0.019\n",
      "Train Epoch: 726 [4096/194182 (2%)]\tLoss: 0.379797\tGrad Norm: 1.159225\tLR: 0.030000\n",
      "Train Epoch: 726 [24576/194182 (12%)]\tLoss: 0.389667\tGrad Norm: 1.471119\tLR: 0.030000\n",
      "Train Epoch: 726 [45056/194182 (23%)]\tLoss: 0.382201\tGrad Norm: 1.284331\tLR: 0.030000\n",
      "Train Epoch: 726 [65536/194182 (33%)]\tLoss: 0.389758\tGrad Norm: 1.106498\tLR: 0.030000\n",
      "Train Epoch: 726 [86016/194182 (44%)]\tLoss: 0.390498\tGrad Norm: 1.322312\tLR: 0.030000\n",
      "Train Epoch: 726 [106496/194182 (54%)]\tLoss: 0.389329\tGrad Norm: 1.440940\tLR: 0.030000\n",
      "Train Epoch: 726 [126976/194182 (65%)]\tLoss: 0.382966\tGrad Norm: 1.143999\tLR: 0.030000\n",
      "Train Epoch: 726 [147456/194182 (75%)]\tLoss: 0.391475\tGrad Norm: 1.246359\tLR: 0.030000\n",
      "Train Epoch: 726 [167936/194182 (85%)]\tLoss: 0.389534\tGrad Norm: 1.387727\tLR: 0.030000\n",
      "Train Epoch: 726 [188416/194182 (96%)]\tLoss: 0.386858\tGrad Norm: 1.432816\tLR: 0.030000\n",
      "Train set: Average loss: 0.3900\n",
      "Test set: Average loss: 0.2456, Average MAE: 0.3570\n",
      "Train Epoch: 727 [4096/194182 (2%)]\tLoss: 0.392748\tGrad Norm: 1.364318\tLR: 0.030000\n",
      "Train Epoch: 727 [24576/194182 (12%)]\tLoss: 0.392778\tGrad Norm: 1.286460\tLR: 0.030000\n",
      "Train Epoch: 727 [45056/194182 (23%)]\tLoss: 0.390009\tGrad Norm: 1.211792\tLR: 0.030000\n",
      "Train Epoch: 727 [65536/194182 (33%)]\tLoss: 0.393043\tGrad Norm: 1.094905\tLR: 0.030000\n",
      "Train Epoch: 727 [86016/194182 (44%)]\tLoss: 0.388234\tGrad Norm: 1.262322\tLR: 0.030000\n",
      "Train Epoch: 727 [106496/194182 (54%)]\tLoss: 0.389645\tGrad Norm: 1.025945\tLR: 0.030000\n",
      "Train Epoch: 727 [126976/194182 (65%)]\tLoss: 0.382999\tGrad Norm: 1.091543\tLR: 0.030000\n",
      "Train Epoch: 727 [147456/194182 (75%)]\tLoss: 0.386731\tGrad Norm: 1.124877\tLR: 0.030000\n",
      "Train Epoch: 727 [167936/194182 (85%)]\tLoss: 0.377366\tGrad Norm: 1.205912\tLR: 0.030000\n",
      "Train Epoch: 727 [188416/194182 (96%)]\tLoss: 0.378511\tGrad Norm: 1.145702\tLR: 0.030000\n",
      "Train set: Average loss: 0.3882\n",
      "Test set: Average loss: 0.2428, Average MAE: 0.3372\n",
      "Train Epoch: 728 [4096/194182 (2%)]\tLoss: 0.394452\tGrad Norm: 1.291074\tLR: 0.030000\n",
      "Train Epoch: 728 [24576/194182 (12%)]\tLoss: 0.393900\tGrad Norm: 1.463751\tLR: 0.030000\n",
      "Train Epoch: 728 [45056/194182 (23%)]\tLoss: 0.379750\tGrad Norm: 1.091319\tLR: 0.030000\n",
      "Train Epoch: 728 [65536/194182 (33%)]\tLoss: 0.387533\tGrad Norm: 1.274983\tLR: 0.030000\n",
      "Train Epoch: 728 [86016/194182 (44%)]\tLoss: 0.390326\tGrad Norm: 1.213452\tLR: 0.030000\n",
      "Train Epoch: 728 [106496/194182 (54%)]\tLoss: 0.384432\tGrad Norm: 0.915757\tLR: 0.030000\n",
      "Train Epoch: 728 [126976/194182 (65%)]\tLoss: 0.385130\tGrad Norm: 1.332228\tLR: 0.030000\n",
      "Train Epoch: 728 [147456/194182 (75%)]\tLoss: 0.380606\tGrad Norm: 1.157256\tLR: 0.030000\n",
      "Train Epoch: 728 [167936/194182 (85%)]\tLoss: 0.391886\tGrad Norm: 1.479287\tLR: 0.030000\n",
      "Train Epoch: 728 [188416/194182 (96%)]\tLoss: 0.392688\tGrad Norm: 1.576157\tLR: 0.030000\n",
      "Train set: Average loss: 0.3896\n",
      "Test set: Average loss: 0.2429, Average MAE: 0.3319\n",
      "Train Epoch: 729 [4096/194182 (2%)]\tLoss: 0.382567\tGrad Norm: 1.195164\tLR: 0.030000\n",
      "Train Epoch: 729 [24576/194182 (12%)]\tLoss: 0.388152\tGrad Norm: 0.902291\tLR: 0.030000\n",
      "Train Epoch: 729 [45056/194182 (23%)]\tLoss: 0.382885\tGrad Norm: 1.047712\tLR: 0.030000\n",
      "Train Epoch: 729 [65536/194182 (33%)]\tLoss: 0.379168\tGrad Norm: 1.111659\tLR: 0.030000\n",
      "Train Epoch: 729 [86016/194182 (44%)]\tLoss: 0.389747\tGrad Norm: 1.131482\tLR: 0.030000\n",
      "Train Epoch: 729 [106496/194182 (54%)]\tLoss: 0.392047\tGrad Norm: 1.241342\tLR: 0.030000\n",
      "Train Epoch: 729 [126976/194182 (65%)]\tLoss: 0.382102\tGrad Norm: 1.211771\tLR: 0.030000\n",
      "Train Epoch: 729 [147456/194182 (75%)]\tLoss: 0.387988\tGrad Norm: 1.136070\tLR: 0.030000\n",
      "Train Epoch: 729 [167936/194182 (85%)]\tLoss: 0.388644\tGrad Norm: 1.158691\tLR: 0.030000\n",
      "Train Epoch: 729 [188416/194182 (96%)]\tLoss: 0.393231\tGrad Norm: 1.784940\tLR: 0.030000\n",
      "Train set: Average loss: 0.3877\n",
      "Test set: Average loss: 0.2462, Average MAE: 0.3585\n",
      "Train Epoch: 730 [4096/194182 (2%)]\tLoss: 0.384479\tGrad Norm: 1.258553\tLR: 0.030000\n",
      "Train Epoch: 730 [24576/194182 (12%)]\tLoss: 0.393086\tGrad Norm: 1.152437\tLR: 0.030000\n",
      "Train Epoch: 730 [45056/194182 (23%)]\tLoss: 0.385845\tGrad Norm: 1.081597\tLR: 0.030000\n",
      "Train Epoch: 730 [65536/194182 (33%)]\tLoss: 0.382470\tGrad Norm: 1.218159\tLR: 0.030000\n",
      "Train Epoch: 730 [86016/194182 (44%)]\tLoss: 0.391179\tGrad Norm: 0.880049\tLR: 0.030000\n",
      "Train Epoch: 730 [106496/194182 (54%)]\tLoss: 0.383359\tGrad Norm: 0.826036\tLR: 0.030000\n",
      "Train Epoch: 730 [126976/194182 (65%)]\tLoss: 0.386183\tGrad Norm: 1.274781\tLR: 0.030000\n",
      "Train Epoch: 730 [147456/194182 (75%)]\tLoss: 0.394072\tGrad Norm: 1.265482\tLR: 0.030000\n",
      "Train Epoch: 730 [167936/194182 (85%)]\tLoss: 0.393794\tGrad Norm: 1.172518\tLR: 0.030000\n",
      "Train Epoch: 730 [188416/194182 (96%)]\tLoss: 0.392217\tGrad Norm: 1.256838\tLR: 0.030000\n",
      "Train set: Average loss: 0.3870\n",
      "Test set: Average loss: 0.2409, Average MAE: 0.3395\n",
      "Epoch 730: Mean reward = 0.062 +/- 0.048\n",
      "Train Epoch: 731 [4096/194182 (2%)]\tLoss: 0.389593\tGrad Norm: 1.144510\tLR: 0.030000\n",
      "Train Epoch: 731 [24576/194182 (12%)]\tLoss: 0.381589\tGrad Norm: 1.182552\tLR: 0.030000\n",
      "Train Epoch: 731 [45056/194182 (23%)]\tLoss: 0.383785\tGrad Norm: 1.542674\tLR: 0.030000\n",
      "Train Epoch: 731 [65536/194182 (33%)]\tLoss: 0.381705\tGrad Norm: 0.905089\tLR: 0.030000\n",
      "Train Epoch: 731 [86016/194182 (44%)]\tLoss: 0.376879\tGrad Norm: 1.121152\tLR: 0.030000\n",
      "Train Epoch: 731 [106496/194182 (54%)]\tLoss: 0.390121\tGrad Norm: 1.353612\tLR: 0.030000\n",
      "Train Epoch: 731 [126976/194182 (65%)]\tLoss: 0.386868\tGrad Norm: 1.038536\tLR: 0.030000\n",
      "Train Epoch: 731 [147456/194182 (75%)]\tLoss: 0.379335\tGrad Norm: 0.740605\tLR: 0.030000\n",
      "Train Epoch: 731 [167936/194182 (85%)]\tLoss: 0.381937\tGrad Norm: 1.106133\tLR: 0.030000\n",
      "Train Epoch: 731 [188416/194182 (96%)]\tLoss: 0.388849\tGrad Norm: 1.099317\tLR: 0.030000\n",
      "Train set: Average loss: 0.3859\n",
      "Test set: Average loss: 0.2423, Average MAE: 0.3406\n",
      "Train Epoch: 732 [4096/194182 (2%)]\tLoss: 0.391041\tGrad Norm: 1.228610\tLR: 0.030000\n",
      "Train Epoch: 732 [24576/194182 (12%)]\tLoss: 0.384568\tGrad Norm: 1.194068\tLR: 0.030000\n",
      "Train Epoch: 732 [45056/194182 (23%)]\tLoss: 0.383100\tGrad Norm: 1.399013\tLR: 0.030000\n",
      "Train Epoch: 732 [65536/194182 (33%)]\tLoss: 0.395132\tGrad Norm: 1.523542\tLR: 0.030000\n",
      "Train Epoch: 732 [86016/194182 (44%)]\tLoss: 0.383713\tGrad Norm: 1.154870\tLR: 0.030000\n",
      "Train Epoch: 732 [106496/194182 (54%)]\tLoss: 0.384626\tGrad Norm: 1.089376\tLR: 0.030000\n",
      "Train Epoch: 732 [126976/194182 (65%)]\tLoss: 0.384817\tGrad Norm: 1.153089\tLR: 0.030000\n",
      "Train Epoch: 732 [147456/194182 (75%)]\tLoss: 0.392226\tGrad Norm: 1.614098\tLR: 0.030000\n",
      "Train Epoch: 732 [167936/194182 (85%)]\tLoss: 0.387125\tGrad Norm: 1.319488\tLR: 0.030000\n",
      "Train Epoch: 732 [188416/194182 (96%)]\tLoss: 0.386586\tGrad Norm: 1.365792\tLR: 0.030000\n",
      "Train set: Average loss: 0.3880\n",
      "Test set: Average loss: 0.2421, Average MAE: 0.3473\n",
      "Train Epoch: 733 [4096/194182 (2%)]\tLoss: 0.386504\tGrad Norm: 1.070007\tLR: 0.030000\n",
      "Train Epoch: 733 [24576/194182 (12%)]\tLoss: 0.380516\tGrad Norm: 0.847979\tLR: 0.030000\n",
      "Train Epoch: 733 [45056/194182 (23%)]\tLoss: 0.386917\tGrad Norm: 1.219094\tLR: 0.030000\n",
      "Train Epoch: 733 [65536/194182 (33%)]\tLoss: 0.385074\tGrad Norm: 1.133003\tLR: 0.030000\n",
      "Train Epoch: 733 [86016/194182 (44%)]\tLoss: 0.389768\tGrad Norm: 1.561086\tLR: 0.030000\n",
      "Train Epoch: 733 [106496/194182 (54%)]\tLoss: 0.397693\tGrad Norm: 1.392152\tLR: 0.030000\n",
      "Train Epoch: 733 [126976/194182 (65%)]\tLoss: 0.392248\tGrad Norm: 1.513487\tLR: 0.030000\n",
      "Train Epoch: 733 [147456/194182 (75%)]\tLoss: 0.397074\tGrad Norm: 1.518715\tLR: 0.030000\n",
      "Train Epoch: 733 [167936/194182 (85%)]\tLoss: 0.388694\tGrad Norm: 1.376094\tLR: 0.030000\n",
      "Train Epoch: 733 [188416/194182 (96%)]\tLoss: 0.384070\tGrad Norm: 1.236768\tLR: 0.030000\n",
      "Train set: Average loss: 0.3879\n",
      "Test set: Average loss: 0.2427, Average MAE: 0.3515\n",
      "Train Epoch: 734 [4096/194182 (2%)]\tLoss: 0.385602\tGrad Norm: 1.064461\tLR: 0.030000\n",
      "Train Epoch: 734 [24576/194182 (12%)]\tLoss: 0.377797\tGrad Norm: 0.904266\tLR: 0.030000\n",
      "Train Epoch: 734 [45056/194182 (23%)]\tLoss: 0.387496\tGrad Norm: 1.462472\tLR: 0.030000\n",
      "Train Epoch: 734 [65536/194182 (33%)]\tLoss: 0.391435\tGrad Norm: 1.119190\tLR: 0.030000\n",
      "Train Epoch: 734 [86016/194182 (44%)]\tLoss: 0.390547\tGrad Norm: 1.243119\tLR: 0.030000\n",
      "Train Epoch: 734 [106496/194182 (54%)]\tLoss: 0.381734\tGrad Norm: 1.045411\tLR: 0.030000\n",
      "Train Epoch: 734 [126976/194182 (65%)]\tLoss: 0.387062\tGrad Norm: 1.354864\tLR: 0.030000\n",
      "Train Epoch: 734 [147456/194182 (75%)]\tLoss: 0.385904\tGrad Norm: 1.361538\tLR: 0.030000\n",
      "Train Epoch: 734 [167936/194182 (85%)]\tLoss: 0.386333\tGrad Norm: 1.283432\tLR: 0.030000\n",
      "Train Epoch: 734 [188416/194182 (96%)]\tLoss: 0.389676\tGrad Norm: 1.286862\tLR: 0.030000\n",
      "Train set: Average loss: 0.3870\n",
      "Test set: Average loss: 0.2433, Average MAE: 0.3548\n",
      "Train Epoch: 735 [4096/194182 (2%)]\tLoss: 0.385114\tGrad Norm: 1.379042\tLR: 0.030000\n",
      "Train Epoch: 735 [24576/194182 (12%)]\tLoss: 0.383726\tGrad Norm: 1.193634\tLR: 0.030000\n",
      "Train Epoch: 735 [45056/194182 (23%)]\tLoss: 0.384327\tGrad Norm: 1.267870\tLR: 0.030000\n",
      "Train Epoch: 735 [65536/194182 (33%)]\tLoss: 0.390837\tGrad Norm: 1.105812\tLR: 0.030000\n",
      "Train Epoch: 735 [86016/194182 (44%)]\tLoss: 0.382413\tGrad Norm: 0.950245\tLR: 0.030000\n",
      "Train Epoch: 735 [106496/194182 (54%)]\tLoss: 0.379107\tGrad Norm: 0.751744\tLR: 0.030000\n",
      "Train Epoch: 735 [126976/194182 (65%)]\tLoss: 0.381551\tGrad Norm: 0.824838\tLR: 0.030000\n",
      "Train Epoch: 735 [147456/194182 (75%)]\tLoss: 0.390578\tGrad Norm: 1.270697\tLR: 0.030000\n",
      "Train Epoch: 735 [167936/194182 (85%)]\tLoss: 0.382247\tGrad Norm: 1.034029\tLR: 0.030000\n",
      "Train Epoch: 735 [188416/194182 (96%)]\tLoss: 0.386114\tGrad Norm: 1.125273\tLR: 0.030000\n",
      "Train set: Average loss: 0.3847\n",
      "Test set: Average loss: 0.2506, Average MAE: 0.3645\n",
      "Epoch 735: Mean reward = 0.056 +/- 0.030\n",
      "Train Epoch: 736 [4096/194182 (2%)]\tLoss: 0.386450\tGrad Norm: 1.729949\tLR: 0.030000\n",
      "Train Epoch: 736 [24576/194182 (12%)]\tLoss: 0.391382\tGrad Norm: 1.248605\tLR: 0.030000\n",
      "Train Epoch: 736 [45056/194182 (23%)]\tLoss: 0.389233\tGrad Norm: 1.525915\tLR: 0.030000\n",
      "Train Epoch: 736 [65536/194182 (33%)]\tLoss: 0.400902\tGrad Norm: 1.775715\tLR: 0.030000\n",
      "Train Epoch: 736 [86016/194182 (44%)]\tLoss: 0.390333\tGrad Norm: 1.091515\tLR: 0.030000\n",
      "Train Epoch: 736 [106496/194182 (54%)]\tLoss: 0.385592\tGrad Norm: 0.958534\tLR: 0.030000\n",
      "Train Epoch: 736 [126976/194182 (65%)]\tLoss: 0.393359\tGrad Norm: 1.276600\tLR: 0.030000\n",
      "Train Epoch: 736 [147456/194182 (75%)]\tLoss: 0.384728\tGrad Norm: 1.206324\tLR: 0.030000\n",
      "Train Epoch: 736 [167936/194182 (85%)]\tLoss: 0.392507\tGrad Norm: 1.378328\tLR: 0.030000\n",
      "Train Epoch: 736 [188416/194182 (96%)]\tLoss: 0.379121\tGrad Norm: 0.961921\tLR: 0.030000\n",
      "Train set: Average loss: 0.3868\n",
      "Test set: Average loss: 0.2408, Average MAE: 0.3447\n",
      "Train Epoch: 737 [4096/194182 (2%)]\tLoss: 0.385551\tGrad Norm: 1.073059\tLR: 0.030000\n",
      "Train Epoch: 737 [24576/194182 (12%)]\tLoss: 0.388876\tGrad Norm: 1.002575\tLR: 0.030000\n",
      "Train Epoch: 737 [45056/194182 (23%)]\tLoss: 0.386661\tGrad Norm: 0.925765\tLR: 0.030000\n",
      "Train Epoch: 737 [65536/194182 (33%)]\tLoss: 0.383681\tGrad Norm: 1.395393\tLR: 0.030000\n",
      "Train Epoch: 737 [86016/194182 (44%)]\tLoss: 0.377745\tGrad Norm: 1.072955\tLR: 0.030000\n",
      "Train Epoch: 737 [106496/194182 (54%)]\tLoss: 0.377614\tGrad Norm: 1.090225\tLR: 0.030000\n",
      "Train Epoch: 737 [126976/194182 (65%)]\tLoss: 0.387225\tGrad Norm: 1.366308\tLR: 0.030000\n",
      "Train Epoch: 737 [147456/194182 (75%)]\tLoss: 0.385727\tGrad Norm: 1.199058\tLR: 0.030000\n",
      "Train Epoch: 737 [167936/194182 (85%)]\tLoss: 0.381487\tGrad Norm: 1.297417\tLR: 0.030000\n",
      "Train Epoch: 737 [188416/194182 (96%)]\tLoss: 0.390991\tGrad Norm: 1.337105\tLR: 0.030000\n",
      "Train set: Average loss: 0.3852\n",
      "Test set: Average loss: 0.2430, Average MAE: 0.3341\n",
      "Train Epoch: 738 [4096/194182 (2%)]\tLoss: 0.392928\tGrad Norm: 1.420404\tLR: 0.030000\n",
      "Train Epoch: 738 [24576/194182 (12%)]\tLoss: 0.389423\tGrad Norm: 0.891017\tLR: 0.030000\n",
      "Train Epoch: 738 [45056/194182 (23%)]\tLoss: 0.377952\tGrad Norm: 1.018910\tLR: 0.030000\n",
      "Train Epoch: 738 [65536/194182 (33%)]\tLoss: 0.393518\tGrad Norm: 1.432248\tLR: 0.030000\n",
      "Train Epoch: 738 [86016/194182 (44%)]\tLoss: 0.400694\tGrad Norm: 1.746261\tLR: 0.030000\n",
      "Train Epoch: 738 [106496/194182 (54%)]\tLoss: 0.386733\tGrad Norm: 1.404241\tLR: 0.030000\n",
      "Train Epoch: 738 [126976/194182 (65%)]\tLoss: 0.388618\tGrad Norm: 1.358095\tLR: 0.030000\n",
      "Train Epoch: 738 [147456/194182 (75%)]\tLoss: 0.390845\tGrad Norm: 1.294872\tLR: 0.030000\n",
      "Train Epoch: 738 [167936/194182 (85%)]\tLoss: 0.384003\tGrad Norm: 1.130882\tLR: 0.030000\n",
      "Train Epoch: 738 [188416/194182 (96%)]\tLoss: 0.391793\tGrad Norm: 0.999877\tLR: 0.030000\n",
      "Train set: Average loss: 0.3866\n",
      "Test set: Average loss: 0.2434, Average MAE: 0.3439\n",
      "Train Epoch: 739 [4096/194182 (2%)]\tLoss: 0.385623\tGrad Norm: 1.286373\tLR: 0.030000\n",
      "Train Epoch: 739 [24576/194182 (12%)]\tLoss: 0.379796\tGrad Norm: 1.344080\tLR: 0.030000\n",
      "Train Epoch: 739 [45056/194182 (23%)]\tLoss: 0.379738\tGrad Norm: 1.024216\tLR: 0.030000\n",
      "Train Epoch: 739 [65536/194182 (33%)]\tLoss: 0.375611\tGrad Norm: 1.019031\tLR: 0.030000\n",
      "Train Epoch: 739 [86016/194182 (44%)]\tLoss: 0.385900\tGrad Norm: 1.069055\tLR: 0.030000\n",
      "Train Epoch: 739 [106496/194182 (54%)]\tLoss: 0.385251\tGrad Norm: 1.006020\tLR: 0.030000\n",
      "Train Epoch: 739 [126976/194182 (65%)]\tLoss: 0.380611\tGrad Norm: 1.137919\tLR: 0.030000\n",
      "Train Epoch: 739 [147456/194182 (75%)]\tLoss: 0.399995\tGrad Norm: 1.671966\tLR: 0.030000\n",
      "Train Epoch: 739 [167936/194182 (85%)]\tLoss: 0.395950\tGrad Norm: 1.758967\tLR: 0.030000\n",
      "Train Epoch: 739 [188416/194182 (96%)]\tLoss: 0.391149\tGrad Norm: 1.426319\tLR: 0.030000\n",
      "Train set: Average loss: 0.3859\n",
      "Test set: Average loss: 0.2443, Average MAE: 0.3555\n",
      "Train Epoch: 740 [4096/194182 (2%)]\tLoss: 0.384200\tGrad Norm: 1.213344\tLR: 0.030000\n",
      "Train Epoch: 740 [24576/194182 (12%)]\tLoss: 0.379492\tGrad Norm: 0.869768\tLR: 0.030000\n",
      "Train Epoch: 740 [45056/194182 (23%)]\tLoss: 0.375913\tGrad Norm: 0.959632\tLR: 0.030000\n",
      "Train Epoch: 740 [65536/194182 (33%)]\tLoss: 0.385283\tGrad Norm: 1.113426\tLR: 0.030000\n",
      "Train Epoch: 740 [86016/194182 (44%)]\tLoss: 0.378841\tGrad Norm: 1.172866\tLR: 0.030000\n",
      "Train Epoch: 740 [106496/194182 (54%)]\tLoss: 0.379177\tGrad Norm: 1.107741\tLR: 0.030000\n",
      "Train Epoch: 740 [126976/194182 (65%)]\tLoss: 0.378888\tGrad Norm: 1.086968\tLR: 0.030000\n",
      "Train Epoch: 740 [147456/194182 (75%)]\tLoss: 0.388781\tGrad Norm: 1.215501\tLR: 0.030000\n",
      "Train Epoch: 740 [167936/194182 (85%)]\tLoss: 0.385970\tGrad Norm: 0.978434\tLR: 0.030000\n",
      "Train Epoch: 740 [188416/194182 (96%)]\tLoss: 0.389670\tGrad Norm: 1.368471\tLR: 0.030000\n",
      "Train set: Average loss: 0.3833\n",
      "Test set: Average loss: 0.2441, Average MAE: 0.3556\n",
      "Epoch 740: Mean reward = 0.062 +/- 0.066\n",
      "Train Epoch: 741 [4096/194182 (2%)]\tLoss: 0.390084\tGrad Norm: 1.231770\tLR: 0.030000\n",
      "Train Epoch: 741 [24576/194182 (12%)]\tLoss: 0.378448\tGrad Norm: 0.999950\tLR: 0.030000\n",
      "Train Epoch: 741 [45056/194182 (23%)]\tLoss: 0.384134\tGrad Norm: 1.052409\tLR: 0.030000\n",
      "Train Epoch: 741 [65536/194182 (33%)]\tLoss: 0.391388\tGrad Norm: 1.585404\tLR: 0.030000\n",
      "Train Epoch: 741 [86016/194182 (44%)]\tLoss: 0.384109\tGrad Norm: 1.048729\tLR: 0.030000\n",
      "Train Epoch: 741 [106496/194182 (54%)]\tLoss: 0.385495\tGrad Norm: 1.386532\tLR: 0.030000\n",
      "Train Epoch: 741 [126976/194182 (65%)]\tLoss: 0.386392\tGrad Norm: 1.168273\tLR: 0.030000\n",
      "Train Epoch: 741 [147456/194182 (75%)]\tLoss: 0.379534\tGrad Norm: 1.289971\tLR: 0.030000\n",
      "Train Epoch: 741 [167936/194182 (85%)]\tLoss: 0.384219\tGrad Norm: 1.376351\tLR: 0.030000\n",
      "Train Epoch: 741 [188416/194182 (96%)]\tLoss: 0.391831\tGrad Norm: 1.207366\tLR: 0.030000\n",
      "Train set: Average loss: 0.3843\n",
      "Test set: Average loss: 0.2403, Average MAE: 0.3358\n",
      "Train Epoch: 742 [4096/194182 (2%)]\tLoss: 0.380406\tGrad Norm: 1.117559\tLR: 0.030000\n",
      "Train Epoch: 742 [24576/194182 (12%)]\tLoss: 0.381831\tGrad Norm: 1.291704\tLR: 0.030000\n",
      "Train Epoch: 742 [45056/194182 (23%)]\tLoss: 0.389619\tGrad Norm: 1.539172\tLR: 0.030000\n",
      "Train Epoch: 742 [65536/194182 (33%)]\tLoss: 0.386193\tGrad Norm: 1.219351\tLR: 0.030000\n",
      "Train Epoch: 742 [86016/194182 (44%)]\tLoss: 0.385318\tGrad Norm: 1.645801\tLR: 0.030000\n",
      "Train Epoch: 742 [106496/194182 (54%)]\tLoss: 0.383716\tGrad Norm: 1.509374\tLR: 0.030000\n",
      "Train Epoch: 742 [126976/194182 (65%)]\tLoss: 0.380105\tGrad Norm: 1.081596\tLR: 0.030000\n",
      "Train Epoch: 742 [147456/194182 (75%)]\tLoss: 0.377916\tGrad Norm: 1.178815\tLR: 0.030000\n",
      "Train Epoch: 742 [167936/194182 (85%)]\tLoss: 0.387362\tGrad Norm: 1.395810\tLR: 0.030000\n",
      "Train Epoch: 742 [188416/194182 (96%)]\tLoss: 0.388612\tGrad Norm: 1.586922\tLR: 0.030000\n",
      "Train set: Average loss: 0.3861\n",
      "Test set: Average loss: 0.2479, Average MAE: 0.3382\n",
      "Train Epoch: 743 [4096/194182 (2%)]\tLoss: 0.400404\tGrad Norm: 1.628943\tLR: 0.030000\n",
      "Train Epoch: 743 [24576/194182 (12%)]\tLoss: 0.377022\tGrad Norm: 1.167516\tLR: 0.030000\n",
      "Train Epoch: 743 [45056/194182 (23%)]\tLoss: 0.385994\tGrad Norm: 0.867581\tLR: 0.030000\n",
      "Train Epoch: 743 [65536/194182 (33%)]\tLoss: 0.383186\tGrad Norm: 1.321807\tLR: 0.030000\n",
      "Train Epoch: 743 [86016/194182 (44%)]\tLoss: 0.384976\tGrad Norm: 0.958813\tLR: 0.030000\n",
      "Train Epoch: 743 [106496/194182 (54%)]\tLoss: 0.385610\tGrad Norm: 1.240808\tLR: 0.030000\n",
      "Train Epoch: 743 [126976/194182 (65%)]\tLoss: 0.389356\tGrad Norm: 1.291516\tLR: 0.030000\n",
      "Train Epoch: 743 [147456/194182 (75%)]\tLoss: 0.379509\tGrad Norm: 1.066256\tLR: 0.030000\n",
      "Train Epoch: 743 [167936/194182 (85%)]\tLoss: 0.385929\tGrad Norm: 1.460256\tLR: 0.030000\n",
      "Train Epoch: 743 [188416/194182 (96%)]\tLoss: 0.379381\tGrad Norm: 1.009950\tLR: 0.030000\n",
      "Train set: Average loss: 0.3840\n",
      "Test set: Average loss: 0.2373, Average MAE: 0.3450\n",
      "Train Epoch: 744 [4096/194182 (2%)]\tLoss: 0.378152\tGrad Norm: 0.900645\tLR: 0.030000\n",
      "Train Epoch: 744 [24576/194182 (12%)]\tLoss: 0.376960\tGrad Norm: 0.766475\tLR: 0.030000\n",
      "Train Epoch: 744 [45056/194182 (23%)]\tLoss: 0.382479\tGrad Norm: 0.988408\tLR: 0.030000\n",
      "Train Epoch: 744 [65536/194182 (33%)]\tLoss: 0.382264\tGrad Norm: 1.321694\tLR: 0.030000\n",
      "Train Epoch: 744 [86016/194182 (44%)]\tLoss: 0.390394\tGrad Norm: 1.247651\tLR: 0.030000\n",
      "Train Epoch: 744 [106496/194182 (54%)]\tLoss: 0.382811\tGrad Norm: 1.385277\tLR: 0.030000\n",
      "Train Epoch: 744 [126976/194182 (65%)]\tLoss: 0.380172\tGrad Norm: 1.105107\tLR: 0.030000\n",
      "Train Epoch: 744 [147456/194182 (75%)]\tLoss: 0.388323\tGrad Norm: 1.376179\tLR: 0.030000\n",
      "Train Epoch: 744 [167936/194182 (85%)]\tLoss: 0.387407\tGrad Norm: 1.342082\tLR: 0.030000\n",
      "Train Epoch: 744 [188416/194182 (96%)]\tLoss: 0.384545\tGrad Norm: 1.159079\tLR: 0.030000\n",
      "Train set: Average loss: 0.3830\n",
      "Test set: Average loss: 0.2412, Average MAE: 0.3482\n",
      "Train Epoch: 745 [4096/194182 (2%)]\tLoss: 0.387342\tGrad Norm: 1.014979\tLR: 0.030000\n",
      "Train Epoch: 745 [24576/194182 (12%)]\tLoss: 0.386844\tGrad Norm: 1.420306\tLR: 0.030000\n",
      "Train Epoch: 745 [45056/194182 (23%)]\tLoss: 0.388925\tGrad Norm: 1.552431\tLR: 0.030000\n",
      "Train Epoch: 745 [65536/194182 (33%)]\tLoss: 0.390481\tGrad Norm: 1.502316\tLR: 0.030000\n",
      "Train Epoch: 745 [86016/194182 (44%)]\tLoss: 0.389929\tGrad Norm: 1.237178\tLR: 0.030000\n",
      "Train Epoch: 745 [106496/194182 (54%)]\tLoss: 0.372275\tGrad Norm: 1.129417\tLR: 0.030000\n",
      "Train Epoch: 745 [126976/194182 (65%)]\tLoss: 0.377825\tGrad Norm: 1.058626\tLR: 0.030000\n",
      "Train Epoch: 745 [147456/194182 (75%)]\tLoss: 0.369956\tGrad Norm: 0.872238\tLR: 0.030000\n",
      "Train Epoch: 745 [167936/194182 (85%)]\tLoss: 0.379481\tGrad Norm: 0.742041\tLR: 0.030000\n",
      "Train Epoch: 745 [188416/194182 (96%)]\tLoss: 0.369760\tGrad Norm: 0.745166\tLR: 0.030000\n",
      "Train set: Average loss: 0.3824\n",
      "Test set: Average loss: 0.2435, Average MAE: 0.3505\n",
      "Epoch 745: Mean reward = 0.057 +/- 0.037\n",
      "Train Epoch: 746 [4096/194182 (2%)]\tLoss: 0.381947\tGrad Norm: 1.110297\tLR: 0.030000\n",
      "Train Epoch: 746 [24576/194182 (12%)]\tLoss: 0.380577\tGrad Norm: 1.044741\tLR: 0.030000\n",
      "Train Epoch: 746 [45056/194182 (23%)]\tLoss: 0.373067\tGrad Norm: 1.031963\tLR: 0.030000\n",
      "Train Epoch: 746 [65536/194182 (33%)]\tLoss: 0.380392\tGrad Norm: 1.441212\tLR: 0.030000\n",
      "Train Epoch: 746 [86016/194182 (44%)]\tLoss: 0.394353\tGrad Norm: 1.663570\tLR: 0.030000\n",
      "Train Epoch: 746 [106496/194182 (54%)]\tLoss: 0.382890\tGrad Norm: 1.337626\tLR: 0.030000\n",
      "Train Epoch: 746 [126976/194182 (65%)]\tLoss: 0.390191\tGrad Norm: 1.140324\tLR: 0.030000\n",
      "Train Epoch: 746 [147456/194182 (75%)]\tLoss: 0.384998\tGrad Norm: 1.447594\tLR: 0.030000\n",
      "Train Epoch: 746 [167936/194182 (85%)]\tLoss: 0.383652\tGrad Norm: 1.242689\tLR: 0.030000\n",
      "Train Epoch: 746 [188416/194182 (96%)]\tLoss: 0.382265\tGrad Norm: 1.131868\tLR: 0.030000\n",
      "Train set: Average loss: 0.3836\n",
      "Test set: Average loss: 0.2417, Average MAE: 0.3358\n",
      "Train Epoch: 747 [4096/194182 (2%)]\tLoss: 0.378788\tGrad Norm: 1.149480\tLR: 0.030000\n",
      "Train Epoch: 747 [24576/194182 (12%)]\tLoss: 0.384928\tGrad Norm: 1.476953\tLR: 0.030000\n",
      "Train Epoch: 747 [45056/194182 (23%)]\tLoss: 0.379531\tGrad Norm: 1.452220\tLR: 0.030000\n",
      "Train Epoch: 747 [65536/194182 (33%)]\tLoss: 0.387326\tGrad Norm: 1.196824\tLR: 0.030000\n",
      "Train Epoch: 747 [86016/194182 (44%)]\tLoss: 0.378033\tGrad Norm: 0.787139\tLR: 0.030000\n",
      "Train Epoch: 747 [106496/194182 (54%)]\tLoss: 0.377332\tGrad Norm: 0.686012\tLR: 0.030000\n",
      "Train Epoch: 747 [126976/194182 (65%)]\tLoss: 0.376309\tGrad Norm: 1.057190\tLR: 0.030000\n",
      "Train Epoch: 747 [147456/194182 (75%)]\tLoss: 0.389360\tGrad Norm: 1.755466\tLR: 0.030000\n",
      "Train Epoch: 747 [167936/194182 (85%)]\tLoss: 0.377389\tGrad Norm: 1.375974\tLR: 0.030000\n",
      "Train Epoch: 747 [188416/194182 (96%)]\tLoss: 0.385736\tGrad Norm: 1.153441\tLR: 0.030000\n",
      "Train set: Average loss: 0.3823\n",
      "Test set: Average loss: 0.2421, Average MAE: 0.3442\n",
      "Train Epoch: 748 [4096/194182 (2%)]\tLoss: 0.385701\tGrad Norm: 1.281409\tLR: 0.030000\n",
      "Train Epoch: 748 [24576/194182 (12%)]\tLoss: 0.375284\tGrad Norm: 1.245019\tLR: 0.030000\n",
      "Train Epoch: 748 [45056/194182 (23%)]\tLoss: 0.391270\tGrad Norm: 1.360997\tLR: 0.030000\n",
      "Train Epoch: 748 [65536/194182 (33%)]\tLoss: 0.386475\tGrad Norm: 1.327368\tLR: 0.030000\n",
      "Train Epoch: 748 [86016/194182 (44%)]\tLoss: 0.378492\tGrad Norm: 0.938881\tLR: 0.030000\n",
      "Train Epoch: 748 [106496/194182 (54%)]\tLoss: 0.380618\tGrad Norm: 1.045178\tLR: 0.030000\n",
      "Train Epoch: 748 [126976/194182 (65%)]\tLoss: 0.380550\tGrad Norm: 1.304055\tLR: 0.030000\n",
      "Train Epoch: 748 [147456/194182 (75%)]\tLoss: 0.384282\tGrad Norm: 0.990925\tLR: 0.030000\n",
      "Train Epoch: 748 [167936/194182 (85%)]\tLoss: 0.385026\tGrad Norm: 1.196522\tLR: 0.030000\n",
      "Train Epoch: 748 [188416/194182 (96%)]\tLoss: 0.395860\tGrad Norm: 1.625409\tLR: 0.030000\n",
      "Train set: Average loss: 0.3828\n",
      "Test set: Average loss: 0.2428, Average MAE: 0.3325\n",
      "Train Epoch: 749 [4096/194182 (2%)]\tLoss: 0.376605\tGrad Norm: 1.339476\tLR: 0.030000\n",
      "Train Epoch: 749 [24576/194182 (12%)]\tLoss: 0.386527\tGrad Norm: 1.335498\tLR: 0.030000\n",
      "Train Epoch: 749 [45056/194182 (23%)]\tLoss: 0.393213\tGrad Norm: 1.476794\tLR: 0.030000\n",
      "Train Epoch: 749 [65536/194182 (33%)]\tLoss: 0.378566\tGrad Norm: 1.139386\tLR: 0.030000\n",
      "Train Epoch: 749 [86016/194182 (44%)]\tLoss: 0.378416\tGrad Norm: 0.962118\tLR: 0.030000\n",
      "Train Epoch: 749 [106496/194182 (54%)]\tLoss: 0.383318\tGrad Norm: 1.164559\tLR: 0.030000\n",
      "Train Epoch: 749 [126976/194182 (65%)]\tLoss: 0.382423\tGrad Norm: 1.288075\tLR: 0.030000\n",
      "Train Epoch: 749 [147456/194182 (75%)]\tLoss: 0.376147\tGrad Norm: 1.105373\tLR: 0.030000\n",
      "Train Epoch: 749 [167936/194182 (85%)]\tLoss: 0.380962\tGrad Norm: 1.254012\tLR: 0.030000\n",
      "Train Epoch: 749 [188416/194182 (96%)]\tLoss: 0.378178\tGrad Norm: 1.020803\tLR: 0.030000\n",
      "Train set: Average loss: 0.3821\n",
      "Test set: Average loss: 0.2401, Average MAE: 0.3456\n",
      "Train Epoch: 750 [4096/194182 (2%)]\tLoss: 0.380186\tGrad Norm: 1.068873\tLR: 0.030000\n",
      "Train Epoch: 750 [24576/194182 (12%)]\tLoss: 0.382142\tGrad Norm: 1.138861\tLR: 0.030000\n",
      "Train Epoch: 750 [45056/194182 (23%)]\tLoss: 0.380706\tGrad Norm: 1.289600\tLR: 0.030000\n",
      "Train Epoch: 750 [65536/194182 (33%)]\tLoss: 0.374844\tGrad Norm: 1.301075\tLR: 0.030000\n",
      "Train Epoch: 750 [86016/194182 (44%)]\tLoss: 0.373550\tGrad Norm: 0.956353\tLR: 0.030000\n",
      "Train Epoch: 750 [106496/194182 (54%)]\tLoss: 0.371028\tGrad Norm: 0.842650\tLR: 0.030000\n",
      "Train Epoch: 750 [126976/194182 (65%)]\tLoss: 0.390756\tGrad Norm: 1.206034\tLR: 0.030000\n",
      "Train Epoch: 750 [147456/194182 (75%)]\tLoss: 0.379207\tGrad Norm: 1.102000\tLR: 0.030000\n",
      "Train Epoch: 750 [167936/194182 (85%)]\tLoss: 0.384821\tGrad Norm: 1.522274\tLR: 0.030000\n",
      "Train Epoch: 750 [188416/194182 (96%)]\tLoss: 0.389197\tGrad Norm: 1.457344\tLR: 0.030000\n",
      "Train set: Average loss: 0.3814\n",
      "Test set: Average loss: 0.2495, Average MAE: 0.3487\n",
      "Epoch 750: Mean reward = 0.060 +/- 0.043\n",
      "Train Epoch: 751 [4096/194182 (2%)]\tLoss: 0.396901\tGrad Norm: 1.612596\tLR: 0.030000\n",
      "Train Epoch: 751 [24576/194182 (12%)]\tLoss: 0.379436\tGrad Norm: 1.261760\tLR: 0.030000\n",
      "Train Epoch: 751 [45056/194182 (23%)]\tLoss: 0.385080\tGrad Norm: 1.009527\tLR: 0.030000\n",
      "Train Epoch: 751 [65536/194182 (33%)]\tLoss: 0.381492\tGrad Norm: 1.115996\tLR: 0.030000\n",
      "Train Epoch: 751 [86016/194182 (44%)]\tLoss: 0.379690\tGrad Norm: 1.154760\tLR: 0.030000\n",
      "Train Epoch: 751 [106496/194182 (54%)]\tLoss: 0.389774\tGrad Norm: 1.458236\tLR: 0.030000\n",
      "Train Epoch: 751 [126976/194182 (65%)]\tLoss: 0.387911\tGrad Norm: 1.491885\tLR: 0.030000\n",
      "Train Epoch: 751 [147456/194182 (75%)]\tLoss: 0.396436\tGrad Norm: 1.921797\tLR: 0.030000\n",
      "Train Epoch: 751 [167936/194182 (85%)]\tLoss: 0.385764\tGrad Norm: 1.355089\tLR: 0.030000\n",
      "Train Epoch: 751 [188416/194182 (96%)]\tLoss: 0.385364\tGrad Norm: 1.225935\tLR: 0.030000\n",
      "Train set: Average loss: 0.3835\n",
      "Test set: Average loss: 0.2411, Average MAE: 0.3519\n",
      "Train Epoch: 752 [4096/194182 (2%)]\tLoss: 0.379884\tGrad Norm: 1.049819\tLR: 0.030000\n",
      "Train Epoch: 752 [24576/194182 (12%)]\tLoss: 0.376425\tGrad Norm: 0.835307\tLR: 0.030000\n",
      "Train Epoch: 752 [45056/194182 (23%)]\tLoss: 0.372965\tGrad Norm: 0.886841\tLR: 0.030000\n",
      "Train Epoch: 752 [65536/194182 (33%)]\tLoss: 0.377008\tGrad Norm: 0.751878\tLR: 0.030000\n",
      "Train Epoch: 752 [86016/194182 (44%)]\tLoss: 0.374713\tGrad Norm: 0.991250\tLR: 0.030000\n",
      "Train Epoch: 752 [106496/194182 (54%)]\tLoss: 0.382071\tGrad Norm: 1.269729\tLR: 0.030000\n",
      "Train Epoch: 752 [126976/194182 (65%)]\tLoss: 0.383797\tGrad Norm: 1.246683\tLR: 0.030000\n",
      "Train Epoch: 752 [147456/194182 (75%)]\tLoss: 0.378217\tGrad Norm: 1.232037\tLR: 0.030000\n",
      "Train Epoch: 752 [167936/194182 (85%)]\tLoss: 0.390238\tGrad Norm: 1.618795\tLR: 0.030000\n",
      "Train Epoch: 752 [188416/194182 (96%)]\tLoss: 0.388788\tGrad Norm: 1.693574\tLR: 0.030000\n",
      "Train set: Average loss: 0.3811\n",
      "Test set: Average loss: 0.2455, Average MAE: 0.3313\n",
      "Train Epoch: 753 [4096/194182 (2%)]\tLoss: 0.384590\tGrad Norm: 1.599104\tLR: 0.030000\n",
      "Train Epoch: 753 [24576/194182 (12%)]\tLoss: 0.380673\tGrad Norm: 0.965130\tLR: 0.030000\n",
      "Train Epoch: 753 [45056/194182 (23%)]\tLoss: 0.375901\tGrad Norm: 1.089477\tLR: 0.030000\n",
      "Train Epoch: 753 [65536/194182 (33%)]\tLoss: 0.376673\tGrad Norm: 1.306043\tLR: 0.030000\n",
      "Train Epoch: 753 [86016/194182 (44%)]\tLoss: 0.383422\tGrad Norm: 1.428694\tLR: 0.030000\n",
      "Train Epoch: 753 [106496/194182 (54%)]\tLoss: 0.381831\tGrad Norm: 1.117171\tLR: 0.030000\n",
      "Train Epoch: 753 [126976/194182 (65%)]\tLoss: 0.381500\tGrad Norm: 1.415072\tLR: 0.030000\n",
      "Train Epoch: 753 [147456/194182 (75%)]\tLoss: 0.383263\tGrad Norm: 1.245380\tLR: 0.030000\n",
      "Train Epoch: 753 [167936/194182 (85%)]\tLoss: 0.376438\tGrad Norm: 1.129506\tLR: 0.030000\n",
      "Train Epoch: 753 [188416/194182 (96%)]\tLoss: 0.388349\tGrad Norm: 1.375606\tLR: 0.030000\n",
      "Train set: Average loss: 0.3814\n",
      "Test set: Average loss: 0.2437, Average MAE: 0.3448\n",
      "Train Epoch: 754 [4096/194182 (2%)]\tLoss: 0.380102\tGrad Norm: 1.274142\tLR: 0.030000\n",
      "Train Epoch: 754 [24576/194182 (12%)]\tLoss: 0.380460\tGrad Norm: 1.235274\tLR: 0.030000\n",
      "Train Epoch: 754 [45056/194182 (23%)]\tLoss: 0.378201\tGrad Norm: 1.265208\tLR: 0.030000\n",
      "Train Epoch: 754 [65536/194182 (33%)]\tLoss: 0.380456\tGrad Norm: 1.329733\tLR: 0.030000\n",
      "Train Epoch: 754 [86016/194182 (44%)]\tLoss: 0.373830\tGrad Norm: 1.230609\tLR: 0.030000\n",
      "Train Epoch: 754 [106496/194182 (54%)]\tLoss: 0.380911\tGrad Norm: 1.256889\tLR: 0.030000\n",
      "Train Epoch: 754 [126976/194182 (65%)]\tLoss: 0.374264\tGrad Norm: 0.930432\tLR: 0.030000\n",
      "Train Epoch: 754 [147456/194182 (75%)]\tLoss: 0.379471\tGrad Norm: 1.268819\tLR: 0.030000\n",
      "Train Epoch: 754 [167936/194182 (85%)]\tLoss: 0.376912\tGrad Norm: 1.183682\tLR: 0.030000\n",
      "Train Epoch: 754 [188416/194182 (96%)]\tLoss: 0.375343\tGrad Norm: 0.939344\tLR: 0.030000\n",
      "Train set: Average loss: 0.3805\n",
      "Test set: Average loss: 0.2388, Average MAE: 0.3425\n",
      "Train Epoch: 755 [4096/194182 (2%)]\tLoss: 0.378504\tGrad Norm: 1.057679\tLR: 0.030000\n",
      "Train Epoch: 755 [24576/194182 (12%)]\tLoss: 0.372776\tGrad Norm: 1.059029\tLR: 0.030000\n",
      "Train Epoch: 755 [45056/194182 (23%)]\tLoss: 0.377000\tGrad Norm: 1.274425\tLR: 0.030000\n",
      "Train Epoch: 755 [65536/194182 (33%)]\tLoss: 0.382328\tGrad Norm: 1.522560\tLR: 0.030000\n",
      "Train Epoch: 755 [86016/194182 (44%)]\tLoss: 0.382557\tGrad Norm: 1.284539\tLR: 0.030000\n",
      "Train Epoch: 755 [106496/194182 (54%)]\tLoss: 0.366744\tGrad Norm: 0.608905\tLR: 0.030000\n",
      "Train Epoch: 755 [126976/194182 (65%)]\tLoss: 0.376602\tGrad Norm: 0.735559\tLR: 0.030000\n",
      "Train Epoch: 755 [147456/194182 (75%)]\tLoss: 0.390472\tGrad Norm: 1.612601\tLR: 0.030000\n",
      "Train Epoch: 755 [167936/194182 (85%)]\tLoss: 0.383561\tGrad Norm: 1.333237\tLR: 0.030000\n",
      "Train Epoch: 755 [188416/194182 (96%)]\tLoss: 0.383600\tGrad Norm: 0.998012\tLR: 0.030000\n",
      "Train set: Average loss: 0.3797\n",
      "Test set: Average loss: 0.2432, Average MAE: 0.3403\n",
      "Epoch 755: Mean reward = 0.056 +/- 0.047\n",
      "Train Epoch: 756 [4096/194182 (2%)]\tLoss: 0.383773\tGrad Norm: 1.210921\tLR: 0.030000\n",
      "Train Epoch: 756 [24576/194182 (12%)]\tLoss: 0.379901\tGrad Norm: 1.238202\tLR: 0.030000\n",
      "Train Epoch: 756 [45056/194182 (23%)]\tLoss: 0.388061\tGrad Norm: 1.303798\tLR: 0.030000\n",
      "Train Epoch: 756 [65536/194182 (33%)]\tLoss: 0.376912\tGrad Norm: 1.168496\tLR: 0.030000\n",
      "Train Epoch: 756 [86016/194182 (44%)]\tLoss: 0.381534\tGrad Norm: 1.246743\tLR: 0.030000\n",
      "Train Epoch: 756 [106496/194182 (54%)]\tLoss: 0.376304\tGrad Norm: 0.815489\tLR: 0.030000\n",
      "Train Epoch: 756 [126976/194182 (65%)]\tLoss: 0.371713\tGrad Norm: 1.246175\tLR: 0.030000\n",
      "Train Epoch: 756 [147456/194182 (75%)]\tLoss: 0.380763\tGrad Norm: 1.688253\tLR: 0.030000\n",
      "Train Epoch: 756 [167936/194182 (85%)]\tLoss: 0.378498\tGrad Norm: 1.252193\tLR: 0.030000\n",
      "Train Epoch: 756 [188416/194182 (96%)]\tLoss: 0.372114\tGrad Norm: 1.159962\tLR: 0.030000\n",
      "Train set: Average loss: 0.3797\n",
      "Test set: Average loss: 0.2402, Average MAE: 0.3328\n",
      "Train Epoch: 757 [4096/194182 (2%)]\tLoss: 0.378848\tGrad Norm: 1.172569\tLR: 0.030000\n",
      "Train Epoch: 757 [24576/194182 (12%)]\tLoss: 0.380993\tGrad Norm: 1.316641\tLR: 0.030000\n",
      "Train Epoch: 757 [45056/194182 (23%)]\tLoss: 0.379199\tGrad Norm: 1.339169\tLR: 0.030000\n",
      "Train Epoch: 757 [65536/194182 (33%)]\tLoss: 0.388764\tGrad Norm: 1.347821\tLR: 0.030000\n",
      "Train Epoch: 757 [86016/194182 (44%)]\tLoss: 0.384196\tGrad Norm: 1.153771\tLR: 0.030000\n",
      "Train Epoch: 757 [106496/194182 (54%)]\tLoss: 0.379742\tGrad Norm: 0.861622\tLR: 0.030000\n",
      "Train Epoch: 757 [126976/194182 (65%)]\tLoss: 0.379980\tGrad Norm: 1.177197\tLR: 0.030000\n",
      "Train Epoch: 757 [147456/194182 (75%)]\tLoss: 0.380476\tGrad Norm: 1.420376\tLR: 0.030000\n",
      "Train Epoch: 757 [167936/194182 (85%)]\tLoss: 0.377279\tGrad Norm: 1.268125\tLR: 0.030000\n",
      "Train Epoch: 757 [188416/194182 (96%)]\tLoss: 0.376950\tGrad Norm: 1.387925\tLR: 0.030000\n",
      "Train set: Average loss: 0.3801\n",
      "Test set: Average loss: 0.2477, Average MAE: 0.3340\n",
      "Train Epoch: 758 [4096/194182 (2%)]\tLoss: 0.393456\tGrad Norm: 1.874078\tLR: 0.030000\n",
      "Train Epoch: 758 [24576/194182 (12%)]\tLoss: 0.382659\tGrad Norm: 1.686791\tLR: 0.030000\n",
      "Train Epoch: 758 [45056/194182 (23%)]\tLoss: 0.371846\tGrad Norm: 1.119515\tLR: 0.030000\n",
      "Train Epoch: 758 [65536/194182 (33%)]\tLoss: 0.374806\tGrad Norm: 1.414732\tLR: 0.030000\n",
      "Train Epoch: 758 [86016/194182 (44%)]\tLoss: 0.375372\tGrad Norm: 1.055202\tLR: 0.030000\n",
      "Train Epoch: 758 [106496/194182 (54%)]\tLoss: 0.380818\tGrad Norm: 1.119194\tLR: 0.030000\n",
      "Train Epoch: 758 [126976/194182 (65%)]\tLoss: 0.392419\tGrad Norm: 1.492301\tLR: 0.030000\n",
      "Train Epoch: 758 [147456/194182 (75%)]\tLoss: 0.384502\tGrad Norm: 1.346033\tLR: 0.030000\n",
      "Train Epoch: 758 [167936/194182 (85%)]\tLoss: 0.380607\tGrad Norm: 1.371359\tLR: 0.030000\n",
      "Train Epoch: 758 [188416/194182 (96%)]\tLoss: 0.376317\tGrad Norm: 1.250354\tLR: 0.030000\n",
      "Train set: Average loss: 0.3817\n",
      "Test set: Average loss: 0.2399, Average MAE: 0.3392\n",
      "Train Epoch: 759 [4096/194182 (2%)]\tLoss: 0.378521\tGrad Norm: 1.070460\tLR: 0.030000\n",
      "Train Epoch: 759 [24576/194182 (12%)]\tLoss: 0.374611\tGrad Norm: 1.117776\tLR: 0.030000\n",
      "Train Epoch: 759 [45056/194182 (23%)]\tLoss: 0.376910\tGrad Norm: 1.184038\tLR: 0.030000\n",
      "Train Epoch: 759 [65536/194182 (33%)]\tLoss: 0.381270\tGrad Norm: 1.234539\tLR: 0.030000\n",
      "Train Epoch: 759 [86016/194182 (44%)]\tLoss: 0.375058\tGrad Norm: 0.762084\tLR: 0.030000\n",
      "Train Epoch: 759 [106496/194182 (54%)]\tLoss: 0.377843\tGrad Norm: 1.252873\tLR: 0.030000\n",
      "Train Epoch: 759 [126976/194182 (65%)]\tLoss: 0.377634\tGrad Norm: 1.551611\tLR: 0.030000\n",
      "Train Epoch: 759 [147456/194182 (75%)]\tLoss: 0.376426\tGrad Norm: 1.326077\tLR: 0.030000\n",
      "Train Epoch: 759 [167936/194182 (85%)]\tLoss: 0.383103\tGrad Norm: 1.127040\tLR: 0.030000\n",
      "Train Epoch: 759 [188416/194182 (96%)]\tLoss: 0.376634\tGrad Norm: 1.177987\tLR: 0.030000\n",
      "Train set: Average loss: 0.3789\n",
      "Test set: Average loss: 0.2431, Average MAE: 0.3321\n",
      "Train Epoch: 760 [4096/194182 (2%)]\tLoss: 0.385234\tGrad Norm: 1.344944\tLR: 0.030000\n",
      "Train Epoch: 760 [24576/194182 (12%)]\tLoss: 0.383176\tGrad Norm: 1.263730\tLR: 0.030000\n",
      "Train Epoch: 760 [45056/194182 (23%)]\tLoss: 0.377708\tGrad Norm: 1.231330\tLR: 0.030000\n",
      "Train Epoch: 760 [65536/194182 (33%)]\tLoss: 0.384781\tGrad Norm: 1.239161\tLR: 0.030000\n",
      "Train Epoch: 760 [86016/194182 (44%)]\tLoss: 0.378615\tGrad Norm: 1.177576\tLR: 0.030000\n",
      "Train Epoch: 760 [106496/194182 (54%)]\tLoss: 0.378752\tGrad Norm: 1.414425\tLR: 0.030000\n",
      "Train Epoch: 760 [126976/194182 (65%)]\tLoss: 0.388695\tGrad Norm: 1.730326\tLR: 0.030000\n",
      "Train Epoch: 760 [147456/194182 (75%)]\tLoss: 0.375310\tGrad Norm: 0.984592\tLR: 0.030000\n",
      "Train Epoch: 760 [167936/194182 (85%)]\tLoss: 0.371170\tGrad Norm: 0.772480\tLR: 0.030000\n",
      "Train Epoch: 760 [188416/194182 (96%)]\tLoss: 0.373408\tGrad Norm: 0.877137\tLR: 0.030000\n",
      "Train set: Average loss: 0.3783\n",
      "Test set: Average loss: 0.2401, Average MAE: 0.3317\n",
      "Epoch 760: Mean reward = 0.045 +/- 0.033\n",
      "Train Epoch: 761 [4096/194182 (2%)]\tLoss: 0.369777\tGrad Norm: 1.270432\tLR: 0.030000\n",
      "Train Epoch: 761 [24576/194182 (12%)]\tLoss: 0.374703\tGrad Norm: 1.233903\tLR: 0.030000\n",
      "Train Epoch: 761 [45056/194182 (23%)]\tLoss: 0.374738\tGrad Norm: 1.192309\tLR: 0.030000\n",
      "Train Epoch: 761 [65536/194182 (33%)]\tLoss: 0.382031\tGrad Norm: 1.154392\tLR: 0.030000\n",
      "Train Epoch: 761 [86016/194182 (44%)]\tLoss: 0.389003\tGrad Norm: 1.501806\tLR: 0.030000\n",
      "Train Epoch: 761 [106496/194182 (54%)]\tLoss: 0.374159\tGrad Norm: 1.194005\tLR: 0.030000\n",
      "Train Epoch: 761 [126976/194182 (65%)]\tLoss: 0.374228\tGrad Norm: 1.176505\tLR: 0.030000\n",
      "Train Epoch: 761 [147456/194182 (75%)]\tLoss: 0.382528\tGrad Norm: 1.529988\tLR: 0.030000\n",
      "Train Epoch: 761 [167936/194182 (85%)]\tLoss: 0.384161\tGrad Norm: 1.559572\tLR: 0.030000\n",
      "Train Epoch: 761 [188416/194182 (96%)]\tLoss: 0.378780\tGrad Norm: 1.009599\tLR: 0.030000\n",
      "Train set: Average loss: 0.3795\n",
      "Test set: Average loss: 0.2416, Average MAE: 0.3516\n",
      "Train Epoch: 762 [4096/194182 (2%)]\tLoss: 0.378755\tGrad Norm: 1.080080\tLR: 0.030000\n",
      "Train Epoch: 762 [24576/194182 (12%)]\tLoss: 0.382905\tGrad Norm: 1.494624\tLR: 0.030000\n",
      "Train Epoch: 762 [45056/194182 (23%)]\tLoss: 0.384650\tGrad Norm: 1.650966\tLR: 0.030000\n",
      "Train Epoch: 762 [65536/194182 (33%)]\tLoss: 0.381277\tGrad Norm: 1.375744\tLR: 0.030000\n",
      "Train Epoch: 762 [86016/194182 (44%)]\tLoss: 0.374815\tGrad Norm: 1.147337\tLR: 0.030000\n",
      "Train Epoch: 762 [106496/194182 (54%)]\tLoss: 0.374732\tGrad Norm: 1.090108\tLR: 0.030000\n",
      "Train Epoch: 762 [126976/194182 (65%)]\tLoss: 0.376927\tGrad Norm: 0.899364\tLR: 0.030000\n",
      "Train Epoch: 762 [147456/194182 (75%)]\tLoss: 0.380024\tGrad Norm: 1.104421\tLR: 0.030000\n",
      "Train Epoch: 762 [167936/194182 (85%)]\tLoss: 0.376402\tGrad Norm: 1.338390\tLR: 0.030000\n",
      "Train Epoch: 762 [188416/194182 (96%)]\tLoss: 0.389068\tGrad Norm: 1.437207\tLR: 0.030000\n",
      "Train set: Average loss: 0.3789\n",
      "Test set: Average loss: 0.2419, Average MAE: 0.3472\n",
      "Train Epoch: 763 [4096/194182 (2%)]\tLoss: 0.372347\tGrad Norm: 1.273974\tLR: 0.030000\n",
      "Train Epoch: 763 [24576/194182 (12%)]\tLoss: 0.373883\tGrad Norm: 1.124832\tLR: 0.030000\n",
      "Train Epoch: 763 [45056/194182 (23%)]\tLoss: 0.377582\tGrad Norm: 1.206401\tLR: 0.030000\n",
      "Train Epoch: 763 [65536/194182 (33%)]\tLoss: 0.380295\tGrad Norm: 1.007048\tLR: 0.030000\n",
      "Train Epoch: 763 [86016/194182 (44%)]\tLoss: 0.376199\tGrad Norm: 1.126341\tLR: 0.030000\n",
      "Train Epoch: 763 [106496/194182 (54%)]\tLoss: 0.376661\tGrad Norm: 1.165394\tLR: 0.030000\n",
      "Train Epoch: 763 [126976/194182 (65%)]\tLoss: 0.372597\tGrad Norm: 1.393035\tLR: 0.030000\n",
      "Train Epoch: 763 [147456/194182 (75%)]\tLoss: 0.378670\tGrad Norm: 1.114538\tLR: 0.030000\n",
      "Train Epoch: 763 [167936/194182 (85%)]\tLoss: 0.372199\tGrad Norm: 1.181293\tLR: 0.030000\n",
      "Train Epoch: 763 [188416/194182 (96%)]\tLoss: 0.375840\tGrad Norm: 1.224205\tLR: 0.030000\n",
      "Train set: Average loss: 0.3782\n",
      "Test set: Average loss: 0.2433, Average MAE: 0.3550\n",
      "Train Epoch: 764 [4096/194182 (2%)]\tLoss: 0.375713\tGrad Norm: 1.287506\tLR: 0.030000\n",
      "Train Epoch: 764 [24576/194182 (12%)]\tLoss: 0.378118\tGrad Norm: 1.345118\tLR: 0.030000\n",
      "Train Epoch: 764 [45056/194182 (23%)]\tLoss: 0.386487\tGrad Norm: 1.421731\tLR: 0.030000\n",
      "Train Epoch: 764 [65536/194182 (33%)]\tLoss: 0.377362\tGrad Norm: 1.191147\tLR: 0.030000\n",
      "Train Epoch: 764 [86016/194182 (44%)]\tLoss: 0.381055\tGrad Norm: 1.068797\tLR: 0.030000\n",
      "Train Epoch: 764 [106496/194182 (54%)]\tLoss: 0.369005\tGrad Norm: 1.131479\tLR: 0.030000\n",
      "Train Epoch: 764 [126976/194182 (65%)]\tLoss: 0.373338\tGrad Norm: 1.059208\tLR: 0.030000\n",
      "Train Epoch: 764 [147456/194182 (75%)]\tLoss: 0.374185\tGrad Norm: 0.986870\tLR: 0.030000\n",
      "Train Epoch: 764 [167936/194182 (85%)]\tLoss: 0.378591\tGrad Norm: 1.193709\tLR: 0.030000\n",
      "Train Epoch: 764 [188416/194182 (96%)]\tLoss: 0.381856\tGrad Norm: 0.877227\tLR: 0.030000\n",
      "Train set: Average loss: 0.3771\n",
      "Test set: Average loss: 0.2438, Average MAE: 0.3424\n",
      "Train Epoch: 765 [4096/194182 (2%)]\tLoss: 0.383427\tGrad Norm: 1.301352\tLR: 0.030000\n",
      "Train Epoch: 765 [24576/194182 (12%)]\tLoss: 0.380772\tGrad Norm: 1.496901\tLR: 0.030000\n",
      "Train Epoch: 765 [45056/194182 (23%)]\tLoss: 0.373102\tGrad Norm: 1.246077\tLR: 0.030000\n",
      "Train Epoch: 765 [65536/194182 (33%)]\tLoss: 0.376228\tGrad Norm: 1.225028\tLR: 0.030000\n",
      "Train Epoch: 765 [86016/194182 (44%)]\tLoss: 0.388603\tGrad Norm: 1.618276\tLR: 0.030000\n",
      "Train Epoch: 765 [106496/194182 (54%)]\tLoss: 0.380215\tGrad Norm: 1.365839\tLR: 0.030000\n",
      "Train Epoch: 765 [126976/194182 (65%)]\tLoss: 0.379746\tGrad Norm: 1.580656\tLR: 0.030000\n",
      "Train Epoch: 765 [147456/194182 (75%)]\tLoss: 0.377000\tGrad Norm: 1.228543\tLR: 0.030000\n",
      "Train Epoch: 765 [167936/194182 (85%)]\tLoss: 0.379203\tGrad Norm: 1.324606\tLR: 0.030000\n",
      "Train Epoch: 765 [188416/194182 (96%)]\tLoss: 0.373310\tGrad Norm: 1.129461\tLR: 0.030000\n",
      "Train set: Average loss: 0.3796\n",
      "Test set: Average loss: 0.2445, Average MAE: 0.3560\n",
      "Epoch 765: Mean reward = 0.053 +/- 0.056\n",
      "Train Epoch: 766 [4096/194182 (2%)]\tLoss: 0.381913\tGrad Norm: 1.408676\tLR: 0.030000\n",
      "Train Epoch: 766 [24576/194182 (12%)]\tLoss: 0.379713\tGrad Norm: 1.435673\tLR: 0.030000\n",
      "Train Epoch: 766 [45056/194182 (23%)]\tLoss: 0.385259\tGrad Norm: 1.474716\tLR: 0.030000\n",
      "Train Epoch: 766 [65536/194182 (33%)]\tLoss: 0.372579\tGrad Norm: 1.290108\tLR: 0.030000\n",
      "Train Epoch: 766 [86016/194182 (44%)]\tLoss: 0.377920\tGrad Norm: 0.975269\tLR: 0.030000\n",
      "Train Epoch: 766 [106496/194182 (54%)]\tLoss: 0.379577\tGrad Norm: 1.181964\tLR: 0.030000\n",
      "Train Epoch: 766 [126976/194182 (65%)]\tLoss: 0.378818\tGrad Norm: 0.976196\tLR: 0.030000\n",
      "Train Epoch: 766 [147456/194182 (75%)]\tLoss: 0.378311\tGrad Norm: 1.250426\tLR: 0.030000\n",
      "Train Epoch: 766 [167936/194182 (85%)]\tLoss: 0.380625\tGrad Norm: 1.420684\tLR: 0.030000\n",
      "Train Epoch: 766 [188416/194182 (96%)]\tLoss: 0.378998\tGrad Norm: 1.498975\tLR: 0.030000\n",
      "Train set: Average loss: 0.3785\n",
      "Test set: Average loss: 0.2439, Average MAE: 0.3408\n",
      "Train Epoch: 767 [4096/194182 (2%)]\tLoss: 0.383861\tGrad Norm: 1.368007\tLR: 0.030000\n",
      "Train Epoch: 767 [24576/194182 (12%)]\tLoss: 0.374613\tGrad Norm: 1.210606\tLR: 0.030000\n",
      "Train Epoch: 767 [45056/194182 (23%)]\tLoss: 0.374519\tGrad Norm: 1.406386\tLR: 0.030000\n",
      "Train Epoch: 767 [65536/194182 (33%)]\tLoss: 0.379791\tGrad Norm: 1.227425\tLR: 0.030000\n",
      "Train Epoch: 767 [86016/194182 (44%)]\tLoss: 0.379477\tGrad Norm: 1.095241\tLR: 0.030000\n",
      "Train Epoch: 767 [106496/194182 (54%)]\tLoss: 0.387495\tGrad Norm: 1.455463\tLR: 0.030000\n",
      "Train Epoch: 767 [126976/194182 (65%)]\tLoss: 0.377072\tGrad Norm: 1.343382\tLR: 0.030000\n",
      "Train Epoch: 767 [147456/194182 (75%)]\tLoss: 0.385258\tGrad Norm: 1.555511\tLR: 0.030000\n",
      "Train Epoch: 767 [167936/194182 (85%)]\tLoss: 0.376689\tGrad Norm: 0.921128\tLR: 0.030000\n",
      "Train Epoch: 767 [188416/194182 (96%)]\tLoss: 0.369507\tGrad Norm: 1.134427\tLR: 0.030000\n",
      "Train set: Average loss: 0.3773\n",
      "Test set: Average loss: 0.2399, Average MAE: 0.3346\n",
      "Train Epoch: 768 [4096/194182 (2%)]\tLoss: 0.366725\tGrad Norm: 1.007013\tLR: 0.030000\n",
      "Train Epoch: 768 [24576/194182 (12%)]\tLoss: 0.377310\tGrad Norm: 1.014344\tLR: 0.030000\n",
      "Train Epoch: 768 [45056/194182 (23%)]\tLoss: 0.372206\tGrad Norm: 1.318374\tLR: 0.030000\n",
      "Train Epoch: 768 [65536/194182 (33%)]\tLoss: 0.373119\tGrad Norm: 0.978648\tLR: 0.030000\n",
      "Train Epoch: 768 [86016/194182 (44%)]\tLoss: 0.376868\tGrad Norm: 1.002235\tLR: 0.030000\n",
      "Train Epoch: 768 [106496/194182 (54%)]\tLoss: 0.371527\tGrad Norm: 0.919023\tLR: 0.030000\n",
      "Train Epoch: 768 [126976/194182 (65%)]\tLoss: 0.374900\tGrad Norm: 1.005244\tLR: 0.030000\n",
      "Train Epoch: 768 [147456/194182 (75%)]\tLoss: 0.383610\tGrad Norm: 1.599446\tLR: 0.030000\n",
      "Train Epoch: 768 [167936/194182 (85%)]\tLoss: 0.390337\tGrad Norm: 1.616515\tLR: 0.030000\n",
      "Train Epoch: 768 [188416/194182 (96%)]\tLoss: 0.379409\tGrad Norm: 1.577966\tLR: 0.030000\n",
      "Train set: Average loss: 0.3769\n",
      "Test set: Average loss: 0.2467, Average MAE: 0.3316\n",
      "Train Epoch: 769 [4096/194182 (2%)]\tLoss: 0.382147\tGrad Norm: 1.620126\tLR: 0.030000\n",
      "Train Epoch: 769 [24576/194182 (12%)]\tLoss: 0.377381\tGrad Norm: 1.647134\tLR: 0.030000\n",
      "Train Epoch: 769 [45056/194182 (23%)]\tLoss: 0.373263\tGrad Norm: 1.137753\tLR: 0.030000\n",
      "Train Epoch: 769 [65536/194182 (33%)]\tLoss: 0.381478\tGrad Norm: 1.435788\tLR: 0.030000\n",
      "Train Epoch: 769 [86016/194182 (44%)]\tLoss: 0.375702\tGrad Norm: 1.219643\tLR: 0.030000\n",
      "Train Epoch: 769 [106496/194182 (54%)]\tLoss: 0.375642\tGrad Norm: 1.081133\tLR: 0.030000\n",
      "Train Epoch: 769 [126976/194182 (65%)]\tLoss: 0.371394\tGrad Norm: 1.249958\tLR: 0.030000\n",
      "Train Epoch: 769 [147456/194182 (75%)]\tLoss: 0.371539\tGrad Norm: 1.275646\tLR: 0.030000\n",
      "Train Epoch: 769 [167936/194182 (85%)]\tLoss: 0.384335\tGrad Norm: 1.214329\tLR: 0.030000\n",
      "Train Epoch: 769 [188416/194182 (96%)]\tLoss: 0.367168\tGrad Norm: 1.146054\tLR: 0.030000\n",
      "Train set: Average loss: 0.3767\n",
      "Test set: Average loss: 0.2365, Average MAE: 0.3408\n",
      "Train Epoch: 770 [4096/194182 (2%)]\tLoss: 0.370627\tGrad Norm: 0.816060\tLR: 0.030000\n",
      "Train Epoch: 770 [24576/194182 (12%)]\tLoss: 0.366654\tGrad Norm: 0.973410\tLR: 0.030000\n",
      "Train Epoch: 770 [45056/194182 (23%)]\tLoss: 0.373152\tGrad Norm: 0.980874\tLR: 0.030000\n",
      "Train Epoch: 770 [65536/194182 (33%)]\tLoss: 0.380165\tGrad Norm: 1.117762\tLR: 0.030000\n",
      "Train Epoch: 770 [86016/194182 (44%)]\tLoss: 0.387639\tGrad Norm: 1.631142\tLR: 0.030000\n",
      "Train Epoch: 770 [106496/194182 (54%)]\tLoss: 0.371308\tGrad Norm: 1.321080\tLR: 0.030000\n",
      "Train Epoch: 770 [126976/194182 (65%)]\tLoss: 0.373399\tGrad Norm: 1.232245\tLR: 0.030000\n",
      "Train Epoch: 770 [147456/194182 (75%)]\tLoss: 0.381856\tGrad Norm: 1.300619\tLR: 0.030000\n",
      "Train Epoch: 770 [167936/194182 (85%)]\tLoss: 0.380377\tGrad Norm: 1.010360\tLR: 0.030000\n",
      "Train Epoch: 770 [188416/194182 (96%)]\tLoss: 0.375110\tGrad Norm: 1.283650\tLR: 0.030000\n",
      "Train set: Average loss: 0.3754\n",
      "Test set: Average loss: 0.2460, Average MAE: 0.3555\n",
      "Epoch 770: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 771 [4096/194182 (2%)]\tLoss: 0.373736\tGrad Norm: 1.601800\tLR: 0.030000\n",
      "Train Epoch: 771 [24576/194182 (12%)]\tLoss: 0.373022\tGrad Norm: 1.098621\tLR: 0.030000\n",
      "Train Epoch: 771 [45056/194182 (23%)]\tLoss: 0.367595\tGrad Norm: 1.204958\tLR: 0.030000\n",
      "Train Epoch: 771 [65536/194182 (33%)]\tLoss: 0.384519\tGrad Norm: 1.427713\tLR: 0.030000\n",
      "Train Epoch: 771 [86016/194182 (44%)]\tLoss: 0.370654\tGrad Norm: 1.181954\tLR: 0.030000\n",
      "Train Epoch: 771 [106496/194182 (54%)]\tLoss: 0.366764\tGrad Norm: 1.203896\tLR: 0.030000\n",
      "Train Epoch: 771 [126976/194182 (65%)]\tLoss: 0.376893\tGrad Norm: 1.049525\tLR: 0.030000\n",
      "Train Epoch: 771 [147456/194182 (75%)]\tLoss: 0.383366\tGrad Norm: 1.475484\tLR: 0.030000\n",
      "Train Epoch: 771 [167936/194182 (85%)]\tLoss: 0.379689\tGrad Norm: 1.436337\tLR: 0.030000\n",
      "Train Epoch: 771 [188416/194182 (96%)]\tLoss: 0.378801\tGrad Norm: 1.210017\tLR: 0.030000\n",
      "Train set: Average loss: 0.3766\n",
      "Test set: Average loss: 0.2438, Average MAE: 0.3497\n",
      "Train Epoch: 772 [4096/194182 (2%)]\tLoss: 0.379891\tGrad Norm: 1.218626\tLR: 0.030000\n",
      "Train Epoch: 772 [24576/194182 (12%)]\tLoss: 0.372679\tGrad Norm: 1.223186\tLR: 0.030000\n",
      "Train Epoch: 772 [45056/194182 (23%)]\tLoss: 0.370440\tGrad Norm: 1.024795\tLR: 0.030000\n",
      "Train Epoch: 772 [65536/194182 (33%)]\tLoss: 0.379874\tGrad Norm: 1.375165\tLR: 0.030000\n",
      "Train Epoch: 772 [86016/194182 (44%)]\tLoss: 0.374905\tGrad Norm: 1.440138\tLR: 0.030000\n",
      "Train Epoch: 772 [106496/194182 (54%)]\tLoss: 0.375848\tGrad Norm: 1.342312\tLR: 0.030000\n",
      "Train Epoch: 772 [126976/194182 (65%)]\tLoss: 0.370391\tGrad Norm: 1.227512\tLR: 0.030000\n",
      "Train Epoch: 772 [147456/194182 (75%)]\tLoss: 0.375051\tGrad Norm: 1.235011\tLR: 0.030000\n",
      "Train Epoch: 772 [167936/194182 (85%)]\tLoss: 0.383793\tGrad Norm: 1.296482\tLR: 0.030000\n",
      "Train Epoch: 772 [188416/194182 (96%)]\tLoss: 0.373724\tGrad Norm: 1.502350\tLR: 0.030000\n",
      "Train set: Average loss: 0.3771\n",
      "Test set: Average loss: 0.2464, Average MAE: 0.3548\n",
      "Train Epoch: 773 [4096/194182 (2%)]\tLoss: 0.380222\tGrad Norm: 1.469172\tLR: 0.030000\n",
      "Train Epoch: 773 [24576/194182 (12%)]\tLoss: 0.384598\tGrad Norm: 1.426707\tLR: 0.030000\n",
      "Train Epoch: 773 [45056/194182 (23%)]\tLoss: 0.371867\tGrad Norm: 1.091885\tLR: 0.030000\n",
      "Train Epoch: 773 [65536/194182 (33%)]\tLoss: 0.377683\tGrad Norm: 0.896815\tLR: 0.030000\n",
      "Train Epoch: 773 [86016/194182 (44%)]\tLoss: 0.375132\tGrad Norm: 1.199243\tLR: 0.030000\n",
      "Train Epoch: 773 [106496/194182 (54%)]\tLoss: 0.372393\tGrad Norm: 1.053490\tLR: 0.030000\n",
      "Train Epoch: 773 [126976/194182 (65%)]\tLoss: 0.368229\tGrad Norm: 1.104797\tLR: 0.030000\n",
      "Train Epoch: 773 [147456/194182 (75%)]\tLoss: 0.381861\tGrad Norm: 1.187232\tLR: 0.030000\n",
      "Train Epoch: 773 [167936/194182 (85%)]\tLoss: 0.380341\tGrad Norm: 1.583939\tLR: 0.030000\n",
      "Train Epoch: 773 [188416/194182 (96%)]\tLoss: 0.374171\tGrad Norm: 1.458059\tLR: 0.030000\n",
      "Train set: Average loss: 0.3753\n",
      "Test set: Average loss: 0.2438, Average MAE: 0.3554\n",
      "Train Epoch: 774 [4096/194182 (2%)]\tLoss: 0.376408\tGrad Norm: 1.327863\tLR: 0.030000\n",
      "Train Epoch: 774 [24576/194182 (12%)]\tLoss: 0.372075\tGrad Norm: 1.157353\tLR: 0.030000\n",
      "Train Epoch: 774 [45056/194182 (23%)]\tLoss: 0.379593\tGrad Norm: 1.674965\tLR: 0.030000\n",
      "Train Epoch: 774 [65536/194182 (33%)]\tLoss: 0.390494\tGrad Norm: 2.040367\tLR: 0.030000\n",
      "Train Epoch: 774 [86016/194182 (44%)]\tLoss: 0.372183\tGrad Norm: 1.543868\tLR: 0.030000\n",
      "Train Epoch: 774 [106496/194182 (54%)]\tLoss: 0.369263\tGrad Norm: 0.668223\tLR: 0.030000\n",
      "Train Epoch: 774 [126976/194182 (65%)]\tLoss: 0.368920\tGrad Norm: 0.718803\tLR: 0.030000\n",
      "Train Epoch: 774 [147456/194182 (75%)]\tLoss: 0.368927\tGrad Norm: 0.701525\tLR: 0.030000\n",
      "Train Epoch: 774 [167936/194182 (85%)]\tLoss: 0.362340\tGrad Norm: 0.971346\tLR: 0.030000\n",
      "Train Epoch: 774 [188416/194182 (96%)]\tLoss: 0.379079\tGrad Norm: 0.956146\tLR: 0.030000\n",
      "Train set: Average loss: 0.3744\n",
      "Test set: Average loss: 0.2388, Average MAE: 0.3375\n",
      "Train Epoch: 775 [4096/194182 (2%)]\tLoss: 0.365955\tGrad Norm: 0.919970\tLR: 0.030000\n",
      "Train Epoch: 775 [24576/194182 (12%)]\tLoss: 0.374147\tGrad Norm: 1.264295\tLR: 0.030000\n",
      "Train Epoch: 775 [45056/194182 (23%)]\tLoss: 0.380393\tGrad Norm: 1.461501\tLR: 0.030000\n",
      "Train Epoch: 775 [65536/194182 (33%)]\tLoss: 0.378633\tGrad Norm: 1.336188\tLR: 0.030000\n",
      "Train Epoch: 775 [86016/194182 (44%)]\tLoss: 0.384587\tGrad Norm: 1.422561\tLR: 0.030000\n",
      "Train Epoch: 775 [106496/194182 (54%)]\tLoss: 0.369658\tGrad Norm: 1.328724\tLR: 0.030000\n",
      "Train Epoch: 775 [126976/194182 (65%)]\tLoss: 0.376281\tGrad Norm: 1.435400\tLR: 0.030000\n",
      "Train Epoch: 775 [147456/194182 (75%)]\tLoss: 0.377187\tGrad Norm: 1.326708\tLR: 0.030000\n",
      "Train Epoch: 775 [167936/194182 (85%)]\tLoss: 0.372050\tGrad Norm: 1.084117\tLR: 0.030000\n",
      "Train Epoch: 775 [188416/194182 (96%)]\tLoss: 0.368072\tGrad Norm: 1.032167\tLR: 0.030000\n",
      "Train set: Average loss: 0.3761\n",
      "Test set: Average loss: 0.2432, Average MAE: 0.3556\n",
      "Epoch 775: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 776 [4096/194182 (2%)]\tLoss: 0.373624\tGrad Norm: 1.335774\tLR: 0.030000\n",
      "Train Epoch: 776 [24576/194182 (12%)]\tLoss: 0.373355\tGrad Norm: 1.283427\tLR: 0.030000\n",
      "Train Epoch: 776 [45056/194182 (23%)]\tLoss: 0.377267\tGrad Norm: 1.416578\tLR: 0.030000\n",
      "Train Epoch: 776 [65536/194182 (33%)]\tLoss: 0.374754\tGrad Norm: 1.488886\tLR: 0.030000\n",
      "Train Epoch: 776 [86016/194182 (44%)]\tLoss: 0.372382\tGrad Norm: 0.992309\tLR: 0.030000\n",
      "Train Epoch: 776 [106496/194182 (54%)]\tLoss: 0.375623\tGrad Norm: 1.205079\tLR: 0.030000\n",
      "Train Epoch: 776 [126976/194182 (65%)]\tLoss: 0.368460\tGrad Norm: 1.236873\tLR: 0.030000\n",
      "Train Epoch: 776 [147456/194182 (75%)]\tLoss: 0.376289\tGrad Norm: 1.368912\tLR: 0.030000\n",
      "Train Epoch: 776 [167936/194182 (85%)]\tLoss: 0.380765\tGrad Norm: 1.210603\tLR: 0.030000\n",
      "Train Epoch: 776 [188416/194182 (96%)]\tLoss: 0.386637\tGrad Norm: 1.458675\tLR: 0.030000\n",
      "Train set: Average loss: 0.3753\n",
      "Test set: Average loss: 0.2415, Average MAE: 0.3545\n",
      "Train Epoch: 777 [4096/194182 (2%)]\tLoss: 0.373215\tGrad Norm: 1.158628\tLR: 0.030000\n",
      "Train Epoch: 777 [24576/194182 (12%)]\tLoss: 0.372501\tGrad Norm: 1.089395\tLR: 0.030000\n",
      "Train Epoch: 777 [45056/194182 (23%)]\tLoss: 0.374808\tGrad Norm: 1.404612\tLR: 0.030000\n",
      "Train Epoch: 777 [65536/194182 (33%)]\tLoss: 0.380560\tGrad Norm: 1.531277\tLR: 0.030000\n",
      "Train Epoch: 777 [86016/194182 (44%)]\tLoss: 0.379945\tGrad Norm: 1.492438\tLR: 0.030000\n",
      "Train Epoch: 777 [106496/194182 (54%)]\tLoss: 0.368161\tGrad Norm: 1.124211\tLR: 0.030000\n",
      "Train Epoch: 777 [126976/194182 (65%)]\tLoss: 0.373102\tGrad Norm: 1.422829\tLR: 0.030000\n",
      "Train Epoch: 777 [147456/194182 (75%)]\tLoss: 0.371993\tGrad Norm: 1.418535\tLR: 0.030000\n",
      "Train Epoch: 777 [167936/194182 (85%)]\tLoss: 0.385768\tGrad Norm: 1.574064\tLR: 0.030000\n",
      "Train Epoch: 777 [188416/194182 (96%)]\tLoss: 0.380146\tGrad Norm: 1.124173\tLR: 0.030000\n",
      "Train set: Average loss: 0.3768\n",
      "Test set: Average loss: 0.2482, Average MAE: 0.3431\n",
      "Train Epoch: 778 [4096/194182 (2%)]\tLoss: 0.378191\tGrad Norm: 1.611917\tLR: 0.030000\n",
      "Train Epoch: 778 [24576/194182 (12%)]\tLoss: 0.366814\tGrad Norm: 1.183009\tLR: 0.030000\n",
      "Train Epoch: 778 [45056/194182 (23%)]\tLoss: 0.371780\tGrad Norm: 0.989465\tLR: 0.030000\n",
      "Train Epoch: 778 [65536/194182 (33%)]\tLoss: 0.372903\tGrad Norm: 1.070203\tLR: 0.030000\n",
      "Train Epoch: 778 [86016/194182 (44%)]\tLoss: 0.367485\tGrad Norm: 0.853375\tLR: 0.030000\n",
      "Train Epoch: 778 [106496/194182 (54%)]\tLoss: 0.374152\tGrad Norm: 0.912109\tLR: 0.030000\n",
      "Train Epoch: 778 [126976/194182 (65%)]\tLoss: 0.368674\tGrad Norm: 1.134521\tLR: 0.030000\n",
      "Train Epoch: 778 [147456/194182 (75%)]\tLoss: 0.369239\tGrad Norm: 1.087190\tLR: 0.030000\n",
      "Train Epoch: 778 [167936/194182 (85%)]\tLoss: 0.375472\tGrad Norm: 1.067509\tLR: 0.030000\n",
      "Train Epoch: 778 [188416/194182 (96%)]\tLoss: 0.370130\tGrad Norm: 1.172200\tLR: 0.030000\n",
      "Train set: Average loss: 0.3723\n",
      "Test set: Average loss: 0.2417, Average MAE: 0.3365\n",
      "Train Epoch: 779 [4096/194182 (2%)]\tLoss: 0.374528\tGrad Norm: 1.249975\tLR: 0.030000\n",
      "Train Epoch: 779 [24576/194182 (12%)]\tLoss: 0.364757\tGrad Norm: 1.109874\tLR: 0.030000\n",
      "Train Epoch: 779 [45056/194182 (23%)]\tLoss: 0.367974\tGrad Norm: 1.148383\tLR: 0.030000\n",
      "Train Epoch: 779 [65536/194182 (33%)]\tLoss: 0.376329\tGrad Norm: 1.646373\tLR: 0.030000\n",
      "Train Epoch: 779 [86016/194182 (44%)]\tLoss: 0.373156\tGrad Norm: 0.941381\tLR: 0.030000\n",
      "Train Epoch: 779 [106496/194182 (54%)]\tLoss: 0.370943\tGrad Norm: 1.159392\tLR: 0.030000\n",
      "Train Epoch: 779 [126976/194182 (65%)]\tLoss: 0.379346\tGrad Norm: 1.535971\tLR: 0.030000\n",
      "Train Epoch: 779 [147456/194182 (75%)]\tLoss: 0.381773\tGrad Norm: 1.178466\tLR: 0.030000\n",
      "Train Epoch: 779 [167936/194182 (85%)]\tLoss: 0.371995\tGrad Norm: 1.031826\tLR: 0.030000\n",
      "Train Epoch: 779 [188416/194182 (96%)]\tLoss: 0.371223\tGrad Norm: 1.153846\tLR: 0.030000\n",
      "Train set: Average loss: 0.3731\n",
      "Test set: Average loss: 0.2431, Average MAE: 0.3366\n",
      "Train Epoch: 780 [4096/194182 (2%)]\tLoss: 0.366375\tGrad Norm: 1.167049\tLR: 0.030000\n",
      "Train Epoch: 780 [24576/194182 (12%)]\tLoss: 0.371464\tGrad Norm: 1.333889\tLR: 0.030000\n",
      "Train Epoch: 780 [45056/194182 (23%)]\tLoss: 0.369488\tGrad Norm: 1.453335\tLR: 0.030000\n",
      "Train Epoch: 780 [65536/194182 (33%)]\tLoss: 0.372533\tGrad Norm: 1.488506\tLR: 0.030000\n",
      "Train Epoch: 780 [86016/194182 (44%)]\tLoss: 0.380761\tGrad Norm: 1.595837\tLR: 0.030000\n",
      "Train Epoch: 780 [106496/194182 (54%)]\tLoss: 0.376004\tGrad Norm: 1.340541\tLR: 0.030000\n",
      "Train Epoch: 780 [126976/194182 (65%)]\tLoss: 0.371123\tGrad Norm: 1.121112\tLR: 0.030000\n",
      "Train Epoch: 780 [147456/194182 (75%)]\tLoss: 0.374442\tGrad Norm: 1.371653\tLR: 0.030000\n",
      "Train Epoch: 780 [167936/194182 (85%)]\tLoss: 0.383566\tGrad Norm: 1.452919\tLR: 0.030000\n",
      "Train Epoch: 780 [188416/194182 (96%)]\tLoss: 0.378004\tGrad Norm: 1.397648\tLR: 0.030000\n",
      "Train set: Average loss: 0.3755\n",
      "Test set: Average loss: 0.2489, Average MAE: 0.3336\n",
      "Epoch 780: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 781 [4096/194182 (2%)]\tLoss: 0.385649\tGrad Norm: 1.714764\tLR: 0.030000\n",
      "Train Epoch: 781 [24576/194182 (12%)]\tLoss: 0.371512\tGrad Norm: 1.109913\tLR: 0.030000\n",
      "Train Epoch: 781 [45056/194182 (23%)]\tLoss: 0.374241\tGrad Norm: 1.320859\tLR: 0.030000\n",
      "Train Epoch: 781 [65536/194182 (33%)]\tLoss: 0.376680\tGrad Norm: 1.543393\tLR: 0.030000\n",
      "Train Epoch: 781 [86016/194182 (44%)]\tLoss: 0.366857\tGrad Norm: 1.056799\tLR: 0.030000\n",
      "Train Epoch: 781 [106496/194182 (54%)]\tLoss: 0.368194\tGrad Norm: 1.193015\tLR: 0.030000\n",
      "Train Epoch: 781 [126976/194182 (65%)]\tLoss: 0.364760\tGrad Norm: 1.270602\tLR: 0.030000\n",
      "Train Epoch: 781 [147456/194182 (75%)]\tLoss: 0.378889\tGrad Norm: 1.369643\tLR: 0.030000\n",
      "Train Epoch: 781 [167936/194182 (85%)]\tLoss: 0.380602\tGrad Norm: 0.958418\tLR: 0.030000\n",
      "Train Epoch: 781 [188416/194182 (96%)]\tLoss: 0.371838\tGrad Norm: 1.160512\tLR: 0.030000\n",
      "Train set: Average loss: 0.3734\n",
      "Test set: Average loss: 0.2428, Average MAE: 0.3349\n",
      "Train Epoch: 782 [4096/194182 (2%)]\tLoss: 0.377529\tGrad Norm: 1.543627\tLR: 0.030000\n",
      "Train Epoch: 782 [24576/194182 (12%)]\tLoss: 0.377187\tGrad Norm: 1.235016\tLR: 0.030000\n",
      "Train Epoch: 782 [45056/194182 (23%)]\tLoss: 0.375745\tGrad Norm: 1.343455\tLR: 0.030000\n",
      "Train Epoch: 782 [65536/194182 (33%)]\tLoss: 0.367867\tGrad Norm: 0.913534\tLR: 0.030000\n",
      "Train Epoch: 782 [86016/194182 (44%)]\tLoss: 0.376205\tGrad Norm: 1.399402\tLR: 0.030000\n",
      "Train Epoch: 782 [106496/194182 (54%)]\tLoss: 0.373463\tGrad Norm: 1.065437\tLR: 0.030000\n",
      "Train Epoch: 782 [126976/194182 (65%)]\tLoss: 0.366134\tGrad Norm: 1.304548\tLR: 0.030000\n",
      "Train Epoch: 782 [147456/194182 (75%)]\tLoss: 0.362611\tGrad Norm: 1.036019\tLR: 0.030000\n",
      "Train Epoch: 782 [167936/194182 (85%)]\tLoss: 0.374089\tGrad Norm: 1.165379\tLR: 0.030000\n",
      "Train Epoch: 782 [188416/194182 (96%)]\tLoss: 0.373620\tGrad Norm: 1.339192\tLR: 0.030000\n",
      "Train set: Average loss: 0.3733\n",
      "Test set: Average loss: 0.2417, Average MAE: 0.3475\n",
      "Train Epoch: 783 [4096/194182 (2%)]\tLoss: 0.370151\tGrad Norm: 1.248409\tLR: 0.030000\n",
      "Train Epoch: 783 [24576/194182 (12%)]\tLoss: 0.374841\tGrad Norm: 1.411984\tLR: 0.030000\n",
      "Train Epoch: 783 [45056/194182 (23%)]\tLoss: 0.374616\tGrad Norm: 1.349885\tLR: 0.030000\n",
      "Train Epoch: 783 [65536/194182 (33%)]\tLoss: 0.371369\tGrad Norm: 0.996238\tLR: 0.030000\n",
      "Train Epoch: 783 [86016/194182 (44%)]\tLoss: 0.370393\tGrad Norm: 1.375978\tLR: 0.030000\n",
      "Train Epoch: 783 [106496/194182 (54%)]\tLoss: 0.369712\tGrad Norm: 1.069983\tLR: 0.030000\n",
      "Train Epoch: 783 [126976/194182 (65%)]\tLoss: 0.369875\tGrad Norm: 0.948220\tLR: 0.030000\n",
      "Train Epoch: 783 [147456/194182 (75%)]\tLoss: 0.370974\tGrad Norm: 1.064349\tLR: 0.030000\n",
      "Train Epoch: 783 [167936/194182 (85%)]\tLoss: 0.374115\tGrad Norm: 1.033799\tLR: 0.030000\n",
      "Train Epoch: 783 [188416/194182 (96%)]\tLoss: 0.374501\tGrad Norm: 1.495903\tLR: 0.030000\n",
      "Train set: Average loss: 0.3723\n",
      "Test set: Average loss: 0.2532, Average MAE: 0.3579\n",
      "Train Epoch: 784 [4096/194182 (2%)]\tLoss: 0.386718\tGrad Norm: 1.771442\tLR: 0.030000\n",
      "Train Epoch: 784 [24576/194182 (12%)]\tLoss: 0.371119\tGrad Norm: 1.355349\tLR: 0.030000\n",
      "Train Epoch: 784 [45056/194182 (23%)]\tLoss: 0.367961\tGrad Norm: 1.032419\tLR: 0.030000\n",
      "Train Epoch: 784 [65536/194182 (33%)]\tLoss: 0.370669\tGrad Norm: 1.233341\tLR: 0.030000\n",
      "Train Epoch: 784 [86016/194182 (44%)]\tLoss: 0.377826\tGrad Norm: 1.295311\tLR: 0.030000\n",
      "Train Epoch: 784 [106496/194182 (54%)]\tLoss: 0.367966\tGrad Norm: 1.259663\tLR: 0.030000\n",
      "Train Epoch: 784 [126976/194182 (65%)]\tLoss: 0.371048\tGrad Norm: 1.196786\tLR: 0.030000\n",
      "Train Epoch: 784 [147456/194182 (75%)]\tLoss: 0.373359\tGrad Norm: 1.448967\tLR: 0.030000\n",
      "Train Epoch: 784 [167936/194182 (85%)]\tLoss: 0.369305\tGrad Norm: 1.250381\tLR: 0.030000\n",
      "Train Epoch: 784 [188416/194182 (96%)]\tLoss: 0.368013\tGrad Norm: 1.201867\tLR: 0.030000\n",
      "Train set: Average loss: 0.3726\n",
      "Test set: Average loss: 0.2435, Average MAE: 0.3324\n",
      "Train Epoch: 785 [4096/194182 (2%)]\tLoss: 0.376962\tGrad Norm: 1.391696\tLR: 0.030000\n",
      "Train Epoch: 785 [24576/194182 (12%)]\tLoss: 0.373731\tGrad Norm: 1.705505\tLR: 0.030000\n",
      "Train Epoch: 785 [45056/194182 (23%)]\tLoss: 0.379597\tGrad Norm: 1.413808\tLR: 0.030000\n",
      "Train Epoch: 785 [65536/194182 (33%)]\tLoss: 0.368359\tGrad Norm: 0.906794\tLR: 0.030000\n",
      "Train Epoch: 785 [86016/194182 (44%)]\tLoss: 0.371592\tGrad Norm: 1.257403\tLR: 0.030000\n",
      "Train Epoch: 785 [106496/194182 (54%)]\tLoss: 0.384087\tGrad Norm: 1.374239\tLR: 0.030000\n",
      "Train Epoch: 785 [126976/194182 (65%)]\tLoss: 0.369991\tGrad Norm: 1.179143\tLR: 0.030000\n",
      "Train Epoch: 785 [147456/194182 (75%)]\tLoss: 0.365393\tGrad Norm: 1.158843\tLR: 0.030000\n",
      "Train Epoch: 785 [167936/194182 (85%)]\tLoss: 0.372428\tGrad Norm: 1.604929\tLR: 0.030000\n",
      "Train Epoch: 785 [188416/194182 (96%)]\tLoss: 0.378978\tGrad Norm: 1.267681\tLR: 0.030000\n",
      "Train set: Average loss: 0.3732\n",
      "Test set: Average loss: 0.2396, Average MAE: 0.3378\n",
      "Epoch 785: Mean reward = 0.036 +/- 0.018\n",
      "Train Epoch: 786 [4096/194182 (2%)]\tLoss: 0.368298\tGrad Norm: 1.083507\tLR: 0.030000\n",
      "Train Epoch: 786 [24576/194182 (12%)]\tLoss: 0.362970\tGrad Norm: 0.892939\tLR: 0.030000\n",
      "Train Epoch: 786 [45056/194182 (23%)]\tLoss: 0.373711\tGrad Norm: 1.153556\tLR: 0.030000\n",
      "Train Epoch: 786 [65536/194182 (33%)]\tLoss: 0.375672\tGrad Norm: 1.450012\tLR: 0.030000\n",
      "Train Epoch: 786 [86016/194182 (44%)]\tLoss: 0.370691\tGrad Norm: 1.257589\tLR: 0.030000\n",
      "Train Epoch: 786 [106496/194182 (54%)]\tLoss: 0.367970\tGrad Norm: 1.031953\tLR: 0.030000\n",
      "Train Epoch: 786 [126976/194182 (65%)]\tLoss: 0.376494\tGrad Norm: 1.091463\tLR: 0.030000\n",
      "Train Epoch: 786 [147456/194182 (75%)]\tLoss: 0.371720\tGrad Norm: 1.243039\tLR: 0.030000\n",
      "Train Epoch: 786 [167936/194182 (85%)]\tLoss: 0.369699\tGrad Norm: 1.377576\tLR: 0.030000\n",
      "Train Epoch: 786 [188416/194182 (96%)]\tLoss: 0.371916\tGrad Norm: 1.322057\tLR: 0.030000\n",
      "Train set: Average loss: 0.3718\n",
      "Test set: Average loss: 0.2423, Average MAE: 0.3453\n",
      "Train Epoch: 787 [4096/194182 (2%)]\tLoss: 0.386035\tGrad Norm: 1.411712\tLR: 0.030000\n",
      "Train Epoch: 787 [24576/194182 (12%)]\tLoss: 0.373490\tGrad Norm: 1.312273\tLR: 0.030000\n",
      "Train Epoch: 787 [45056/194182 (23%)]\tLoss: 0.369295\tGrad Norm: 1.497457\tLR: 0.030000\n",
      "Train Epoch: 787 [65536/194182 (33%)]\tLoss: 0.373954\tGrad Norm: 1.223676\tLR: 0.030000\n",
      "Train Epoch: 787 [86016/194182 (44%)]\tLoss: 0.370236\tGrad Norm: 1.375679\tLR: 0.030000\n",
      "Train Epoch: 787 [106496/194182 (54%)]\tLoss: 0.372535\tGrad Norm: 1.501564\tLR: 0.030000\n",
      "Train Epoch: 787 [126976/194182 (65%)]\tLoss: 0.377226\tGrad Norm: 1.262499\tLR: 0.030000\n",
      "Train Epoch: 787 [147456/194182 (75%)]\tLoss: 0.373520\tGrad Norm: 1.442217\tLR: 0.030000\n",
      "Train Epoch: 787 [167936/194182 (85%)]\tLoss: 0.384993\tGrad Norm: 1.822765\tLR: 0.030000\n",
      "Train Epoch: 787 [188416/194182 (96%)]\tLoss: 0.379161\tGrad Norm: 1.737566\tLR: 0.030000\n",
      "Train set: Average loss: 0.3756\n",
      "Test set: Average loss: 0.2429, Average MAE: 0.3321\n",
      "Train Epoch: 788 [4096/194182 (2%)]\tLoss: 0.369291\tGrad Norm: 1.288366\tLR: 0.030000\n",
      "Train Epoch: 788 [24576/194182 (12%)]\tLoss: 0.373678\tGrad Norm: 1.290443\tLR: 0.030000\n",
      "Train Epoch: 788 [45056/194182 (23%)]\tLoss: 0.370643\tGrad Norm: 1.436970\tLR: 0.030000\n",
      "Train Epoch: 788 [65536/194182 (33%)]\tLoss: 0.364402\tGrad Norm: 0.790507\tLR: 0.030000\n",
      "Train Epoch: 788 [86016/194182 (44%)]\tLoss: 0.372405\tGrad Norm: 0.801639\tLR: 0.030000\n",
      "Train Epoch: 788 [106496/194182 (54%)]\tLoss: 0.361087\tGrad Norm: 1.089962\tLR: 0.030000\n",
      "Train Epoch: 788 [126976/194182 (65%)]\tLoss: 0.366764\tGrad Norm: 1.138852\tLR: 0.030000\n",
      "Train Epoch: 788 [147456/194182 (75%)]\tLoss: 0.374611\tGrad Norm: 1.175040\tLR: 0.030000\n",
      "Train Epoch: 788 [167936/194182 (85%)]\tLoss: 0.376019\tGrad Norm: 1.232402\tLR: 0.030000\n",
      "Train Epoch: 788 [188416/194182 (96%)]\tLoss: 0.375246\tGrad Norm: 1.199276\tLR: 0.030000\n",
      "Train set: Average loss: 0.3697\n",
      "Test set: Average loss: 0.2412, Average MAE: 0.3483\n",
      "Train Epoch: 789 [4096/194182 (2%)]\tLoss: 0.363004\tGrad Norm: 1.077766\tLR: 0.030000\n",
      "Train Epoch: 789 [24576/194182 (12%)]\tLoss: 0.373695\tGrad Norm: 1.343190\tLR: 0.030000\n",
      "Train Epoch: 789 [45056/194182 (23%)]\tLoss: 0.372324\tGrad Norm: 1.365821\tLR: 0.030000\n",
      "Train Epoch: 789 [65536/194182 (33%)]\tLoss: 0.375506\tGrad Norm: 1.398867\tLR: 0.030000\n",
      "Train Epoch: 789 [86016/194182 (44%)]\tLoss: 0.369336\tGrad Norm: 1.103664\tLR: 0.030000\n",
      "Train Epoch: 789 [106496/194182 (54%)]\tLoss: 0.377342\tGrad Norm: 1.508869\tLR: 0.030000\n",
      "Train Epoch: 789 [126976/194182 (65%)]\tLoss: 0.369970\tGrad Norm: 1.614571\tLR: 0.030000\n",
      "Train Epoch: 789 [147456/194182 (75%)]\tLoss: 0.368234\tGrad Norm: 1.314440\tLR: 0.030000\n",
      "Train Epoch: 789 [167936/194182 (85%)]\tLoss: 0.371725\tGrad Norm: 1.266590\tLR: 0.030000\n",
      "Train Epoch: 789 [188416/194182 (96%)]\tLoss: 0.371235\tGrad Norm: 1.353012\tLR: 0.030000\n",
      "Train set: Average loss: 0.3719\n",
      "Test set: Average loss: 0.2469, Average MAE: 0.3574\n",
      "Train Epoch: 790 [4096/194182 (2%)]\tLoss: 0.371530\tGrad Norm: 1.564617\tLR: 0.030000\n",
      "Train Epoch: 790 [24576/194182 (12%)]\tLoss: 0.374159\tGrad Norm: 1.394699\tLR: 0.030000\n",
      "Train Epoch: 790 [45056/194182 (23%)]\tLoss: 0.365773\tGrad Norm: 1.076703\tLR: 0.030000\n",
      "Train Epoch: 790 [65536/194182 (33%)]\tLoss: 0.362862\tGrad Norm: 0.924828\tLR: 0.030000\n",
      "Train Epoch: 790 [86016/194182 (44%)]\tLoss: 0.364312\tGrad Norm: 1.194186\tLR: 0.030000\n",
      "Train Epoch: 790 [106496/194182 (54%)]\tLoss: 0.375771\tGrad Norm: 1.473704\tLR: 0.030000\n",
      "Train Epoch: 790 [126976/194182 (65%)]\tLoss: 0.371228\tGrad Norm: 1.176971\tLR: 0.030000\n",
      "Train Epoch: 790 [147456/194182 (75%)]\tLoss: 0.369624\tGrad Norm: 1.347308\tLR: 0.030000\n",
      "Train Epoch: 790 [167936/194182 (85%)]\tLoss: 0.371565\tGrad Norm: 1.633143\tLR: 0.030000\n",
      "Train Epoch: 790 [188416/194182 (96%)]\tLoss: 0.365135\tGrad Norm: 0.989571\tLR: 0.030000\n",
      "Train set: Average loss: 0.3713\n",
      "Test set: Average loss: 0.2430, Average MAE: 0.3353\n",
      "Epoch 790: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 791 [4096/194182 (2%)]\tLoss: 0.369277\tGrad Norm: 1.272838\tLR: 0.030000\n",
      "Train Epoch: 791 [24576/194182 (12%)]\tLoss: 0.366574\tGrad Norm: 1.218195\tLR: 0.030000\n",
      "Train Epoch: 791 [45056/194182 (23%)]\tLoss: 0.364534\tGrad Norm: 0.977709\tLR: 0.030000\n",
      "Train Epoch: 791 [65536/194182 (33%)]\tLoss: 0.364356\tGrad Norm: 1.066469\tLR: 0.030000\n",
      "Train Epoch: 791 [86016/194182 (44%)]\tLoss: 0.376347\tGrad Norm: 1.709598\tLR: 0.030000\n",
      "Train Epoch: 791 [106496/194182 (54%)]\tLoss: 0.371141\tGrad Norm: 1.342064\tLR: 0.030000\n",
      "Train Epoch: 791 [126976/194182 (65%)]\tLoss: 0.372323\tGrad Norm: 1.233642\tLR: 0.030000\n",
      "Train Epoch: 791 [147456/194182 (75%)]\tLoss: 0.377258\tGrad Norm: 1.470206\tLR: 0.030000\n",
      "Train Epoch: 791 [167936/194182 (85%)]\tLoss: 0.367859\tGrad Norm: 1.211339\tLR: 0.030000\n",
      "Train Epoch: 791 [188416/194182 (96%)]\tLoss: 0.368434\tGrad Norm: 1.214448\tLR: 0.030000\n",
      "Train set: Average loss: 0.3712\n",
      "Test set: Average loss: 0.2428, Average MAE: 0.3441\n",
      "Train Epoch: 792 [4096/194182 (2%)]\tLoss: 0.363003\tGrad Norm: 1.214199\tLR: 0.030000\n",
      "Train Epoch: 792 [24576/194182 (12%)]\tLoss: 0.367717\tGrad Norm: 1.106528\tLR: 0.030000\n",
      "Train Epoch: 792 [45056/194182 (23%)]\tLoss: 0.367129\tGrad Norm: 1.210232\tLR: 0.030000\n",
      "Train Epoch: 792 [65536/194182 (33%)]\tLoss: 0.358643\tGrad Norm: 0.878322\tLR: 0.030000\n",
      "Train Epoch: 792 [86016/194182 (44%)]\tLoss: 0.368137\tGrad Norm: 1.047988\tLR: 0.030000\n",
      "Train Epoch: 792 [106496/194182 (54%)]\tLoss: 0.368067\tGrad Norm: 1.082850\tLR: 0.030000\n",
      "Train Epoch: 792 [126976/194182 (65%)]\tLoss: 0.376012\tGrad Norm: 1.108088\tLR: 0.030000\n",
      "Train Epoch: 792 [147456/194182 (75%)]\tLoss: 0.368249\tGrad Norm: 0.941046\tLR: 0.030000\n",
      "Train Epoch: 792 [167936/194182 (85%)]\tLoss: 0.380632\tGrad Norm: 1.330655\tLR: 0.030000\n",
      "Train Epoch: 792 [188416/194182 (96%)]\tLoss: 0.361703\tGrad Norm: 1.152085\tLR: 0.030000\n",
      "Train set: Average loss: 0.3687\n",
      "Test set: Average loss: 0.2384, Average MAE: 0.3460\n",
      "Train Epoch: 793 [4096/194182 (2%)]\tLoss: 0.364115\tGrad Norm: 0.970977\tLR: 0.030000\n",
      "Train Epoch: 793 [24576/194182 (12%)]\tLoss: 0.368625\tGrad Norm: 1.023625\tLR: 0.030000\n",
      "Train Epoch: 793 [45056/194182 (23%)]\tLoss: 0.369641\tGrad Norm: 1.334632\tLR: 0.030000\n",
      "Train Epoch: 793 [65536/194182 (33%)]\tLoss: 0.370179\tGrad Norm: 1.287524\tLR: 0.030000\n",
      "Train Epoch: 793 [86016/194182 (44%)]\tLoss: 0.376558\tGrad Norm: 1.248433\tLR: 0.030000\n",
      "Train Epoch: 793 [106496/194182 (54%)]\tLoss: 0.384763\tGrad Norm: 1.904152\tLR: 0.030000\n",
      "Train Epoch: 793 [126976/194182 (65%)]\tLoss: 0.365125\tGrad Norm: 1.560801\tLR: 0.030000\n",
      "Train Epoch: 793 [147456/194182 (75%)]\tLoss: 0.373695\tGrad Norm: 1.234578\tLR: 0.030000\n",
      "Train Epoch: 793 [167936/194182 (85%)]\tLoss: 0.376086\tGrad Norm: 1.131322\tLR: 0.030000\n",
      "Train Epoch: 793 [188416/194182 (96%)]\tLoss: 0.366335\tGrad Norm: 1.133490\tLR: 0.030000\n",
      "Train set: Average loss: 0.3706\n",
      "Test set: Average loss: 0.2432, Average MAE: 0.3510\n",
      "Train Epoch: 794 [4096/194182 (2%)]\tLoss: 0.369127\tGrad Norm: 1.291320\tLR: 0.030000\n",
      "Train Epoch: 794 [24576/194182 (12%)]\tLoss: 0.370508\tGrad Norm: 1.429789\tLR: 0.030000\n",
      "Train Epoch: 794 [45056/194182 (23%)]\tLoss: 0.374873\tGrad Norm: 1.558033\tLR: 0.030000\n",
      "Train Epoch: 794 [65536/194182 (33%)]\tLoss: 0.366643\tGrad Norm: 1.298567\tLR: 0.030000\n",
      "Train Epoch: 794 [86016/194182 (44%)]\tLoss: 0.371626\tGrad Norm: 1.209857\tLR: 0.030000\n",
      "Train Epoch: 794 [106496/194182 (54%)]\tLoss: 0.374670\tGrad Norm: 1.570788\tLR: 0.030000\n",
      "Train Epoch: 794 [126976/194182 (65%)]\tLoss: 0.358998\tGrad Norm: 1.100785\tLR: 0.030000\n",
      "Train Epoch: 794 [147456/194182 (75%)]\tLoss: 0.367292\tGrad Norm: 1.501524\tLR: 0.030000\n",
      "Train Epoch: 794 [167936/194182 (85%)]\tLoss: 0.361583\tGrad Norm: 0.861450\tLR: 0.030000\n",
      "Train Epoch: 794 [188416/194182 (96%)]\tLoss: 0.377607\tGrad Norm: 1.439061\tLR: 0.030000\n",
      "Train set: Average loss: 0.3710\n",
      "Test set: Average loss: 0.2473, Average MAE: 0.3348\n",
      "Train Epoch: 795 [4096/194182 (2%)]\tLoss: 0.373065\tGrad Norm: 1.509732\tLR: 0.030000\n",
      "Train Epoch: 795 [24576/194182 (12%)]\tLoss: 0.364401\tGrad Norm: 1.214934\tLR: 0.030000\n",
      "Train Epoch: 795 [45056/194182 (23%)]\tLoss: 0.377427\tGrad Norm: 1.430254\tLR: 0.030000\n",
      "Train Epoch: 795 [65536/194182 (33%)]\tLoss: 0.373658\tGrad Norm: 1.212434\tLR: 0.030000\n",
      "Train Epoch: 795 [86016/194182 (44%)]\tLoss: 0.366533\tGrad Norm: 1.224901\tLR: 0.030000\n",
      "Train Epoch: 795 [106496/194182 (54%)]\tLoss: 0.370289\tGrad Norm: 1.205660\tLR: 0.030000\n",
      "Train Epoch: 795 [126976/194182 (65%)]\tLoss: 0.368384\tGrad Norm: 0.929736\tLR: 0.030000\n",
      "Train Epoch: 795 [147456/194182 (75%)]\tLoss: 0.377002\tGrad Norm: 1.024021\tLR: 0.030000\n",
      "Train Epoch: 795 [167936/194182 (85%)]\tLoss: 0.366744\tGrad Norm: 1.301759\tLR: 0.030000\n",
      "Train Epoch: 795 [188416/194182 (96%)]\tLoss: 0.362142\tGrad Norm: 1.126318\tLR: 0.030000\n",
      "Train set: Average loss: 0.3689\n",
      "Test set: Average loss: 0.2398, Average MAE: 0.3415\n",
      "Epoch 795: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 796 [4096/194182 (2%)]\tLoss: 0.365435\tGrad Norm: 1.048492\tLR: 0.030000\n",
      "Train Epoch: 796 [24576/194182 (12%)]\tLoss: 0.374065\tGrad Norm: 1.496094\tLR: 0.030000\n",
      "Train Epoch: 796 [45056/194182 (23%)]\tLoss: 0.369242\tGrad Norm: 1.204151\tLR: 0.030000\n",
      "Train Epoch: 796 [65536/194182 (33%)]\tLoss: 0.363302\tGrad Norm: 1.099333\tLR: 0.030000\n",
      "Train Epoch: 796 [86016/194182 (44%)]\tLoss: 0.360813\tGrad Norm: 0.945007\tLR: 0.030000\n",
      "Train Epoch: 796 [106496/194182 (54%)]\tLoss: 0.367353\tGrad Norm: 1.059391\tLR: 0.030000\n",
      "Train Epoch: 796 [126976/194182 (65%)]\tLoss: 0.372364\tGrad Norm: 1.182268\tLR: 0.030000\n",
      "Train Epoch: 796 [147456/194182 (75%)]\tLoss: 0.366483\tGrad Norm: 1.284663\tLR: 0.030000\n",
      "Train Epoch: 796 [167936/194182 (85%)]\tLoss: 0.369874\tGrad Norm: 1.346305\tLR: 0.030000\n",
      "Train Epoch: 796 [188416/194182 (96%)]\tLoss: 0.371762\tGrad Norm: 1.631559\tLR: 0.030000\n",
      "Train set: Average loss: 0.3695\n",
      "Test set: Average loss: 0.2427, Average MAE: 0.3524\n",
      "Train Epoch: 797 [4096/194182 (2%)]\tLoss: 0.361812\tGrad Norm: 1.275906\tLR: 0.030000\n",
      "Train Epoch: 797 [24576/194182 (12%)]\tLoss: 0.364966\tGrad Norm: 1.604596\tLR: 0.030000\n",
      "Train Epoch: 797 [45056/194182 (23%)]\tLoss: 0.365399\tGrad Norm: 1.352652\tLR: 0.030000\n",
      "Train Epoch: 797 [65536/194182 (33%)]\tLoss: 0.371794\tGrad Norm: 1.698613\tLR: 0.030000\n",
      "Train Epoch: 797 [86016/194182 (44%)]\tLoss: 0.383269\tGrad Norm: 1.282485\tLR: 0.030000\n",
      "Train Epoch: 797 [106496/194182 (54%)]\tLoss: 0.366998\tGrad Norm: 0.888176\tLR: 0.030000\n",
      "Train Epoch: 797 [126976/194182 (65%)]\tLoss: 0.362758\tGrad Norm: 0.945195\tLR: 0.030000\n",
      "Train Epoch: 797 [147456/194182 (75%)]\tLoss: 0.362236\tGrad Norm: 1.365665\tLR: 0.030000\n",
      "Train Epoch: 797 [167936/194182 (85%)]\tLoss: 0.368416\tGrad Norm: 1.232568\tLR: 0.030000\n",
      "Train Epoch: 797 [188416/194182 (96%)]\tLoss: 0.375461\tGrad Norm: 1.548523\tLR: 0.030000\n",
      "Train set: Average loss: 0.3702\n",
      "Test set: Average loss: 0.2428, Average MAE: 0.3525\n",
      "Train Epoch: 798 [4096/194182 (2%)]\tLoss: 0.370461\tGrad Norm: 1.221904\tLR: 0.030000\n",
      "Train Epoch: 798 [24576/194182 (12%)]\tLoss: 0.367324\tGrad Norm: 1.125823\tLR: 0.030000\n",
      "Train Epoch: 798 [45056/194182 (23%)]\tLoss: 0.374589\tGrad Norm: 1.278121\tLR: 0.030000\n",
      "Train Epoch: 798 [65536/194182 (33%)]\tLoss: 0.369551\tGrad Norm: 1.432073\tLR: 0.030000\n",
      "Train Epoch: 798 [86016/194182 (44%)]\tLoss: 0.369610\tGrad Norm: 1.485688\tLR: 0.030000\n",
      "Train Epoch: 798 [106496/194182 (54%)]\tLoss: 0.376180\tGrad Norm: 1.640725\tLR: 0.030000\n",
      "Train Epoch: 798 [126976/194182 (65%)]\tLoss: 0.365900\tGrad Norm: 1.283351\tLR: 0.030000\n",
      "Train Epoch: 798 [147456/194182 (75%)]\tLoss: 0.366513\tGrad Norm: 0.998939\tLR: 0.030000\n",
      "Train Epoch: 798 [167936/194182 (85%)]\tLoss: 0.366252\tGrad Norm: 0.999165\tLR: 0.030000\n",
      "Train Epoch: 798 [188416/194182 (96%)]\tLoss: 0.371902\tGrad Norm: 1.158335\tLR: 0.030000\n",
      "Train set: Average loss: 0.3683\n",
      "Test set: Average loss: 0.2430, Average MAE: 0.3539\n",
      "Train Epoch: 799 [4096/194182 (2%)]\tLoss: 0.364934\tGrad Norm: 1.207395\tLR: 0.030000\n",
      "Train Epoch: 799 [24576/194182 (12%)]\tLoss: 0.361491\tGrad Norm: 0.934366\tLR: 0.030000\n",
      "Train Epoch: 799 [45056/194182 (23%)]\tLoss: 0.370388\tGrad Norm: 0.965055\tLR: 0.030000\n",
      "Train Epoch: 799 [65536/194182 (33%)]\tLoss: 0.365542\tGrad Norm: 1.073436\tLR: 0.030000\n",
      "Train Epoch: 799 [86016/194182 (44%)]\tLoss: 0.370494\tGrad Norm: 1.111883\tLR: 0.030000\n",
      "Train Epoch: 799 [106496/194182 (54%)]\tLoss: 0.366448\tGrad Norm: 1.188017\tLR: 0.030000\n",
      "Train Epoch: 799 [126976/194182 (65%)]\tLoss: 0.372571\tGrad Norm: 1.271540\tLR: 0.030000\n",
      "Train Epoch: 799 [147456/194182 (75%)]\tLoss: 0.372230\tGrad Norm: 1.541197\tLR: 0.030000\n",
      "Train Epoch: 799 [167936/194182 (85%)]\tLoss: 0.364770\tGrad Norm: 1.200974\tLR: 0.030000\n",
      "Train Epoch: 799 [188416/194182 (96%)]\tLoss: 0.367222\tGrad Norm: 1.184259\tLR: 0.030000\n",
      "Train set: Average loss: 0.3679\n",
      "Test set: Average loss: 0.2413, Average MAE: 0.3412\n",
      "Train Epoch: 800 [4096/194182 (2%)]\tLoss: 0.367635\tGrad Norm: 1.139507\tLR: 0.030000\n",
      "Train Epoch: 800 [24576/194182 (12%)]\tLoss: 0.376097\tGrad Norm: 1.275837\tLR: 0.030000\n",
      "Train Epoch: 800 [45056/194182 (23%)]\tLoss: 0.371086\tGrad Norm: 1.451737\tLR: 0.030000\n",
      "Train Epoch: 800 [65536/194182 (33%)]\tLoss: 0.369983\tGrad Norm: 1.446267\tLR: 0.030000\n",
      "Train Epoch: 800 [86016/194182 (44%)]\tLoss: 0.365117\tGrad Norm: 1.309007\tLR: 0.030000\n",
      "Train Epoch: 800 [106496/194182 (54%)]\tLoss: 0.371159\tGrad Norm: 1.393149\tLR: 0.030000\n",
      "Train Epoch: 800 [126976/194182 (65%)]\tLoss: 0.377903\tGrad Norm: 1.437071\tLR: 0.030000\n",
      "Train Epoch: 800 [147456/194182 (75%)]\tLoss: 0.372562\tGrad Norm: 1.643085\tLR: 0.030000\n",
      "Train Epoch: 800 [167936/194182 (85%)]\tLoss: 0.377634\tGrad Norm: 1.756913\tLR: 0.030000\n",
      "Train Epoch: 800 [188416/194182 (96%)]\tLoss: 0.374190\tGrad Norm: 1.601866\tLR: 0.030000\n",
      "Train set: Average loss: 0.3711\n",
      "Test set: Average loss: 0.2540, Average MAE: 0.3699\n",
      "Epoch 800: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 801 [4096/194182 (2%)]\tLoss: 0.374337\tGrad Norm: 1.998649\tLR: 0.030000\n",
      "Train Epoch: 801 [24576/194182 (12%)]\tLoss: 0.371996\tGrad Norm: 1.362962\tLR: 0.030000\n",
      "Train Epoch: 801 [45056/194182 (23%)]\tLoss: 0.365424\tGrad Norm: 1.158549\tLR: 0.030000\n",
      "Train Epoch: 801 [65536/194182 (33%)]\tLoss: 0.374085\tGrad Norm: 1.081260\tLR: 0.030000\n",
      "Train Epoch: 801 [86016/194182 (44%)]\tLoss: 0.363634\tGrad Norm: 1.332419\tLR: 0.030000\n",
      "Train Epoch: 801 [106496/194182 (54%)]\tLoss: 0.369321\tGrad Norm: 1.328226\tLR: 0.030000\n",
      "Train Epoch: 801 [126976/194182 (65%)]\tLoss: 0.370451\tGrad Norm: 1.465022\tLR: 0.030000\n",
      "Train Epoch: 801 [147456/194182 (75%)]\tLoss: 0.376948\tGrad Norm: 1.739875\tLR: 0.030000\n",
      "Train Epoch: 801 [167936/194182 (85%)]\tLoss: 0.373332\tGrad Norm: 1.254047\tLR: 0.030000\n",
      "Train Epoch: 801 [188416/194182 (96%)]\tLoss: 0.355635\tGrad Norm: 0.863077\tLR: 0.030000\n",
      "Train set: Average loss: 0.3698\n",
      "Test set: Average loss: 0.2391, Average MAE: 0.3449\n",
      "Train Epoch: 802 [4096/194182 (2%)]\tLoss: 0.362379\tGrad Norm: 0.835019\tLR: 0.030000\n",
      "Train Epoch: 802 [24576/194182 (12%)]\tLoss: 0.362442\tGrad Norm: 1.044820\tLR: 0.030000\n",
      "Train Epoch: 802 [45056/194182 (23%)]\tLoss: 0.374185\tGrad Norm: 1.398975\tLR: 0.030000\n",
      "Train Epoch: 802 [65536/194182 (33%)]\tLoss: 0.366205\tGrad Norm: 1.175536\tLR: 0.030000\n",
      "Train Epoch: 802 [86016/194182 (44%)]\tLoss: 0.371703\tGrad Norm: 1.226001\tLR: 0.030000\n",
      "Train Epoch: 802 [106496/194182 (54%)]\tLoss: 0.369611\tGrad Norm: 1.269820\tLR: 0.030000\n",
      "Train Epoch: 802 [126976/194182 (65%)]\tLoss: 0.368149\tGrad Norm: 1.233831\tLR: 0.030000\n",
      "Train Epoch: 802 [147456/194182 (75%)]\tLoss: 0.366342\tGrad Norm: 0.839913\tLR: 0.030000\n",
      "Train Epoch: 802 [167936/194182 (85%)]\tLoss: 0.364227\tGrad Norm: 1.088163\tLR: 0.030000\n",
      "Train Epoch: 802 [188416/194182 (96%)]\tLoss: 0.372513\tGrad Norm: 1.335284\tLR: 0.030000\n",
      "Train set: Average loss: 0.3659\n",
      "Test set: Average loss: 0.2421, Average MAE: 0.3493\n",
      "Train Epoch: 803 [4096/194182 (2%)]\tLoss: 0.369099\tGrad Norm: 1.258216\tLR: 0.030000\n",
      "Train Epoch: 803 [24576/194182 (12%)]\tLoss: 0.373336\tGrad Norm: 1.497126\tLR: 0.030000\n",
      "Train Epoch: 803 [45056/194182 (23%)]\tLoss: 0.363780\tGrad Norm: 1.159457\tLR: 0.030000\n",
      "Train Epoch: 803 [65536/194182 (33%)]\tLoss: 0.368663\tGrad Norm: 1.328091\tLR: 0.030000\n",
      "Train Epoch: 803 [86016/194182 (44%)]\tLoss: 0.366207\tGrad Norm: 1.379230\tLR: 0.030000\n",
      "Train Epoch: 803 [106496/194182 (54%)]\tLoss: 0.366288\tGrad Norm: 1.123108\tLR: 0.030000\n",
      "Train Epoch: 803 [126976/194182 (65%)]\tLoss: 0.369773\tGrad Norm: 1.371520\tLR: 0.030000\n",
      "Train Epoch: 803 [147456/194182 (75%)]\tLoss: 0.367187\tGrad Norm: 1.189083\tLR: 0.030000\n",
      "Train Epoch: 803 [167936/194182 (85%)]\tLoss: 0.362234\tGrad Norm: 1.013380\tLR: 0.030000\n",
      "Train Epoch: 803 [188416/194182 (96%)]\tLoss: 0.362933\tGrad Norm: 0.800682\tLR: 0.030000\n",
      "Train set: Average loss: 0.3668\n",
      "Test set: Average loss: 0.2364, Average MAE: 0.3385\n",
      "Train Epoch: 804 [4096/194182 (2%)]\tLoss: 0.359599\tGrad Norm: 0.698795\tLR: 0.030000\n",
      "Train Epoch: 804 [24576/194182 (12%)]\tLoss: 0.369442\tGrad Norm: 1.161826\tLR: 0.030000\n",
      "Train Epoch: 804 [45056/194182 (23%)]\tLoss: 0.367313\tGrad Norm: 1.146750\tLR: 0.030000\n",
      "Train Epoch: 804 [65536/194182 (33%)]\tLoss: 0.359601\tGrad Norm: 1.170936\tLR: 0.030000\n",
      "Train Epoch: 804 [86016/194182 (44%)]\tLoss: 0.361705\tGrad Norm: 1.343644\tLR: 0.030000\n",
      "Train Epoch: 804 [106496/194182 (54%)]\tLoss: 0.372460\tGrad Norm: 1.592403\tLR: 0.030000\n",
      "Train Epoch: 804 [126976/194182 (65%)]\tLoss: 0.365230\tGrad Norm: 1.151088\tLR: 0.030000\n",
      "Train Epoch: 804 [147456/194182 (75%)]\tLoss: 0.370701\tGrad Norm: 1.264730\tLR: 0.030000\n",
      "Train Epoch: 804 [167936/194182 (85%)]\tLoss: 0.361581\tGrad Norm: 1.310829\tLR: 0.030000\n",
      "Train Epoch: 804 [188416/194182 (96%)]\tLoss: 0.375480\tGrad Norm: 1.533881\tLR: 0.030000\n",
      "Train set: Average loss: 0.3675\n",
      "Test set: Average loss: 0.2515, Average MAE: 0.3639\n",
      "Train Epoch: 805 [4096/194182 (2%)]\tLoss: 0.380403\tGrad Norm: 1.860314\tLR: 0.030000\n",
      "Train Epoch: 805 [24576/194182 (12%)]\tLoss: 0.366252\tGrad Norm: 1.292888\tLR: 0.030000\n",
      "Train Epoch: 805 [45056/194182 (23%)]\tLoss: 0.363522\tGrad Norm: 1.194045\tLR: 0.030000\n",
      "Train Epoch: 805 [65536/194182 (33%)]\tLoss: 0.362416\tGrad Norm: 1.405312\tLR: 0.030000\n",
      "Train Epoch: 805 [86016/194182 (44%)]\tLoss: 0.373507\tGrad Norm: 1.778510\tLR: 0.030000\n",
      "Train Epoch: 805 [106496/194182 (54%)]\tLoss: 0.374719\tGrad Norm: 1.304246\tLR: 0.030000\n",
      "Train Epoch: 805 [126976/194182 (65%)]\tLoss: 0.369140\tGrad Norm: 1.226386\tLR: 0.030000\n",
      "Train Epoch: 805 [147456/194182 (75%)]\tLoss: 0.367988\tGrad Norm: 1.344876\tLR: 0.030000\n",
      "Train Epoch: 805 [167936/194182 (85%)]\tLoss: 0.363406\tGrad Norm: 1.443272\tLR: 0.030000\n",
      "Train Epoch: 805 [188416/194182 (96%)]\tLoss: 0.361417\tGrad Norm: 1.056063\tLR: 0.030000\n",
      "Train set: Average loss: 0.3688\n",
      "Test set: Average loss: 0.2378, Average MAE: 0.3403\n",
      "Epoch 805: Mean reward = 0.045 +/- 0.033\n",
      "Train Epoch: 806 [4096/194182 (2%)]\tLoss: 0.360733\tGrad Norm: 0.924022\tLR: 0.030000\n",
      "Train Epoch: 806 [24576/194182 (12%)]\tLoss: 0.368156\tGrad Norm: 1.218574\tLR: 0.030000\n",
      "Train Epoch: 806 [45056/194182 (23%)]\tLoss: 0.366472\tGrad Norm: 1.383898\tLR: 0.030000\n",
      "Train Epoch: 806 [65536/194182 (33%)]\tLoss: 0.362625\tGrad Norm: 1.263870\tLR: 0.030000\n",
      "Train Epoch: 806 [86016/194182 (44%)]\tLoss: 0.368711\tGrad Norm: 1.253469\tLR: 0.030000\n",
      "Train Epoch: 806 [106496/194182 (54%)]\tLoss: 0.362899\tGrad Norm: 1.083524\tLR: 0.030000\n",
      "Train Epoch: 806 [126976/194182 (65%)]\tLoss: 0.371047\tGrad Norm: 1.198050\tLR: 0.030000\n",
      "Train Epoch: 806 [147456/194182 (75%)]\tLoss: 0.361660\tGrad Norm: 1.116273\tLR: 0.030000\n",
      "Train Epoch: 806 [167936/194182 (85%)]\tLoss: 0.360344\tGrad Norm: 0.954997\tLR: 0.030000\n",
      "Train Epoch: 806 [188416/194182 (96%)]\tLoss: 0.375915\tGrad Norm: 1.545549\tLR: 0.030000\n",
      "Train set: Average loss: 0.3666\n",
      "Test set: Average loss: 0.2438, Average MAE: 0.3495\n",
      "Train Epoch: 807 [4096/194182 (2%)]\tLoss: 0.369397\tGrad Norm: 1.238195\tLR: 0.030000\n",
      "Train Epoch: 807 [24576/194182 (12%)]\tLoss: 0.370364\tGrad Norm: 1.238947\tLR: 0.030000\n",
      "Train Epoch: 807 [45056/194182 (23%)]\tLoss: 0.365060\tGrad Norm: 1.293425\tLR: 0.030000\n",
      "Train Epoch: 807 [65536/194182 (33%)]\tLoss: 0.371985\tGrad Norm: 1.731644\tLR: 0.030000\n",
      "Train Epoch: 807 [86016/194182 (44%)]\tLoss: 0.374188\tGrad Norm: 1.705513\tLR: 0.030000\n",
      "Train Epoch: 807 [106496/194182 (54%)]\tLoss: 0.368703\tGrad Norm: 1.283926\tLR: 0.030000\n",
      "Train Epoch: 807 [126976/194182 (65%)]\tLoss: 0.358523\tGrad Norm: 1.074258\tLR: 0.030000\n",
      "Train Epoch: 807 [147456/194182 (75%)]\tLoss: 0.358835\tGrad Norm: 1.017683\tLR: 0.030000\n",
      "Train Epoch: 807 [167936/194182 (85%)]\tLoss: 0.364192\tGrad Norm: 1.108234\tLR: 0.030000\n",
      "Train Epoch: 807 [188416/194182 (96%)]\tLoss: 0.366669\tGrad Norm: 1.183513\tLR: 0.030000\n",
      "Train set: Average loss: 0.3668\n",
      "Test set: Average loss: 0.2428, Average MAE: 0.3370\n",
      "Train Epoch: 808 [4096/194182 (2%)]\tLoss: 0.376049\tGrad Norm: 1.251299\tLR: 0.030000\n",
      "Train Epoch: 808 [24576/194182 (12%)]\tLoss: 0.365169\tGrad Norm: 1.031956\tLR: 0.030000\n",
      "Train Epoch: 808 [45056/194182 (23%)]\tLoss: 0.365933\tGrad Norm: 1.370987\tLR: 0.030000\n",
      "Train Epoch: 808 [65536/194182 (33%)]\tLoss: 0.368865\tGrad Norm: 1.199100\tLR: 0.030000\n",
      "Train Epoch: 808 [86016/194182 (44%)]\tLoss: 0.363950\tGrad Norm: 1.256159\tLR: 0.030000\n",
      "Train Epoch: 808 [106496/194182 (54%)]\tLoss: 0.364891\tGrad Norm: 1.208058\tLR: 0.030000\n",
      "Train Epoch: 808 [126976/194182 (65%)]\tLoss: 0.355523\tGrad Norm: 1.124240\tLR: 0.030000\n",
      "Train Epoch: 808 [147456/194182 (75%)]\tLoss: 0.365197\tGrad Norm: 1.447743\tLR: 0.030000\n",
      "Train Epoch: 808 [167936/194182 (85%)]\tLoss: 0.361750\tGrad Norm: 1.199371\tLR: 0.030000\n",
      "Train Epoch: 808 [188416/194182 (96%)]\tLoss: 0.367615\tGrad Norm: 1.043157\tLR: 0.030000\n",
      "Train set: Average loss: 0.3657\n",
      "Test set: Average loss: 0.2388, Average MAE: 0.3486\n",
      "Train Epoch: 809 [4096/194182 (2%)]\tLoss: 0.361863\tGrad Norm: 0.937555\tLR: 0.030000\n",
      "Train Epoch: 809 [24576/194182 (12%)]\tLoss: 0.374349\tGrad Norm: 1.506714\tLR: 0.030000\n",
      "Train Epoch: 809 [45056/194182 (23%)]\tLoss: 0.378130\tGrad Norm: 1.506927\tLR: 0.030000\n",
      "Train Epoch: 809 [65536/194182 (33%)]\tLoss: 0.365214\tGrad Norm: 1.103799\tLR: 0.030000\n",
      "Train Epoch: 809 [86016/194182 (44%)]\tLoss: 0.368839\tGrad Norm: 1.361528\tLR: 0.030000\n",
      "Train Epoch: 809 [106496/194182 (54%)]\tLoss: 0.366344\tGrad Norm: 1.570273\tLR: 0.030000\n",
      "Train Epoch: 809 [126976/194182 (65%)]\tLoss: 0.368495\tGrad Norm: 1.357374\tLR: 0.030000\n",
      "Train Epoch: 809 [147456/194182 (75%)]\tLoss: 0.365602\tGrad Norm: 1.297557\tLR: 0.030000\n",
      "Train Epoch: 809 [167936/194182 (85%)]\tLoss: 0.351484\tGrad Norm: 1.112604\tLR: 0.030000\n",
      "Train Epoch: 809 [188416/194182 (96%)]\tLoss: 0.356799\tGrad Norm: 1.061530\tLR: 0.030000\n",
      "Train set: Average loss: 0.3663\n",
      "Test set: Average loss: 0.2411, Average MAE: 0.3345\n",
      "Train Epoch: 810 [4096/194182 (2%)]\tLoss: 0.363093\tGrad Norm: 1.225330\tLR: 0.030000\n",
      "Train Epoch: 810 [24576/194182 (12%)]\tLoss: 0.372773\tGrad Norm: 1.514314\tLR: 0.030000\n",
      "Train Epoch: 810 [45056/194182 (23%)]\tLoss: 0.358572\tGrad Norm: 1.199275\tLR: 0.030000\n",
      "Train Epoch: 810 [65536/194182 (33%)]\tLoss: 0.362431\tGrad Norm: 1.065447\tLR: 0.030000\n",
      "Train Epoch: 810 [86016/194182 (44%)]\tLoss: 0.355931\tGrad Norm: 0.998200\tLR: 0.030000\n",
      "Train Epoch: 810 [106496/194182 (54%)]\tLoss: 0.370825\tGrad Norm: 1.321570\tLR: 0.030000\n",
      "Train Epoch: 810 [126976/194182 (65%)]\tLoss: 0.377099\tGrad Norm: 1.704983\tLR: 0.030000\n",
      "Train Epoch: 810 [147456/194182 (75%)]\tLoss: 0.363797\tGrad Norm: 1.265401\tLR: 0.030000\n",
      "Train Epoch: 810 [167936/194182 (85%)]\tLoss: 0.362738\tGrad Norm: 1.280563\tLR: 0.030000\n",
      "Train Epoch: 810 [188416/194182 (96%)]\tLoss: 0.371109\tGrad Norm: 1.189560\tLR: 0.030000\n",
      "Train set: Average loss: 0.3660\n",
      "Test set: Average loss: 0.2409, Average MAE: 0.3367\n",
      "Epoch 810: Mean reward = 0.048 +/- 0.054\n",
      "Train Epoch: 811 [4096/194182 (2%)]\tLoss: 0.363960\tGrad Norm: 1.109282\tLR: 0.030000\n",
      "Train Epoch: 811 [24576/194182 (12%)]\tLoss: 0.365246\tGrad Norm: 1.350248\tLR: 0.030000\n",
      "Train Epoch: 811 [45056/194182 (23%)]\tLoss: 0.368999\tGrad Norm: 1.552559\tLR: 0.030000\n",
      "Train Epoch: 811 [65536/194182 (33%)]\tLoss: 0.359765\tGrad Norm: 1.230801\tLR: 0.030000\n",
      "Train Epoch: 811 [86016/194182 (44%)]\tLoss: 0.364733\tGrad Norm: 1.041344\tLR: 0.030000\n",
      "Train Epoch: 811 [106496/194182 (54%)]\tLoss: 0.373298\tGrad Norm: 1.468968\tLR: 0.030000\n",
      "Train Epoch: 811 [126976/194182 (65%)]\tLoss: 0.361599\tGrad Norm: 1.123972\tLR: 0.030000\n",
      "Train Epoch: 811 [147456/194182 (75%)]\tLoss: 0.358819\tGrad Norm: 0.777127\tLR: 0.030000\n",
      "Train Epoch: 811 [167936/194182 (85%)]\tLoss: 0.361443\tGrad Norm: 0.702208\tLR: 0.030000\n",
      "Train Epoch: 811 [188416/194182 (96%)]\tLoss: 0.364257\tGrad Norm: 1.222870\tLR: 0.030000\n",
      "Train set: Average loss: 0.3640\n",
      "Test set: Average loss: 0.2400, Average MAE: 0.3414\n",
      "Train Epoch: 812 [4096/194182 (2%)]\tLoss: 0.361696\tGrad Norm: 1.214261\tLR: 0.030000\n",
      "Train Epoch: 812 [24576/194182 (12%)]\tLoss: 0.367562\tGrad Norm: 0.998161\tLR: 0.030000\n",
      "Train Epoch: 812 [45056/194182 (23%)]\tLoss: 0.363149\tGrad Norm: 1.218690\tLR: 0.030000\n",
      "Train Epoch: 812 [65536/194182 (33%)]\tLoss: 0.365973\tGrad Norm: 1.286918\tLR: 0.030000\n",
      "Train Epoch: 812 [86016/194182 (44%)]\tLoss: 0.365816\tGrad Norm: 1.371526\tLR: 0.030000\n",
      "Train Epoch: 812 [106496/194182 (54%)]\tLoss: 0.367209\tGrad Norm: 1.187949\tLR: 0.030000\n",
      "Train Epoch: 812 [126976/194182 (65%)]\tLoss: 0.362667\tGrad Norm: 1.065573\tLR: 0.030000\n",
      "Train Epoch: 812 [147456/194182 (75%)]\tLoss: 0.360790\tGrad Norm: 1.247054\tLR: 0.030000\n",
      "Train Epoch: 812 [167936/194182 (85%)]\tLoss: 0.361097\tGrad Norm: 0.833897\tLR: 0.030000\n",
      "Train Epoch: 812 [188416/194182 (96%)]\tLoss: 0.362674\tGrad Norm: 1.104192\tLR: 0.030000\n",
      "Train set: Average loss: 0.3638\n",
      "Test set: Average loss: 0.2427, Average MAE: 0.3477\n",
      "Train Epoch: 813 [4096/194182 (2%)]\tLoss: 0.363373\tGrad Norm: 1.304607\tLR: 0.030000\n",
      "Train Epoch: 813 [24576/194182 (12%)]\tLoss: 0.361949\tGrad Norm: 1.321577\tLR: 0.030000\n",
      "Train Epoch: 813 [45056/194182 (23%)]\tLoss: 0.367883\tGrad Norm: 1.257841\tLR: 0.030000\n",
      "Train Epoch: 813 [65536/194182 (33%)]\tLoss: 0.363636\tGrad Norm: 0.962440\tLR: 0.030000\n",
      "Train Epoch: 813 [86016/194182 (44%)]\tLoss: 0.367094\tGrad Norm: 1.455525\tLR: 0.030000\n",
      "Train Epoch: 813 [106496/194182 (54%)]\tLoss: 0.362729\tGrad Norm: 1.481902\tLR: 0.030000\n",
      "Train Epoch: 813 [126976/194182 (65%)]\tLoss: 0.369851\tGrad Norm: 1.284499\tLR: 0.030000\n",
      "Train Epoch: 813 [147456/194182 (75%)]\tLoss: 0.362699\tGrad Norm: 1.221029\tLR: 0.030000\n",
      "Train Epoch: 813 [167936/194182 (85%)]\tLoss: 0.358326\tGrad Norm: 1.199685\tLR: 0.030000\n",
      "Train Epoch: 813 [188416/194182 (96%)]\tLoss: 0.365815\tGrad Norm: 1.160994\tLR: 0.030000\n",
      "Train set: Average loss: 0.3655\n",
      "Test set: Average loss: 0.2384, Average MAE: 0.3393\n",
      "Train Epoch: 814 [4096/194182 (2%)]\tLoss: 0.366207\tGrad Norm: 0.965876\tLR: 0.030000\n",
      "Train Epoch: 814 [24576/194182 (12%)]\tLoss: 0.363814\tGrad Norm: 1.351186\tLR: 0.030000\n",
      "Train Epoch: 814 [45056/194182 (23%)]\tLoss: 0.372566\tGrad Norm: 1.804852\tLR: 0.030000\n",
      "Train Epoch: 814 [65536/194182 (33%)]\tLoss: 0.370712\tGrad Norm: 1.567637\tLR: 0.030000\n",
      "Train Epoch: 814 [86016/194182 (44%)]\tLoss: 0.365488\tGrad Norm: 1.194958\tLR: 0.030000\n",
      "Train Epoch: 814 [106496/194182 (54%)]\tLoss: 0.362367\tGrad Norm: 1.052381\tLR: 0.030000\n",
      "Train Epoch: 814 [126976/194182 (65%)]\tLoss: 0.367047\tGrad Norm: 1.298645\tLR: 0.030000\n",
      "Train Epoch: 814 [147456/194182 (75%)]\tLoss: 0.362549\tGrad Norm: 1.428161\tLR: 0.030000\n",
      "Train Epoch: 814 [167936/194182 (85%)]\tLoss: 0.371109\tGrad Norm: 1.557882\tLR: 0.030000\n",
      "Train Epoch: 814 [188416/194182 (96%)]\tLoss: 0.369503\tGrad Norm: 1.340539\tLR: 0.030000\n",
      "Train set: Average loss: 0.3665\n",
      "Test set: Average loss: 0.2378, Average MAE: 0.3322\n",
      "Train Epoch: 815 [4096/194182 (2%)]\tLoss: 0.364550\tGrad Norm: 1.280300\tLR: 0.030000\n",
      "Train Epoch: 815 [24576/194182 (12%)]\tLoss: 0.358551\tGrad Norm: 1.081032\tLR: 0.030000\n",
      "Train Epoch: 815 [45056/194182 (23%)]\tLoss: 0.361893\tGrad Norm: 1.433234\tLR: 0.030000\n",
      "Train Epoch: 815 [65536/194182 (33%)]\tLoss: 0.368053\tGrad Norm: 1.208358\tLR: 0.030000\n",
      "Train Epoch: 815 [86016/194182 (44%)]\tLoss: 0.362343\tGrad Norm: 1.327882\tLR: 0.030000\n",
      "Train Epoch: 815 [106496/194182 (54%)]\tLoss: 0.360012\tGrad Norm: 1.326018\tLR: 0.030000\n",
      "Train Epoch: 815 [126976/194182 (65%)]\tLoss: 0.361810\tGrad Norm: 1.549283\tLR: 0.030000\n",
      "Train Epoch: 815 [147456/194182 (75%)]\tLoss: 0.375560\tGrad Norm: 1.193566\tLR: 0.030000\n",
      "Train Epoch: 815 [167936/194182 (85%)]\tLoss: 0.370767\tGrad Norm: 1.413876\tLR: 0.030000\n",
      "Train Epoch: 815 [188416/194182 (96%)]\tLoss: 0.365352\tGrad Norm: 1.438153\tLR: 0.030000\n",
      "Train set: Average loss: 0.3652\n",
      "Test set: Average loss: 0.2464, Average MAE: 0.3553\n",
      "Epoch 815: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 816 [4096/194182 (2%)]\tLoss: 0.369211\tGrad Norm: 1.632269\tLR: 0.030000\n",
      "Train Epoch: 816 [24576/194182 (12%)]\tLoss: 0.366415\tGrad Norm: 1.209948\tLR: 0.030000\n",
      "Train Epoch: 816 [45056/194182 (23%)]\tLoss: 0.362345\tGrad Norm: 1.166512\tLR: 0.030000\n",
      "Train Epoch: 816 [65536/194182 (33%)]\tLoss: 0.364214\tGrad Norm: 1.189405\tLR: 0.030000\n",
      "Train Epoch: 816 [86016/194182 (44%)]\tLoss: 0.365032\tGrad Norm: 1.409337\tLR: 0.030000\n",
      "Train Epoch: 816 [106496/194182 (54%)]\tLoss: 0.368268\tGrad Norm: 2.117638\tLR: 0.030000\n",
      "Train Epoch: 816 [126976/194182 (65%)]\tLoss: 0.359072\tGrad Norm: 1.157059\tLR: 0.030000\n",
      "Train Epoch: 816 [147456/194182 (75%)]\tLoss: 0.376318\tGrad Norm: 1.629698\tLR: 0.030000\n",
      "Train Epoch: 816 [167936/194182 (85%)]\tLoss: 0.366700\tGrad Norm: 1.577316\tLR: 0.030000\n",
      "Train Epoch: 816 [188416/194182 (96%)]\tLoss: 0.367044\tGrad Norm: 1.710163\tLR: 0.030000\n",
      "Train set: Average loss: 0.3666\n",
      "Test set: Average loss: 0.2451, Average MAE: 0.3570\n",
      "Train Epoch: 817 [4096/194182 (2%)]\tLoss: 0.361806\tGrad Norm: 1.385510\tLR: 0.030000\n",
      "Train Epoch: 817 [24576/194182 (12%)]\tLoss: 0.369019\tGrad Norm: 1.171408\tLR: 0.030000\n",
      "Train Epoch: 817 [45056/194182 (23%)]\tLoss: 0.353222\tGrad Norm: 0.974571\tLR: 0.030000\n",
      "Train Epoch: 817 [65536/194182 (33%)]\tLoss: 0.358609\tGrad Norm: 0.940280\tLR: 0.030000\n",
      "Train Epoch: 817 [86016/194182 (44%)]\tLoss: 0.359290\tGrad Norm: 1.304501\tLR: 0.030000\n",
      "Train Epoch: 817 [106496/194182 (54%)]\tLoss: 0.368387\tGrad Norm: 1.405103\tLR: 0.030000\n",
      "Train Epoch: 817 [126976/194182 (65%)]\tLoss: 0.358250\tGrad Norm: 1.262201\tLR: 0.030000\n",
      "Train Epoch: 817 [147456/194182 (75%)]\tLoss: 0.364212\tGrad Norm: 1.099314\tLR: 0.030000\n",
      "Train Epoch: 817 [167936/194182 (85%)]\tLoss: 0.363046\tGrad Norm: 1.221085\tLR: 0.030000\n",
      "Train Epoch: 817 [188416/194182 (96%)]\tLoss: 0.352237\tGrad Norm: 0.864179\tLR: 0.030000\n",
      "Train set: Average loss: 0.3638\n",
      "Test set: Average loss: 0.2408, Average MAE: 0.3348\n",
      "Train Epoch: 818 [4096/194182 (2%)]\tLoss: 0.362746\tGrad Norm: 1.210814\tLR: 0.030000\n",
      "Train Epoch: 818 [24576/194182 (12%)]\tLoss: 0.356343\tGrad Norm: 1.110086\tLR: 0.030000\n",
      "Train Epoch: 818 [45056/194182 (23%)]\tLoss: 0.367608\tGrad Norm: 1.450940\tLR: 0.030000\n",
      "Train Epoch: 818 [65536/194182 (33%)]\tLoss: 0.353104\tGrad Norm: 1.127651\tLR: 0.030000\n",
      "Train Epoch: 818 [86016/194182 (44%)]\tLoss: 0.361184\tGrad Norm: 1.495675\tLR: 0.030000\n",
      "Train Epoch: 818 [106496/194182 (54%)]\tLoss: 0.365198\tGrad Norm: 1.296776\tLR: 0.030000\n",
      "Train Epoch: 818 [126976/194182 (65%)]\tLoss: 0.357104\tGrad Norm: 1.282413\tLR: 0.030000\n",
      "Train Epoch: 818 [147456/194182 (75%)]\tLoss: 0.367009\tGrad Norm: 1.367867\tLR: 0.030000\n",
      "Train Epoch: 818 [167936/194182 (85%)]\tLoss: 0.362711\tGrad Norm: 1.547564\tLR: 0.030000\n",
      "Train Epoch: 818 [188416/194182 (96%)]\tLoss: 0.361649\tGrad Norm: 1.278617\tLR: 0.030000\n",
      "Train set: Average loss: 0.3642\n",
      "Test set: Average loss: 0.2454, Average MAE: 0.3351\n",
      "Train Epoch: 819 [4096/194182 (2%)]\tLoss: 0.359608\tGrad Norm: 1.533308\tLR: 0.030000\n",
      "Train Epoch: 819 [24576/194182 (12%)]\tLoss: 0.354317\tGrad Norm: 1.079350\tLR: 0.030000\n",
      "Train Epoch: 819 [45056/194182 (23%)]\tLoss: 0.372609\tGrad Norm: 1.536114\tLR: 0.030000\n",
      "Train Epoch: 819 [65536/194182 (33%)]\tLoss: 0.366260\tGrad Norm: 1.396562\tLR: 0.030000\n",
      "Train Epoch: 819 [86016/194182 (44%)]\tLoss: 0.359351\tGrad Norm: 1.286650\tLR: 0.030000\n",
      "Train Epoch: 819 [106496/194182 (54%)]\tLoss: 0.365676\tGrad Norm: 1.169331\tLR: 0.030000\n",
      "Train Epoch: 819 [126976/194182 (65%)]\tLoss: 0.362144\tGrad Norm: 1.196182\tLR: 0.030000\n",
      "Train Epoch: 819 [147456/194182 (75%)]\tLoss: 0.372082\tGrad Norm: 1.274247\tLR: 0.030000\n",
      "Train Epoch: 819 [167936/194182 (85%)]\tLoss: 0.368806\tGrad Norm: 1.445346\tLR: 0.030000\n",
      "Train Epoch: 819 [188416/194182 (96%)]\tLoss: 0.365978\tGrad Norm: 1.247115\tLR: 0.030000\n",
      "Train set: Average loss: 0.3637\n",
      "Test set: Average loss: 0.2411, Average MAE: 0.3458\n",
      "Train Epoch: 820 [4096/194182 (2%)]\tLoss: 0.355265\tGrad Norm: 1.262339\tLR: 0.030000\n",
      "Train Epoch: 820 [24576/194182 (12%)]\tLoss: 0.357036\tGrad Norm: 1.398256\tLR: 0.030000\n",
      "Train Epoch: 820 [45056/194182 (23%)]\tLoss: 0.367662\tGrad Norm: 1.391998\tLR: 0.030000\n",
      "Train Epoch: 820 [65536/194182 (33%)]\tLoss: 0.365935\tGrad Norm: 1.106941\tLR: 0.030000\n",
      "Train Epoch: 820 [86016/194182 (44%)]\tLoss: 0.359000\tGrad Norm: 0.989376\tLR: 0.030000\n",
      "Train Epoch: 820 [106496/194182 (54%)]\tLoss: 0.349258\tGrad Norm: 0.810263\tLR: 0.030000\n",
      "Train Epoch: 820 [126976/194182 (65%)]\tLoss: 0.362593\tGrad Norm: 1.074886\tLR: 0.030000\n",
      "Train Epoch: 820 [147456/194182 (75%)]\tLoss: 0.368501\tGrad Norm: 1.268734\tLR: 0.030000\n",
      "Train Epoch: 820 [167936/194182 (85%)]\tLoss: 0.367596\tGrad Norm: 1.540736\tLR: 0.030000\n",
      "Train Epoch: 820 [188416/194182 (96%)]\tLoss: 0.363491\tGrad Norm: 1.250395\tLR: 0.030000\n",
      "Train set: Average loss: 0.3626\n",
      "Test set: Average loss: 0.2421, Average MAE: 0.3462\n",
      "Epoch 820: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 821 [4096/194182 (2%)]\tLoss: 0.369332\tGrad Norm: 1.196398\tLR: 0.030000\n",
      "Train Epoch: 821 [24576/194182 (12%)]\tLoss: 0.358829\tGrad Norm: 1.343839\tLR: 0.030000\n",
      "Train Epoch: 821 [45056/194182 (23%)]\tLoss: 0.371006\tGrad Norm: 1.697820\tLR: 0.030000\n",
      "Train Epoch: 821 [65536/194182 (33%)]\tLoss: 0.363078\tGrad Norm: 1.224436\tLR: 0.030000\n",
      "Train Epoch: 821 [86016/194182 (44%)]\tLoss: 0.366795\tGrad Norm: 1.201815\tLR: 0.030000\n",
      "Train Epoch: 821 [106496/194182 (54%)]\tLoss: 0.365641\tGrad Norm: 0.896374\tLR: 0.030000\n",
      "Train Epoch: 821 [126976/194182 (65%)]\tLoss: 0.353750\tGrad Norm: 0.957740\tLR: 0.030000\n",
      "Train Epoch: 821 [147456/194182 (75%)]\tLoss: 0.369499\tGrad Norm: 1.301579\tLR: 0.030000\n",
      "Train Epoch: 821 [167936/194182 (85%)]\tLoss: 0.358859\tGrad Norm: 1.218485\tLR: 0.030000\n",
      "Train Epoch: 821 [188416/194182 (96%)]\tLoss: 0.361721\tGrad Norm: 0.893732\tLR: 0.030000\n",
      "Train set: Average loss: 0.3625\n",
      "Test set: Average loss: 0.2374, Average MAE: 0.3444\n",
      "Train Epoch: 822 [4096/194182 (2%)]\tLoss: 0.350954\tGrad Norm: 0.878660\tLR: 0.030000\n",
      "Train Epoch: 822 [24576/194182 (12%)]\tLoss: 0.357095\tGrad Norm: 1.206926\tLR: 0.030000\n",
      "Train Epoch: 822 [45056/194182 (23%)]\tLoss: 0.361807\tGrad Norm: 1.259881\tLR: 0.030000\n",
      "Train Epoch: 822 [65536/194182 (33%)]\tLoss: 0.357892\tGrad Norm: 1.313334\tLR: 0.030000\n",
      "Train Epoch: 822 [86016/194182 (44%)]\tLoss: 0.364108\tGrad Norm: 1.045784\tLR: 0.030000\n",
      "Train Epoch: 822 [106496/194182 (54%)]\tLoss: 0.365175\tGrad Norm: 1.257005\tLR: 0.030000\n",
      "Train Epoch: 822 [126976/194182 (65%)]\tLoss: 0.354891\tGrad Norm: 1.262637\tLR: 0.030000\n",
      "Train Epoch: 822 [147456/194182 (75%)]\tLoss: 0.358114\tGrad Norm: 1.382761\tLR: 0.030000\n",
      "Train Epoch: 822 [167936/194182 (85%)]\tLoss: 0.362546\tGrad Norm: 1.131057\tLR: 0.030000\n",
      "Train Epoch: 822 [188416/194182 (96%)]\tLoss: 0.365956\tGrad Norm: 1.457192\tLR: 0.030000\n",
      "Train set: Average loss: 0.3616\n",
      "Test set: Average loss: 0.2434, Average MAE: 0.3516\n",
      "Train Epoch: 823 [4096/194182 (2%)]\tLoss: 0.354413\tGrad Norm: 1.271335\tLR: 0.030000\n",
      "Train Epoch: 823 [24576/194182 (12%)]\tLoss: 0.374990\tGrad Norm: 1.566393\tLR: 0.030000\n",
      "Train Epoch: 823 [45056/194182 (23%)]\tLoss: 0.371575\tGrad Norm: 1.692576\tLR: 0.030000\n",
      "Train Epoch: 823 [65536/194182 (33%)]\tLoss: 0.355409\tGrad Norm: 1.213450\tLR: 0.030000\n",
      "Train Epoch: 823 [86016/194182 (44%)]\tLoss: 0.360193\tGrad Norm: 1.108683\tLR: 0.030000\n",
      "Train Epoch: 823 [106496/194182 (54%)]\tLoss: 0.373887\tGrad Norm: 1.605098\tLR: 0.030000\n",
      "Train Epoch: 823 [126976/194182 (65%)]\tLoss: 0.361440\tGrad Norm: 1.209181\tLR: 0.030000\n",
      "Train Epoch: 823 [147456/194182 (75%)]\tLoss: 0.354415\tGrad Norm: 1.325025\tLR: 0.030000\n",
      "Train Epoch: 823 [167936/194182 (85%)]\tLoss: 0.360329\tGrad Norm: 0.646921\tLR: 0.030000\n",
      "Train Epoch: 823 [188416/194182 (96%)]\tLoss: 0.356957\tGrad Norm: 0.835241\tLR: 0.030000\n",
      "Train set: Average loss: 0.3631\n",
      "Test set: Average loss: 0.2382, Average MAE: 0.3392\n",
      "Train Epoch: 824 [4096/194182 (2%)]\tLoss: 0.353398\tGrad Norm: 0.917882\tLR: 0.030000\n",
      "Train Epoch: 824 [24576/194182 (12%)]\tLoss: 0.358620\tGrad Norm: 1.320683\tLR: 0.030000\n",
      "Train Epoch: 824 [45056/194182 (23%)]\tLoss: 0.372608\tGrad Norm: 1.343806\tLR: 0.030000\n",
      "Train Epoch: 824 [65536/194182 (33%)]\tLoss: 0.361599\tGrad Norm: 1.329667\tLR: 0.030000\n",
      "Train Epoch: 824 [86016/194182 (44%)]\tLoss: 0.352769\tGrad Norm: 1.106204\tLR: 0.030000\n",
      "Train Epoch: 824 [106496/194182 (54%)]\tLoss: 0.358207\tGrad Norm: 0.896922\tLR: 0.030000\n",
      "Train Epoch: 824 [126976/194182 (65%)]\tLoss: 0.365457\tGrad Norm: 1.239872\tLR: 0.030000\n",
      "Train Epoch: 824 [147456/194182 (75%)]\tLoss: 0.362139\tGrad Norm: 1.219983\tLR: 0.030000\n",
      "Train Epoch: 824 [167936/194182 (85%)]\tLoss: 0.368981\tGrad Norm: 1.113700\tLR: 0.030000\n",
      "Train Epoch: 824 [188416/194182 (96%)]\tLoss: 0.363539\tGrad Norm: 1.203230\tLR: 0.030000\n",
      "Train set: Average loss: 0.3616\n",
      "Test set: Average loss: 0.2409, Average MAE: 0.3338\n",
      "Train Epoch: 825 [4096/194182 (2%)]\tLoss: 0.361378\tGrad Norm: 1.163130\tLR: 0.030000\n",
      "Train Epoch: 825 [24576/194182 (12%)]\tLoss: 0.364651\tGrad Norm: 1.293476\tLR: 0.030000\n",
      "Train Epoch: 825 [45056/194182 (23%)]\tLoss: 0.355417\tGrad Norm: 1.169787\tLR: 0.030000\n",
      "Train Epoch: 825 [65536/194182 (33%)]\tLoss: 0.369773\tGrad Norm: 1.685469\tLR: 0.030000\n",
      "Train Epoch: 825 [86016/194182 (44%)]\tLoss: 0.365415\tGrad Norm: 1.285248\tLR: 0.030000\n",
      "Train Epoch: 825 [106496/194182 (54%)]\tLoss: 0.358503\tGrad Norm: 1.124222\tLR: 0.030000\n",
      "Train Epoch: 825 [126976/194182 (65%)]\tLoss: 0.361815\tGrad Norm: 1.230001\tLR: 0.030000\n",
      "Train Epoch: 825 [147456/194182 (75%)]\tLoss: 0.359947\tGrad Norm: 0.969008\tLR: 0.030000\n",
      "Train Epoch: 825 [167936/194182 (85%)]\tLoss: 0.357207\tGrad Norm: 1.051497\tLR: 0.030000\n",
      "Train Epoch: 825 [188416/194182 (96%)]\tLoss: 0.349127\tGrad Norm: 1.171170\tLR: 0.030000\n",
      "Train set: Average loss: 0.3610\n",
      "Test set: Average loss: 0.2399, Average MAE: 0.3358\n",
      "Epoch 825: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 826 [4096/194182 (2%)]\tLoss: 0.358317\tGrad Norm: 1.161056\tLR: 0.030000\n",
      "Train Epoch: 826 [24576/194182 (12%)]\tLoss: 0.362972\tGrad Norm: 1.524586\tLR: 0.030000\n",
      "Train Epoch: 826 [45056/194182 (23%)]\tLoss: 0.363286\tGrad Norm: 1.276565\tLR: 0.030000\n",
      "Train Epoch: 826 [65536/194182 (33%)]\tLoss: 0.363755\tGrad Norm: 1.490418\tLR: 0.030000\n",
      "Train Epoch: 826 [86016/194182 (44%)]\tLoss: 0.363898\tGrad Norm: 1.216747\tLR: 0.030000\n",
      "Train Epoch: 826 [106496/194182 (54%)]\tLoss: 0.356782\tGrad Norm: 1.447205\tLR: 0.030000\n",
      "Train Epoch: 826 [126976/194182 (65%)]\tLoss: 0.363115\tGrad Norm: 1.621409\tLR: 0.030000\n",
      "Train Epoch: 826 [147456/194182 (75%)]\tLoss: 0.363181\tGrad Norm: 1.343555\tLR: 0.030000\n",
      "Train Epoch: 826 [167936/194182 (85%)]\tLoss: 0.369114\tGrad Norm: 1.301523\tLR: 0.030000\n",
      "Train Epoch: 826 [188416/194182 (96%)]\tLoss: 0.355172\tGrad Norm: 1.070865\tLR: 0.030000\n",
      "Train set: Average loss: 0.3635\n",
      "Test set: Average loss: 0.2440, Average MAE: 0.3417\n",
      "Train Epoch: 827 [4096/194182 (2%)]\tLoss: 0.361997\tGrad Norm: 1.389413\tLR: 0.030000\n",
      "Train Epoch: 827 [24576/194182 (12%)]\tLoss: 0.361404\tGrad Norm: 1.387544\tLR: 0.030000\n",
      "Train Epoch: 827 [45056/194182 (23%)]\tLoss: 0.365320\tGrad Norm: 1.454241\tLR: 0.030000\n",
      "Train Epoch: 827 [65536/194182 (33%)]\tLoss: 0.365744\tGrad Norm: 1.446466\tLR: 0.030000\n",
      "Train Epoch: 827 [86016/194182 (44%)]\tLoss: 0.369492\tGrad Norm: 1.269909\tLR: 0.030000\n",
      "Train Epoch: 827 [106496/194182 (54%)]\tLoss: 0.365892\tGrad Norm: 1.282370\tLR: 0.030000\n",
      "Train Epoch: 827 [126976/194182 (65%)]\tLoss: 0.368784\tGrad Norm: 1.263485\tLR: 0.030000\n",
      "Train Epoch: 827 [147456/194182 (75%)]\tLoss: 0.365476\tGrad Norm: 1.487086\tLR: 0.030000\n",
      "Train Epoch: 827 [167936/194182 (85%)]\tLoss: 0.363248\tGrad Norm: 1.641211\tLR: 0.030000\n",
      "Train Epoch: 827 [188416/194182 (96%)]\tLoss: 0.365866\tGrad Norm: 1.299264\tLR: 0.030000\n",
      "Train set: Average loss: 0.3631\n",
      "Test set: Average loss: 0.2409, Average MAE: 0.3507\n",
      "Train Epoch: 828 [4096/194182 (2%)]\tLoss: 0.366373\tGrad Norm: 1.058024\tLR: 0.030000\n",
      "Train Epoch: 828 [24576/194182 (12%)]\tLoss: 0.363683\tGrad Norm: 1.362065\tLR: 0.030000\n",
      "Train Epoch: 828 [45056/194182 (23%)]\tLoss: 0.359889\tGrad Norm: 1.422696\tLR: 0.030000\n",
      "Train Epoch: 828 [65536/194182 (33%)]\tLoss: 0.353969\tGrad Norm: 1.300503\tLR: 0.030000\n",
      "Train Epoch: 828 [86016/194182 (44%)]\tLoss: 0.359274\tGrad Norm: 1.298015\tLR: 0.030000\n",
      "Train Epoch: 828 [106496/194182 (54%)]\tLoss: 0.357471\tGrad Norm: 1.027845\tLR: 0.030000\n",
      "Train Epoch: 828 [126976/194182 (65%)]\tLoss: 0.362718\tGrad Norm: 1.360434\tLR: 0.030000\n",
      "Train Epoch: 828 [147456/194182 (75%)]\tLoss: 0.363520\tGrad Norm: 1.263306\tLR: 0.030000\n",
      "Train Epoch: 828 [167936/194182 (85%)]\tLoss: 0.359576\tGrad Norm: 1.533793\tLR: 0.030000\n",
      "Train Epoch: 828 [188416/194182 (96%)]\tLoss: 0.364973\tGrad Norm: 1.175254\tLR: 0.030000\n",
      "Train set: Average loss: 0.3611\n",
      "Test set: Average loss: 0.2409, Average MAE: 0.3323\n",
      "Train Epoch: 829 [4096/194182 (2%)]\tLoss: 0.365568\tGrad Norm: 1.261886\tLR: 0.030000\n",
      "Train Epoch: 829 [24576/194182 (12%)]\tLoss: 0.353661\tGrad Norm: 0.951916\tLR: 0.030000\n",
      "Train Epoch: 829 [45056/194182 (23%)]\tLoss: 0.358105\tGrad Norm: 1.074966\tLR: 0.030000\n",
      "Train Epoch: 829 [65536/194182 (33%)]\tLoss: 0.363592\tGrad Norm: 1.306133\tLR: 0.030000\n",
      "Train Epoch: 829 [86016/194182 (44%)]\tLoss: 0.358784\tGrad Norm: 1.318348\tLR: 0.030000\n",
      "Train Epoch: 829 [106496/194182 (54%)]\tLoss: 0.359535\tGrad Norm: 1.035942\tLR: 0.030000\n",
      "Train Epoch: 829 [126976/194182 (65%)]\tLoss: 0.357618\tGrad Norm: 1.184224\tLR: 0.030000\n",
      "Train Epoch: 829 [147456/194182 (75%)]\tLoss: 0.368902\tGrad Norm: 1.549732\tLR: 0.030000\n",
      "Train Epoch: 829 [167936/194182 (85%)]\tLoss: 0.358295\tGrad Norm: 1.216575\tLR: 0.030000\n",
      "Train Epoch: 829 [188416/194182 (96%)]\tLoss: 0.362611\tGrad Norm: 1.223372\tLR: 0.030000\n",
      "Train set: Average loss: 0.3601\n",
      "Test set: Average loss: 0.2393, Average MAE: 0.3450\n",
      "Train Epoch: 830 [4096/194182 (2%)]\tLoss: 0.353126\tGrad Norm: 1.183851\tLR: 0.030000\n",
      "Train Epoch: 830 [24576/194182 (12%)]\tLoss: 0.357607\tGrad Norm: 1.108925\tLR: 0.030000\n",
      "Train Epoch: 830 [45056/194182 (23%)]\tLoss: 0.361449\tGrad Norm: 1.303875\tLR: 0.030000\n",
      "Train Epoch: 830 [65536/194182 (33%)]\tLoss: 0.364171\tGrad Norm: 1.342312\tLR: 0.030000\n",
      "Train Epoch: 830 [86016/194182 (44%)]\tLoss: 0.359971\tGrad Norm: 1.314106\tLR: 0.030000\n",
      "Train Epoch: 830 [106496/194182 (54%)]\tLoss: 0.363614\tGrad Norm: 1.070459\tLR: 0.030000\n",
      "Train Epoch: 830 [126976/194182 (65%)]\tLoss: 0.361488\tGrad Norm: 1.421898\tLR: 0.030000\n",
      "Train Epoch: 830 [147456/194182 (75%)]\tLoss: 0.361949\tGrad Norm: 1.344333\tLR: 0.030000\n",
      "Train Epoch: 830 [167936/194182 (85%)]\tLoss: 0.373166\tGrad Norm: 1.815631\tLR: 0.030000\n",
      "Train Epoch: 830 [188416/194182 (96%)]\tLoss: 0.369532\tGrad Norm: 1.506681\tLR: 0.030000\n",
      "Train set: Average loss: 0.3621\n",
      "Test set: Average loss: 0.2392, Average MAE: 0.3469\n",
      "Epoch 830: Mean reward = 0.044 +/- 0.030\n",
      "Train Epoch: 831 [4096/194182 (2%)]\tLoss: 0.354910\tGrad Norm: 0.941784\tLR: 0.030000\n",
      "Train Epoch: 831 [24576/194182 (12%)]\tLoss: 0.353766\tGrad Norm: 1.075976\tLR: 0.030000\n",
      "Train Epoch: 831 [45056/194182 (23%)]\tLoss: 0.365289\tGrad Norm: 1.218742\tLR: 0.030000\n",
      "Train Epoch: 831 [65536/194182 (33%)]\tLoss: 0.359167\tGrad Norm: 1.237887\tLR: 0.030000\n",
      "Train Epoch: 831 [86016/194182 (44%)]\tLoss: 0.353347\tGrad Norm: 1.027670\tLR: 0.030000\n",
      "Train Epoch: 831 [106496/194182 (54%)]\tLoss: 0.364779\tGrad Norm: 1.143116\tLR: 0.030000\n",
      "Train Epoch: 831 [126976/194182 (65%)]\tLoss: 0.367101\tGrad Norm: 1.374699\tLR: 0.030000\n",
      "Train Epoch: 831 [147456/194182 (75%)]\tLoss: 0.364930\tGrad Norm: 1.033798\tLR: 0.030000\n",
      "Train Epoch: 831 [167936/194182 (85%)]\tLoss: 0.350409\tGrad Norm: 0.626250\tLR: 0.030000\n",
      "Train Epoch: 831 [188416/194182 (96%)]\tLoss: 0.348836\tGrad Norm: 0.916038\tLR: 0.030000\n",
      "Train set: Average loss: 0.3576\n",
      "Test set: Average loss: 0.2421, Average MAE: 0.3353\n",
      "Train Epoch: 832 [4096/194182 (2%)]\tLoss: 0.364062\tGrad Norm: 1.212736\tLR: 0.030000\n",
      "Train Epoch: 832 [24576/194182 (12%)]\tLoss: 0.366893\tGrad Norm: 1.431719\tLR: 0.030000\n",
      "Train Epoch: 832 [45056/194182 (23%)]\tLoss: 0.356084\tGrad Norm: 1.148679\tLR: 0.030000\n",
      "Train Epoch: 832 [65536/194182 (33%)]\tLoss: 0.370826\tGrad Norm: 1.464292\tLR: 0.030000\n",
      "Train Epoch: 832 [86016/194182 (44%)]\tLoss: 0.352564\tGrad Norm: 1.274824\tLR: 0.030000\n",
      "Train Epoch: 832 [106496/194182 (54%)]\tLoss: 0.353079\tGrad Norm: 1.275393\tLR: 0.030000\n",
      "Train Epoch: 832 [126976/194182 (65%)]\tLoss: 0.355825\tGrad Norm: 1.113965\tLR: 0.030000\n",
      "Train Epoch: 832 [147456/194182 (75%)]\tLoss: 0.361337\tGrad Norm: 1.179370\tLR: 0.030000\n",
      "Train Epoch: 832 [167936/194182 (85%)]\tLoss: 0.359939\tGrad Norm: 1.174195\tLR: 0.030000\n",
      "Train Epoch: 832 [188416/194182 (96%)]\tLoss: 0.355835\tGrad Norm: 1.088407\tLR: 0.030000\n",
      "Train set: Average loss: 0.3604\n",
      "Test set: Average loss: 0.2422, Average MAE: 0.3535\n",
      "Train Epoch: 833 [4096/194182 (2%)]\tLoss: 0.359898\tGrad Norm: 1.210560\tLR: 0.030000\n",
      "Train Epoch: 833 [24576/194182 (12%)]\tLoss: 0.357389\tGrad Norm: 1.125165\tLR: 0.030000\n",
      "Train Epoch: 833 [45056/194182 (23%)]\tLoss: 0.356782\tGrad Norm: 1.177867\tLR: 0.030000\n",
      "Train Epoch: 833 [65536/194182 (33%)]\tLoss: 0.370343\tGrad Norm: 1.708703\tLR: 0.030000\n",
      "Train Epoch: 833 [86016/194182 (44%)]\tLoss: 0.363948\tGrad Norm: 1.526886\tLR: 0.030000\n",
      "Train Epoch: 833 [106496/194182 (54%)]\tLoss: 0.358144\tGrad Norm: 1.147264\tLR: 0.030000\n",
      "Train Epoch: 833 [126976/194182 (65%)]\tLoss: 0.361242\tGrad Norm: 1.225954\tLR: 0.030000\n",
      "Train Epoch: 833 [147456/194182 (75%)]\tLoss: 0.353356\tGrad Norm: 1.076180\tLR: 0.030000\n",
      "Train Epoch: 833 [167936/194182 (85%)]\tLoss: 0.362482\tGrad Norm: 1.656363\tLR: 0.030000\n",
      "Train Epoch: 833 [188416/194182 (96%)]\tLoss: 0.365232\tGrad Norm: 1.210178\tLR: 0.030000\n",
      "Train set: Average loss: 0.3600\n",
      "Test set: Average loss: 0.2449, Average MAE: 0.3518\n",
      "Train Epoch: 834 [4096/194182 (2%)]\tLoss: 0.351066\tGrad Norm: 1.448800\tLR: 0.030000\n",
      "Train Epoch: 834 [24576/194182 (12%)]\tLoss: 0.358458\tGrad Norm: 1.450885\tLR: 0.030000\n",
      "Train Epoch: 834 [45056/194182 (23%)]\tLoss: 0.354812\tGrad Norm: 1.277196\tLR: 0.030000\n",
      "Train Epoch: 834 [65536/194182 (33%)]\tLoss: 0.363513\tGrad Norm: 1.456617\tLR: 0.030000\n",
      "Train Epoch: 834 [86016/194182 (44%)]\tLoss: 0.353134\tGrad Norm: 1.075832\tLR: 0.030000\n",
      "Train Epoch: 834 [106496/194182 (54%)]\tLoss: 0.352996\tGrad Norm: 1.023005\tLR: 0.030000\n",
      "Train Epoch: 834 [126976/194182 (65%)]\tLoss: 0.353027\tGrad Norm: 0.905422\tLR: 0.030000\n",
      "Train Epoch: 834 [147456/194182 (75%)]\tLoss: 0.356524\tGrad Norm: 1.199374\tLR: 0.030000\n",
      "Train Epoch: 834 [167936/194182 (85%)]\tLoss: 0.361359\tGrad Norm: 1.324523\tLR: 0.030000\n",
      "Train Epoch: 834 [188416/194182 (96%)]\tLoss: 0.371949\tGrad Norm: 1.518496\tLR: 0.030000\n",
      "Train set: Average loss: 0.3600\n",
      "Test set: Average loss: 0.2409, Average MAE: 0.3301\n",
      "Train Epoch: 835 [4096/194182 (2%)]\tLoss: 0.354168\tGrad Norm: 1.193218\tLR: 0.030000\n",
      "Train Epoch: 835 [24576/194182 (12%)]\tLoss: 0.357593\tGrad Norm: 1.065503\tLR: 0.030000\n",
      "Train Epoch: 835 [45056/194182 (23%)]\tLoss: 0.357463\tGrad Norm: 1.317346\tLR: 0.030000\n",
      "Train Epoch: 835 [65536/194182 (33%)]\tLoss: 0.354476\tGrad Norm: 1.033155\tLR: 0.030000\n",
      "Train Epoch: 835 [86016/194182 (44%)]\tLoss: 0.350567\tGrad Norm: 1.046096\tLR: 0.030000\n",
      "Train Epoch: 835 [106496/194182 (54%)]\tLoss: 0.360160\tGrad Norm: 1.050786\tLR: 0.030000\n",
      "Train Epoch: 835 [126976/194182 (65%)]\tLoss: 0.355965\tGrad Norm: 1.142221\tLR: 0.030000\n",
      "Train Epoch: 835 [147456/194182 (75%)]\tLoss: 0.359782\tGrad Norm: 0.929814\tLR: 0.030000\n",
      "Train Epoch: 835 [167936/194182 (85%)]\tLoss: 0.356384\tGrad Norm: 1.266889\tLR: 0.030000\n",
      "Train Epoch: 835 [188416/194182 (96%)]\tLoss: 0.359318\tGrad Norm: 1.545440\tLR: 0.030000\n",
      "Train set: Average loss: 0.3576\n",
      "Test set: Average loss: 0.2482, Average MAE: 0.3472\n",
      "Epoch 835: Mean reward = 0.037 +/- 0.019\n",
      "Train Epoch: 836 [4096/194182 (2%)]\tLoss: 0.369001\tGrad Norm: 1.548106\tLR: 0.030000\n",
      "Train Epoch: 836 [24576/194182 (12%)]\tLoss: 0.360116\tGrad Norm: 1.240688\tLR: 0.030000\n",
      "Train Epoch: 836 [45056/194182 (23%)]\tLoss: 0.356799\tGrad Norm: 1.504972\tLR: 0.030000\n",
      "Train Epoch: 836 [65536/194182 (33%)]\tLoss: 0.362404\tGrad Norm: 1.300976\tLR: 0.030000\n",
      "Train Epoch: 836 [86016/194182 (44%)]\tLoss: 0.356529\tGrad Norm: 0.798132\tLR: 0.030000\n",
      "Train Epoch: 836 [106496/194182 (54%)]\tLoss: 0.358576\tGrad Norm: 0.927193\tLR: 0.030000\n",
      "Train Epoch: 836 [126976/194182 (65%)]\tLoss: 0.363749\tGrad Norm: 1.640727\tLR: 0.030000\n",
      "Train Epoch: 836 [147456/194182 (75%)]\tLoss: 0.361902\tGrad Norm: 1.642175\tLR: 0.030000\n",
      "Train Epoch: 836 [167936/194182 (85%)]\tLoss: 0.367292\tGrad Norm: 1.631843\tLR: 0.030000\n",
      "Train Epoch: 836 [188416/194182 (96%)]\tLoss: 0.356226\tGrad Norm: 1.228071\tLR: 0.030000\n",
      "Train set: Average loss: 0.3600\n",
      "Test set: Average loss: 0.2367, Average MAE: 0.3345\n",
      "Train Epoch: 837 [4096/194182 (2%)]\tLoss: 0.356116\tGrad Norm: 1.013423\tLR: 0.030000\n",
      "Train Epoch: 837 [24576/194182 (12%)]\tLoss: 0.355056\tGrad Norm: 0.957541\tLR: 0.030000\n",
      "Train Epoch: 837 [45056/194182 (23%)]\tLoss: 0.354745\tGrad Norm: 0.979891\tLR: 0.030000\n",
      "Train Epoch: 837 [65536/194182 (33%)]\tLoss: 0.355612\tGrad Norm: 1.073063\tLR: 0.030000\n",
      "Train Epoch: 837 [86016/194182 (44%)]\tLoss: 0.359594\tGrad Norm: 1.081835\tLR: 0.030000\n",
      "Train Epoch: 837 [106496/194182 (54%)]\tLoss: 0.358522\tGrad Norm: 1.286457\tLR: 0.030000\n",
      "Train Epoch: 837 [126976/194182 (65%)]\tLoss: 0.357784\tGrad Norm: 1.493992\tLR: 0.030000\n",
      "Train Epoch: 837 [147456/194182 (75%)]\tLoss: 0.361595\tGrad Norm: 1.354442\tLR: 0.030000\n",
      "Train Epoch: 837 [167936/194182 (85%)]\tLoss: 0.353547\tGrad Norm: 1.293248\tLR: 0.030000\n",
      "Train Epoch: 837 [188416/194182 (96%)]\tLoss: 0.359697\tGrad Norm: 1.198999\tLR: 0.030000\n",
      "Train set: Average loss: 0.3573\n",
      "Test set: Average loss: 0.2452, Average MAE: 0.3393\n",
      "Train Epoch: 838 [4096/194182 (2%)]\tLoss: 0.365238\tGrad Norm: 1.694267\tLR: 0.030000\n",
      "Train Epoch: 838 [24576/194182 (12%)]\tLoss: 0.362250\tGrad Norm: 1.621212\tLR: 0.030000\n",
      "Train Epoch: 838 [45056/194182 (23%)]\tLoss: 0.350390\tGrad Norm: 1.055937\tLR: 0.030000\n",
      "Train Epoch: 838 [65536/194182 (33%)]\tLoss: 0.357698\tGrad Norm: 1.222462\tLR: 0.030000\n",
      "Train Epoch: 838 [86016/194182 (44%)]\tLoss: 0.351749\tGrad Norm: 1.032692\tLR: 0.030000\n",
      "Train Epoch: 838 [106496/194182 (54%)]\tLoss: 0.362618\tGrad Norm: 1.513227\tLR: 0.030000\n",
      "Train Epoch: 838 [126976/194182 (65%)]\tLoss: 0.366573\tGrad Norm: 1.613181\tLR: 0.030000\n",
      "Train Epoch: 838 [147456/194182 (75%)]\tLoss: 0.365683\tGrad Norm: 1.239959\tLR: 0.030000\n",
      "Train Epoch: 838 [167936/194182 (85%)]\tLoss: 0.362165\tGrad Norm: 1.442020\tLR: 0.030000\n",
      "Train Epoch: 838 [188416/194182 (96%)]\tLoss: 0.358709\tGrad Norm: 1.378918\tLR: 0.030000\n",
      "Train set: Average loss: 0.3598\n",
      "Test set: Average loss: 0.2427, Average MAE: 0.3296\n",
      "Train Epoch: 839 [4096/194182 (2%)]\tLoss: 0.360689\tGrad Norm: 1.479439\tLR: 0.030000\n",
      "Train Epoch: 839 [24576/194182 (12%)]\tLoss: 0.353533\tGrad Norm: 0.970876\tLR: 0.030000\n",
      "Train Epoch: 839 [45056/194182 (23%)]\tLoss: 0.358738\tGrad Norm: 1.245724\tLR: 0.030000\n",
      "Train Epoch: 839 [65536/194182 (33%)]\tLoss: 0.356261\tGrad Norm: 1.423965\tLR: 0.030000\n",
      "Train Epoch: 839 [86016/194182 (44%)]\tLoss: 0.360034\tGrad Norm: 1.372200\tLR: 0.030000\n",
      "Train Epoch: 839 [106496/194182 (54%)]\tLoss: 0.360964\tGrad Norm: 1.541970\tLR: 0.030000\n",
      "Train Epoch: 839 [126976/194182 (65%)]\tLoss: 0.361434\tGrad Norm: 1.473654\tLR: 0.030000\n",
      "Train Epoch: 839 [147456/194182 (75%)]\tLoss: 0.366545\tGrad Norm: 1.212818\tLR: 0.030000\n",
      "Train Epoch: 839 [167936/194182 (85%)]\tLoss: 0.366500\tGrad Norm: 1.632091\tLR: 0.030000\n",
      "Train Epoch: 839 [188416/194182 (96%)]\tLoss: 0.352205\tGrad Norm: 1.245928\tLR: 0.030000\n",
      "Train set: Average loss: 0.3597\n",
      "Test set: Average loss: 0.2453, Average MAE: 0.3399\n",
      "Train Epoch: 840 [4096/194182 (2%)]\tLoss: 0.357390\tGrad Norm: 1.432196\tLR: 0.030000\n",
      "Train Epoch: 840 [24576/194182 (12%)]\tLoss: 0.362066\tGrad Norm: 1.380693\tLR: 0.030000\n",
      "Train Epoch: 840 [45056/194182 (23%)]\tLoss: 0.371680\tGrad Norm: 1.517189\tLR: 0.030000\n",
      "Train Epoch: 840 [65536/194182 (33%)]\tLoss: 0.370936\tGrad Norm: 1.687585\tLR: 0.030000\n",
      "Train Epoch: 840 [86016/194182 (44%)]\tLoss: 0.351018\tGrad Norm: 1.119240\tLR: 0.030000\n",
      "Train Epoch: 840 [106496/194182 (54%)]\tLoss: 0.351649\tGrad Norm: 0.910383\tLR: 0.030000\n",
      "Train Epoch: 840 [126976/194182 (65%)]\tLoss: 0.348989\tGrad Norm: 0.916122\tLR: 0.030000\n",
      "Train Epoch: 840 [147456/194182 (75%)]\tLoss: 0.357852\tGrad Norm: 1.053746\tLR: 0.030000\n",
      "Train Epoch: 840 [167936/194182 (85%)]\tLoss: 0.364403\tGrad Norm: 1.220502\tLR: 0.030000\n",
      "Train Epoch: 840 [188416/194182 (96%)]\tLoss: 0.355847\tGrad Norm: 1.095975\tLR: 0.030000\n",
      "Train set: Average loss: 0.3586\n",
      "Test set: Average loss: 0.2431, Average MAE: 0.3558\n",
      "Epoch 840: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 841 [4096/194182 (2%)]\tLoss: 0.361158\tGrad Norm: 1.255721\tLR: 0.030000\n",
      "Train Epoch: 841 [24576/194182 (12%)]\tLoss: 0.358218\tGrad Norm: 1.407018\tLR: 0.030000\n",
      "Train Epoch: 841 [45056/194182 (23%)]\tLoss: 0.368876\tGrad Norm: 1.640917\tLR: 0.030000\n",
      "Train Epoch: 841 [65536/194182 (33%)]\tLoss: 0.357543\tGrad Norm: 1.035444\tLR: 0.030000\n",
      "Train Epoch: 841 [86016/194182 (44%)]\tLoss: 0.351701\tGrad Norm: 1.114753\tLR: 0.030000\n",
      "Train Epoch: 841 [106496/194182 (54%)]\tLoss: 0.352754\tGrad Norm: 1.154623\tLR: 0.030000\n",
      "Train Epoch: 841 [126976/194182 (65%)]\tLoss: 0.358731\tGrad Norm: 1.158575\tLR: 0.030000\n",
      "Train Epoch: 841 [147456/194182 (75%)]\tLoss: 0.353532\tGrad Norm: 1.102655\tLR: 0.030000\n",
      "Train Epoch: 841 [167936/194182 (85%)]\tLoss: 0.367801\tGrad Norm: 1.468495\tLR: 0.030000\n",
      "Train Epoch: 841 [188416/194182 (96%)]\tLoss: 0.358318\tGrad Norm: 1.369126\tLR: 0.030000\n",
      "Train set: Average loss: 0.3578\n",
      "Test set: Average loss: 0.2413, Average MAE: 0.3323\n",
      "Train Epoch: 842 [4096/194182 (2%)]\tLoss: 0.361817\tGrad Norm: 1.403956\tLR: 0.030000\n",
      "Train Epoch: 842 [24576/194182 (12%)]\tLoss: 0.359209\tGrad Norm: 1.265355\tLR: 0.030000\n",
      "Train Epoch: 842 [45056/194182 (23%)]\tLoss: 0.343415\tGrad Norm: 1.062677\tLR: 0.030000\n",
      "Train Epoch: 842 [65536/194182 (33%)]\tLoss: 0.365592\tGrad Norm: 1.586454\tLR: 0.030000\n",
      "Train Epoch: 842 [86016/194182 (44%)]\tLoss: 0.353586\tGrad Norm: 1.158432\tLR: 0.030000\n",
      "Train Epoch: 842 [106496/194182 (54%)]\tLoss: 0.363670\tGrad Norm: 1.104010\tLR: 0.030000\n",
      "Train Epoch: 842 [126976/194182 (65%)]\tLoss: 0.357477\tGrad Norm: 1.044735\tLR: 0.030000\n",
      "Train Epoch: 842 [147456/194182 (75%)]\tLoss: 0.355627\tGrad Norm: 1.297385\tLR: 0.030000\n",
      "Train Epoch: 842 [167936/194182 (85%)]\tLoss: 0.357412\tGrad Norm: 1.341050\tLR: 0.030000\n",
      "Train Epoch: 842 [188416/194182 (96%)]\tLoss: 0.354985\tGrad Norm: 1.099367\tLR: 0.030000\n",
      "Train set: Average loss: 0.3565\n",
      "Test set: Average loss: 0.2377, Average MAE: 0.3364\n",
      "Train Epoch: 843 [4096/194182 (2%)]\tLoss: 0.356427\tGrad Norm: 0.960176\tLR: 0.030000\n",
      "Train Epoch: 843 [24576/194182 (12%)]\tLoss: 0.355893\tGrad Norm: 0.942354\tLR: 0.030000\n",
      "Train Epoch: 843 [45056/194182 (23%)]\tLoss: 0.360047\tGrad Norm: 1.095455\tLR: 0.030000\n",
      "Train Epoch: 843 [65536/194182 (33%)]\tLoss: 0.352112\tGrad Norm: 1.093482\tLR: 0.030000\n",
      "Train Epoch: 843 [86016/194182 (44%)]\tLoss: 0.352632\tGrad Norm: 1.162411\tLR: 0.030000\n",
      "Train Epoch: 843 [106496/194182 (54%)]\tLoss: 0.372596\tGrad Norm: 1.786394\tLR: 0.030000\n",
      "Train Epoch: 843 [126976/194182 (65%)]\tLoss: 0.356594\tGrad Norm: 1.497776\tLR: 0.030000\n",
      "Train Epoch: 843 [147456/194182 (75%)]\tLoss: 0.356545\tGrad Norm: 1.444435\tLR: 0.030000\n",
      "Train Epoch: 843 [167936/194182 (85%)]\tLoss: 0.358035\tGrad Norm: 1.340111\tLR: 0.030000\n",
      "Train Epoch: 843 [188416/194182 (96%)]\tLoss: 0.358527\tGrad Norm: 1.226648\tLR: 0.030000\n",
      "Train set: Average loss: 0.3574\n",
      "Test set: Average loss: 0.2404, Average MAE: 0.3397\n",
      "Train Epoch: 844 [4096/194182 (2%)]\tLoss: 0.357824\tGrad Norm: 1.095082\tLR: 0.030000\n",
      "Train Epoch: 844 [24576/194182 (12%)]\tLoss: 0.364702\tGrad Norm: 1.677736\tLR: 0.030000\n",
      "Train Epoch: 844 [45056/194182 (23%)]\tLoss: 0.348236\tGrad Norm: 1.062573\tLR: 0.030000\n",
      "Train Epoch: 844 [65536/194182 (33%)]\tLoss: 0.362341\tGrad Norm: 1.175905\tLR: 0.030000\n",
      "Train Epoch: 844 [86016/194182 (44%)]\tLoss: 0.353357\tGrad Norm: 0.953251\tLR: 0.030000\n",
      "Train Epoch: 844 [106496/194182 (54%)]\tLoss: 0.363782\tGrad Norm: 1.361573\tLR: 0.030000\n",
      "Train Epoch: 844 [126976/194182 (65%)]\tLoss: 0.355746\tGrad Norm: 1.340669\tLR: 0.030000\n",
      "Train Epoch: 844 [147456/194182 (75%)]\tLoss: 0.366594\tGrad Norm: 1.721571\tLR: 0.030000\n",
      "Train Epoch: 844 [167936/194182 (85%)]\tLoss: 0.354489\tGrad Norm: 1.422942\tLR: 0.030000\n",
      "Train Epoch: 844 [188416/194182 (96%)]\tLoss: 0.357928\tGrad Norm: 1.419908\tLR: 0.030000\n",
      "Train set: Average loss: 0.3577\n",
      "Test set: Average loss: 0.2482, Average MAE: 0.3320\n",
      "Train Epoch: 845 [4096/194182 (2%)]\tLoss: 0.358735\tGrad Norm: 1.962800\tLR: 0.030000\n",
      "Train Epoch: 845 [24576/194182 (12%)]\tLoss: 0.364391\tGrad Norm: 1.420735\tLR: 0.030000\n",
      "Train Epoch: 845 [45056/194182 (23%)]\tLoss: 0.353749\tGrad Norm: 1.225433\tLR: 0.030000\n",
      "Train Epoch: 845 [65536/194182 (33%)]\tLoss: 0.352652\tGrad Norm: 1.407751\tLR: 0.030000\n",
      "Train Epoch: 845 [86016/194182 (44%)]\tLoss: 0.361416\tGrad Norm: 1.193825\tLR: 0.030000\n",
      "Train Epoch: 845 [106496/194182 (54%)]\tLoss: 0.349085\tGrad Norm: 1.083609\tLR: 0.030000\n",
      "Train Epoch: 845 [126976/194182 (65%)]\tLoss: 0.355371\tGrad Norm: 1.187315\tLR: 0.030000\n",
      "Train Epoch: 845 [147456/194182 (75%)]\tLoss: 0.359734\tGrad Norm: 1.718569\tLR: 0.030000\n",
      "Train Epoch: 845 [167936/194182 (85%)]\tLoss: 0.361541\tGrad Norm: 1.341004\tLR: 0.030000\n",
      "Train Epoch: 845 [188416/194182 (96%)]\tLoss: 0.346278\tGrad Norm: 0.960720\tLR: 0.030000\n",
      "Train set: Average loss: 0.3570\n",
      "Test set: Average loss: 0.2379, Average MAE: 0.3441\n",
      "Epoch 845: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 846 [4096/194182 (2%)]\tLoss: 0.352449\tGrad Norm: 0.994472\tLR: 0.030000\n",
      "Train Epoch: 846 [24576/194182 (12%)]\tLoss: 0.355770\tGrad Norm: 1.138943\tLR: 0.030000\n",
      "Train Epoch: 846 [45056/194182 (23%)]\tLoss: 0.351621\tGrad Norm: 0.948349\tLR: 0.030000\n",
      "Train Epoch: 846 [65536/194182 (33%)]\tLoss: 0.356108\tGrad Norm: 1.295856\tLR: 0.030000\n",
      "Train Epoch: 846 [86016/194182 (44%)]\tLoss: 0.361629\tGrad Norm: 1.320215\tLR: 0.030000\n",
      "Train Epoch: 846 [106496/194182 (54%)]\tLoss: 0.354382\tGrad Norm: 1.218346\tLR: 0.030000\n",
      "Train Epoch: 846 [126976/194182 (65%)]\tLoss: 0.355557\tGrad Norm: 1.319267\tLR: 0.030000\n",
      "Train Epoch: 846 [147456/194182 (75%)]\tLoss: 0.365296\tGrad Norm: 1.646251\tLR: 0.030000\n",
      "Train Epoch: 846 [167936/194182 (85%)]\tLoss: 0.363168\tGrad Norm: 1.516284\tLR: 0.030000\n",
      "Train Epoch: 846 [188416/194182 (96%)]\tLoss: 0.348312\tGrad Norm: 1.172654\tLR: 0.030000\n",
      "Train set: Average loss: 0.3565\n",
      "Test set: Average loss: 0.2410, Average MAE: 0.3423\n",
      "Train Epoch: 847 [4096/194182 (2%)]\tLoss: 0.353870\tGrad Norm: 1.086904\tLR: 0.030000\n",
      "Train Epoch: 847 [24576/194182 (12%)]\tLoss: 0.348626\tGrad Norm: 0.874109\tLR: 0.030000\n",
      "Train Epoch: 847 [45056/194182 (23%)]\tLoss: 0.353548\tGrad Norm: 1.325886\tLR: 0.030000\n",
      "Train Epoch: 847 [65536/194182 (33%)]\tLoss: 0.368168\tGrad Norm: 1.783990\tLR: 0.030000\n",
      "Train Epoch: 847 [86016/194182 (44%)]\tLoss: 0.359620\tGrad Norm: 1.110934\tLR: 0.030000\n",
      "Train Epoch: 847 [106496/194182 (54%)]\tLoss: 0.361595\tGrad Norm: 1.262017\tLR: 0.030000\n",
      "Train Epoch: 847 [126976/194182 (65%)]\tLoss: 0.359840\tGrad Norm: 1.808400\tLR: 0.030000\n",
      "Train Epoch: 847 [147456/194182 (75%)]\tLoss: 0.353155\tGrad Norm: 1.327014\tLR: 0.030000\n",
      "Train Epoch: 847 [167936/194182 (85%)]\tLoss: 0.356618\tGrad Norm: 1.153293\tLR: 0.030000\n",
      "Train Epoch: 847 [188416/194182 (96%)]\tLoss: 0.358494\tGrad Norm: 1.072947\tLR: 0.030000\n",
      "Train set: Average loss: 0.3560\n",
      "Test set: Average loss: 0.2388, Average MAE: 0.3375\n",
      "Train Epoch: 848 [4096/194182 (2%)]\tLoss: 0.346574\tGrad Norm: 1.076869\tLR: 0.030000\n",
      "Train Epoch: 848 [24576/194182 (12%)]\tLoss: 0.363031\tGrad Norm: 1.407968\tLR: 0.030000\n",
      "Train Epoch: 848 [45056/194182 (23%)]\tLoss: 0.354306\tGrad Norm: 1.226664\tLR: 0.030000\n",
      "Train Epoch: 848 [65536/194182 (33%)]\tLoss: 0.356468\tGrad Norm: 1.304721\tLR: 0.030000\n",
      "Train Epoch: 848 [86016/194182 (44%)]\tLoss: 0.357213\tGrad Norm: 1.090676\tLR: 0.030000\n",
      "Train Epoch: 848 [106496/194182 (54%)]\tLoss: 0.359907\tGrad Norm: 1.502412\tLR: 0.030000\n",
      "Train Epoch: 848 [126976/194182 (65%)]\tLoss: 0.362579\tGrad Norm: 1.547027\tLR: 0.030000\n",
      "Train Epoch: 848 [147456/194182 (75%)]\tLoss: 0.357251\tGrad Norm: 1.333212\tLR: 0.030000\n",
      "Train Epoch: 848 [167936/194182 (85%)]\tLoss: 0.351375\tGrad Norm: 1.347770\tLR: 0.030000\n",
      "Train Epoch: 848 [188416/194182 (96%)]\tLoss: 0.354907\tGrad Norm: 1.190587\tLR: 0.030000\n",
      "Train set: Average loss: 0.3567\n",
      "Test set: Average loss: 0.2417, Average MAE: 0.3343\n",
      "Train Epoch: 849 [4096/194182 (2%)]\tLoss: 0.352117\tGrad Norm: 1.307930\tLR: 0.030000\n",
      "Train Epoch: 849 [24576/194182 (12%)]\tLoss: 0.355298\tGrad Norm: 1.215591\tLR: 0.030000\n",
      "Train Epoch: 849 [45056/194182 (23%)]\tLoss: 0.355255\tGrad Norm: 1.260593\tLR: 0.030000\n",
      "Train Epoch: 849 [65536/194182 (33%)]\tLoss: 0.356423\tGrad Norm: 1.202044\tLR: 0.030000\n",
      "Train Epoch: 849 [86016/194182 (44%)]\tLoss: 0.353686\tGrad Norm: 1.018455\tLR: 0.030000\n",
      "Train Epoch: 849 [106496/194182 (54%)]\tLoss: 0.349347\tGrad Norm: 1.131835\tLR: 0.030000\n",
      "Train Epoch: 849 [126976/194182 (65%)]\tLoss: 0.362512\tGrad Norm: 1.464839\tLR: 0.030000\n",
      "Train Epoch: 849 [147456/194182 (75%)]\tLoss: 0.360520\tGrad Norm: 1.720174\tLR: 0.030000\n",
      "Train Epoch: 849 [167936/194182 (85%)]\tLoss: 0.353308\tGrad Norm: 1.018770\tLR: 0.030000\n",
      "Train Epoch: 849 [188416/194182 (96%)]\tLoss: 0.349143\tGrad Norm: 1.169178\tLR: 0.030000\n",
      "Train set: Average loss: 0.3555\n",
      "Test set: Average loss: 0.2464, Average MAE: 0.3530\n",
      "Train Epoch: 850 [4096/194182 (2%)]\tLoss: 0.359782\tGrad Norm: 1.542079\tLR: 0.030000\n",
      "Train Epoch: 850 [24576/194182 (12%)]\tLoss: 0.364858\tGrad Norm: 1.267967\tLR: 0.030000\n",
      "Train Epoch: 850 [45056/194182 (23%)]\tLoss: 0.355934\tGrad Norm: 1.338050\tLR: 0.030000\n",
      "Train Epoch: 850 [65536/194182 (33%)]\tLoss: 0.358955\tGrad Norm: 1.281945\tLR: 0.030000\n",
      "Train Epoch: 850 [86016/194182 (44%)]\tLoss: 0.351826\tGrad Norm: 1.351860\tLR: 0.030000\n",
      "Train Epoch: 850 [106496/194182 (54%)]\tLoss: 0.360769\tGrad Norm: 1.470907\tLR: 0.030000\n",
      "Train Epoch: 850 [126976/194182 (65%)]\tLoss: 0.353573\tGrad Norm: 1.493385\tLR: 0.030000\n",
      "Train Epoch: 850 [147456/194182 (75%)]\tLoss: 0.359855\tGrad Norm: 1.391726\tLR: 0.030000\n",
      "Train Epoch: 850 [167936/194182 (85%)]\tLoss: 0.351516\tGrad Norm: 1.168370\tLR: 0.030000\n",
      "Train Epoch: 850 [188416/194182 (96%)]\tLoss: 0.354563\tGrad Norm: 1.437357\tLR: 0.030000\n",
      "Train set: Average loss: 0.3573\n",
      "Test set: Average loss: 0.2472, Average MAE: 0.3534\n",
      "Epoch 850: Mean reward = 0.075 +/- 0.074\n",
      "Train Epoch: 851 [4096/194182 (2%)]\tLoss: 0.354015\tGrad Norm: 1.564268\tLR: 0.030000\n",
      "Train Epoch: 851 [24576/194182 (12%)]\tLoss: 0.350828\tGrad Norm: 1.185139\tLR: 0.030000\n",
      "Train Epoch: 851 [45056/194182 (23%)]\tLoss: 0.341661\tGrad Norm: 0.935908\tLR: 0.030000\n",
      "Train Epoch: 851 [65536/194182 (33%)]\tLoss: 0.349250\tGrad Norm: 1.034906\tLR: 0.030000\n",
      "Train Epoch: 851 [86016/194182 (44%)]\tLoss: 0.347791\tGrad Norm: 1.026437\tLR: 0.030000\n",
      "Train Epoch: 851 [106496/194182 (54%)]\tLoss: 0.360352\tGrad Norm: 1.195031\tLR: 0.030000\n",
      "Train Epoch: 851 [126976/194182 (65%)]\tLoss: 0.353025\tGrad Norm: 1.228990\tLR: 0.030000\n",
      "Train Epoch: 851 [147456/194182 (75%)]\tLoss: 0.363147\tGrad Norm: 1.251189\tLR: 0.030000\n",
      "Train Epoch: 851 [167936/194182 (85%)]\tLoss: 0.364239\tGrad Norm: 1.416721\tLR: 0.030000\n",
      "Train Epoch: 851 [188416/194182 (96%)]\tLoss: 0.353182\tGrad Norm: 1.153833\tLR: 0.030000\n",
      "Train set: Average loss: 0.3548\n",
      "Test set: Average loss: 0.2408, Average MAE: 0.3507\n",
      "Train Epoch: 852 [4096/194182 (2%)]\tLoss: 0.354658\tGrad Norm: 1.113884\tLR: 0.030000\n",
      "Train Epoch: 852 [24576/194182 (12%)]\tLoss: 0.345242\tGrad Norm: 0.883732\tLR: 0.030000\n",
      "Train Epoch: 852 [45056/194182 (23%)]\tLoss: 0.351944\tGrad Norm: 1.478720\tLR: 0.030000\n",
      "Train Epoch: 852 [65536/194182 (33%)]\tLoss: 0.357999\tGrad Norm: 1.595816\tLR: 0.030000\n",
      "Train Epoch: 852 [86016/194182 (44%)]\tLoss: 0.353325\tGrad Norm: 1.117450\tLR: 0.030000\n",
      "Train Epoch: 852 [106496/194182 (54%)]\tLoss: 0.359956\tGrad Norm: 1.226396\tLR: 0.030000\n",
      "Train Epoch: 852 [126976/194182 (65%)]\tLoss: 0.350600\tGrad Norm: 1.224037\tLR: 0.030000\n",
      "Train Epoch: 852 [147456/194182 (75%)]\tLoss: 0.356091\tGrad Norm: 1.074431\tLR: 0.030000\n",
      "Train Epoch: 852 [167936/194182 (85%)]\tLoss: 0.352202\tGrad Norm: 1.159149\tLR: 0.030000\n",
      "Train Epoch: 852 [188416/194182 (96%)]\tLoss: 0.350706\tGrad Norm: 1.162327\tLR: 0.030000\n",
      "Train set: Average loss: 0.3540\n",
      "Test set: Average loss: 0.2397, Average MAE: 0.3427\n",
      "Train Epoch: 853 [4096/194182 (2%)]\tLoss: 0.344319\tGrad Norm: 1.039658\tLR: 0.030000\n",
      "Train Epoch: 853 [24576/194182 (12%)]\tLoss: 0.357265\tGrad Norm: 1.246502\tLR: 0.030000\n",
      "Train Epoch: 853 [45056/194182 (23%)]\tLoss: 0.352462\tGrad Norm: 1.399113\tLR: 0.030000\n",
      "Train Epoch: 853 [65536/194182 (33%)]\tLoss: 0.362731\tGrad Norm: 1.506009\tLR: 0.030000\n",
      "Train Epoch: 853 [86016/194182 (44%)]\tLoss: 0.354374\tGrad Norm: 1.351452\tLR: 0.030000\n",
      "Train Epoch: 853 [106496/194182 (54%)]\tLoss: 0.352616\tGrad Norm: 1.554995\tLR: 0.030000\n",
      "Train Epoch: 853 [126976/194182 (65%)]\tLoss: 0.352891\tGrad Norm: 1.382597\tLR: 0.030000\n",
      "Train Epoch: 853 [147456/194182 (75%)]\tLoss: 0.363862\tGrad Norm: 1.330481\tLR: 0.030000\n",
      "Train Epoch: 853 [167936/194182 (85%)]\tLoss: 0.362147\tGrad Norm: 1.259638\tLR: 0.030000\n",
      "Train Epoch: 853 [188416/194182 (96%)]\tLoss: 0.355364\tGrad Norm: 1.377546\tLR: 0.030000\n",
      "Train set: Average loss: 0.3564\n",
      "Test set: Average loss: 0.2436, Average MAE: 0.3447\n",
      "Train Epoch: 854 [4096/194182 (2%)]\tLoss: 0.357681\tGrad Norm: 1.329429\tLR: 0.030000\n",
      "Train Epoch: 854 [24576/194182 (12%)]\tLoss: 0.358717\tGrad Norm: 1.610017\tLR: 0.030000\n",
      "Train Epoch: 854 [45056/194182 (23%)]\tLoss: 0.359146\tGrad Norm: 1.453667\tLR: 0.030000\n",
      "Train Epoch: 854 [65536/194182 (33%)]\tLoss: 0.354647\tGrad Norm: 0.991791\tLR: 0.030000\n",
      "Train Epoch: 854 [86016/194182 (44%)]\tLoss: 0.352626\tGrad Norm: 1.125120\tLR: 0.030000\n",
      "Train Epoch: 854 [106496/194182 (54%)]\tLoss: 0.351554\tGrad Norm: 1.062864\tLR: 0.030000\n",
      "Train Epoch: 854 [126976/194182 (65%)]\tLoss: 0.356036\tGrad Norm: 1.357050\tLR: 0.030000\n",
      "Train Epoch: 854 [147456/194182 (75%)]\tLoss: 0.358991\tGrad Norm: 1.471108\tLR: 0.030000\n",
      "Train Epoch: 854 [167936/194182 (85%)]\tLoss: 0.364005\tGrad Norm: 1.880888\tLR: 0.030000\n",
      "Train Epoch: 854 [188416/194182 (96%)]\tLoss: 0.350487\tGrad Norm: 1.241316\tLR: 0.030000\n",
      "Train set: Average loss: 0.3562\n",
      "Test set: Average loss: 0.2396, Average MAE: 0.3460\n",
      "Train Epoch: 855 [4096/194182 (2%)]\tLoss: 0.346202\tGrad Norm: 1.122639\tLR: 0.030000\n",
      "Train Epoch: 855 [24576/194182 (12%)]\tLoss: 0.349876\tGrad Norm: 1.162004\tLR: 0.030000\n",
      "Train Epoch: 855 [45056/194182 (23%)]\tLoss: 0.358011\tGrad Norm: 1.378711\tLR: 0.030000\n",
      "Train Epoch: 855 [65536/194182 (33%)]\tLoss: 0.352777\tGrad Norm: 1.411271\tLR: 0.030000\n",
      "Train Epoch: 855 [86016/194182 (44%)]\tLoss: 0.348988\tGrad Norm: 1.180393\tLR: 0.030000\n",
      "Train Epoch: 855 [106496/194182 (54%)]\tLoss: 0.353995\tGrad Norm: 1.248994\tLR: 0.030000\n",
      "Train Epoch: 855 [126976/194182 (65%)]\tLoss: 0.356801\tGrad Norm: 1.172037\tLR: 0.030000\n",
      "Train Epoch: 855 [147456/194182 (75%)]\tLoss: 0.352806\tGrad Norm: 1.464999\tLR: 0.030000\n",
      "Train Epoch: 855 [167936/194182 (85%)]\tLoss: 0.355266\tGrad Norm: 1.272942\tLR: 0.030000\n",
      "Train Epoch: 855 [188416/194182 (96%)]\tLoss: 0.349146\tGrad Norm: 1.134451\tLR: 0.030000\n",
      "Train set: Average loss: 0.3539\n",
      "Test set: Average loss: 0.2418, Average MAE: 0.3488\n",
      "Epoch 855: Mean reward = 0.050 +/- 0.045\n",
      "Train Epoch: 856 [4096/194182 (2%)]\tLoss: 0.345921\tGrad Norm: 1.087763\tLR: 0.030000\n",
      "Train Epoch: 856 [24576/194182 (12%)]\tLoss: 0.349676\tGrad Norm: 1.331370\tLR: 0.030000\n",
      "Train Epoch: 856 [45056/194182 (23%)]\tLoss: 0.358040\tGrad Norm: 1.555358\tLR: 0.030000\n",
      "Train Epoch: 856 [65536/194182 (33%)]\tLoss: 0.361077\tGrad Norm: 1.661597\tLR: 0.030000\n",
      "Train Epoch: 856 [86016/194182 (44%)]\tLoss: 0.367434\tGrad Norm: 1.535130\tLR: 0.030000\n",
      "Train Epoch: 856 [106496/194182 (54%)]\tLoss: 0.356699\tGrad Norm: 1.327702\tLR: 0.030000\n",
      "Train Epoch: 856 [126976/194182 (65%)]\tLoss: 0.361763\tGrad Norm: 1.549832\tLR: 0.030000\n",
      "Train Epoch: 856 [147456/194182 (75%)]\tLoss: 0.346871\tGrad Norm: 1.180915\tLR: 0.030000\n",
      "Train Epoch: 856 [167936/194182 (85%)]\tLoss: 0.349032\tGrad Norm: 1.068577\tLR: 0.030000\n",
      "Train Epoch: 856 [188416/194182 (96%)]\tLoss: 0.355437\tGrad Norm: 1.433703\tLR: 0.030000\n",
      "Train set: Average loss: 0.3549\n",
      "Test set: Average loss: 0.2435, Average MAE: 0.3559\n",
      "Train Epoch: 857 [4096/194182 (2%)]\tLoss: 0.354085\tGrad Norm: 1.393775\tLR: 0.030000\n",
      "Train Epoch: 857 [24576/194182 (12%)]\tLoss: 0.365556\tGrad Norm: 1.383767\tLR: 0.030000\n",
      "Train Epoch: 857 [45056/194182 (23%)]\tLoss: 0.349814\tGrad Norm: 1.262793\tLR: 0.030000\n",
      "Train Epoch: 857 [65536/194182 (33%)]\tLoss: 0.341844\tGrad Norm: 1.092361\tLR: 0.030000\n",
      "Train Epoch: 857 [86016/194182 (44%)]\tLoss: 0.349795\tGrad Norm: 1.227329\tLR: 0.030000\n",
      "Train Epoch: 857 [106496/194182 (54%)]\tLoss: 0.354380\tGrad Norm: 1.230036\tLR: 0.030000\n",
      "Train Epoch: 857 [126976/194182 (65%)]\tLoss: 0.355436\tGrad Norm: 1.532600\tLR: 0.030000\n",
      "Train Epoch: 857 [147456/194182 (75%)]\tLoss: 0.354884\tGrad Norm: 1.275000\tLR: 0.030000\n",
      "Train Epoch: 857 [167936/194182 (85%)]\tLoss: 0.352890\tGrad Norm: 0.943344\tLR: 0.030000\n",
      "Train Epoch: 857 [188416/194182 (96%)]\tLoss: 0.356923\tGrad Norm: 1.185911\tLR: 0.030000\n",
      "Train set: Average loss: 0.3535\n",
      "Test set: Average loss: 0.2413, Average MAE: 0.3440\n",
      "Train Epoch: 858 [4096/194182 (2%)]\tLoss: 0.358335\tGrad Norm: 1.273473\tLR: 0.030000\n",
      "Train Epoch: 858 [24576/194182 (12%)]\tLoss: 0.348838\tGrad Norm: 1.094597\tLR: 0.030000\n",
      "Train Epoch: 858 [45056/194182 (23%)]\tLoss: 0.348928\tGrad Norm: 1.110200\tLR: 0.030000\n",
      "Train Epoch: 858 [65536/194182 (33%)]\tLoss: 0.351870\tGrad Norm: 1.221235\tLR: 0.030000\n",
      "Train Epoch: 858 [86016/194182 (44%)]\tLoss: 0.348485\tGrad Norm: 1.517287\tLR: 0.030000\n",
      "Train Epoch: 858 [106496/194182 (54%)]\tLoss: 0.356981\tGrad Norm: 1.380085\tLR: 0.030000\n",
      "Train Epoch: 858 [126976/194182 (65%)]\tLoss: 0.355764\tGrad Norm: 1.232903\tLR: 0.030000\n",
      "Train Epoch: 858 [147456/194182 (75%)]\tLoss: 0.350711\tGrad Norm: 1.241286\tLR: 0.030000\n",
      "Train Epoch: 858 [167936/194182 (85%)]\tLoss: 0.349908\tGrad Norm: 1.142786\tLR: 0.030000\n",
      "Train Epoch: 858 [188416/194182 (96%)]\tLoss: 0.355323\tGrad Norm: 1.422387\tLR: 0.030000\n",
      "Train set: Average loss: 0.3537\n",
      "Test set: Average loss: 0.2497, Average MAE: 0.3625\n",
      "Train Epoch: 859 [4096/194182 (2%)]\tLoss: 0.364615\tGrad Norm: 1.744582\tLR: 0.030000\n",
      "Train Epoch: 859 [24576/194182 (12%)]\tLoss: 0.353822\tGrad Norm: 1.512535\tLR: 0.030000\n",
      "Train Epoch: 859 [45056/194182 (23%)]\tLoss: 0.358090\tGrad Norm: 1.321625\tLR: 0.030000\n",
      "Train Epoch: 859 [65536/194182 (33%)]\tLoss: 0.363800\tGrad Norm: 1.527985\tLR: 0.030000\n",
      "Train Epoch: 859 [86016/194182 (44%)]\tLoss: 0.357044\tGrad Norm: 1.569424\tLR: 0.030000\n",
      "Train Epoch: 859 [106496/194182 (54%)]\tLoss: 0.364039\tGrad Norm: 1.534953\tLR: 0.030000\n",
      "Train Epoch: 859 [126976/194182 (65%)]\tLoss: 0.351497\tGrad Norm: 1.443280\tLR: 0.030000\n",
      "Train Epoch: 859 [147456/194182 (75%)]\tLoss: 0.353606\tGrad Norm: 1.267857\tLR: 0.030000\n",
      "Train Epoch: 859 [167936/194182 (85%)]\tLoss: 0.348613\tGrad Norm: 1.240372\tLR: 0.030000\n",
      "Train Epoch: 859 [188416/194182 (96%)]\tLoss: 0.357424\tGrad Norm: 1.223379\tLR: 0.030000\n",
      "Train set: Average loss: 0.3554\n",
      "Test set: Average loss: 0.2468, Average MAE: 0.3512\n",
      "Train Epoch: 860 [4096/194182 (2%)]\tLoss: 0.353917\tGrad Norm: 1.429876\tLR: 0.030000\n",
      "Train Epoch: 860 [24576/194182 (12%)]\tLoss: 0.353670\tGrad Norm: 1.208129\tLR: 0.030000\n",
      "Train Epoch: 860 [45056/194182 (23%)]\tLoss: 0.352118\tGrad Norm: 1.175720\tLR: 0.030000\n",
      "Train Epoch: 860 [65536/194182 (33%)]\tLoss: 0.358288\tGrad Norm: 1.340765\tLR: 0.030000\n",
      "Train Epoch: 860 [86016/194182 (44%)]\tLoss: 0.347216\tGrad Norm: 1.079432\tLR: 0.030000\n",
      "Train Epoch: 860 [106496/194182 (54%)]\tLoss: 0.347700\tGrad Norm: 1.261360\tLR: 0.030000\n",
      "Train Epoch: 860 [126976/194182 (65%)]\tLoss: 0.365325\tGrad Norm: 1.544272\tLR: 0.030000\n",
      "Train Epoch: 860 [147456/194182 (75%)]\tLoss: 0.350464\tGrad Norm: 1.268886\tLR: 0.030000\n",
      "Train Epoch: 860 [167936/194182 (85%)]\tLoss: 0.358964\tGrad Norm: 1.416594\tLR: 0.030000\n",
      "Train Epoch: 860 [188416/194182 (96%)]\tLoss: 0.352594\tGrad Norm: 1.194264\tLR: 0.030000\n",
      "Train set: Average loss: 0.3535\n",
      "Test set: Average loss: 0.2398, Average MAE: 0.3331\n",
      "Epoch 860: Mean reward = 0.070 +/- 0.054\n",
      "Train Epoch: 861 [4096/194182 (2%)]\tLoss: 0.357382\tGrad Norm: 1.101048\tLR: 0.030000\n",
      "Train Epoch: 861 [24576/194182 (12%)]\tLoss: 0.348473\tGrad Norm: 0.882492\tLR: 0.030000\n",
      "Train Epoch: 861 [45056/194182 (23%)]\tLoss: 0.354024\tGrad Norm: 1.015249\tLR: 0.030000\n",
      "Train Epoch: 861 [65536/194182 (33%)]\tLoss: 0.346319\tGrad Norm: 1.303217\tLR: 0.030000\n",
      "Train Epoch: 861 [86016/194182 (44%)]\tLoss: 0.348491\tGrad Norm: 1.350245\tLR: 0.030000\n",
      "Train Epoch: 861 [106496/194182 (54%)]\tLoss: 0.346574\tGrad Norm: 1.336948\tLR: 0.030000\n",
      "Train Epoch: 861 [126976/194182 (65%)]\tLoss: 0.360794\tGrad Norm: 1.507268\tLR: 0.030000\n",
      "Train Epoch: 861 [147456/194182 (75%)]\tLoss: 0.354164\tGrad Norm: 1.202004\tLR: 0.030000\n",
      "Train Epoch: 861 [167936/194182 (85%)]\tLoss: 0.359901\tGrad Norm: 1.435955\tLR: 0.030000\n",
      "Train Epoch: 861 [188416/194182 (96%)]\tLoss: 0.349122\tGrad Norm: 0.966689\tLR: 0.030000\n",
      "Train set: Average loss: 0.3524\n",
      "Test set: Average loss: 0.2406, Average MAE: 0.3476\n",
      "Train Epoch: 862 [4096/194182 (2%)]\tLoss: 0.347748\tGrad Norm: 1.251890\tLR: 0.030000\n",
      "Train Epoch: 862 [24576/194182 (12%)]\tLoss: 0.344676\tGrad Norm: 1.210517\tLR: 0.030000\n",
      "Train Epoch: 862 [45056/194182 (23%)]\tLoss: 0.356907\tGrad Norm: 1.741143\tLR: 0.030000\n",
      "Train Epoch: 862 [65536/194182 (33%)]\tLoss: 0.349795\tGrad Norm: 1.660258\tLR: 0.030000\n",
      "Train Epoch: 862 [86016/194182 (44%)]\tLoss: 0.352647\tGrad Norm: 1.433850\tLR: 0.030000\n",
      "Train Epoch: 862 [106496/194182 (54%)]\tLoss: 0.351233\tGrad Norm: 1.147914\tLR: 0.030000\n",
      "Train Epoch: 862 [126976/194182 (65%)]\tLoss: 0.349403\tGrad Norm: 1.356893\tLR: 0.030000\n",
      "Train Epoch: 862 [147456/194182 (75%)]\tLoss: 0.359876\tGrad Norm: 1.470479\tLR: 0.030000\n",
      "Train Epoch: 862 [167936/194182 (85%)]\tLoss: 0.348567\tGrad Norm: 1.172991\tLR: 0.030000\n",
      "Train Epoch: 862 [188416/194182 (96%)]\tLoss: 0.351409\tGrad Norm: 1.016463\tLR: 0.030000\n",
      "Train set: Average loss: 0.3535\n",
      "Test set: Average loss: 0.2360, Average MAE: 0.3400\n",
      "Saved best model to checkpoints/imitation_PPO_20240514/model_best.pt\n",
      "Saved best optimizer to checkpoints/imitation_PPO_20240514/optimizer_best.pt\n",
      "Train Epoch: 863 [4096/194182 (2%)]\tLoss: 0.349665\tGrad Norm: 0.861977\tLR: 0.030000\n",
      "Train Epoch: 863 [24576/194182 (12%)]\tLoss: 0.347501\tGrad Norm: 1.108355\tLR: 0.030000\n",
      "Train Epoch: 863 [45056/194182 (23%)]\tLoss: 0.359720\tGrad Norm: 1.381982\tLR: 0.030000\n",
      "Train Epoch: 863 [65536/194182 (33%)]\tLoss: 0.355045\tGrad Norm: 1.457518\tLR: 0.030000\n",
      "Train Epoch: 863 [86016/194182 (44%)]\tLoss: 0.353050\tGrad Norm: 1.405216\tLR: 0.030000\n",
      "Train Epoch: 863 [106496/194182 (54%)]\tLoss: 0.355490\tGrad Norm: 1.239673\tLR: 0.030000\n",
      "Train Epoch: 863 [126976/194182 (65%)]\tLoss: 0.359308\tGrad Norm: 1.314730\tLR: 0.030000\n",
      "Train Epoch: 863 [147456/194182 (75%)]\tLoss: 0.342992\tGrad Norm: 1.110629\tLR: 0.030000\n",
      "Train Epoch: 863 [167936/194182 (85%)]\tLoss: 0.350914\tGrad Norm: 1.358736\tLR: 0.030000\n",
      "Train Epoch: 863 [188416/194182 (96%)]\tLoss: 0.340752\tGrad Norm: 1.016601\tLR: 0.030000\n",
      "Train set: Average loss: 0.3514\n",
      "Test set: Average loss: 0.2402, Average MAE: 0.3313\n",
      "Train Epoch: 864 [4096/194182 (2%)]\tLoss: 0.348498\tGrad Norm: 1.215746\tLR: 0.030000\n",
      "Train Epoch: 864 [24576/194182 (12%)]\tLoss: 0.356992\tGrad Norm: 1.322235\tLR: 0.030000\n",
      "Train Epoch: 864 [45056/194182 (23%)]\tLoss: 0.359305\tGrad Norm: 1.081863\tLR: 0.030000\n",
      "Train Epoch: 864 [65536/194182 (33%)]\tLoss: 0.355006\tGrad Norm: 1.227337\tLR: 0.030000\n",
      "Train Epoch: 864 [86016/194182 (44%)]\tLoss: 0.357727\tGrad Norm: 1.311702\tLR: 0.030000\n",
      "Train Epoch: 864 [106496/194182 (54%)]\tLoss: 0.343578\tGrad Norm: 1.131752\tLR: 0.030000\n",
      "Train Epoch: 864 [126976/194182 (65%)]\tLoss: 0.353913\tGrad Norm: 1.416880\tLR: 0.030000\n",
      "Train Epoch: 864 [147456/194182 (75%)]\tLoss: 0.363100\tGrad Norm: 1.928911\tLR: 0.030000\n",
      "Train Epoch: 864 [167936/194182 (85%)]\tLoss: 0.345310\tGrad Norm: 1.098819\tLR: 0.030000\n",
      "Train Epoch: 864 [188416/194182 (96%)]\tLoss: 0.350030\tGrad Norm: 1.231346\tLR: 0.030000\n",
      "Train set: Average loss: 0.3516\n",
      "Test set: Average loss: 0.2427, Average MAE: 0.3468\n",
      "Train Epoch: 865 [4096/194182 (2%)]\tLoss: 0.348908\tGrad Norm: 1.290347\tLR: 0.030000\n",
      "Train Epoch: 865 [24576/194182 (12%)]\tLoss: 0.350596\tGrad Norm: 1.134144\tLR: 0.030000\n",
      "Train Epoch: 865 [45056/194182 (23%)]\tLoss: 0.351022\tGrad Norm: 1.333724\tLR: 0.030000\n",
      "Train Epoch: 865 [65536/194182 (33%)]\tLoss: 0.351663\tGrad Norm: 1.206162\tLR: 0.030000\n",
      "Train Epoch: 865 [86016/194182 (44%)]\tLoss: 0.346859\tGrad Norm: 1.109480\tLR: 0.030000\n",
      "Train Epoch: 865 [106496/194182 (54%)]\tLoss: 0.357878\tGrad Norm: 1.219641\tLR: 0.030000\n",
      "Train Epoch: 865 [126976/194182 (65%)]\tLoss: 0.353701\tGrad Norm: 1.512637\tLR: 0.030000\n",
      "Train Epoch: 865 [147456/194182 (75%)]\tLoss: 0.353734\tGrad Norm: 1.282711\tLR: 0.030000\n",
      "Train Epoch: 865 [167936/194182 (85%)]\tLoss: 0.346787\tGrad Norm: 1.102029\tLR: 0.030000\n",
      "Train Epoch: 865 [188416/194182 (96%)]\tLoss: 0.355716\tGrad Norm: 1.233884\tLR: 0.030000\n",
      "Train set: Average loss: 0.3512\n",
      "Test set: Average loss: 0.2458, Average MAE: 0.3566\n",
      "Epoch 865: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 866 [4096/194182 (2%)]\tLoss: 0.350061\tGrad Norm: 1.371136\tLR: 0.030000\n",
      "Train Epoch: 866 [24576/194182 (12%)]\tLoss: 0.348355\tGrad Norm: 1.170558\tLR: 0.030000\n",
      "Train Epoch: 866 [45056/194182 (23%)]\tLoss: 0.353527\tGrad Norm: 1.238643\tLR: 0.030000\n",
      "Train Epoch: 866 [65536/194182 (33%)]\tLoss: 0.356677\tGrad Norm: 1.592078\tLR: 0.030000\n",
      "Train Epoch: 866 [86016/194182 (44%)]\tLoss: 0.355653\tGrad Norm: 1.411863\tLR: 0.030000\n",
      "Train Epoch: 866 [106496/194182 (54%)]\tLoss: 0.345094\tGrad Norm: 0.906220\tLR: 0.030000\n",
      "Train Epoch: 866 [126976/194182 (65%)]\tLoss: 0.349800\tGrad Norm: 1.359989\tLR: 0.030000\n",
      "Train Epoch: 866 [147456/194182 (75%)]\tLoss: 0.350190\tGrad Norm: 1.339598\tLR: 0.030000\n",
      "Train Epoch: 866 [167936/194182 (85%)]\tLoss: 0.348491\tGrad Norm: 1.366311\tLR: 0.030000\n",
      "Train Epoch: 866 [188416/194182 (96%)]\tLoss: 0.350561\tGrad Norm: 1.252171\tLR: 0.030000\n",
      "Train set: Average loss: 0.3517\n",
      "Test set: Average loss: 0.2418, Average MAE: 0.3482\n",
      "Train Epoch: 867 [4096/194182 (2%)]\tLoss: 0.353691\tGrad Norm: 1.175807\tLR: 0.030000\n",
      "Train Epoch: 867 [24576/194182 (12%)]\tLoss: 0.343366\tGrad Norm: 1.109613\tLR: 0.030000\n",
      "Train Epoch: 867 [45056/194182 (23%)]\tLoss: 0.351683\tGrad Norm: 1.477069\tLR: 0.030000\n",
      "Train Epoch: 867 [65536/194182 (33%)]\tLoss: 0.353330\tGrad Norm: 1.308717\tLR: 0.030000\n",
      "Train Epoch: 867 [86016/194182 (44%)]\tLoss: 0.353012\tGrad Norm: 1.314879\tLR: 0.030000\n",
      "Train Epoch: 867 [106496/194182 (54%)]\tLoss: 0.350923\tGrad Norm: 1.350072\tLR: 0.030000\n",
      "Train Epoch: 867 [126976/194182 (65%)]\tLoss: 0.348615\tGrad Norm: 1.403866\tLR: 0.030000\n",
      "Train Epoch: 867 [147456/194182 (75%)]\tLoss: 0.359680\tGrad Norm: 1.608326\tLR: 0.030000\n",
      "Train Epoch: 867 [167936/194182 (85%)]\tLoss: 0.358686\tGrad Norm: 1.459600\tLR: 0.030000\n",
      "Train Epoch: 867 [188416/194182 (96%)]\tLoss: 0.352742\tGrad Norm: 1.339478\tLR: 0.030000\n",
      "Train set: Average loss: 0.3527\n",
      "Test set: Average loss: 0.2436, Average MAE: 0.3501\n",
      "Train Epoch: 868 [4096/194182 (2%)]\tLoss: 0.348300\tGrad Norm: 1.289410\tLR: 0.030000\n",
      "Train Epoch: 868 [24576/194182 (12%)]\tLoss: 0.353401\tGrad Norm: 1.165408\tLR: 0.030000\n",
      "Train Epoch: 868 [45056/194182 (23%)]\tLoss: 0.352483\tGrad Norm: 1.084924\tLR: 0.030000\n",
      "Train Epoch: 868 [65536/194182 (33%)]\tLoss: 0.342464\tGrad Norm: 1.059846\tLR: 0.030000\n",
      "Train Epoch: 868 [86016/194182 (44%)]\tLoss: 0.349749\tGrad Norm: 1.167132\tLR: 0.030000\n",
      "Train Epoch: 868 [106496/194182 (54%)]\tLoss: 0.354505\tGrad Norm: 1.387742\tLR: 0.030000\n",
      "Train Epoch: 868 [126976/194182 (65%)]\tLoss: 0.356025\tGrad Norm: 1.319788\tLR: 0.030000\n",
      "Train Epoch: 868 [147456/194182 (75%)]\tLoss: 0.349866\tGrad Norm: 1.215812\tLR: 0.030000\n",
      "Train Epoch: 868 [167936/194182 (85%)]\tLoss: 0.353918\tGrad Norm: 1.272492\tLR: 0.030000\n",
      "Train Epoch: 868 [188416/194182 (96%)]\tLoss: 0.352445\tGrad Norm: 1.283774\tLR: 0.030000\n",
      "Train set: Average loss: 0.3507\n",
      "Test set: Average loss: 0.2469, Average MAE: 0.3392\n",
      "Train Epoch: 869 [4096/194182 (2%)]\tLoss: 0.351165\tGrad Norm: 1.659987\tLR: 0.030000\n",
      "Train Epoch: 869 [24576/194182 (12%)]\tLoss: 0.351514\tGrad Norm: 1.293495\tLR: 0.030000\n",
      "Train Epoch: 869 [45056/194182 (23%)]\tLoss: 0.348427\tGrad Norm: 1.132674\tLR: 0.030000\n",
      "Train Epoch: 869 [65536/194182 (33%)]\tLoss: 0.342908\tGrad Norm: 1.095981\tLR: 0.030000\n",
      "Train Epoch: 869 [86016/194182 (44%)]\tLoss: 0.353031\tGrad Norm: 1.215058\tLR: 0.030000\n",
      "Train Epoch: 869 [106496/194182 (54%)]\tLoss: 0.357303\tGrad Norm: 1.521532\tLR: 0.030000\n",
      "Train Epoch: 869 [126976/194182 (65%)]\tLoss: 0.346168\tGrad Norm: 1.171015\tLR: 0.030000\n",
      "Train Epoch: 869 [147456/194182 (75%)]\tLoss: 0.344720\tGrad Norm: 1.532534\tLR: 0.030000\n",
      "Train Epoch: 869 [167936/194182 (85%)]\tLoss: 0.349702\tGrad Norm: 1.393397\tLR: 0.030000\n",
      "Train Epoch: 869 [188416/194182 (96%)]\tLoss: 0.348270\tGrad Norm: 1.289739\tLR: 0.030000\n",
      "Train set: Average loss: 0.3514\n",
      "Test set: Average loss: 0.2470, Average MAE: 0.3325\n",
      "Train Epoch: 870 [4096/194182 (2%)]\tLoss: 0.362222\tGrad Norm: 1.657640\tLR: 0.030000\n",
      "Train Epoch: 870 [24576/194182 (12%)]\tLoss: 0.358582\tGrad Norm: 1.428280\tLR: 0.030000\n",
      "Train Epoch: 870 [45056/194182 (23%)]\tLoss: 0.347566\tGrad Norm: 1.438175\tLR: 0.030000\n",
      "Train Epoch: 870 [65536/194182 (33%)]\tLoss: 0.355828\tGrad Norm: 1.592597\tLR: 0.030000\n",
      "Train Epoch: 870 [86016/194182 (44%)]\tLoss: 0.357683\tGrad Norm: 1.746744\tLR: 0.030000\n",
      "Train Epoch: 870 [106496/194182 (54%)]\tLoss: 0.350174\tGrad Norm: 1.250772\tLR: 0.030000\n",
      "Train Epoch: 870 [126976/194182 (65%)]\tLoss: 0.344235\tGrad Norm: 1.221426\tLR: 0.030000\n",
      "Train Epoch: 870 [147456/194182 (75%)]\tLoss: 0.348025\tGrad Norm: 1.197011\tLR: 0.030000\n",
      "Train Epoch: 870 [167936/194182 (85%)]\tLoss: 0.355370\tGrad Norm: 1.368835\tLR: 0.030000\n",
      "Train Epoch: 870 [188416/194182 (96%)]\tLoss: 0.346913\tGrad Norm: 1.054083\tLR: 0.030000\n",
      "Train set: Average loss: 0.3519\n",
      "Test set: Average loss: 0.2428, Average MAE: 0.3423\n",
      "Epoch 870: Mean reward = 0.046 +/- 0.001\n",
      "Train Epoch: 871 [4096/194182 (2%)]\tLoss: 0.354329\tGrad Norm: 1.384966\tLR: 0.030000\n",
      "Train Epoch: 871 [24576/194182 (12%)]\tLoss: 0.352326\tGrad Norm: 1.317469\tLR: 0.030000\n",
      "Train Epoch: 871 [45056/194182 (23%)]\tLoss: 0.343788\tGrad Norm: 0.941991\tLR: 0.030000\n",
      "Train Epoch: 871 [65536/194182 (33%)]\tLoss: 0.345843\tGrad Norm: 1.164510\tLR: 0.030000\n",
      "Train Epoch: 871 [86016/194182 (44%)]\tLoss: 0.352279\tGrad Norm: 1.503229\tLR: 0.030000\n",
      "Train Epoch: 871 [106496/194182 (54%)]\tLoss: 0.349006\tGrad Norm: 1.189180\tLR: 0.030000\n",
      "Train Epoch: 871 [126976/194182 (65%)]\tLoss: 0.348921\tGrad Norm: 1.263749\tLR: 0.030000\n",
      "Train Epoch: 871 [147456/194182 (75%)]\tLoss: 0.345915\tGrad Norm: 0.985669\tLR: 0.030000\n",
      "Train Epoch: 871 [167936/194182 (85%)]\tLoss: 0.351104\tGrad Norm: 1.009398\tLR: 0.030000\n",
      "Train Epoch: 871 [188416/194182 (96%)]\tLoss: 0.342680\tGrad Norm: 1.221076\tLR: 0.030000\n",
      "Train set: Average loss: 0.3490\n",
      "Test set: Average loss: 0.2394, Average MAE: 0.3326\n",
      "Train Epoch: 872 [4096/194182 (2%)]\tLoss: 0.351533\tGrad Norm: 1.130149\tLR: 0.030000\n",
      "Train Epoch: 872 [24576/194182 (12%)]\tLoss: 0.347073\tGrad Norm: 1.245421\tLR: 0.030000\n",
      "Train Epoch: 872 [45056/194182 (23%)]\tLoss: 0.347748\tGrad Norm: 1.406059\tLR: 0.030000\n",
      "Train Epoch: 872 [65536/194182 (33%)]\tLoss: 0.345878\tGrad Norm: 1.105773\tLR: 0.030000\n",
      "Train Epoch: 872 [86016/194182 (44%)]\tLoss: 0.354923\tGrad Norm: 0.972764\tLR: 0.030000\n",
      "Train Epoch: 872 [106496/194182 (54%)]\tLoss: 0.347253\tGrad Norm: 1.027788\tLR: 0.030000\n",
      "Train Epoch: 872 [126976/194182 (65%)]\tLoss: 0.352075\tGrad Norm: 1.534175\tLR: 0.030000\n",
      "Train Epoch: 872 [147456/194182 (75%)]\tLoss: 0.355307\tGrad Norm: 1.594796\tLR: 0.030000\n",
      "Train Epoch: 872 [167936/194182 (85%)]\tLoss: 0.353874\tGrad Norm: 1.542514\tLR: 0.030000\n",
      "Train Epoch: 872 [188416/194182 (96%)]\tLoss: 0.350279\tGrad Norm: 1.355688\tLR: 0.030000\n",
      "Train set: Average loss: 0.3506\n",
      "Test set: Average loss: 0.2415, Average MAE: 0.3481\n",
      "Train Epoch: 873 [4096/194182 (2%)]\tLoss: 0.348726\tGrad Norm: 1.177129\tLR: 0.030000\n",
      "Train Epoch: 873 [24576/194182 (12%)]\tLoss: 0.346829\tGrad Norm: 1.034552\tLR: 0.030000\n",
      "Train Epoch: 873 [45056/194182 (23%)]\tLoss: 0.353195\tGrad Norm: 1.036055\tLR: 0.030000\n",
      "Train Epoch: 873 [65536/194182 (33%)]\tLoss: 0.341411\tGrad Norm: 0.925452\tLR: 0.030000\n",
      "Train Epoch: 873 [86016/194182 (44%)]\tLoss: 0.350320\tGrad Norm: 1.269285\tLR: 0.030000\n",
      "Train Epoch: 873 [106496/194182 (54%)]\tLoss: 0.352822\tGrad Norm: 1.220871\tLR: 0.030000\n",
      "Train Epoch: 873 [126976/194182 (65%)]\tLoss: 0.353435\tGrad Norm: 1.178616\tLR: 0.030000\n",
      "Train Epoch: 873 [147456/194182 (75%)]\tLoss: 0.352021\tGrad Norm: 1.303660\tLR: 0.030000\n",
      "Train Epoch: 873 [167936/194182 (85%)]\tLoss: 0.347384\tGrad Norm: 1.389109\tLR: 0.030000\n",
      "Train Epoch: 873 [188416/194182 (96%)]\tLoss: 0.348399\tGrad Norm: 1.421771\tLR: 0.030000\n",
      "Train set: Average loss: 0.3487\n",
      "Test set: Average loss: 0.2535, Average MAE: 0.3685\n",
      "Train Epoch: 874 [4096/194182 (2%)]\tLoss: 0.371983\tGrad Norm: 1.810562\tLR: 0.030000\n",
      "Train Epoch: 874 [24576/194182 (12%)]\tLoss: 0.348621\tGrad Norm: 1.165908\tLR: 0.030000\n",
      "Train Epoch: 874 [45056/194182 (23%)]\tLoss: 0.354967\tGrad Norm: 1.335976\tLR: 0.030000\n",
      "Train Epoch: 874 [65536/194182 (33%)]\tLoss: 0.347128\tGrad Norm: 1.284590\tLR: 0.030000\n",
      "Train Epoch: 874 [86016/194182 (44%)]\tLoss: 0.351876\tGrad Norm: 1.111683\tLR: 0.030000\n",
      "Train Epoch: 874 [106496/194182 (54%)]\tLoss: 0.344727\tGrad Norm: 1.075779\tLR: 0.030000\n",
      "Train Epoch: 874 [126976/194182 (65%)]\tLoss: 0.344311\tGrad Norm: 1.387514\tLR: 0.030000\n",
      "Train Epoch: 874 [147456/194182 (75%)]\tLoss: 0.354192\tGrad Norm: 1.869809\tLR: 0.030000\n",
      "Train Epoch: 874 [167936/194182 (85%)]\tLoss: 0.362310\tGrad Norm: 1.698179\tLR: 0.030000\n",
      "Train Epoch: 874 [188416/194182 (96%)]\tLoss: 0.353943\tGrad Norm: 1.342721\tLR: 0.030000\n",
      "Train set: Average loss: 0.3513\n",
      "Test set: Average loss: 0.2428, Average MAE: 0.3457\n",
      "Train Epoch: 875 [4096/194182 (2%)]\tLoss: 0.355519\tGrad Norm: 1.402777\tLR: 0.030000\n",
      "Train Epoch: 875 [24576/194182 (12%)]\tLoss: 0.347572\tGrad Norm: 1.082501\tLR: 0.030000\n",
      "Train Epoch: 875 [45056/194182 (23%)]\tLoss: 0.348124\tGrad Norm: 1.442420\tLR: 0.030000\n",
      "Train Epoch: 875 [65536/194182 (33%)]\tLoss: 0.345141\tGrad Norm: 1.024334\tLR: 0.030000\n",
      "Train Epoch: 875 [86016/194182 (44%)]\tLoss: 0.351678\tGrad Norm: 1.150432\tLR: 0.030000\n",
      "Train Epoch: 875 [106496/194182 (54%)]\tLoss: 0.349450\tGrad Norm: 1.413896\tLR: 0.030000\n",
      "Train Epoch: 875 [126976/194182 (65%)]\tLoss: 0.348465\tGrad Norm: 1.358654\tLR: 0.030000\n",
      "Train Epoch: 875 [147456/194182 (75%)]\tLoss: 0.353221\tGrad Norm: 1.371341\tLR: 0.030000\n",
      "Train Epoch: 875 [167936/194182 (85%)]\tLoss: 0.341294\tGrad Norm: 1.055386\tLR: 0.030000\n",
      "Train Epoch: 875 [188416/194182 (96%)]\tLoss: 0.342792\tGrad Norm: 1.073853\tLR: 0.030000\n",
      "Train set: Average loss: 0.3482\n",
      "Test set: Average loss: 0.2418, Average MAE: 0.3449\n",
      "Epoch 875: Mean reward = 0.037 +/- 0.019\n",
      "Train Epoch: 876 [4096/194182 (2%)]\tLoss: 0.343622\tGrad Norm: 1.179281\tLR: 0.030000\n",
      "Train Epoch: 876 [24576/194182 (12%)]\tLoss: 0.361859\tGrad Norm: 1.587766\tLR: 0.030000\n",
      "Train Epoch: 876 [45056/194182 (23%)]\tLoss: 0.342281\tGrad Norm: 1.302097\tLR: 0.030000\n",
      "Train Epoch: 876 [65536/194182 (33%)]\tLoss: 0.349715\tGrad Norm: 1.180348\tLR: 0.030000\n",
      "Train Epoch: 876 [86016/194182 (44%)]\tLoss: 0.346191\tGrad Norm: 1.340585\tLR: 0.030000\n",
      "Train Epoch: 876 [106496/194182 (54%)]\tLoss: 0.347020\tGrad Norm: 1.255850\tLR: 0.030000\n",
      "Train Epoch: 876 [126976/194182 (65%)]\tLoss: 0.344609\tGrad Norm: 1.182708\tLR: 0.030000\n",
      "Train Epoch: 876 [147456/194182 (75%)]\tLoss: 0.352505\tGrad Norm: 1.564890\tLR: 0.030000\n",
      "Train Epoch: 876 [167936/194182 (85%)]\tLoss: 0.351995\tGrad Norm: 0.862331\tLR: 0.030000\n",
      "Train Epoch: 876 [188416/194182 (96%)]\tLoss: 0.346069\tGrad Norm: 1.405609\tLR: 0.030000\n",
      "Train set: Average loss: 0.3494\n",
      "Test set: Average loss: 0.2540, Average MAE: 0.3643\n",
      "Train Epoch: 877 [4096/194182 (2%)]\tLoss: 0.359008\tGrad Norm: 1.943910\tLR: 0.030000\n",
      "Train Epoch: 877 [24576/194182 (12%)]\tLoss: 0.345252\tGrad Norm: 1.516809\tLR: 0.030000\n",
      "Train Epoch: 877 [45056/194182 (23%)]\tLoss: 0.348763\tGrad Norm: 0.812781\tLR: 0.030000\n",
      "Train Epoch: 877 [65536/194182 (33%)]\tLoss: 0.343737\tGrad Norm: 0.885109\tLR: 0.030000\n",
      "Train Epoch: 877 [86016/194182 (44%)]\tLoss: 0.351661\tGrad Norm: 0.857481\tLR: 0.030000\n",
      "Train Epoch: 877 [106496/194182 (54%)]\tLoss: 0.352410\tGrad Norm: 1.508127\tLR: 0.030000\n",
      "Train Epoch: 877 [126976/194182 (65%)]\tLoss: 0.343026\tGrad Norm: 1.534812\tLR: 0.030000\n",
      "Train Epoch: 877 [147456/194182 (75%)]\tLoss: 0.341543\tGrad Norm: 1.176302\tLR: 0.030000\n",
      "Train Epoch: 877 [167936/194182 (85%)]\tLoss: 0.336578\tGrad Norm: 1.160047\tLR: 0.030000\n",
      "Train Epoch: 877 [188416/194182 (96%)]\tLoss: 0.347090\tGrad Norm: 1.221075\tLR: 0.030000\n",
      "Train set: Average loss: 0.3481\n",
      "Test set: Average loss: 0.2393, Average MAE: 0.3415\n",
      "Train Epoch: 878 [4096/194182 (2%)]\tLoss: 0.340184\tGrad Norm: 1.213164\tLR: 0.030000\n",
      "Train Epoch: 878 [24576/194182 (12%)]\tLoss: 0.349042\tGrad Norm: 1.090597\tLR: 0.030000\n",
      "Train Epoch: 878 [45056/194182 (23%)]\tLoss: 0.346738\tGrad Norm: 1.456922\tLR: 0.030000\n",
      "Train Epoch: 878 [65536/194182 (33%)]\tLoss: 0.351679\tGrad Norm: 1.450554\tLR: 0.030000\n",
      "Train Epoch: 878 [86016/194182 (44%)]\tLoss: 0.345201\tGrad Norm: 1.277040\tLR: 0.030000\n",
      "Train Epoch: 878 [106496/194182 (54%)]\tLoss: 0.344215\tGrad Norm: 1.332296\tLR: 0.030000\n",
      "Train Epoch: 878 [126976/194182 (65%)]\tLoss: 0.352213\tGrad Norm: 1.619794\tLR: 0.030000\n",
      "Train Epoch: 878 [147456/194182 (75%)]\tLoss: 0.347342\tGrad Norm: 1.461358\tLR: 0.030000\n",
      "Train Epoch: 878 [167936/194182 (85%)]\tLoss: 0.352627\tGrad Norm: 1.361929\tLR: 0.030000\n",
      "Train Epoch: 878 [188416/194182 (96%)]\tLoss: 0.355510\tGrad Norm: 1.458254\tLR: 0.030000\n",
      "Train set: Average loss: 0.3495\n",
      "Test set: Average loss: 0.2424, Average MAE: 0.3452\n",
      "Train Epoch: 879 [4096/194182 (2%)]\tLoss: 0.353316\tGrad Norm: 1.277879\tLR: 0.030000\n",
      "Train Epoch: 879 [24576/194182 (12%)]\tLoss: 0.348531\tGrad Norm: 1.366845\tLR: 0.030000\n",
      "Train Epoch: 879 [45056/194182 (23%)]\tLoss: 0.354781\tGrad Norm: 1.525386\tLR: 0.030000\n",
      "Train Epoch: 879 [65536/194182 (33%)]\tLoss: 0.347528\tGrad Norm: 1.219003\tLR: 0.030000\n",
      "Train Epoch: 879 [86016/194182 (44%)]\tLoss: 0.339054\tGrad Norm: 0.911544\tLR: 0.030000\n",
      "Train Epoch: 879 [106496/194182 (54%)]\tLoss: 0.343850\tGrad Norm: 1.393152\tLR: 0.030000\n",
      "Train Epoch: 879 [126976/194182 (65%)]\tLoss: 0.350446\tGrad Norm: 1.556486\tLR: 0.030000\n",
      "Train Epoch: 879 [147456/194182 (75%)]\tLoss: 0.348668\tGrad Norm: 1.496030\tLR: 0.030000\n",
      "Train Epoch: 879 [167936/194182 (85%)]\tLoss: 0.352236\tGrad Norm: 1.456268\tLR: 0.030000\n",
      "Train Epoch: 879 [188416/194182 (96%)]\tLoss: 0.353817\tGrad Norm: 1.388583\tLR: 0.030000\n",
      "Train set: Average loss: 0.3492\n",
      "Test set: Average loss: 0.2489, Average MAE: 0.3310\n",
      "Train Epoch: 880 [4096/194182 (2%)]\tLoss: 0.355797\tGrad Norm: 1.724478\tLR: 0.030000\n",
      "Train Epoch: 880 [24576/194182 (12%)]\tLoss: 0.342412\tGrad Norm: 1.156070\tLR: 0.030000\n",
      "Train Epoch: 880 [45056/194182 (23%)]\tLoss: 0.338796\tGrad Norm: 1.166104\tLR: 0.030000\n",
      "Train Epoch: 880 [65536/194182 (33%)]\tLoss: 0.339138\tGrad Norm: 0.899035\tLR: 0.030000\n",
      "Train Epoch: 880 [86016/194182 (44%)]\tLoss: 0.350647\tGrad Norm: 1.331757\tLR: 0.030000\n",
      "Train Epoch: 880 [106496/194182 (54%)]\tLoss: 0.349991\tGrad Norm: 1.284033\tLR: 0.030000\n",
      "Train Epoch: 880 [126976/194182 (65%)]\tLoss: 0.349897\tGrad Norm: 1.496649\tLR: 0.030000\n",
      "Train Epoch: 880 [147456/194182 (75%)]\tLoss: 0.343856\tGrad Norm: 1.178403\tLR: 0.030000\n",
      "Train Epoch: 880 [167936/194182 (85%)]\tLoss: 0.352970\tGrad Norm: 1.172619\tLR: 0.030000\n",
      "Train Epoch: 880 [188416/194182 (96%)]\tLoss: 0.353675\tGrad Norm: 1.140267\tLR: 0.030000\n",
      "Train set: Average loss: 0.3476\n",
      "Test set: Average loss: 0.2393, Average MAE: 0.3392\n",
      "Epoch 880: Mean reward = 0.062 +/- 0.049\n",
      "Train Epoch: 881 [4096/194182 (2%)]\tLoss: 0.346518\tGrad Norm: 1.017124\tLR: 0.030000\n",
      "Train Epoch: 881 [24576/194182 (12%)]\tLoss: 0.354734\tGrad Norm: 1.365830\tLR: 0.030000\n",
      "Train Epoch: 881 [45056/194182 (23%)]\tLoss: 0.336726\tGrad Norm: 1.132862\tLR: 0.030000\n",
      "Train Epoch: 881 [65536/194182 (33%)]\tLoss: 0.347261\tGrad Norm: 1.074995\tLR: 0.030000\n",
      "Train Epoch: 881 [86016/194182 (44%)]\tLoss: 0.348362\tGrad Norm: 1.563860\tLR: 0.030000\n",
      "Train Epoch: 881 [106496/194182 (54%)]\tLoss: 0.341932\tGrad Norm: 1.122016\tLR: 0.030000\n",
      "Train Epoch: 881 [126976/194182 (65%)]\tLoss: 0.348732\tGrad Norm: 1.349760\tLR: 0.030000\n",
      "Train Epoch: 881 [147456/194182 (75%)]\tLoss: 0.352300\tGrad Norm: 1.246969\tLR: 0.030000\n",
      "Train Epoch: 881 [167936/194182 (85%)]\tLoss: 0.346369\tGrad Norm: 1.382669\tLR: 0.030000\n",
      "Train Epoch: 881 [188416/194182 (96%)]\tLoss: 0.345703\tGrad Norm: 1.121751\tLR: 0.030000\n",
      "Train set: Average loss: 0.3472\n",
      "Test set: Average loss: 0.2434, Average MAE: 0.3477\n",
      "Train Epoch: 882 [4096/194182 (2%)]\tLoss: 0.341894\tGrad Norm: 1.281552\tLR: 0.030000\n",
      "Train Epoch: 882 [24576/194182 (12%)]\tLoss: 0.344015\tGrad Norm: 1.078177\tLR: 0.030000\n",
      "Train Epoch: 882 [45056/194182 (23%)]\tLoss: 0.346799\tGrad Norm: 1.383858\tLR: 0.030000\n",
      "Train Epoch: 882 [65536/194182 (33%)]\tLoss: 0.351701\tGrad Norm: 1.613892\tLR: 0.030000\n",
      "Train Epoch: 882 [86016/194182 (44%)]\tLoss: 0.346900\tGrad Norm: 1.233871\tLR: 0.030000\n",
      "Train Epoch: 882 [106496/194182 (54%)]\tLoss: 0.348754\tGrad Norm: 1.290901\tLR: 0.030000\n",
      "Train Epoch: 882 [126976/194182 (65%)]\tLoss: 0.346591\tGrad Norm: 1.462072\tLR: 0.030000\n",
      "Train Epoch: 882 [147456/194182 (75%)]\tLoss: 0.355048\tGrad Norm: 1.364615\tLR: 0.030000\n",
      "Train Epoch: 882 [167936/194182 (85%)]\tLoss: 0.350816\tGrad Norm: 1.438600\tLR: 0.030000\n",
      "Train Epoch: 882 [188416/194182 (96%)]\tLoss: 0.338382\tGrad Norm: 1.539090\tLR: 0.030000\n",
      "Train set: Average loss: 0.3486\n",
      "Test set: Average loss: 0.2453, Average MAE: 0.3305\n",
      "Train Epoch: 883 [4096/194182 (2%)]\tLoss: 0.344749\tGrad Norm: 1.382326\tLR: 0.030000\n",
      "Train Epoch: 883 [24576/194182 (12%)]\tLoss: 0.346663\tGrad Norm: 1.351436\tLR: 0.030000\n",
      "Train Epoch: 883 [45056/194182 (23%)]\tLoss: 0.342407\tGrad Norm: 1.271561\tLR: 0.030000\n",
      "Train Epoch: 883 [65536/194182 (33%)]\tLoss: 0.350748\tGrad Norm: 1.306856\tLR: 0.030000\n",
      "Train Epoch: 883 [86016/194182 (44%)]\tLoss: 0.344999\tGrad Norm: 1.269692\tLR: 0.030000\n",
      "Train Epoch: 883 [106496/194182 (54%)]\tLoss: 0.356941\tGrad Norm: 1.490555\tLR: 0.030000\n",
      "Train Epoch: 883 [126976/194182 (65%)]\tLoss: 0.348562\tGrad Norm: 1.407334\tLR: 0.030000\n",
      "Train Epoch: 883 [147456/194182 (75%)]\tLoss: 0.345351\tGrad Norm: 1.041894\tLR: 0.030000\n",
      "Train Epoch: 883 [167936/194182 (85%)]\tLoss: 0.345607\tGrad Norm: 1.017162\tLR: 0.030000\n",
      "Train Epoch: 883 [188416/194182 (96%)]\tLoss: 0.345412\tGrad Norm: 1.130584\tLR: 0.030000\n",
      "Train set: Average loss: 0.3472\n",
      "Test set: Average loss: 0.2379, Average MAE: 0.3408\n",
      "Train Epoch: 884 [4096/194182 (2%)]\tLoss: 0.345940\tGrad Norm: 0.915056\tLR: 0.030000\n",
      "Train Epoch: 884 [24576/194182 (12%)]\tLoss: 0.342297\tGrad Norm: 1.265856\tLR: 0.030000\n",
      "Train Epoch: 884 [45056/194182 (23%)]\tLoss: 0.354054\tGrad Norm: 1.507736\tLR: 0.030000\n",
      "Train Epoch: 884 [65536/194182 (33%)]\tLoss: 0.348003\tGrad Norm: 1.629771\tLR: 0.030000\n",
      "Train Epoch: 884 [86016/194182 (44%)]\tLoss: 0.347025\tGrad Norm: 1.216271\tLR: 0.030000\n",
      "Train Epoch: 884 [106496/194182 (54%)]\tLoss: 0.345175\tGrad Norm: 1.091234\tLR: 0.030000\n",
      "Train Epoch: 884 [126976/194182 (65%)]\tLoss: 0.346194\tGrad Norm: 1.380435\tLR: 0.030000\n",
      "Train Epoch: 884 [147456/194182 (75%)]\tLoss: 0.350169\tGrad Norm: 1.550311\tLR: 0.030000\n",
      "Train Epoch: 884 [167936/194182 (85%)]\tLoss: 0.348454\tGrad Norm: 1.208864\tLR: 0.030000\n",
      "Train Epoch: 884 [188416/194182 (96%)]\tLoss: 0.354004\tGrad Norm: 1.311204\tLR: 0.030000\n",
      "Train set: Average loss: 0.3475\n",
      "Test set: Average loss: 0.2540, Average MAE: 0.3666\n",
      "Train Epoch: 885 [4096/194182 (2%)]\tLoss: 0.351046\tGrad Norm: 2.078068\tLR: 0.030000\n",
      "Train Epoch: 885 [24576/194182 (12%)]\tLoss: 0.352566\tGrad Norm: 1.317141\tLR: 0.030000\n",
      "Train Epoch: 885 [45056/194182 (23%)]\tLoss: 0.352528\tGrad Norm: 1.377673\tLR: 0.030000\n",
      "Train Epoch: 885 [65536/194182 (33%)]\tLoss: 0.342075\tGrad Norm: 1.222159\tLR: 0.030000\n",
      "Train Epoch: 885 [86016/194182 (44%)]\tLoss: 0.337467\tGrad Norm: 1.112956\tLR: 0.030000\n",
      "Train Epoch: 885 [106496/194182 (54%)]\tLoss: 0.346172\tGrad Norm: 1.046951\tLR: 0.030000\n",
      "Train Epoch: 885 [126976/194182 (65%)]\tLoss: 0.343998\tGrad Norm: 1.191751\tLR: 0.030000\n",
      "Train Epoch: 885 [147456/194182 (75%)]\tLoss: 0.359421\tGrad Norm: 1.903192\tLR: 0.030000\n",
      "Train Epoch: 885 [167936/194182 (85%)]\tLoss: 0.337903\tGrad Norm: 1.183350\tLR: 0.030000\n",
      "Train Epoch: 885 [188416/194182 (96%)]\tLoss: 0.341616\tGrad Norm: 0.919674\tLR: 0.030000\n",
      "Train set: Average loss: 0.3471\n",
      "Test set: Average loss: 0.2395, Average MAE: 0.3472\n",
      "Epoch 885: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 886 [4096/194182 (2%)]\tLoss: 0.346046\tGrad Norm: 1.076871\tLR: 0.030000\n",
      "Train Epoch: 886 [24576/194182 (12%)]\tLoss: 0.349022\tGrad Norm: 1.413082\tLR: 0.030000\n",
      "Train Epoch: 886 [45056/194182 (23%)]\tLoss: 0.338963\tGrad Norm: 0.897444\tLR: 0.030000\n",
      "Train Epoch: 886 [65536/194182 (33%)]\tLoss: 0.347422\tGrad Norm: 1.669194\tLR: 0.030000\n",
      "Train Epoch: 886 [86016/194182 (44%)]\tLoss: 0.348993\tGrad Norm: 1.538437\tLR: 0.030000\n",
      "Train Epoch: 886 [106496/194182 (54%)]\tLoss: 0.355207\tGrad Norm: 1.753616\tLR: 0.030000\n",
      "Train Epoch: 886 [126976/194182 (65%)]\tLoss: 0.345797\tGrad Norm: 1.319071\tLR: 0.030000\n",
      "Train Epoch: 886 [147456/194182 (75%)]\tLoss: 0.343956\tGrad Norm: 1.117714\tLR: 0.030000\n",
      "Train Epoch: 886 [167936/194182 (85%)]\tLoss: 0.346089\tGrad Norm: 0.762142\tLR: 0.030000\n",
      "Train Epoch: 886 [188416/194182 (96%)]\tLoss: 0.342266\tGrad Norm: 0.554500\tLR: 0.030000\n",
      "Train set: Average loss: 0.3462\n",
      "Test set: Average loss: 0.2395, Average MAE: 0.3466\n",
      "Train Epoch: 887 [4096/194182 (2%)]\tLoss: 0.346303\tGrad Norm: 1.079358\tLR: 0.030000\n",
      "Train Epoch: 887 [24576/194182 (12%)]\tLoss: 0.349830\tGrad Norm: 1.388694\tLR: 0.030000\n",
      "Train Epoch: 887 [45056/194182 (23%)]\tLoss: 0.352969\tGrad Norm: 1.574088\tLR: 0.030000\n",
      "Train Epoch: 887 [65536/194182 (33%)]\tLoss: 0.346099\tGrad Norm: 1.241312\tLR: 0.030000\n",
      "Train Epoch: 887 [86016/194182 (44%)]\tLoss: 0.346763\tGrad Norm: 1.091273\tLR: 0.030000\n",
      "Train Epoch: 887 [106496/194182 (54%)]\tLoss: 0.344623\tGrad Norm: 1.430633\tLR: 0.030000\n",
      "Train Epoch: 887 [126976/194182 (65%)]\tLoss: 0.355586\tGrad Norm: 1.540011\tLR: 0.030000\n",
      "Train Epoch: 887 [147456/194182 (75%)]\tLoss: 0.361194\tGrad Norm: 1.810356\tLR: 0.030000\n",
      "Train Epoch: 887 [167936/194182 (85%)]\tLoss: 0.355086\tGrad Norm: 1.552430\tLR: 0.030000\n",
      "Train Epoch: 887 [188416/194182 (96%)]\tLoss: 0.347200\tGrad Norm: 1.282762\tLR: 0.030000\n",
      "Train set: Average loss: 0.3484\n",
      "Test set: Average loss: 0.2387, Average MAE: 0.3485\n",
      "Train Epoch: 888 [4096/194182 (2%)]\tLoss: 0.336695\tGrad Norm: 0.984947\tLR: 0.030000\n",
      "Train Epoch: 888 [24576/194182 (12%)]\tLoss: 0.339115\tGrad Norm: 1.069887\tLR: 0.030000\n",
      "Train Epoch: 888 [45056/194182 (23%)]\tLoss: 0.354383\tGrad Norm: 1.439850\tLR: 0.030000\n",
      "Train Epoch: 888 [65536/194182 (33%)]\tLoss: 0.340873\tGrad Norm: 1.241542\tLR: 0.030000\n",
      "Train Epoch: 888 [86016/194182 (44%)]\tLoss: 0.349197\tGrad Norm: 0.950537\tLR: 0.030000\n",
      "Train Epoch: 888 [106496/194182 (54%)]\tLoss: 0.336391\tGrad Norm: 0.882728\tLR: 0.030000\n",
      "Train Epoch: 888 [126976/194182 (65%)]\tLoss: 0.342958\tGrad Norm: 1.173478\tLR: 0.030000\n",
      "Train Epoch: 888 [147456/194182 (75%)]\tLoss: 0.345099\tGrad Norm: 1.352879\tLR: 0.030000\n",
      "Train Epoch: 888 [167936/194182 (85%)]\tLoss: 0.344308\tGrad Norm: 1.306623\tLR: 0.030000\n",
      "Train Epoch: 888 [188416/194182 (96%)]\tLoss: 0.346914\tGrad Norm: 1.320070\tLR: 0.030000\n",
      "Train set: Average loss: 0.3453\n",
      "Test set: Average loss: 0.2511, Average MAE: 0.3441\n",
      "Train Epoch: 889 [4096/194182 (2%)]\tLoss: 0.364481\tGrad Norm: 1.724874\tLR: 0.030000\n",
      "Train Epoch: 889 [24576/194182 (12%)]\tLoss: 0.340885\tGrad Norm: 1.028555\tLR: 0.030000\n",
      "Train Epoch: 889 [45056/194182 (23%)]\tLoss: 0.346152\tGrad Norm: 1.060458\tLR: 0.030000\n",
      "Train Epoch: 889 [65536/194182 (33%)]\tLoss: 0.339738\tGrad Norm: 0.873694\tLR: 0.030000\n",
      "Train Epoch: 889 [86016/194182 (44%)]\tLoss: 0.344225\tGrad Norm: 1.176159\tLR: 0.030000\n",
      "Train Epoch: 889 [106496/194182 (54%)]\tLoss: 0.347125\tGrad Norm: 1.194533\tLR: 0.030000\n",
      "Train Epoch: 889 [126976/194182 (65%)]\tLoss: 0.345518\tGrad Norm: 1.578455\tLR: 0.030000\n",
      "Train Epoch: 889 [147456/194182 (75%)]\tLoss: 0.352463\tGrad Norm: 1.603250\tLR: 0.030000\n",
      "Train Epoch: 889 [167936/194182 (85%)]\tLoss: 0.352177\tGrad Norm: 1.615254\tLR: 0.030000\n",
      "Train Epoch: 889 [188416/194182 (96%)]\tLoss: 0.349823\tGrad Norm: 1.534214\tLR: 0.030000\n",
      "Train set: Average loss: 0.3458\n",
      "Test set: Average loss: 0.2437, Average MAE: 0.3354\n",
      "Train Epoch: 890 [4096/194182 (2%)]\tLoss: 0.350110\tGrad Norm: 1.765937\tLR: 0.030000\n",
      "Train Epoch: 890 [24576/194182 (12%)]\tLoss: 0.343647\tGrad Norm: 1.381431\tLR: 0.030000\n",
      "Train Epoch: 890 [45056/194182 (23%)]\tLoss: 0.332170\tGrad Norm: 1.260464\tLR: 0.030000\n",
      "Train Epoch: 890 [65536/194182 (33%)]\tLoss: 0.346850\tGrad Norm: 1.314502\tLR: 0.030000\n",
      "Train Epoch: 890 [86016/194182 (44%)]\tLoss: 0.349444\tGrad Norm: 1.372376\tLR: 0.030000\n",
      "Train Epoch: 890 [106496/194182 (54%)]\tLoss: 0.347015\tGrad Norm: 1.204838\tLR: 0.030000\n",
      "Train Epoch: 890 [126976/194182 (65%)]\tLoss: 0.344967\tGrad Norm: 1.206957\tLR: 0.030000\n",
      "Train Epoch: 890 [147456/194182 (75%)]\tLoss: 0.340144\tGrad Norm: 1.211522\tLR: 0.030000\n",
      "Train Epoch: 890 [167936/194182 (85%)]\tLoss: 0.354229\tGrad Norm: 1.261929\tLR: 0.030000\n",
      "Train Epoch: 890 [188416/194182 (96%)]\tLoss: 0.351282\tGrad Norm: 1.322088\tLR: 0.030000\n",
      "Train set: Average loss: 0.3461\n",
      "Test set: Average loss: 0.2442, Average MAE: 0.3373\n",
      "Epoch 890: Mean reward = 0.047 +/- 0.052\n",
      "Train Epoch: 891 [4096/194182 (2%)]\tLoss: 0.350437\tGrad Norm: 1.358129\tLR: 0.030000\n",
      "Train Epoch: 891 [24576/194182 (12%)]\tLoss: 0.342396\tGrad Norm: 1.300975\tLR: 0.030000\n",
      "Train Epoch: 891 [45056/194182 (23%)]\tLoss: 0.344827\tGrad Norm: 1.713540\tLR: 0.030000\n",
      "Train Epoch: 891 [65536/194182 (33%)]\tLoss: 0.333659\tGrad Norm: 1.161286\tLR: 0.030000\n",
      "Train Epoch: 891 [86016/194182 (44%)]\tLoss: 0.341388\tGrad Norm: 1.086816\tLR: 0.030000\n",
      "Train Epoch: 891 [106496/194182 (54%)]\tLoss: 0.364323\tGrad Norm: 1.623366\tLR: 0.030000\n",
      "Train Epoch: 891 [126976/194182 (65%)]\tLoss: 0.350130\tGrad Norm: 1.598050\tLR: 0.030000\n",
      "Train Epoch: 891 [147456/194182 (75%)]\tLoss: 0.352963\tGrad Norm: 1.520527\tLR: 0.030000\n",
      "Train Epoch: 891 [167936/194182 (85%)]\tLoss: 0.344989\tGrad Norm: 1.226695\tLR: 0.030000\n",
      "Train Epoch: 891 [188416/194182 (96%)]\tLoss: 0.343867\tGrad Norm: 1.250984\tLR: 0.030000\n",
      "Train set: Average loss: 0.3471\n",
      "Test set: Average loss: 0.2407, Average MAE: 0.3513\n",
      "Train Epoch: 892 [4096/194182 (2%)]\tLoss: 0.342488\tGrad Norm: 1.199490\tLR: 0.030000\n",
      "Train Epoch: 892 [24576/194182 (12%)]\tLoss: 0.343450\tGrad Norm: 1.285529\tLR: 0.030000\n",
      "Train Epoch: 892 [45056/194182 (23%)]\tLoss: 0.346828\tGrad Norm: 1.298562\tLR: 0.030000\n",
      "Train Epoch: 892 [65536/194182 (33%)]\tLoss: 0.337556\tGrad Norm: 1.092711\tLR: 0.030000\n",
      "Train Epoch: 892 [86016/194182 (44%)]\tLoss: 0.357029\tGrad Norm: 1.786651\tLR: 0.030000\n",
      "Train Epoch: 892 [106496/194182 (54%)]\tLoss: 0.351530\tGrad Norm: 1.666113\tLR: 0.030000\n",
      "Train Epoch: 892 [126976/194182 (65%)]\tLoss: 0.338445\tGrad Norm: 1.190503\tLR: 0.030000\n",
      "Train Epoch: 892 [147456/194182 (75%)]\tLoss: 0.330005\tGrad Norm: 0.941234\tLR: 0.030000\n",
      "Train Epoch: 892 [167936/194182 (85%)]\tLoss: 0.349323\tGrad Norm: 1.196687\tLR: 0.030000\n",
      "Train Epoch: 892 [188416/194182 (96%)]\tLoss: 0.349578\tGrad Norm: 1.119722\tLR: 0.030000\n",
      "Train set: Average loss: 0.3456\n",
      "Test set: Average loss: 0.2365, Average MAE: 0.3324\n",
      "Train Epoch: 893 [4096/194182 (2%)]\tLoss: 0.343902\tGrad Norm: 0.804755\tLR: 0.030000\n",
      "Train Epoch: 893 [24576/194182 (12%)]\tLoss: 0.338675\tGrad Norm: 1.404731\tLR: 0.030000\n",
      "Train Epoch: 893 [45056/194182 (23%)]\tLoss: 0.342221\tGrad Norm: 1.211171\tLR: 0.030000\n",
      "Train Epoch: 893 [65536/194182 (33%)]\tLoss: 0.350098\tGrad Norm: 1.544450\tLR: 0.030000\n",
      "Train Epoch: 893 [86016/194182 (44%)]\tLoss: 0.350994\tGrad Norm: 1.356554\tLR: 0.030000\n",
      "Train Epoch: 893 [106496/194182 (54%)]\tLoss: 0.349596\tGrad Norm: 1.234223\tLR: 0.030000\n",
      "Train Epoch: 893 [126976/194182 (65%)]\tLoss: 0.343973\tGrad Norm: 1.403118\tLR: 0.030000\n",
      "Train Epoch: 893 [147456/194182 (75%)]\tLoss: 0.346958\tGrad Norm: 1.411573\tLR: 0.030000\n",
      "Train Epoch: 893 [167936/194182 (85%)]\tLoss: 0.342255\tGrad Norm: 0.868199\tLR: 0.030000\n",
      "Train Epoch: 893 [188416/194182 (96%)]\tLoss: 0.341343\tGrad Norm: 0.928249\tLR: 0.030000\n",
      "Train set: Average loss: 0.3442\n",
      "Test set: Average loss: 0.2410, Average MAE: 0.3380\n",
      "Train Epoch: 894 [4096/194182 (2%)]\tLoss: 0.346812\tGrad Norm: 1.206526\tLR: 0.030000\n",
      "Train Epoch: 894 [24576/194182 (12%)]\tLoss: 0.348396\tGrad Norm: 1.452844\tLR: 0.030000\n",
      "Train Epoch: 894 [45056/194182 (23%)]\tLoss: 0.345496\tGrad Norm: 1.367699\tLR: 0.030000\n",
      "Train Epoch: 894 [65536/194182 (33%)]\tLoss: 0.345032\tGrad Norm: 1.264084\tLR: 0.030000\n",
      "Train Epoch: 894 [86016/194182 (44%)]\tLoss: 0.344352\tGrad Norm: 1.424466\tLR: 0.030000\n",
      "Train Epoch: 894 [106496/194182 (54%)]\tLoss: 0.347772\tGrad Norm: 1.305398\tLR: 0.030000\n",
      "Train Epoch: 894 [126976/194182 (65%)]\tLoss: 0.347922\tGrad Norm: 1.301476\tLR: 0.030000\n",
      "Train Epoch: 894 [147456/194182 (75%)]\tLoss: 0.356212\tGrad Norm: 1.597494\tLR: 0.030000\n",
      "Train Epoch: 894 [167936/194182 (85%)]\tLoss: 0.352286\tGrad Norm: 1.551152\tLR: 0.030000\n",
      "Train Epoch: 894 [188416/194182 (96%)]\tLoss: 0.345894\tGrad Norm: 1.504337\tLR: 0.030000\n",
      "Train set: Average loss: 0.3462\n",
      "Test set: Average loss: 0.2388, Average MAE: 0.3346\n",
      "Train Epoch: 895 [4096/194182 (2%)]\tLoss: 0.339729\tGrad Norm: 1.071357\tLR: 0.030000\n",
      "Train Epoch: 895 [24576/194182 (12%)]\tLoss: 0.353854\tGrad Norm: 1.496374\tLR: 0.030000\n",
      "Train Epoch: 895 [45056/194182 (23%)]\tLoss: 0.349547\tGrad Norm: 1.431106\tLR: 0.030000\n",
      "Train Epoch: 895 [65536/194182 (33%)]\tLoss: 0.347111\tGrad Norm: 1.346334\tLR: 0.030000\n",
      "Train Epoch: 895 [86016/194182 (44%)]\tLoss: 0.346795\tGrad Norm: 1.305548\tLR: 0.030000\n",
      "Train Epoch: 895 [106496/194182 (54%)]\tLoss: 0.344846\tGrad Norm: 1.544394\tLR: 0.030000\n",
      "Train Epoch: 895 [126976/194182 (65%)]\tLoss: 0.351207\tGrad Norm: 1.764899\tLR: 0.030000\n",
      "Train Epoch: 895 [147456/194182 (75%)]\tLoss: 0.347117\tGrad Norm: 1.315250\tLR: 0.030000\n",
      "Train Epoch: 895 [167936/194182 (85%)]\tLoss: 0.339538\tGrad Norm: 1.287133\tLR: 0.030000\n",
      "Train Epoch: 895 [188416/194182 (96%)]\tLoss: 0.353341\tGrad Norm: 1.489176\tLR: 0.030000\n",
      "Train set: Average loss: 0.3463\n",
      "Test set: Average loss: 0.2440, Average MAE: 0.3314\n",
      "Epoch 895: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 896 [4096/194182 (2%)]\tLoss: 0.348875\tGrad Norm: 1.584296\tLR: 0.030000\n",
      "Train Epoch: 896 [24576/194182 (12%)]\tLoss: 0.354613\tGrad Norm: 1.593381\tLR: 0.030000\n",
      "Train Epoch: 896 [45056/194182 (23%)]\tLoss: 0.343472\tGrad Norm: 1.121080\tLR: 0.030000\n",
      "Train Epoch: 896 [65536/194182 (33%)]\tLoss: 0.336182\tGrad Norm: 1.226251\tLR: 0.030000\n",
      "Train Epoch: 896 [86016/194182 (44%)]\tLoss: 0.342826\tGrad Norm: 1.134126\tLR: 0.030000\n",
      "Train Epoch: 896 [106496/194182 (54%)]\tLoss: 0.339124\tGrad Norm: 1.121555\tLR: 0.030000\n",
      "Train Epoch: 896 [126976/194182 (65%)]\tLoss: 0.340262\tGrad Norm: 1.232374\tLR: 0.030000\n",
      "Train Epoch: 896 [147456/194182 (75%)]\tLoss: 0.343789\tGrad Norm: 1.373402\tLR: 0.030000\n",
      "Train Epoch: 896 [167936/194182 (85%)]\tLoss: 0.346387\tGrad Norm: 1.245227\tLR: 0.030000\n",
      "Train Epoch: 896 [188416/194182 (96%)]\tLoss: 0.345018\tGrad Norm: 1.287419\tLR: 0.030000\n",
      "Train set: Average loss: 0.3451\n",
      "Test set: Average loss: 0.2403, Average MAE: 0.3317\n",
      "Train Epoch: 897 [4096/194182 (2%)]\tLoss: 0.339306\tGrad Norm: 1.172090\tLR: 0.030000\n",
      "Train Epoch: 897 [24576/194182 (12%)]\tLoss: 0.346909\tGrad Norm: 1.521065\tLR: 0.030000\n",
      "Train Epoch: 897 [45056/194182 (23%)]\tLoss: 0.346732\tGrad Norm: 1.672105\tLR: 0.030000\n",
      "Train Epoch: 897 [65536/194182 (33%)]\tLoss: 0.348012\tGrad Norm: 1.439197\tLR: 0.030000\n",
      "Train Epoch: 897 [86016/194182 (44%)]\tLoss: 0.343328\tGrad Norm: 1.388200\tLR: 0.030000\n",
      "Train Epoch: 897 [106496/194182 (54%)]\tLoss: 0.331205\tGrad Norm: 1.075900\tLR: 0.030000\n",
      "Train Epoch: 897 [126976/194182 (65%)]\tLoss: 0.342857\tGrad Norm: 0.825907\tLR: 0.030000\n",
      "Train Epoch: 897 [147456/194182 (75%)]\tLoss: 0.348694\tGrad Norm: 1.373849\tLR: 0.030000\n",
      "Train Epoch: 897 [167936/194182 (85%)]\tLoss: 0.347523\tGrad Norm: 1.154583\tLR: 0.030000\n",
      "Train Epoch: 897 [188416/194182 (96%)]\tLoss: 0.348981\tGrad Norm: 1.458063\tLR: 0.030000\n",
      "Train set: Average loss: 0.3449\n",
      "Test set: Average loss: 0.2458, Average MAE: 0.3490\n",
      "Train Epoch: 898 [4096/194182 (2%)]\tLoss: 0.343485\tGrad Norm: 1.414795\tLR: 0.030000\n",
      "Train Epoch: 898 [24576/194182 (12%)]\tLoss: 0.346036\tGrad Norm: 1.312457\tLR: 0.030000\n",
      "Train Epoch: 898 [45056/194182 (23%)]\tLoss: 0.344774\tGrad Norm: 1.318061\tLR: 0.030000\n",
      "Train Epoch: 898 [65536/194182 (33%)]\tLoss: 0.346553\tGrad Norm: 1.371710\tLR: 0.030000\n",
      "Train Epoch: 898 [86016/194182 (44%)]\tLoss: 0.339109\tGrad Norm: 0.815973\tLR: 0.030000\n",
      "Train Epoch: 898 [106496/194182 (54%)]\tLoss: 0.349292\tGrad Norm: 1.272620\tLR: 0.030000\n",
      "Train Epoch: 898 [126976/194182 (65%)]\tLoss: 0.344131\tGrad Norm: 1.111537\tLR: 0.030000\n",
      "Train Epoch: 898 [147456/194182 (75%)]\tLoss: 0.343399\tGrad Norm: 1.245168\tLR: 0.030000\n",
      "Train Epoch: 898 [167936/194182 (85%)]\tLoss: 0.347382\tGrad Norm: 1.515992\tLR: 0.030000\n",
      "Train Epoch: 898 [188416/194182 (96%)]\tLoss: 0.344618\tGrad Norm: 1.389501\tLR: 0.030000\n",
      "Train set: Average loss: 0.3438\n",
      "Test set: Average loss: 0.2393, Average MAE: 0.3304\n",
      "Train Epoch: 899 [4096/194182 (2%)]\tLoss: 0.339215\tGrad Norm: 1.203135\tLR: 0.030000\n",
      "Train Epoch: 899 [24576/194182 (12%)]\tLoss: 0.344554\tGrad Norm: 1.319080\tLR: 0.030000\n",
      "Train Epoch: 899 [45056/194182 (23%)]\tLoss: 0.339276\tGrad Norm: 0.891201\tLR: 0.030000\n",
      "Train Epoch: 899 [65536/194182 (33%)]\tLoss: 0.349493\tGrad Norm: 1.326807\tLR: 0.030000\n",
      "Train Epoch: 899 [86016/194182 (44%)]\tLoss: 0.344655\tGrad Norm: 1.313506\tLR: 0.030000\n",
      "Train Epoch: 899 [106496/194182 (54%)]\tLoss: 0.340946\tGrad Norm: 1.121882\tLR: 0.030000\n",
      "Train Epoch: 899 [126976/194182 (65%)]\tLoss: 0.341635\tGrad Norm: 1.193193\tLR: 0.030000\n",
      "Train Epoch: 899 [147456/194182 (75%)]\tLoss: 0.349515\tGrad Norm: 1.154418\tLR: 0.030000\n",
      "Train Epoch: 899 [167936/194182 (85%)]\tLoss: 0.353425\tGrad Norm: 1.604396\tLR: 0.030000\n",
      "Train Epoch: 899 [188416/194182 (96%)]\tLoss: 0.347702\tGrad Norm: 1.546612\tLR: 0.030000\n",
      "Train set: Average loss: 0.3432\n",
      "Test set: Average loss: 0.2428, Average MAE: 0.3321\n",
      "Train Epoch: 900 [4096/194182 (2%)]\tLoss: 0.348106\tGrad Norm: 1.488689\tLR: 0.030000\n",
      "Train Epoch: 900 [24576/194182 (12%)]\tLoss: 0.352635\tGrad Norm: 1.347566\tLR: 0.030000\n",
      "Train Epoch: 900 [45056/194182 (23%)]\tLoss: 0.341556\tGrad Norm: 1.510386\tLR: 0.030000\n",
      "Train Epoch: 900 [65536/194182 (33%)]\tLoss: 0.337884\tGrad Norm: 1.112421\tLR: 0.030000\n",
      "Train Epoch: 900 [86016/194182 (44%)]\tLoss: 0.340734\tGrad Norm: 1.071737\tLR: 0.030000\n",
      "Train Epoch: 900 [106496/194182 (54%)]\tLoss: 0.340928\tGrad Norm: 1.288373\tLR: 0.030000\n",
      "Train Epoch: 900 [126976/194182 (65%)]\tLoss: 0.342205\tGrad Norm: 1.437044\tLR: 0.030000\n",
      "Train Epoch: 900 [147456/194182 (75%)]\tLoss: 0.349347\tGrad Norm: 1.456704\tLR: 0.030000\n",
      "Train Epoch: 900 [167936/194182 (85%)]\tLoss: 0.338913\tGrad Norm: 1.044734\tLR: 0.030000\n",
      "Train Epoch: 900 [188416/194182 (96%)]\tLoss: 0.350961\tGrad Norm: 1.260540\tLR: 0.030000\n",
      "Train set: Average loss: 0.3434\n",
      "Test set: Average loss: 0.2463, Average MAE: 0.3566\n",
      "Epoch 900: Mean reward = 0.046 +/- 0.001\n",
      "Train Epoch: 901 [4096/194182 (2%)]\tLoss: 0.342870\tGrad Norm: 1.493953\tLR: 0.030000\n",
      "Train Epoch: 901 [24576/194182 (12%)]\tLoss: 0.344344\tGrad Norm: 1.441674\tLR: 0.030000\n",
      "Train Epoch: 901 [45056/194182 (23%)]\tLoss: 0.344568\tGrad Norm: 1.305179\tLR: 0.030000\n",
      "Train Epoch: 901 [65536/194182 (33%)]\tLoss: 0.351248\tGrad Norm: 1.450061\tLR: 0.030000\n",
      "Train Epoch: 901 [86016/194182 (44%)]\tLoss: 0.352722\tGrad Norm: 1.722901\tLR: 0.030000\n",
      "Train Epoch: 901 [106496/194182 (54%)]\tLoss: 0.337214\tGrad Norm: 1.240368\tLR: 0.030000\n",
      "Train Epoch: 901 [126976/194182 (65%)]\tLoss: 0.333212\tGrad Norm: 1.247819\tLR: 0.030000\n",
      "Train Epoch: 901 [147456/194182 (75%)]\tLoss: 0.337794\tGrad Norm: 0.925796\tLR: 0.030000\n",
      "Train Epoch: 901 [167936/194182 (85%)]\tLoss: 0.351180\tGrad Norm: 1.602320\tLR: 0.030000\n",
      "Train Epoch: 901 [188416/194182 (96%)]\tLoss: 0.349862\tGrad Norm: 1.482692\tLR: 0.030000\n",
      "Train set: Average loss: 0.3446\n",
      "Test set: Average loss: 0.2452, Average MAE: 0.3538\n",
      "Train Epoch: 902 [4096/194182 (2%)]\tLoss: 0.355409\tGrad Norm: 1.393860\tLR: 0.030000\n",
      "Train Epoch: 902 [24576/194182 (12%)]\tLoss: 0.341358\tGrad Norm: 1.529904\tLR: 0.030000\n",
      "Train Epoch: 902 [45056/194182 (23%)]\tLoss: 0.339644\tGrad Norm: 1.365436\tLR: 0.030000\n",
      "Train Epoch: 902 [65536/194182 (33%)]\tLoss: 0.342282\tGrad Norm: 1.154376\tLR: 0.030000\n",
      "Train Epoch: 902 [86016/194182 (44%)]\tLoss: 0.340999\tGrad Norm: 0.882405\tLR: 0.030000\n",
      "Train Epoch: 902 [106496/194182 (54%)]\tLoss: 0.342131\tGrad Norm: 1.123066\tLR: 0.030000\n",
      "Train Epoch: 902 [126976/194182 (65%)]\tLoss: 0.344055\tGrad Norm: 1.593456\tLR: 0.030000\n",
      "Train Epoch: 902 [147456/194182 (75%)]\tLoss: 0.349070\tGrad Norm: 1.644978\tLR: 0.030000\n",
      "Train Epoch: 902 [167936/194182 (85%)]\tLoss: 0.352653\tGrad Norm: 1.575346\tLR: 0.030000\n",
      "Train Epoch: 902 [188416/194182 (96%)]\tLoss: 0.347415\tGrad Norm: 1.494518\tLR: 0.030000\n",
      "Train set: Average loss: 0.3441\n",
      "Test set: Average loss: 0.2530, Average MAE: 0.3585\n",
      "Train Epoch: 903 [4096/194182 (2%)]\tLoss: 0.352196\tGrad Norm: 1.663767\tLR: 0.030000\n",
      "Train Epoch: 903 [24576/194182 (12%)]\tLoss: 0.344056\tGrad Norm: 0.991753\tLR: 0.030000\n",
      "Train Epoch: 903 [45056/194182 (23%)]\tLoss: 0.342904\tGrad Norm: 1.352134\tLR: 0.030000\n",
      "Train Epoch: 903 [65536/194182 (33%)]\tLoss: 0.339201\tGrad Norm: 1.277707\tLR: 0.030000\n",
      "Train Epoch: 903 [86016/194182 (44%)]\tLoss: 0.341095\tGrad Norm: 1.299004\tLR: 0.030000\n",
      "Train Epoch: 903 [106496/194182 (54%)]\tLoss: 0.347254\tGrad Norm: 1.359123\tLR: 0.030000\n",
      "Train Epoch: 903 [126976/194182 (65%)]\tLoss: 0.340092\tGrad Norm: 1.212782\tLR: 0.030000\n",
      "Train Epoch: 903 [147456/194182 (75%)]\tLoss: 0.352096\tGrad Norm: 1.500291\tLR: 0.030000\n",
      "Train Epoch: 903 [167936/194182 (85%)]\tLoss: 0.341553\tGrad Norm: 1.058392\tLR: 0.030000\n",
      "Train Epoch: 903 [188416/194182 (96%)]\tLoss: 0.335223\tGrad Norm: 1.202735\tLR: 0.030000\n",
      "Train set: Average loss: 0.3423\n",
      "Test set: Average loss: 0.2464, Average MAE: 0.3449\n",
      "Train Epoch: 904 [4096/194182 (2%)]\tLoss: 0.340597\tGrad Norm: 1.498415\tLR: 0.030000\n",
      "Train Epoch: 904 [24576/194182 (12%)]\tLoss: 0.344865\tGrad Norm: 1.391425\tLR: 0.030000\n",
      "Train Epoch: 904 [45056/194182 (23%)]\tLoss: 0.337134\tGrad Norm: 1.352146\tLR: 0.030000\n",
      "Train Epoch: 904 [65536/194182 (33%)]\tLoss: 0.344208\tGrad Norm: 1.703119\tLR: 0.030000\n",
      "Train Epoch: 904 [86016/194182 (44%)]\tLoss: 0.344550\tGrad Norm: 1.570061\tLR: 0.030000\n",
      "Train Epoch: 904 [106496/194182 (54%)]\tLoss: 0.341331\tGrad Norm: 1.545577\tLR: 0.030000\n",
      "Train Epoch: 904 [126976/194182 (65%)]\tLoss: 0.353695\tGrad Norm: 1.300027\tLR: 0.030000\n",
      "Train Epoch: 904 [147456/194182 (75%)]\tLoss: 0.350343\tGrad Norm: 1.686646\tLR: 0.030000\n",
      "Train Epoch: 904 [167936/194182 (85%)]\tLoss: 0.347043\tGrad Norm: 1.391792\tLR: 0.030000\n",
      "Train Epoch: 904 [188416/194182 (96%)]\tLoss: 0.344371\tGrad Norm: 1.568246\tLR: 0.030000\n",
      "Train set: Average loss: 0.3447\n",
      "Test set: Average loss: 0.2494, Average MAE: 0.3347\n",
      "Train Epoch: 905 [4096/194182 (2%)]\tLoss: 0.350377\tGrad Norm: 1.778366\tLR: 0.030000\n",
      "Train Epoch: 905 [24576/194182 (12%)]\tLoss: 0.345994\tGrad Norm: 1.188434\tLR: 0.030000\n",
      "Train Epoch: 905 [45056/194182 (23%)]\tLoss: 0.349704\tGrad Norm: 1.327315\tLR: 0.030000\n",
      "Train Epoch: 905 [65536/194182 (33%)]\tLoss: 0.347354\tGrad Norm: 1.278932\tLR: 0.030000\n",
      "Train Epoch: 905 [86016/194182 (44%)]\tLoss: 0.342295\tGrad Norm: 0.995920\tLR: 0.030000\n",
      "Train Epoch: 905 [106496/194182 (54%)]\tLoss: 0.333160\tGrad Norm: 1.049919\tLR: 0.030000\n",
      "Train Epoch: 905 [126976/194182 (65%)]\tLoss: 0.347155\tGrad Norm: 1.218446\tLR: 0.030000\n",
      "Train Epoch: 905 [147456/194182 (75%)]\tLoss: 0.336419\tGrad Norm: 1.376617\tLR: 0.030000\n",
      "Train Epoch: 905 [167936/194182 (85%)]\tLoss: 0.341450\tGrad Norm: 1.273424\tLR: 0.030000\n",
      "Train Epoch: 905 [188416/194182 (96%)]\tLoss: 0.344037\tGrad Norm: 1.463467\tLR: 0.030000\n",
      "Train set: Average loss: 0.3424\n",
      "Test set: Average loss: 0.2463, Average MAE: 0.3570\n",
      "Epoch 905: Mean reward = 0.058 +/- 0.037\n",
      "Train Epoch: 906 [4096/194182 (2%)]\tLoss: 0.349030\tGrad Norm: 1.460705\tLR: 0.030000\n",
      "Train Epoch: 906 [24576/194182 (12%)]\tLoss: 0.329617\tGrad Norm: 0.971219\tLR: 0.030000\n",
      "Train Epoch: 906 [45056/194182 (23%)]\tLoss: 0.337282\tGrad Norm: 0.990041\tLR: 0.030000\n",
      "Train Epoch: 906 [65536/194182 (33%)]\tLoss: 0.350510\tGrad Norm: 1.310145\tLR: 0.030000\n",
      "Train Epoch: 906 [86016/194182 (44%)]\tLoss: 0.339616\tGrad Norm: 1.138332\tLR: 0.030000\n",
      "Train Epoch: 906 [106496/194182 (54%)]\tLoss: 0.340323\tGrad Norm: 1.076622\tLR: 0.030000\n",
      "Train Epoch: 906 [126976/194182 (65%)]\tLoss: 0.343980\tGrad Norm: 1.426204\tLR: 0.030000\n",
      "Train Epoch: 906 [147456/194182 (75%)]\tLoss: 0.344918\tGrad Norm: 1.208036\tLR: 0.030000\n",
      "Train Epoch: 906 [167936/194182 (85%)]\tLoss: 0.349575\tGrad Norm: 1.387921\tLR: 0.030000\n",
      "Train Epoch: 906 [188416/194182 (96%)]\tLoss: 0.350010\tGrad Norm: 1.616176\tLR: 0.030000\n",
      "Train set: Average loss: 0.3415\n",
      "Test set: Average loss: 0.2440, Average MAE: 0.3322\n",
      "Train Epoch: 907 [4096/194182 (2%)]\tLoss: 0.339871\tGrad Norm: 1.539309\tLR: 0.030000\n",
      "Train Epoch: 907 [24576/194182 (12%)]\tLoss: 0.345493\tGrad Norm: 1.379265\tLR: 0.030000\n",
      "Train Epoch: 907 [45056/194182 (23%)]\tLoss: 0.346106\tGrad Norm: 1.123440\tLR: 0.030000\n",
      "Train Epoch: 907 [65536/194182 (33%)]\tLoss: 0.338877\tGrad Norm: 1.574718\tLR: 0.030000\n",
      "Train Epoch: 907 [86016/194182 (44%)]\tLoss: 0.347977\tGrad Norm: 1.451764\tLR: 0.030000\n",
      "Train Epoch: 907 [106496/194182 (54%)]\tLoss: 0.335041\tGrad Norm: 1.348314\tLR: 0.030000\n",
      "Train Epoch: 907 [126976/194182 (65%)]\tLoss: 0.338394\tGrad Norm: 1.147147\tLR: 0.030000\n",
      "Train Epoch: 907 [147456/194182 (75%)]\tLoss: 0.349779\tGrad Norm: 1.296950\tLR: 0.030000\n",
      "Train Epoch: 907 [167936/194182 (85%)]\tLoss: 0.349560\tGrad Norm: 1.524263\tLR: 0.030000\n",
      "Train Epoch: 907 [188416/194182 (96%)]\tLoss: 0.347257\tGrad Norm: 1.448981\tLR: 0.030000\n",
      "Train set: Average loss: 0.3433\n",
      "Test set: Average loss: 0.2423, Average MAE: 0.3346\n",
      "Train Epoch: 908 [4096/194182 (2%)]\tLoss: 0.339481\tGrad Norm: 1.255663\tLR: 0.030000\n",
      "Train Epoch: 908 [24576/194182 (12%)]\tLoss: 0.334119\tGrad Norm: 1.106425\tLR: 0.030000\n",
      "Train Epoch: 908 [45056/194182 (23%)]\tLoss: 0.332419\tGrad Norm: 0.824153\tLR: 0.030000\n",
      "Train Epoch: 908 [65536/194182 (33%)]\tLoss: 0.336553\tGrad Norm: 0.910068\tLR: 0.030000\n",
      "Train Epoch: 908 [86016/194182 (44%)]\tLoss: 0.339407\tGrad Norm: 1.051842\tLR: 0.030000\n",
      "Train Epoch: 908 [106496/194182 (54%)]\tLoss: 0.338235\tGrad Norm: 1.182510\tLR: 0.030000\n",
      "Train Epoch: 908 [126976/194182 (65%)]\tLoss: 0.333501\tGrad Norm: 1.349387\tLR: 0.030000\n",
      "Train Epoch: 908 [147456/194182 (75%)]\tLoss: 0.340925\tGrad Norm: 1.403933\tLR: 0.030000\n",
      "Train Epoch: 908 [167936/194182 (85%)]\tLoss: 0.352589\tGrad Norm: 1.516091\tLR: 0.030000\n",
      "Train Epoch: 908 [188416/194182 (96%)]\tLoss: 0.337098\tGrad Norm: 1.542066\tLR: 0.030000\n",
      "Train set: Average loss: 0.3402\n",
      "Test set: Average loss: 0.2412, Average MAE: 0.3395\n",
      "Train Epoch: 909 [4096/194182 (2%)]\tLoss: 0.338421\tGrad Norm: 1.275326\tLR: 0.030000\n",
      "Train Epoch: 909 [24576/194182 (12%)]\tLoss: 0.340545\tGrad Norm: 1.557750\tLR: 0.030000\n",
      "Train Epoch: 909 [45056/194182 (23%)]\tLoss: 0.350645\tGrad Norm: 1.325581\tLR: 0.030000\n",
      "Train Epoch: 909 [65536/194182 (33%)]\tLoss: 0.337114\tGrad Norm: 1.000951\tLR: 0.030000\n",
      "Train Epoch: 909 [86016/194182 (44%)]\tLoss: 0.333976\tGrad Norm: 1.182137\tLR: 0.030000\n",
      "Train Epoch: 909 [106496/194182 (54%)]\tLoss: 0.348443\tGrad Norm: 1.572217\tLR: 0.030000\n",
      "Train Epoch: 909 [126976/194182 (65%)]\tLoss: 0.344873\tGrad Norm: 1.596620\tLR: 0.030000\n",
      "Train Epoch: 909 [147456/194182 (75%)]\tLoss: 0.335141\tGrad Norm: 1.205012\tLR: 0.030000\n",
      "Train Epoch: 909 [167936/194182 (85%)]\tLoss: 0.339213\tGrad Norm: 1.363484\tLR: 0.030000\n",
      "Train Epoch: 909 [188416/194182 (96%)]\tLoss: 0.336641\tGrad Norm: 1.340905\tLR: 0.030000\n",
      "Train set: Average loss: 0.3416\n",
      "Test set: Average loss: 0.2451, Average MAE: 0.3516\n",
      "Train Epoch: 910 [4096/194182 (2%)]\tLoss: 0.348841\tGrad Norm: 1.490688\tLR: 0.030000\n",
      "Train Epoch: 910 [24576/194182 (12%)]\tLoss: 0.341350\tGrad Norm: 1.373213\tLR: 0.030000\n",
      "Train Epoch: 910 [45056/194182 (23%)]\tLoss: 0.337113\tGrad Norm: 1.471142\tLR: 0.030000\n",
      "Train Epoch: 910 [65536/194182 (33%)]\tLoss: 0.355712\tGrad Norm: 1.928893\tLR: 0.030000\n",
      "Train Epoch: 910 [86016/194182 (44%)]\tLoss: 0.342637\tGrad Norm: 1.372817\tLR: 0.030000\n",
      "Train Epoch: 910 [106496/194182 (54%)]\tLoss: 0.338915\tGrad Norm: 1.606809\tLR: 0.030000\n",
      "Train Epoch: 910 [126976/194182 (65%)]\tLoss: 0.339860\tGrad Norm: 1.186112\tLR: 0.030000\n",
      "Train Epoch: 910 [147456/194182 (75%)]\tLoss: 0.347094\tGrad Norm: 1.317943\tLR: 0.030000\n",
      "Train Epoch: 910 [167936/194182 (85%)]\tLoss: 0.332169\tGrad Norm: 1.294790\tLR: 0.030000\n",
      "Train Epoch: 910 [188416/194182 (96%)]\tLoss: 0.345820\tGrad Norm: 1.262441\tLR: 0.030000\n",
      "Train set: Average loss: 0.3426\n",
      "Test set: Average loss: 0.2373, Average MAE: 0.3443\n",
      "Epoch 910: Mean reward = 0.046 +/- 0.001\n",
      "Train Epoch: 911 [4096/194182 (2%)]\tLoss: 0.333405\tGrad Norm: 0.909519\tLR: 0.030000\n",
      "Train Epoch: 911 [24576/194182 (12%)]\tLoss: 0.347967\tGrad Norm: 1.547052\tLR: 0.030000\n",
      "Train Epoch: 911 [45056/194182 (23%)]\tLoss: 0.341950\tGrad Norm: 1.316739\tLR: 0.030000\n",
      "Train Epoch: 911 [65536/194182 (33%)]\tLoss: 0.337142\tGrad Norm: 1.081229\tLR: 0.030000\n",
      "Train Epoch: 911 [86016/194182 (44%)]\tLoss: 0.335904\tGrad Norm: 1.159217\tLR: 0.030000\n",
      "Train Epoch: 911 [106496/194182 (54%)]\tLoss: 0.344621\tGrad Norm: 1.216726\tLR: 0.030000\n",
      "Train Epoch: 911 [126976/194182 (65%)]\tLoss: 0.346146\tGrad Norm: 1.507743\tLR: 0.030000\n",
      "Train Epoch: 911 [147456/194182 (75%)]\tLoss: 0.340196\tGrad Norm: 1.225420\tLR: 0.030000\n",
      "Train Epoch: 911 [167936/194182 (85%)]\tLoss: 0.333388\tGrad Norm: 1.171617\tLR: 0.030000\n",
      "Train Epoch: 911 [188416/194182 (96%)]\tLoss: 0.345476\tGrad Norm: 1.407652\tLR: 0.030000\n",
      "Train set: Average loss: 0.3403\n",
      "Test set: Average loss: 0.2430, Average MAE: 0.3525\n",
      "Train Epoch: 912 [4096/194182 (2%)]\tLoss: 0.336884\tGrad Norm: 1.274759\tLR: 0.030000\n",
      "Train Epoch: 912 [24576/194182 (12%)]\tLoss: 0.344805\tGrad Norm: 1.311668\tLR: 0.030000\n",
      "Train Epoch: 912 [45056/194182 (23%)]\tLoss: 0.341999\tGrad Norm: 1.146968\tLR: 0.030000\n",
      "Train Epoch: 912 [65536/194182 (33%)]\tLoss: 0.335343\tGrad Norm: 1.031820\tLR: 0.030000\n",
      "Train Epoch: 912 [86016/194182 (44%)]\tLoss: 0.337940\tGrad Norm: 1.293416\tLR: 0.030000\n",
      "Train Epoch: 912 [106496/194182 (54%)]\tLoss: 0.349582\tGrad Norm: 1.708769\tLR: 0.030000\n",
      "Train Epoch: 912 [126976/194182 (65%)]\tLoss: 0.338771\tGrad Norm: 1.315215\tLR: 0.030000\n",
      "Train Epoch: 912 [147456/194182 (75%)]\tLoss: 0.329273\tGrad Norm: 1.120971\tLR: 0.030000\n",
      "Train Epoch: 912 [167936/194182 (85%)]\tLoss: 0.341137\tGrad Norm: 1.224736\tLR: 0.030000\n",
      "Train Epoch: 912 [188416/194182 (96%)]\tLoss: 0.333752\tGrad Norm: 1.110184\tLR: 0.030000\n",
      "Train set: Average loss: 0.3396\n",
      "Test set: Average loss: 0.2432, Average MAE: 0.3465\n",
      "Train Epoch: 913 [4096/194182 (2%)]\tLoss: 0.340942\tGrad Norm: 1.321740\tLR: 0.030000\n",
      "Train Epoch: 913 [24576/194182 (12%)]\tLoss: 0.336623\tGrad Norm: 1.533563\tLR: 0.030000\n",
      "Train Epoch: 913 [45056/194182 (23%)]\tLoss: 0.343325\tGrad Norm: 1.440652\tLR: 0.030000\n",
      "Train Epoch: 913 [65536/194182 (33%)]\tLoss: 0.341104\tGrad Norm: 1.395628\tLR: 0.030000\n",
      "Train Epoch: 913 [86016/194182 (44%)]\tLoss: 0.336486\tGrad Norm: 1.175708\tLR: 0.030000\n",
      "Train Epoch: 913 [106496/194182 (54%)]\tLoss: 0.335404\tGrad Norm: 1.318001\tLR: 0.030000\n",
      "Train Epoch: 913 [126976/194182 (65%)]\tLoss: 0.333423\tGrad Norm: 1.209339\tLR: 0.030000\n",
      "Train Epoch: 913 [147456/194182 (75%)]\tLoss: 0.338862\tGrad Norm: 1.317363\tLR: 0.030000\n",
      "Train Epoch: 913 [167936/194182 (85%)]\tLoss: 0.346216\tGrad Norm: 1.538200\tLR: 0.030000\n",
      "Train Epoch: 913 [188416/194182 (96%)]\tLoss: 0.339089\tGrad Norm: 1.235905\tLR: 0.030000\n",
      "Train set: Average loss: 0.3413\n",
      "Test set: Average loss: 0.2408, Average MAE: 0.3409\n",
      "Train Epoch: 914 [4096/194182 (2%)]\tLoss: 0.335628\tGrad Norm: 1.228762\tLR: 0.030000\n",
      "Train Epoch: 914 [24576/194182 (12%)]\tLoss: 0.331572\tGrad Norm: 0.938836\tLR: 0.030000\n",
      "Train Epoch: 914 [45056/194182 (23%)]\tLoss: 0.336725\tGrad Norm: 0.927756\tLR: 0.030000\n",
      "Train Epoch: 914 [65536/194182 (33%)]\tLoss: 0.342999\tGrad Norm: 1.437207\tLR: 0.030000\n",
      "Train Epoch: 914 [86016/194182 (44%)]\tLoss: 0.336299\tGrad Norm: 1.289826\tLR: 0.030000\n",
      "Train Epoch: 914 [106496/194182 (54%)]\tLoss: 0.348407\tGrad Norm: 1.497469\tLR: 0.030000\n",
      "Train Epoch: 914 [126976/194182 (65%)]\tLoss: 0.345104\tGrad Norm: 1.450934\tLR: 0.030000\n",
      "Train Epoch: 914 [147456/194182 (75%)]\tLoss: 0.339077\tGrad Norm: 1.338458\tLR: 0.030000\n",
      "Train Epoch: 914 [167936/194182 (85%)]\tLoss: 0.338164\tGrad Norm: 1.354954\tLR: 0.030000\n",
      "Train Epoch: 914 [188416/194182 (96%)]\tLoss: 0.344211\tGrad Norm: 1.440906\tLR: 0.030000\n",
      "Train set: Average loss: 0.3400\n",
      "Test set: Average loss: 0.2468, Average MAE: 0.3358\n",
      "Train Epoch: 915 [4096/194182 (2%)]\tLoss: 0.350053\tGrad Norm: 1.698330\tLR: 0.030000\n",
      "Train Epoch: 915 [24576/194182 (12%)]\tLoss: 0.339444\tGrad Norm: 1.501885\tLR: 0.030000\n",
      "Train Epoch: 915 [45056/194182 (23%)]\tLoss: 0.339162\tGrad Norm: 1.363715\tLR: 0.030000\n",
      "Train Epoch: 915 [65536/194182 (33%)]\tLoss: 0.333481\tGrad Norm: 1.124767\tLR: 0.030000\n",
      "Train Epoch: 915 [86016/194182 (44%)]\tLoss: 0.330379\tGrad Norm: 1.203335\tLR: 0.030000\n",
      "Train Epoch: 915 [106496/194182 (54%)]\tLoss: 0.340652\tGrad Norm: 1.116865\tLR: 0.030000\n",
      "Train Epoch: 915 [126976/194182 (65%)]\tLoss: 0.333134\tGrad Norm: 1.063625\tLR: 0.030000\n",
      "Train Epoch: 915 [147456/194182 (75%)]\tLoss: 0.326324\tGrad Norm: 1.028354\tLR: 0.030000\n",
      "Train Epoch: 915 [167936/194182 (85%)]\tLoss: 0.337722\tGrad Norm: 1.058921\tLR: 0.030000\n",
      "Train Epoch: 915 [188416/194182 (96%)]\tLoss: 0.331906\tGrad Norm: 1.115680\tLR: 0.030000\n",
      "Train set: Average loss: 0.3385\n",
      "Test set: Average loss: 0.2421, Average MAE: 0.3298\n",
      "Epoch 915: Mean reward = 0.066 +/- 0.072\n",
      "Train Epoch: 916 [4096/194182 (2%)]\tLoss: 0.331254\tGrad Norm: 1.362682\tLR: 0.030000\n",
      "Train Epoch: 916 [24576/194182 (12%)]\tLoss: 0.341479\tGrad Norm: 1.377059\tLR: 0.030000\n",
      "Train Epoch: 916 [45056/194182 (23%)]\tLoss: 0.348122\tGrad Norm: 1.515761\tLR: 0.030000\n",
      "Train Epoch: 916 [65536/194182 (33%)]\tLoss: 0.347174\tGrad Norm: 1.214903\tLR: 0.030000\n",
      "Train Epoch: 916 [86016/194182 (44%)]\tLoss: 0.339366\tGrad Norm: 1.406736\tLR: 0.030000\n",
      "Train Epoch: 916 [106496/194182 (54%)]\tLoss: 0.342384\tGrad Norm: 1.717301\tLR: 0.030000\n",
      "Train Epoch: 916 [126976/194182 (65%)]\tLoss: 0.345285\tGrad Norm: 1.440595\tLR: 0.030000\n",
      "Train Epoch: 916 [147456/194182 (75%)]\tLoss: 0.342299\tGrad Norm: 1.344102\tLR: 0.030000\n",
      "Train Epoch: 916 [167936/194182 (85%)]\tLoss: 0.338292\tGrad Norm: 1.281017\tLR: 0.030000\n",
      "Train Epoch: 916 [188416/194182 (96%)]\tLoss: 0.343082\tGrad Norm: 1.587063\tLR: 0.030000\n",
      "Train set: Average loss: 0.3407\n",
      "Test set: Average loss: 0.2411, Average MAE: 0.3381\n",
      "Train Epoch: 917 [4096/194182 (2%)]\tLoss: 0.342625\tGrad Norm: 1.126870\tLR: 0.030000\n",
      "Train Epoch: 917 [24576/194182 (12%)]\tLoss: 0.333139\tGrad Norm: 1.235196\tLR: 0.030000\n",
      "Train Epoch: 917 [45056/194182 (23%)]\tLoss: 0.325893\tGrad Norm: 1.089228\tLR: 0.030000\n",
      "Train Epoch: 917 [65536/194182 (33%)]\tLoss: 0.327520\tGrad Norm: 0.945749\tLR: 0.030000\n",
      "Train Epoch: 917 [86016/194182 (44%)]\tLoss: 0.330645\tGrad Norm: 1.365161\tLR: 0.030000\n",
      "Train Epoch: 917 [106496/194182 (54%)]\tLoss: 0.344535\tGrad Norm: 1.392579\tLR: 0.030000\n",
      "Train Epoch: 917 [126976/194182 (65%)]\tLoss: 0.333999\tGrad Norm: 1.205860\tLR: 0.030000\n",
      "Train Epoch: 917 [147456/194182 (75%)]\tLoss: 0.339443\tGrad Norm: 1.110456\tLR: 0.030000\n",
      "Train Epoch: 917 [167936/194182 (85%)]\tLoss: 0.333584\tGrad Norm: 1.050479\tLR: 0.030000\n",
      "Train Epoch: 917 [188416/194182 (96%)]\tLoss: 0.331789\tGrad Norm: 1.407651\tLR: 0.030000\n",
      "Train set: Average loss: 0.3373\n",
      "Test set: Average loss: 0.2474, Average MAE: 0.3532\n",
      "Train Epoch: 918 [4096/194182 (2%)]\tLoss: 0.346699\tGrad Norm: 1.669210\tLR: 0.030000\n",
      "Train Epoch: 918 [24576/194182 (12%)]\tLoss: 0.346085\tGrad Norm: 1.415330\tLR: 0.030000\n",
      "Train Epoch: 918 [45056/194182 (23%)]\tLoss: 0.331968\tGrad Norm: 1.123521\tLR: 0.030000\n",
      "Train Epoch: 918 [65536/194182 (33%)]\tLoss: 0.341947\tGrad Norm: 1.188446\tLR: 0.030000\n",
      "Train Epoch: 918 [86016/194182 (44%)]\tLoss: 0.347950\tGrad Norm: 1.621956\tLR: 0.030000\n",
      "Train Epoch: 918 [106496/194182 (54%)]\tLoss: 0.333676\tGrad Norm: 1.258475\tLR: 0.030000\n",
      "Train Epoch: 918 [126976/194182 (65%)]\tLoss: 0.342830\tGrad Norm: 1.501847\tLR: 0.030000\n",
      "Train Epoch: 918 [147456/194182 (75%)]\tLoss: 0.355098\tGrad Norm: 2.037483\tLR: 0.030000\n",
      "Train Epoch: 918 [167936/194182 (85%)]\tLoss: 0.337047\tGrad Norm: 1.536619\tLR: 0.030000\n",
      "Train Epoch: 918 [188416/194182 (96%)]\tLoss: 0.343884\tGrad Norm: 1.903975\tLR: 0.030000\n",
      "Train set: Average loss: 0.3424\n",
      "Test set: Average loss: 0.2411, Average MAE: 0.3291\n",
      "Train Epoch: 919 [4096/194182 (2%)]\tLoss: 0.336054\tGrad Norm: 1.267903\tLR: 0.030000\n",
      "Train Epoch: 919 [24576/194182 (12%)]\tLoss: 0.333392\tGrad Norm: 0.857886\tLR: 0.030000\n",
      "Train Epoch: 919 [45056/194182 (23%)]\tLoss: 0.340146\tGrad Norm: 1.583271\tLR: 0.030000\n",
      "Train Epoch: 919 [65536/194182 (33%)]\tLoss: 0.343365\tGrad Norm: 1.452686\tLR: 0.030000\n",
      "Train Epoch: 919 [86016/194182 (44%)]\tLoss: 0.349445\tGrad Norm: 1.644438\tLR: 0.030000\n",
      "Train Epoch: 919 [106496/194182 (54%)]\tLoss: 0.337819\tGrad Norm: 1.312204\tLR: 0.030000\n",
      "Train Epoch: 919 [126976/194182 (65%)]\tLoss: 0.334901\tGrad Norm: 1.171685\tLR: 0.030000\n",
      "Train Epoch: 919 [147456/194182 (75%)]\tLoss: 0.336460\tGrad Norm: 1.332466\tLR: 0.030000\n",
      "Train Epoch: 919 [167936/194182 (85%)]\tLoss: 0.341907\tGrad Norm: 1.202496\tLR: 0.030000\n",
      "Train Epoch: 919 [188416/194182 (96%)]\tLoss: 0.339492\tGrad Norm: 1.344549\tLR: 0.030000\n",
      "Train set: Average loss: 0.3387\n",
      "Test set: Average loss: 0.2414, Average MAE: 0.3320\n",
      "Train Epoch: 920 [4096/194182 (2%)]\tLoss: 0.337976\tGrad Norm: 1.202914\tLR: 0.030000\n",
      "Train Epoch: 920 [24576/194182 (12%)]\tLoss: 0.332948\tGrad Norm: 1.014678\tLR: 0.030000\n",
      "Train Epoch: 920 [45056/194182 (23%)]\tLoss: 0.341587\tGrad Norm: 1.433556\tLR: 0.030000\n",
      "Train Epoch: 920 [65536/194182 (33%)]\tLoss: 0.338505\tGrad Norm: 1.458783\tLR: 0.030000\n",
      "Train Epoch: 920 [86016/194182 (44%)]\tLoss: 0.341870\tGrad Norm: 1.632281\tLR: 0.030000\n",
      "Train Epoch: 920 [106496/194182 (54%)]\tLoss: 0.348800\tGrad Norm: 1.675018\tLR: 0.030000\n",
      "Train Epoch: 920 [126976/194182 (65%)]\tLoss: 0.334124\tGrad Norm: 1.123403\tLR: 0.030000\n",
      "Train Epoch: 920 [147456/194182 (75%)]\tLoss: 0.340238\tGrad Norm: 1.346633\tLR: 0.030000\n",
      "Train Epoch: 920 [167936/194182 (85%)]\tLoss: 0.337378\tGrad Norm: 1.470211\tLR: 0.030000\n",
      "Train Epoch: 920 [188416/194182 (96%)]\tLoss: 0.343425\tGrad Norm: 1.221320\tLR: 0.030000\n",
      "Train set: Average loss: 0.3402\n",
      "Test set: Average loss: 0.2432, Average MAE: 0.3529\n",
      "Epoch 920: Mean reward = 0.044 +/- 0.031\n",
      "Train Epoch: 921 [4096/194182 (2%)]\tLoss: 0.332399\tGrad Norm: 1.444284\tLR: 0.030000\n",
      "Train Epoch: 921 [24576/194182 (12%)]\tLoss: 0.341857\tGrad Norm: 1.328583\tLR: 0.030000\n",
      "Train Epoch: 921 [45056/194182 (23%)]\tLoss: 0.334462\tGrad Norm: 1.106695\tLR: 0.030000\n",
      "Train Epoch: 921 [65536/194182 (33%)]\tLoss: 0.333851\tGrad Norm: 1.456077\tLR: 0.030000\n",
      "Train Epoch: 921 [86016/194182 (44%)]\tLoss: 0.339869\tGrad Norm: 1.627952\tLR: 0.030000\n",
      "Train Epoch: 921 [106496/194182 (54%)]\tLoss: 0.342166\tGrad Norm: 1.535847\tLR: 0.030000\n",
      "Train Epoch: 921 [126976/194182 (65%)]\tLoss: 0.341228\tGrad Norm: 1.198451\tLR: 0.030000\n",
      "Train Epoch: 921 [147456/194182 (75%)]\tLoss: 0.337962\tGrad Norm: 1.254775\tLR: 0.030000\n",
      "Train Epoch: 921 [167936/194182 (85%)]\tLoss: 0.342477\tGrad Norm: 1.312024\tLR: 0.030000\n",
      "Train Epoch: 921 [188416/194182 (96%)]\tLoss: 0.344312\tGrad Norm: 1.622327\tLR: 0.030000\n",
      "Train set: Average loss: 0.3400\n",
      "Test set: Average loss: 0.2505, Average MAE: 0.3415\n",
      "Train Epoch: 922 [4096/194182 (2%)]\tLoss: 0.346058\tGrad Norm: 1.759214\tLR: 0.030000\n",
      "Train Epoch: 922 [24576/194182 (12%)]\tLoss: 0.333489\tGrad Norm: 0.931895\tLR: 0.030000\n",
      "Train Epoch: 922 [45056/194182 (23%)]\tLoss: 0.340030\tGrad Norm: 1.310777\tLR: 0.030000\n",
      "Train Epoch: 922 [65536/194182 (33%)]\tLoss: 0.329525\tGrad Norm: 1.062233\tLR: 0.030000\n",
      "Train Epoch: 922 [86016/194182 (44%)]\tLoss: 0.340050\tGrad Norm: 1.216861\tLR: 0.030000\n",
      "Train Epoch: 922 [106496/194182 (54%)]\tLoss: 0.342252\tGrad Norm: 1.630050\tLR: 0.030000\n",
      "Train Epoch: 922 [126976/194182 (65%)]\tLoss: 0.332761\tGrad Norm: 1.396253\tLR: 0.030000\n",
      "Train Epoch: 922 [147456/194182 (75%)]\tLoss: 0.337797\tGrad Norm: 1.145853\tLR: 0.030000\n",
      "Train Epoch: 922 [167936/194182 (85%)]\tLoss: 0.339259\tGrad Norm: 1.504885\tLR: 0.030000\n",
      "Train Epoch: 922 [188416/194182 (96%)]\tLoss: 0.338965\tGrad Norm: 1.316670\tLR: 0.030000\n",
      "Train set: Average loss: 0.3380\n",
      "Test set: Average loss: 0.2444, Average MAE: 0.3530\n",
      "Train Epoch: 923 [4096/194182 (2%)]\tLoss: 0.339658\tGrad Norm: 1.358691\tLR: 0.030000\n",
      "Train Epoch: 923 [24576/194182 (12%)]\tLoss: 0.336691\tGrad Norm: 1.417621\tLR: 0.030000\n",
      "Train Epoch: 923 [45056/194182 (23%)]\tLoss: 0.347817\tGrad Norm: 1.341369\tLR: 0.030000\n",
      "Train Epoch: 923 [65536/194182 (33%)]\tLoss: 0.352958\tGrad Norm: 1.658215\tLR: 0.030000\n",
      "Train Epoch: 923 [86016/194182 (44%)]\tLoss: 0.329051\tGrad Norm: 1.044081\tLR: 0.030000\n",
      "Train Epoch: 923 [106496/194182 (54%)]\tLoss: 0.328443\tGrad Norm: 0.940664\tLR: 0.030000\n",
      "Train Epoch: 923 [126976/194182 (65%)]\tLoss: 0.335973\tGrad Norm: 1.257995\tLR: 0.030000\n",
      "Train Epoch: 923 [147456/194182 (75%)]\tLoss: 0.338277\tGrad Norm: 1.328079\tLR: 0.030000\n",
      "Train Epoch: 923 [167936/194182 (85%)]\tLoss: 0.342306\tGrad Norm: 1.347367\tLR: 0.030000\n",
      "Train Epoch: 923 [188416/194182 (96%)]\tLoss: 0.336375\tGrad Norm: 1.402782\tLR: 0.030000\n",
      "Train set: Average loss: 0.3377\n",
      "Test set: Average loss: 0.2492, Average MAE: 0.3380\n",
      "Train Epoch: 924 [4096/194182 (2%)]\tLoss: 0.354128\tGrad Norm: 1.783128\tLR: 0.030000\n",
      "Train Epoch: 924 [24576/194182 (12%)]\tLoss: 0.336782\tGrad Norm: 0.935069\tLR: 0.030000\n",
      "Train Epoch: 924 [45056/194182 (23%)]\tLoss: 0.333546\tGrad Norm: 1.270618\tLR: 0.030000\n",
      "Train Epoch: 924 [65536/194182 (33%)]\tLoss: 0.333756\tGrad Norm: 1.428252\tLR: 0.030000\n",
      "Train Epoch: 924 [86016/194182 (44%)]\tLoss: 0.337062\tGrad Norm: 1.466166\tLR: 0.030000\n",
      "Train Epoch: 924 [106496/194182 (54%)]\tLoss: 0.341421\tGrad Norm: 1.571552\tLR: 0.030000\n",
      "Train Epoch: 924 [126976/194182 (65%)]\tLoss: 0.345526\tGrad Norm: 1.378045\tLR: 0.030000\n",
      "Train Epoch: 924 [147456/194182 (75%)]\tLoss: 0.341338\tGrad Norm: 1.315578\tLR: 0.030000\n",
      "Train Epoch: 924 [167936/194182 (85%)]\tLoss: 0.335139\tGrad Norm: 1.335706\tLR: 0.030000\n",
      "Train Epoch: 924 [188416/194182 (96%)]\tLoss: 0.336323\tGrad Norm: 1.399616\tLR: 0.030000\n",
      "Train set: Average loss: 0.3388\n",
      "Test set: Average loss: 0.2409, Average MAE: 0.3297\n",
      "Train Epoch: 925 [4096/194182 (2%)]\tLoss: 0.327161\tGrad Norm: 1.238907\tLR: 0.030000\n",
      "Train Epoch: 925 [24576/194182 (12%)]\tLoss: 0.334070\tGrad Norm: 0.977326\tLR: 0.030000\n",
      "Train Epoch: 925 [45056/194182 (23%)]\tLoss: 0.350128\tGrad Norm: 1.767038\tLR: 0.030000\n",
      "Train Epoch: 925 [65536/194182 (33%)]\tLoss: 0.340097\tGrad Norm: 1.382996\tLR: 0.030000\n",
      "Train Epoch: 925 [86016/194182 (44%)]\tLoss: 0.338467\tGrad Norm: 1.265315\tLR: 0.030000\n",
      "Train Epoch: 925 [106496/194182 (54%)]\tLoss: 0.336988\tGrad Norm: 1.098488\tLR: 0.030000\n",
      "Train Epoch: 925 [126976/194182 (65%)]\tLoss: 0.330492\tGrad Norm: 1.074095\tLR: 0.030000\n",
      "Train Epoch: 925 [147456/194182 (75%)]\tLoss: 0.340349\tGrad Norm: 1.362413\tLR: 0.030000\n",
      "Train Epoch: 925 [167936/194182 (85%)]\tLoss: 0.339854\tGrad Norm: 1.253356\tLR: 0.030000\n",
      "Train Epoch: 925 [188416/194182 (96%)]\tLoss: 0.331151\tGrad Norm: 1.199197\tLR: 0.030000\n",
      "Train set: Average loss: 0.3364\n",
      "Test set: Average loss: 0.2428, Average MAE: 0.3469\n",
      "Epoch 925: Mean reward = 0.046 +/- 0.001\n",
      "Train Epoch: 926 [4096/194182 (2%)]\tLoss: 0.342010\tGrad Norm: 1.191694\tLR: 0.030000\n",
      "Train Epoch: 926 [24576/194182 (12%)]\tLoss: 0.339100\tGrad Norm: 1.453818\tLR: 0.030000\n",
      "Train Epoch: 926 [45056/194182 (23%)]\tLoss: 0.333695\tGrad Norm: 1.558728\tLR: 0.030000\n",
      "Train Epoch: 926 [65536/194182 (33%)]\tLoss: 0.335296\tGrad Norm: 1.439228\tLR: 0.030000\n",
      "Train Epoch: 926 [86016/194182 (44%)]\tLoss: 0.334419\tGrad Norm: 0.970300\tLR: 0.030000\n",
      "Train Epoch: 926 [106496/194182 (54%)]\tLoss: 0.336542\tGrad Norm: 1.089666\tLR: 0.030000\n",
      "Train Epoch: 926 [126976/194182 (65%)]\tLoss: 0.333303\tGrad Norm: 1.058962\tLR: 0.030000\n",
      "Train Epoch: 926 [147456/194182 (75%)]\tLoss: 0.328790\tGrad Norm: 0.942222\tLR: 0.030000\n",
      "Train Epoch: 926 [167936/194182 (85%)]\tLoss: 0.345853\tGrad Norm: 1.136039\tLR: 0.030000\n",
      "Train Epoch: 926 [188416/194182 (96%)]\tLoss: 0.340427\tGrad Norm: 1.524005\tLR: 0.030000\n",
      "Train set: Average loss: 0.3362\n",
      "Test set: Average loss: 0.2479, Average MAE: 0.3613\n",
      "Train Epoch: 927 [4096/194182 (2%)]\tLoss: 0.338830\tGrad Norm: 1.696246\tLR: 0.030000\n",
      "Train Epoch: 927 [24576/194182 (12%)]\tLoss: 0.331437\tGrad Norm: 1.295508\tLR: 0.030000\n",
      "Train Epoch: 927 [45056/194182 (23%)]\tLoss: 0.332252\tGrad Norm: 1.146961\tLR: 0.030000\n",
      "Train Epoch: 927 [65536/194182 (33%)]\tLoss: 0.332527\tGrad Norm: 1.164868\tLR: 0.030000\n",
      "Train Epoch: 927 [86016/194182 (44%)]\tLoss: 0.342333\tGrad Norm: 1.415280\tLR: 0.030000\n",
      "Train Epoch: 927 [106496/194182 (54%)]\tLoss: 0.334506\tGrad Norm: 1.418947\tLR: 0.030000\n",
      "Train Epoch: 927 [126976/194182 (65%)]\tLoss: 0.333862\tGrad Norm: 1.251023\tLR: 0.030000\n",
      "Train Epoch: 927 [147456/194182 (75%)]\tLoss: 0.341275\tGrad Norm: 1.446655\tLR: 0.030000\n",
      "Train Epoch: 927 [167936/194182 (85%)]\tLoss: 0.342801\tGrad Norm: 1.454182\tLR: 0.030000\n",
      "Train Epoch: 927 [188416/194182 (96%)]\tLoss: 0.333288\tGrad Norm: 1.408756\tLR: 0.030000\n",
      "Train set: Average loss: 0.3375\n",
      "Test set: Average loss: 0.2458, Average MAE: 0.3329\n",
      "Train Epoch: 928 [4096/194182 (2%)]\tLoss: 0.338016\tGrad Norm: 1.465778\tLR: 0.030000\n",
      "Train Epoch: 928 [24576/194182 (12%)]\tLoss: 0.341701\tGrad Norm: 1.624379\tLR: 0.030000\n",
      "Train Epoch: 928 [45056/194182 (23%)]\tLoss: 0.338112\tGrad Norm: 1.785453\tLR: 0.030000\n",
      "Train Epoch: 928 [65536/194182 (33%)]\tLoss: 0.335893\tGrad Norm: 1.560006\tLR: 0.030000\n",
      "Train Epoch: 928 [86016/194182 (44%)]\tLoss: 0.341740\tGrad Norm: 1.417084\tLR: 0.030000\n",
      "Train Epoch: 928 [106496/194182 (54%)]\tLoss: 0.345194\tGrad Norm: 1.613923\tLR: 0.030000\n",
      "Train Epoch: 928 [126976/194182 (65%)]\tLoss: 0.342685\tGrad Norm: 1.188639\tLR: 0.030000\n",
      "Train Epoch: 928 [147456/194182 (75%)]\tLoss: 0.332917\tGrad Norm: 0.966594\tLR: 0.030000\n",
      "Train Epoch: 928 [167936/194182 (85%)]\tLoss: 0.334674\tGrad Norm: 1.239285\tLR: 0.030000\n",
      "Train Epoch: 928 [188416/194182 (96%)]\tLoss: 0.343106\tGrad Norm: 1.463809\tLR: 0.030000\n",
      "Train set: Average loss: 0.3380\n",
      "Test set: Average loss: 0.2414, Average MAE: 0.3395\n",
      "Train Epoch: 929 [4096/194182 (2%)]\tLoss: 0.342378\tGrad Norm: 1.229052\tLR: 0.030000\n",
      "Train Epoch: 929 [24576/194182 (12%)]\tLoss: 0.341200\tGrad Norm: 1.406085\tLR: 0.030000\n",
      "Train Epoch: 929 [45056/194182 (23%)]\tLoss: 0.337080\tGrad Norm: 1.327846\tLR: 0.030000\n",
      "Train Epoch: 929 [65536/194182 (33%)]\tLoss: 0.338982\tGrad Norm: 1.341673\tLR: 0.030000\n",
      "Train Epoch: 929 [86016/194182 (44%)]\tLoss: 0.340633\tGrad Norm: 1.434019\tLR: 0.030000\n",
      "Train Epoch: 929 [106496/194182 (54%)]\tLoss: 0.333877\tGrad Norm: 1.229965\tLR: 0.030000\n",
      "Train Epoch: 929 [126976/194182 (65%)]\tLoss: 0.333460\tGrad Norm: 1.333318\tLR: 0.030000\n",
      "Train Epoch: 929 [147456/194182 (75%)]\tLoss: 0.342816\tGrad Norm: 1.477414\tLR: 0.030000\n",
      "Train Epoch: 929 [167936/194182 (85%)]\tLoss: 0.338101\tGrad Norm: 1.769990\tLR: 0.030000\n",
      "Train Epoch: 929 [188416/194182 (96%)]\tLoss: 0.350924\tGrad Norm: 1.723751\tLR: 0.030000\n",
      "Train set: Average loss: 0.3385\n",
      "Test set: Average loss: 0.2425, Average MAE: 0.3550\n",
      "Train Epoch: 930 [4096/194182 (2%)]\tLoss: 0.334540\tGrad Norm: 1.357019\tLR: 0.030000\n",
      "Train Epoch: 930 [24576/194182 (12%)]\tLoss: 0.341262\tGrad Norm: 1.434906\tLR: 0.030000\n",
      "Train Epoch: 930 [45056/194182 (23%)]\tLoss: 0.334554\tGrad Norm: 0.974438\tLR: 0.030000\n",
      "Train Epoch: 930 [65536/194182 (33%)]\tLoss: 0.345191\tGrad Norm: 1.301447\tLR: 0.030000\n",
      "Train Epoch: 930 [86016/194182 (44%)]\tLoss: 0.338228\tGrad Norm: 1.284624\tLR: 0.030000\n",
      "Train Epoch: 930 [106496/194182 (54%)]\tLoss: 0.340103\tGrad Norm: 1.184366\tLR: 0.030000\n",
      "Train Epoch: 930 [126976/194182 (65%)]\tLoss: 0.332937\tGrad Norm: 1.342232\tLR: 0.030000\n",
      "Train Epoch: 930 [147456/194182 (75%)]\tLoss: 0.338263\tGrad Norm: 1.354085\tLR: 0.030000\n",
      "Train Epoch: 930 [167936/194182 (85%)]\tLoss: 0.356143\tGrad Norm: 1.718733\tLR: 0.030000\n",
      "Train Epoch: 930 [188416/194182 (96%)]\tLoss: 0.344714\tGrad Norm: 1.358243\tLR: 0.030000\n",
      "Train set: Average loss: 0.3367\n",
      "Test set: Average loss: 0.2399, Average MAE: 0.3311\n",
      "Epoch 930: Mean reward = 0.059 +/- 0.044\n",
      "Train Epoch: 931 [4096/194182 (2%)]\tLoss: 0.338068\tGrad Norm: 1.252327\tLR: 0.030000\n",
      "Train Epoch: 931 [24576/194182 (12%)]\tLoss: 0.332967\tGrad Norm: 1.585207\tLR: 0.030000\n",
      "Train Epoch: 931 [45056/194182 (23%)]\tLoss: 0.344820\tGrad Norm: 1.452103\tLR: 0.030000\n",
      "Train Epoch: 931 [65536/194182 (33%)]\tLoss: 0.336227\tGrad Norm: 1.443108\tLR: 0.030000\n",
      "Train Epoch: 931 [86016/194182 (44%)]\tLoss: 0.339713\tGrad Norm: 1.552698\tLR: 0.030000\n",
      "Train Epoch: 931 [106496/194182 (54%)]\tLoss: 0.336884\tGrad Norm: 1.445872\tLR: 0.030000\n",
      "Train Epoch: 931 [126976/194182 (65%)]\tLoss: 0.333802\tGrad Norm: 0.922529\tLR: 0.030000\n",
      "Train Epoch: 931 [147456/194182 (75%)]\tLoss: 0.332757\tGrad Norm: 0.982266\tLR: 0.030000\n",
      "Train Epoch: 931 [167936/194182 (85%)]\tLoss: 0.338687\tGrad Norm: 1.196412\tLR: 0.030000\n",
      "Train Epoch: 931 [188416/194182 (96%)]\tLoss: 0.326724\tGrad Norm: 1.174111\tLR: 0.030000\n",
      "Train set: Average loss: 0.3356\n",
      "Test set: Average loss: 0.2408, Average MAE: 0.3402\n",
      "Train Epoch: 932 [4096/194182 (2%)]\tLoss: 0.329589\tGrad Norm: 1.195594\tLR: 0.030000\n",
      "Train Epoch: 932 [24576/194182 (12%)]\tLoss: 0.339504\tGrad Norm: 1.358161\tLR: 0.030000\n",
      "Train Epoch: 932 [45056/194182 (23%)]\tLoss: 0.342495\tGrad Norm: 1.499704\tLR: 0.030000\n",
      "Train Epoch: 932 [65536/194182 (33%)]\tLoss: 0.332586\tGrad Norm: 1.196007\tLR: 0.030000\n",
      "Train Epoch: 932 [86016/194182 (44%)]\tLoss: 0.332969\tGrad Norm: 1.098196\tLR: 0.030000\n",
      "Train Epoch: 932 [106496/194182 (54%)]\tLoss: 0.338704\tGrad Norm: 1.879720\tLR: 0.030000\n",
      "Train Epoch: 932 [126976/194182 (65%)]\tLoss: 0.340398\tGrad Norm: 1.609192\tLR: 0.030000\n",
      "Train Epoch: 932 [147456/194182 (75%)]\tLoss: 0.341677\tGrad Norm: 1.310702\tLR: 0.030000\n",
      "Train Epoch: 932 [167936/194182 (85%)]\tLoss: 0.335755\tGrad Norm: 1.066567\tLR: 0.030000\n",
      "Train Epoch: 932 [188416/194182 (96%)]\tLoss: 0.337081\tGrad Norm: 1.175233\tLR: 0.030000\n",
      "Train set: Average loss: 0.3361\n",
      "Test set: Average loss: 0.2445, Average MAE: 0.3348\n",
      "Train Epoch: 933 [4096/194182 (2%)]\tLoss: 0.345069\tGrad Norm: 1.618768\tLR: 0.030000\n",
      "Train Epoch: 933 [24576/194182 (12%)]\tLoss: 0.333202\tGrad Norm: 1.152294\tLR: 0.030000\n",
      "Train Epoch: 933 [45056/194182 (23%)]\tLoss: 0.329039\tGrad Norm: 1.085483\tLR: 0.030000\n",
      "Train Epoch: 933 [65536/194182 (33%)]\tLoss: 0.345313\tGrad Norm: 1.511197\tLR: 0.030000\n",
      "Train Epoch: 933 [86016/194182 (44%)]\tLoss: 0.335440\tGrad Norm: 1.232960\tLR: 0.030000\n",
      "Train Epoch: 933 [106496/194182 (54%)]\tLoss: 0.336613\tGrad Norm: 1.427691\tLR: 0.030000\n",
      "Train Epoch: 933 [126976/194182 (65%)]\tLoss: 0.332516\tGrad Norm: 1.343304\tLR: 0.030000\n",
      "Train Epoch: 933 [147456/194182 (75%)]\tLoss: 0.332812\tGrad Norm: 1.408057\tLR: 0.030000\n",
      "Train Epoch: 933 [167936/194182 (85%)]\tLoss: 0.335682\tGrad Norm: 1.125978\tLR: 0.030000\n",
      "Train Epoch: 933 [188416/194182 (96%)]\tLoss: 0.331126\tGrad Norm: 1.145559\tLR: 0.030000\n",
      "Train set: Average loss: 0.3354\n",
      "Test set: Average loss: 0.2410, Average MAE: 0.3497\n",
      "Train Epoch: 934 [4096/194182 (2%)]\tLoss: 0.326896\tGrad Norm: 1.154697\tLR: 0.030000\n",
      "Train Epoch: 934 [24576/194182 (12%)]\tLoss: 0.335482\tGrad Norm: 1.549786\tLR: 0.030000\n",
      "Train Epoch: 934 [45056/194182 (23%)]\tLoss: 0.330819\tGrad Norm: 1.481406\tLR: 0.030000\n",
      "Train Epoch: 934 [65536/194182 (33%)]\tLoss: 0.341227\tGrad Norm: 1.440359\tLR: 0.030000\n",
      "Train Epoch: 934 [86016/194182 (44%)]\tLoss: 0.343830\tGrad Norm: 1.787735\tLR: 0.030000\n",
      "Train Epoch: 934 [106496/194182 (54%)]\tLoss: 0.343659\tGrad Norm: 1.335062\tLR: 0.030000\n",
      "Train Epoch: 934 [126976/194182 (65%)]\tLoss: 0.332173\tGrad Norm: 0.972564\tLR: 0.030000\n",
      "Train Epoch: 934 [147456/194182 (75%)]\tLoss: 0.328195\tGrad Norm: 0.937593\tLR: 0.030000\n",
      "Train Epoch: 934 [167936/194182 (85%)]\tLoss: 0.342090\tGrad Norm: 1.403233\tLR: 0.030000\n",
      "Train Epoch: 934 [188416/194182 (96%)]\tLoss: 0.334250\tGrad Norm: 1.392750\tLR: 0.030000\n",
      "Train set: Average loss: 0.3371\n",
      "Test set: Average loss: 0.2455, Average MAE: 0.3500\n",
      "Train Epoch: 935 [4096/194182 (2%)]\tLoss: 0.334563\tGrad Norm: 1.364337\tLR: 0.030000\n",
      "Train Epoch: 935 [24576/194182 (12%)]\tLoss: 0.335318\tGrad Norm: 1.277233\tLR: 0.030000\n",
      "Train Epoch: 935 [45056/194182 (23%)]\tLoss: 0.343474\tGrad Norm: 1.326414\tLR: 0.030000\n",
      "Train Epoch: 935 [65536/194182 (33%)]\tLoss: 0.341081\tGrad Norm: 1.479112\tLR: 0.030000\n",
      "Train Epoch: 935 [86016/194182 (44%)]\tLoss: 0.333048\tGrad Norm: 1.387985\tLR: 0.030000\n",
      "Train Epoch: 935 [106496/194182 (54%)]\tLoss: 0.333203\tGrad Norm: 1.482488\tLR: 0.030000\n",
      "Train Epoch: 935 [126976/194182 (65%)]\tLoss: 0.327161\tGrad Norm: 1.152803\tLR: 0.030000\n",
      "Train Epoch: 935 [147456/194182 (75%)]\tLoss: 0.335340\tGrad Norm: 1.337696\tLR: 0.030000\n",
      "Train Epoch: 935 [167936/194182 (85%)]\tLoss: 0.333786\tGrad Norm: 1.263571\tLR: 0.030000\n",
      "Train Epoch: 935 [188416/194182 (96%)]\tLoss: 0.338679\tGrad Norm: 1.670750\tLR: 0.030000\n",
      "Train set: Average loss: 0.3365\n",
      "Test set: Average loss: 0.2472, Average MAE: 0.3602\n",
      "Epoch 935: Mean reward = 0.052 +/- 0.018\n",
      "Train Epoch: 936 [4096/194182 (2%)]\tLoss: 0.341707\tGrad Norm: 1.488392\tLR: 0.030000\n",
      "Train Epoch: 936 [24576/194182 (12%)]\tLoss: 0.336328\tGrad Norm: 1.414592\tLR: 0.030000\n",
      "Train Epoch: 936 [45056/194182 (23%)]\tLoss: 0.335642\tGrad Norm: 1.327057\tLR: 0.030000\n",
      "Train Epoch: 936 [65536/194182 (33%)]\tLoss: 0.338950\tGrad Norm: 1.325607\tLR: 0.030000\n",
      "Train Epoch: 936 [86016/194182 (44%)]\tLoss: 0.331870\tGrad Norm: 1.052164\tLR: 0.030000\n",
      "Train Epoch: 936 [106496/194182 (54%)]\tLoss: 0.334934\tGrad Norm: 1.006289\tLR: 0.030000\n",
      "Train Epoch: 936 [126976/194182 (65%)]\tLoss: 0.336443\tGrad Norm: 1.433306\tLR: 0.030000\n",
      "Train Epoch: 936 [147456/194182 (75%)]\tLoss: 0.340112\tGrad Norm: 1.503182\tLR: 0.030000\n",
      "Train Epoch: 936 [167936/194182 (85%)]\tLoss: 0.345158\tGrad Norm: 1.568298\tLR: 0.030000\n",
      "Train Epoch: 936 [188416/194182 (96%)]\tLoss: 0.334351\tGrad Norm: 1.401943\tLR: 0.030000\n",
      "Train set: Average loss: 0.3355\n",
      "Test set: Average loss: 0.2431, Average MAE: 0.3363\n",
      "Train Epoch: 937 [4096/194182 (2%)]\tLoss: 0.340350\tGrad Norm: 1.409755\tLR: 0.030000\n",
      "Train Epoch: 937 [24576/194182 (12%)]\tLoss: 0.335849\tGrad Norm: 0.896630\tLR: 0.030000\n",
      "Train Epoch: 937 [45056/194182 (23%)]\tLoss: 0.329346\tGrad Norm: 1.110938\tLR: 0.030000\n",
      "Train Epoch: 937 [65536/194182 (33%)]\tLoss: 0.335883\tGrad Norm: 1.194858\tLR: 0.030000\n",
      "Train Epoch: 937 [86016/194182 (44%)]\tLoss: 0.331434\tGrad Norm: 1.155814\tLR: 0.030000\n",
      "Train Epoch: 937 [106496/194182 (54%)]\tLoss: 0.338774\tGrad Norm: 1.317692\tLR: 0.030000\n",
      "Train Epoch: 937 [126976/194182 (65%)]\tLoss: 0.335523\tGrad Norm: 1.348166\tLR: 0.030000\n",
      "Train Epoch: 937 [147456/194182 (75%)]\tLoss: 0.347273\tGrad Norm: 1.702670\tLR: 0.030000\n",
      "Train Epoch: 937 [167936/194182 (85%)]\tLoss: 0.338429\tGrad Norm: 1.589290\tLR: 0.030000\n",
      "Train Epoch: 937 [188416/194182 (96%)]\tLoss: 0.342673\tGrad Norm: 1.463396\tLR: 0.030000\n",
      "Train set: Average loss: 0.3354\n",
      "Test set: Average loss: 0.2409, Average MAE: 0.3323\n",
      "Train Epoch: 938 [4096/194182 (2%)]\tLoss: 0.333925\tGrad Norm: 1.227194\tLR: 0.030000\n",
      "Train Epoch: 938 [24576/194182 (12%)]\tLoss: 0.335503\tGrad Norm: 1.236758\tLR: 0.030000\n",
      "Train Epoch: 938 [45056/194182 (23%)]\tLoss: 0.333725\tGrad Norm: 1.738527\tLR: 0.030000\n",
      "Train Epoch: 938 [65536/194182 (33%)]\tLoss: 0.341902\tGrad Norm: 1.839034\tLR: 0.030000\n",
      "Train Epoch: 938 [86016/194182 (44%)]\tLoss: 0.341346\tGrad Norm: 1.491544\tLR: 0.030000\n",
      "Train Epoch: 938 [106496/194182 (54%)]\tLoss: 0.339227\tGrad Norm: 1.817637\tLR: 0.030000\n",
      "Train Epoch: 938 [126976/194182 (65%)]\tLoss: 0.336302\tGrad Norm: 1.892659\tLR: 0.030000\n",
      "Train Epoch: 938 [147456/194182 (75%)]\tLoss: 0.329177\tGrad Norm: 1.146807\tLR: 0.030000\n",
      "Train Epoch: 938 [167936/194182 (85%)]\tLoss: 0.337879\tGrad Norm: 1.382600\tLR: 0.030000\n",
      "Train Epoch: 938 [188416/194182 (96%)]\tLoss: 0.335223\tGrad Norm: 1.397326\tLR: 0.030000\n",
      "Train set: Average loss: 0.3371\n",
      "Test set: Average loss: 0.2409, Average MAE: 0.3350\n",
      "Train Epoch: 939 [4096/194182 (2%)]\tLoss: 0.334955\tGrad Norm: 1.522223\tLR: 0.030000\n",
      "Train Epoch: 939 [24576/194182 (12%)]\tLoss: 0.339527\tGrad Norm: 1.144279\tLR: 0.030000\n",
      "Train Epoch: 939 [45056/194182 (23%)]\tLoss: 0.337618\tGrad Norm: 1.050259\tLR: 0.030000\n",
      "Train Epoch: 939 [65536/194182 (33%)]\tLoss: 0.328280\tGrad Norm: 1.065198\tLR: 0.030000\n",
      "Train Epoch: 939 [86016/194182 (44%)]\tLoss: 0.336261\tGrad Norm: 0.812460\tLR: 0.030000\n",
      "Train Epoch: 939 [106496/194182 (54%)]\tLoss: 0.329308\tGrad Norm: 0.960208\tLR: 0.030000\n",
      "Train Epoch: 939 [126976/194182 (65%)]\tLoss: 0.331153\tGrad Norm: 1.356764\tLR: 0.030000\n",
      "Train Epoch: 939 [147456/194182 (75%)]\tLoss: 0.333529\tGrad Norm: 1.131445\tLR: 0.030000\n",
      "Train Epoch: 939 [167936/194182 (85%)]\tLoss: 0.332464\tGrad Norm: 1.225150\tLR: 0.030000\n",
      "Train Epoch: 939 [188416/194182 (96%)]\tLoss: 0.340888\tGrad Norm: 1.429302\tLR: 0.030000\n",
      "Train set: Average loss: 0.3323\n",
      "Test set: Average loss: 0.2467, Average MAE: 0.3582\n",
      "Train Epoch: 940 [4096/194182 (2%)]\tLoss: 0.337810\tGrad Norm: 1.506381\tLR: 0.030000\n",
      "Train Epoch: 940 [24576/194182 (12%)]\tLoss: 0.339257\tGrad Norm: 1.348530\tLR: 0.030000\n",
      "Train Epoch: 940 [45056/194182 (23%)]\tLoss: 0.337984\tGrad Norm: 1.521015\tLR: 0.030000\n",
      "Train Epoch: 940 [65536/194182 (33%)]\tLoss: 0.341552\tGrad Norm: 1.471936\tLR: 0.030000\n",
      "Train Epoch: 940 [86016/194182 (44%)]\tLoss: 0.336736\tGrad Norm: 1.092221\tLR: 0.030000\n",
      "Train Epoch: 940 [106496/194182 (54%)]\tLoss: 0.328880\tGrad Norm: 1.197895\tLR: 0.030000\n",
      "Train Epoch: 940 [126976/194182 (65%)]\tLoss: 0.335006\tGrad Norm: 1.508065\tLR: 0.030000\n",
      "Train Epoch: 940 [147456/194182 (75%)]\tLoss: 0.333835\tGrad Norm: 1.434351\tLR: 0.030000\n",
      "Train Epoch: 940 [167936/194182 (85%)]\tLoss: 0.335245\tGrad Norm: 1.288682\tLR: 0.030000\n",
      "Train Epoch: 940 [188416/194182 (96%)]\tLoss: 0.340413\tGrad Norm: 1.594415\tLR: 0.030000\n",
      "Train set: Average loss: 0.3353\n",
      "Test set: Average loss: 0.2469, Average MAE: 0.3335\n",
      "Epoch 940: Mean reward = 0.037 +/- 0.019\n",
      "Train Epoch: 941 [4096/194182 (2%)]\tLoss: 0.337815\tGrad Norm: 1.587712\tLR: 0.030000\n",
      "Train Epoch: 941 [24576/194182 (12%)]\tLoss: 0.331515\tGrad Norm: 1.597675\tLR: 0.030000\n",
      "Train Epoch: 941 [45056/194182 (23%)]\tLoss: 0.334186\tGrad Norm: 1.185766\tLR: 0.030000\n",
      "Train Epoch: 941 [65536/194182 (33%)]\tLoss: 0.325897\tGrad Norm: 1.304091\tLR: 0.030000\n",
      "Train Epoch: 941 [86016/194182 (44%)]\tLoss: 0.333078\tGrad Norm: 1.217982\tLR: 0.030000\n",
      "Train Epoch: 941 [106496/194182 (54%)]\tLoss: 0.338545\tGrad Norm: 1.691254\tLR: 0.030000\n",
      "Train Epoch: 941 [126976/194182 (65%)]\tLoss: 0.334758\tGrad Norm: 1.150544\tLR: 0.030000\n",
      "Train Epoch: 941 [147456/194182 (75%)]\tLoss: 0.328673\tGrad Norm: 1.050213\tLR: 0.030000\n",
      "Train Epoch: 941 [167936/194182 (85%)]\tLoss: 0.338920\tGrad Norm: 1.566081\tLR: 0.030000\n",
      "Train Epoch: 941 [188416/194182 (96%)]\tLoss: 0.335047\tGrad Norm: 1.308632\tLR: 0.030000\n",
      "Train set: Average loss: 0.3351\n",
      "Test set: Average loss: 0.2432, Average MAE: 0.3423\n",
      "Train Epoch: 942 [4096/194182 (2%)]\tLoss: 0.342160\tGrad Norm: 1.255424\tLR: 0.030000\n",
      "Train Epoch: 942 [24576/194182 (12%)]\tLoss: 0.337421\tGrad Norm: 1.449132\tLR: 0.030000\n",
      "Train Epoch: 942 [45056/194182 (23%)]\tLoss: 0.345036\tGrad Norm: 1.321993\tLR: 0.030000\n",
      "Train Epoch: 942 [65536/194182 (33%)]\tLoss: 0.329139\tGrad Norm: 1.321868\tLR: 0.030000\n",
      "Train Epoch: 942 [86016/194182 (44%)]\tLoss: 0.331694\tGrad Norm: 1.200611\tLR: 0.030000\n",
      "Train Epoch: 942 [106496/194182 (54%)]\tLoss: 0.331936\tGrad Norm: 1.036785\tLR: 0.030000\n",
      "Train Epoch: 942 [126976/194182 (65%)]\tLoss: 0.333992\tGrad Norm: 0.943429\tLR: 0.030000\n",
      "Train Epoch: 942 [147456/194182 (75%)]\tLoss: 0.328431\tGrad Norm: 1.114975\tLR: 0.030000\n",
      "Train Epoch: 942 [167936/194182 (85%)]\tLoss: 0.325579\tGrad Norm: 1.659380\tLR: 0.030000\n",
      "Train Epoch: 942 [188416/194182 (96%)]\tLoss: 0.347100\tGrad Norm: 1.679718\tLR: 0.030000\n",
      "Train set: Average loss: 0.3336\n",
      "Test set: Average loss: 0.2453, Average MAE: 0.3331\n",
      "Train Epoch: 943 [4096/194182 (2%)]\tLoss: 0.334011\tGrad Norm: 1.544340\tLR: 0.030000\n",
      "Train Epoch: 943 [24576/194182 (12%)]\tLoss: 0.335266\tGrad Norm: 1.557366\tLR: 0.030000\n",
      "Train Epoch: 943 [45056/194182 (23%)]\tLoss: 0.340210\tGrad Norm: 1.486815\tLR: 0.030000\n",
      "Train Epoch: 943 [65536/194182 (33%)]\tLoss: 0.332363\tGrad Norm: 1.139071\tLR: 0.030000\n",
      "Train Epoch: 943 [86016/194182 (44%)]\tLoss: 0.336998\tGrad Norm: 1.420111\tLR: 0.030000\n",
      "Train Epoch: 943 [106496/194182 (54%)]\tLoss: 0.338233\tGrad Norm: 1.334425\tLR: 0.030000\n",
      "Train Epoch: 943 [126976/194182 (65%)]\tLoss: 0.330460\tGrad Norm: 1.048996\tLR: 0.030000\n",
      "Train Epoch: 943 [147456/194182 (75%)]\tLoss: 0.326064\tGrad Norm: 1.224117\tLR: 0.030000\n",
      "Train Epoch: 943 [167936/194182 (85%)]\tLoss: 0.330163\tGrad Norm: 1.046001\tLR: 0.030000\n",
      "Train Epoch: 943 [188416/194182 (96%)]\tLoss: 0.334582\tGrad Norm: 1.446061\tLR: 0.030000\n",
      "Train set: Average loss: 0.3335\n",
      "Test set: Average loss: 0.2444, Average MAE: 0.3450\n",
      "Train Epoch: 944 [4096/194182 (2%)]\tLoss: 0.331406\tGrad Norm: 1.352011\tLR: 0.030000\n",
      "Train Epoch: 944 [24576/194182 (12%)]\tLoss: 0.336207\tGrad Norm: 1.493395\tLR: 0.030000\n",
      "Train Epoch: 944 [45056/194182 (23%)]\tLoss: 0.337709\tGrad Norm: 1.475623\tLR: 0.030000\n",
      "Train Epoch: 944 [65536/194182 (33%)]\tLoss: 0.331740\tGrad Norm: 1.093698\tLR: 0.030000\n",
      "Train Epoch: 944 [86016/194182 (44%)]\tLoss: 0.334339\tGrad Norm: 1.234879\tLR: 0.030000\n",
      "Train Epoch: 944 [106496/194182 (54%)]\tLoss: 0.330053\tGrad Norm: 1.406089\tLR: 0.030000\n",
      "Train Epoch: 944 [126976/194182 (65%)]\tLoss: 0.337255\tGrad Norm: 1.168437\tLR: 0.030000\n",
      "Train Epoch: 944 [147456/194182 (75%)]\tLoss: 0.333106\tGrad Norm: 0.972026\tLR: 0.030000\n",
      "Train Epoch: 944 [167936/194182 (85%)]\tLoss: 0.329921\tGrad Norm: 1.333564\tLR: 0.030000\n",
      "Train Epoch: 944 [188416/194182 (96%)]\tLoss: 0.338438\tGrad Norm: 1.641738\tLR: 0.030000\n",
      "Train set: Average loss: 0.3341\n",
      "Test set: Average loss: 0.2527, Average MAE: 0.3667\n",
      "Train Epoch: 945 [4096/194182 (2%)]\tLoss: 0.346654\tGrad Norm: 1.869358\tLR: 0.030000\n",
      "Train Epoch: 945 [24576/194182 (12%)]\tLoss: 0.332610\tGrad Norm: 1.407199\tLR: 0.030000\n",
      "Train Epoch: 945 [45056/194182 (23%)]\tLoss: 0.339214\tGrad Norm: 1.315174\tLR: 0.030000\n",
      "Train Epoch: 945 [65536/194182 (33%)]\tLoss: 0.342520\tGrad Norm: 1.952956\tLR: 0.030000\n",
      "Train Epoch: 945 [86016/194182 (44%)]\tLoss: 0.336653\tGrad Norm: 1.508655\tLR: 0.030000\n",
      "Train Epoch: 945 [106496/194182 (54%)]\tLoss: 0.323048\tGrad Norm: 1.289276\tLR: 0.030000\n",
      "Train Epoch: 945 [126976/194182 (65%)]\tLoss: 0.329354\tGrad Norm: 1.240302\tLR: 0.030000\n",
      "Train Epoch: 945 [147456/194182 (75%)]\tLoss: 0.337274\tGrad Norm: 1.315502\tLR: 0.030000\n",
      "Train Epoch: 945 [167936/194182 (85%)]\tLoss: 0.338779\tGrad Norm: 1.611907\tLR: 0.030000\n",
      "Train Epoch: 945 [188416/194182 (96%)]\tLoss: 0.336536\tGrad Norm: 1.689693\tLR: 0.030000\n",
      "Train set: Average loss: 0.3356\n",
      "Test set: Average loss: 0.2464, Average MAE: 0.3568\n",
      "Epoch 945: Mean reward = 0.066 +/- 0.056\n",
      "Train Epoch: 946 [4096/194182 (2%)]\tLoss: 0.341752\tGrad Norm: 1.464192\tLR: 0.030000\n",
      "Train Epoch: 946 [24576/194182 (12%)]\tLoss: 0.334891\tGrad Norm: 1.465601\tLR: 0.030000\n",
      "Train Epoch: 946 [45056/194182 (23%)]\tLoss: 0.335932\tGrad Norm: 1.446086\tLR: 0.030000\n",
      "Train Epoch: 946 [65536/194182 (33%)]\tLoss: 0.328943\tGrad Norm: 1.306184\tLR: 0.030000\n",
      "Train Epoch: 946 [86016/194182 (44%)]\tLoss: 0.332462\tGrad Norm: 1.200886\tLR: 0.030000\n",
      "Train Epoch: 946 [106496/194182 (54%)]\tLoss: 0.333217\tGrad Norm: 1.245027\tLR: 0.030000\n",
      "Train Epoch: 946 [126976/194182 (65%)]\tLoss: 0.336141\tGrad Norm: 1.305197\tLR: 0.030000\n",
      "Train Epoch: 946 [147456/194182 (75%)]\tLoss: 0.328682\tGrad Norm: 1.392133\tLR: 0.030000\n",
      "Train Epoch: 946 [167936/194182 (85%)]\tLoss: 0.336687\tGrad Norm: 1.406011\tLR: 0.030000\n",
      "Train Epoch: 946 [188416/194182 (96%)]\tLoss: 0.335342\tGrad Norm: 1.207179\tLR: 0.030000\n",
      "Train set: Average loss: 0.3337\n",
      "Test set: Average loss: 0.2403, Average MAE: 0.3337\n",
      "Train Epoch: 947 [4096/194182 (2%)]\tLoss: 0.331909\tGrad Norm: 1.145785\tLR: 0.030000\n",
      "Train Epoch: 947 [24576/194182 (12%)]\tLoss: 0.330433\tGrad Norm: 1.357404\tLR: 0.030000\n",
      "Train Epoch: 947 [45056/194182 (23%)]\tLoss: 0.333592\tGrad Norm: 1.305967\tLR: 0.030000\n",
      "Train Epoch: 947 [65536/194182 (33%)]\tLoss: 0.333393\tGrad Norm: 1.512865\tLR: 0.030000\n",
      "Train Epoch: 947 [86016/194182 (44%)]\tLoss: 0.330554\tGrad Norm: 1.187865\tLR: 0.030000\n",
      "Train Epoch: 947 [106496/194182 (54%)]\tLoss: 0.340373\tGrad Norm: 1.634433\tLR: 0.030000\n",
      "Train Epoch: 947 [126976/194182 (65%)]\tLoss: 0.336722\tGrad Norm: 1.368086\tLR: 0.030000\n",
      "Train Epoch: 947 [147456/194182 (75%)]\tLoss: 0.325606\tGrad Norm: 1.315029\tLR: 0.030000\n",
      "Train Epoch: 947 [167936/194182 (85%)]\tLoss: 0.338884\tGrad Norm: 1.664507\tLR: 0.030000\n",
      "Train Epoch: 947 [188416/194182 (96%)]\tLoss: 0.342921\tGrad Norm: 1.845595\tLR: 0.030000\n",
      "Train set: Average loss: 0.3345\n",
      "Test set: Average loss: 0.2417, Average MAE: 0.3334\n",
      "Train Epoch: 948 [4096/194182 (2%)]\tLoss: 0.327010\tGrad Norm: 1.382608\tLR: 0.030000\n",
      "Train Epoch: 948 [24576/194182 (12%)]\tLoss: 0.332179\tGrad Norm: 1.139997\tLR: 0.030000\n",
      "Train Epoch: 948 [45056/194182 (23%)]\tLoss: 0.333697\tGrad Norm: 1.392903\tLR: 0.030000\n",
      "Train Epoch: 948 [65536/194182 (33%)]\tLoss: 0.330436\tGrad Norm: 1.359829\tLR: 0.030000\n",
      "Train Epoch: 948 [86016/194182 (44%)]\tLoss: 0.324326\tGrad Norm: 1.175528\tLR: 0.030000\n",
      "Train Epoch: 948 [106496/194182 (54%)]\tLoss: 0.326816\tGrad Norm: 1.227676\tLR: 0.030000\n",
      "Train Epoch: 948 [126976/194182 (65%)]\tLoss: 0.337285\tGrad Norm: 1.275817\tLR: 0.030000\n",
      "Train Epoch: 948 [147456/194182 (75%)]\tLoss: 0.334815\tGrad Norm: 1.474090\tLR: 0.030000\n",
      "Train Epoch: 948 [167936/194182 (85%)]\tLoss: 0.335932\tGrad Norm: 1.372374\tLR: 0.030000\n",
      "Train Epoch: 948 [188416/194182 (96%)]\tLoss: 0.331147\tGrad Norm: 1.179430\tLR: 0.030000\n",
      "Train set: Average loss: 0.3328\n",
      "Test set: Average loss: 0.2382, Average MAE: 0.3362\n",
      "Train Epoch: 949 [4096/194182 (2%)]\tLoss: 0.334610\tGrad Norm: 0.980785\tLR: 0.030000\n",
      "Train Epoch: 949 [24576/194182 (12%)]\tLoss: 0.332288\tGrad Norm: 1.367530\tLR: 0.030000\n",
      "Train Epoch: 949 [45056/194182 (23%)]\tLoss: 0.334448\tGrad Norm: 1.389356\tLR: 0.030000\n",
      "Train Epoch: 949 [65536/194182 (33%)]\tLoss: 0.336922\tGrad Norm: 1.583146\tLR: 0.030000\n",
      "Train Epoch: 949 [86016/194182 (44%)]\tLoss: 0.323651\tGrad Norm: 0.993537\tLR: 0.030000\n",
      "Train Epoch: 949 [106496/194182 (54%)]\tLoss: 0.332150\tGrad Norm: 1.049120\tLR: 0.030000\n",
      "Train Epoch: 949 [126976/194182 (65%)]\tLoss: 0.337037\tGrad Norm: 1.326833\tLR: 0.030000\n",
      "Train Epoch: 949 [147456/194182 (75%)]\tLoss: 0.339262\tGrad Norm: 1.671319\tLR: 0.030000\n",
      "Train Epoch: 949 [167936/194182 (85%)]\tLoss: 0.329626\tGrad Norm: 1.187414\tLR: 0.030000\n",
      "Train Epoch: 949 [188416/194182 (96%)]\tLoss: 0.334283\tGrad Norm: 1.245370\tLR: 0.030000\n",
      "Train set: Average loss: 0.3323\n",
      "Test set: Average loss: 0.2457, Average MAE: 0.3302\n",
      "Train Epoch: 950 [4096/194182 (2%)]\tLoss: 0.332912\tGrad Norm: 1.618256\tLR: 0.030000\n",
      "Train Epoch: 950 [24576/194182 (12%)]\tLoss: 0.329201\tGrad Norm: 1.302805\tLR: 0.030000\n",
      "Train Epoch: 950 [45056/194182 (23%)]\tLoss: 0.331777\tGrad Norm: 1.538011\tLR: 0.030000\n",
      "Train Epoch: 950 [65536/194182 (33%)]\tLoss: 0.327624\tGrad Norm: 1.171076\tLR: 0.030000\n",
      "Train Epoch: 950 [86016/194182 (44%)]\tLoss: 0.331704\tGrad Norm: 1.377873\tLR: 0.030000\n",
      "Train Epoch: 950 [106496/194182 (54%)]\tLoss: 0.324259\tGrad Norm: 1.241447\tLR: 0.030000\n",
      "Train Epoch: 950 [126976/194182 (65%)]\tLoss: 0.333107\tGrad Norm: 1.187631\tLR: 0.030000\n",
      "Train Epoch: 950 [147456/194182 (75%)]\tLoss: 0.326522\tGrad Norm: 1.062237\tLR: 0.030000\n",
      "Train Epoch: 950 [167936/194182 (85%)]\tLoss: 0.328949\tGrad Norm: 1.162012\tLR: 0.030000\n",
      "Train Epoch: 950 [188416/194182 (96%)]\tLoss: 0.333891\tGrad Norm: 1.472150\tLR: 0.030000\n",
      "Train set: Average loss: 0.3319\n",
      "Test set: Average loss: 0.2405, Average MAE: 0.3474\n",
      "Epoch 950: Mean reward = 0.066 +/- 0.043\n",
      "Train Epoch: 951 [4096/194182 (2%)]\tLoss: 0.332487\tGrad Norm: 1.061182\tLR: 0.030000\n",
      "Train Epoch: 951 [24576/194182 (12%)]\tLoss: 0.324318\tGrad Norm: 1.280455\tLR: 0.030000\n",
      "Train Epoch: 951 [45056/194182 (23%)]\tLoss: 0.336695\tGrad Norm: 1.773478\tLR: 0.030000\n",
      "Train Epoch: 951 [65536/194182 (33%)]\tLoss: 0.338628\tGrad Norm: 1.665092\tLR: 0.030000\n",
      "Train Epoch: 951 [86016/194182 (44%)]\tLoss: 0.332229\tGrad Norm: 1.186706\tLR: 0.030000\n",
      "Train Epoch: 951 [106496/194182 (54%)]\tLoss: 0.324647\tGrad Norm: 1.071905\tLR: 0.030000\n",
      "Train Epoch: 951 [126976/194182 (65%)]\tLoss: 0.331866\tGrad Norm: 1.246029\tLR: 0.030000\n",
      "Train Epoch: 951 [147456/194182 (75%)]\tLoss: 0.336247\tGrad Norm: 1.316843\tLR: 0.030000\n",
      "Train Epoch: 951 [167936/194182 (85%)]\tLoss: 0.322870\tGrad Norm: 1.087475\tLR: 0.030000\n",
      "Train Epoch: 951 [188416/194182 (96%)]\tLoss: 0.324444\tGrad Norm: 1.423759\tLR: 0.030000\n",
      "Train set: Average loss: 0.3320\n",
      "Test set: Average loss: 0.2470, Average MAE: 0.3482\n",
      "Train Epoch: 952 [4096/194182 (2%)]\tLoss: 0.335101\tGrad Norm: 1.503557\tLR: 0.030000\n",
      "Train Epoch: 952 [24576/194182 (12%)]\tLoss: 0.329189\tGrad Norm: 1.291139\tLR: 0.030000\n",
      "Train Epoch: 952 [45056/194182 (23%)]\tLoss: 0.329007\tGrad Norm: 1.321722\tLR: 0.030000\n",
      "Train Epoch: 952 [65536/194182 (33%)]\tLoss: 0.331695\tGrad Norm: 1.231696\tLR: 0.030000\n",
      "Train Epoch: 952 [86016/194182 (44%)]\tLoss: 0.326380\tGrad Norm: 1.223094\tLR: 0.030000\n",
      "Train Epoch: 952 [106496/194182 (54%)]\tLoss: 0.334532\tGrad Norm: 1.129843\tLR: 0.030000\n",
      "Train Epoch: 952 [126976/194182 (65%)]\tLoss: 0.331436\tGrad Norm: 1.173321\tLR: 0.030000\n",
      "Train Epoch: 952 [147456/194182 (75%)]\tLoss: 0.340190\tGrad Norm: 1.223728\tLR: 0.030000\n",
      "Train Epoch: 952 [167936/194182 (85%)]\tLoss: 0.332367\tGrad Norm: 1.433165\tLR: 0.030000\n",
      "Train Epoch: 952 [188416/194182 (96%)]\tLoss: 0.328956\tGrad Norm: 1.524855\tLR: 0.030000\n",
      "Train set: Average loss: 0.3317\n",
      "Test set: Average loss: 0.2463, Average MAE: 0.3314\n",
      "Train Epoch: 953 [4096/194182 (2%)]\tLoss: 0.333783\tGrad Norm: 1.624507\tLR: 0.030000\n",
      "Train Epoch: 953 [24576/194182 (12%)]\tLoss: 0.332689\tGrad Norm: 1.289623\tLR: 0.030000\n",
      "Train Epoch: 953 [45056/194182 (23%)]\tLoss: 0.337174\tGrad Norm: 1.533217\tLR: 0.030000\n",
      "Train Epoch: 953 [65536/194182 (33%)]\tLoss: 0.331924\tGrad Norm: 1.722547\tLR: 0.030000\n",
      "Train Epoch: 953 [86016/194182 (44%)]\tLoss: 0.338724\tGrad Norm: 1.603436\tLR: 0.030000\n",
      "Train Epoch: 953 [106496/194182 (54%)]\tLoss: 0.336521\tGrad Norm: 1.416893\tLR: 0.030000\n",
      "Train Epoch: 953 [126976/194182 (65%)]\tLoss: 0.333337\tGrad Norm: 1.592276\tLR: 0.030000\n",
      "Train Epoch: 953 [147456/194182 (75%)]\tLoss: 0.333876\tGrad Norm: 1.338126\tLR: 0.030000\n",
      "Train Epoch: 953 [167936/194182 (85%)]\tLoss: 0.324597\tGrad Norm: 1.264252\tLR: 0.030000\n",
      "Train Epoch: 953 [188416/194182 (96%)]\tLoss: 0.333496\tGrad Norm: 1.310756\tLR: 0.030000\n",
      "Train set: Average loss: 0.3332\n",
      "Test set: Average loss: 0.2408, Average MAE: 0.3298\n",
      "Train Epoch: 954 [4096/194182 (2%)]\tLoss: 0.330514\tGrad Norm: 1.417331\tLR: 0.030000\n",
      "Train Epoch: 954 [24576/194182 (12%)]\tLoss: 0.332966\tGrad Norm: 1.298129\tLR: 0.030000\n",
      "Train Epoch: 954 [45056/194182 (23%)]\tLoss: 0.328311\tGrad Norm: 1.328046\tLR: 0.030000\n",
      "Train Epoch: 954 [65536/194182 (33%)]\tLoss: 0.329803\tGrad Norm: 1.551010\tLR: 0.030000\n",
      "Train Epoch: 954 [86016/194182 (44%)]\tLoss: 0.336971\tGrad Norm: 1.524622\tLR: 0.030000\n",
      "Train Epoch: 954 [106496/194182 (54%)]\tLoss: 0.328565\tGrad Norm: 1.238531\tLR: 0.030000\n",
      "Train Epoch: 954 [126976/194182 (65%)]\tLoss: 0.330165\tGrad Norm: 1.445444\tLR: 0.030000\n",
      "Train Epoch: 954 [147456/194182 (75%)]\tLoss: 0.331343\tGrad Norm: 1.054417\tLR: 0.030000\n",
      "Train Epoch: 954 [167936/194182 (85%)]\tLoss: 0.328459\tGrad Norm: 1.367757\tLR: 0.030000\n",
      "Train Epoch: 954 [188416/194182 (96%)]\tLoss: 0.326071\tGrad Norm: 1.106066\tLR: 0.030000\n",
      "Train set: Average loss: 0.3317\n",
      "Test set: Average loss: 0.2409, Average MAE: 0.3487\n",
      "Train Epoch: 955 [4096/194182 (2%)]\tLoss: 0.329776\tGrad Norm: 1.229516\tLR: 0.030000\n",
      "Train Epoch: 955 [24576/194182 (12%)]\tLoss: 0.321297\tGrad Norm: 1.359093\tLR: 0.030000\n",
      "Train Epoch: 955 [45056/194182 (23%)]\tLoss: 0.337160\tGrad Norm: 1.607831\tLR: 0.030000\n",
      "Train Epoch: 955 [65536/194182 (33%)]\tLoss: 0.334122\tGrad Norm: 1.522421\tLR: 0.030000\n",
      "Train Epoch: 955 [86016/194182 (44%)]\tLoss: 0.330891\tGrad Norm: 1.053541\tLR: 0.030000\n",
      "Train Epoch: 955 [106496/194182 (54%)]\tLoss: 0.329772\tGrad Norm: 1.032423\tLR: 0.030000\n",
      "Train Epoch: 955 [126976/194182 (65%)]\tLoss: 0.327161\tGrad Norm: 1.449426\tLR: 0.030000\n",
      "Train Epoch: 955 [147456/194182 (75%)]\tLoss: 0.324512\tGrad Norm: 1.791070\tLR: 0.030000\n",
      "Train Epoch: 955 [167936/194182 (85%)]\tLoss: 0.338235\tGrad Norm: 1.595761\tLR: 0.030000\n",
      "Train Epoch: 955 [188416/194182 (96%)]\tLoss: 0.324917\tGrad Norm: 1.461584\tLR: 0.030000\n",
      "Train set: Average loss: 0.3325\n",
      "Test set: Average loss: 0.2470, Average MAE: 0.3566\n",
      "Epoch 955: Mean reward = 0.059 +/- 0.038\n",
      "Train Epoch: 956 [4096/194182 (2%)]\tLoss: 0.330856\tGrad Norm: 1.540779\tLR: 0.030000\n",
      "Train Epoch: 956 [24576/194182 (12%)]\tLoss: 0.333577\tGrad Norm: 1.370179\tLR: 0.030000\n",
      "Train Epoch: 956 [45056/194182 (23%)]\tLoss: 0.328664\tGrad Norm: 1.314652\tLR: 0.030000\n",
      "Train Epoch: 956 [65536/194182 (33%)]\tLoss: 0.338277\tGrad Norm: 1.450783\tLR: 0.030000\n",
      "Train Epoch: 956 [86016/194182 (44%)]\tLoss: 0.326168\tGrad Norm: 1.377847\tLR: 0.030000\n",
      "Train Epoch: 956 [106496/194182 (54%)]\tLoss: 0.329166\tGrad Norm: 0.953154\tLR: 0.030000\n",
      "Train Epoch: 956 [126976/194182 (65%)]\tLoss: 0.330343\tGrad Norm: 1.383645\tLR: 0.030000\n",
      "Train Epoch: 956 [147456/194182 (75%)]\tLoss: 0.328598\tGrad Norm: 1.118837\tLR: 0.030000\n",
      "Train Epoch: 956 [167936/194182 (85%)]\tLoss: 0.327950\tGrad Norm: 1.198698\tLR: 0.030000\n",
      "Train Epoch: 956 [188416/194182 (96%)]\tLoss: 0.329474\tGrad Norm: 1.115717\tLR: 0.030000\n",
      "Train set: Average loss: 0.3305\n",
      "Test set: Average loss: 0.2424, Average MAE: 0.3349\n",
      "Train Epoch: 957 [4096/194182 (2%)]\tLoss: 0.330549\tGrad Norm: 1.263272\tLR: 0.030000\n",
      "Train Epoch: 957 [24576/194182 (12%)]\tLoss: 0.325237\tGrad Norm: 1.204832\tLR: 0.030000\n",
      "Train Epoch: 957 [45056/194182 (23%)]\tLoss: 0.331610\tGrad Norm: 1.459972\tLR: 0.030000\n",
      "Train Epoch: 957 [65536/194182 (33%)]\tLoss: 0.336288\tGrad Norm: 1.451328\tLR: 0.030000\n",
      "Train Epoch: 957 [86016/194182 (44%)]\tLoss: 0.327993\tGrad Norm: 1.353670\tLR: 0.030000\n",
      "Train Epoch: 957 [106496/194182 (54%)]\tLoss: 0.333455\tGrad Norm: 1.470022\tLR: 0.030000\n",
      "Train Epoch: 957 [126976/194182 (65%)]\tLoss: 0.339858\tGrad Norm: 1.570852\tLR: 0.030000\n",
      "Train Epoch: 957 [147456/194182 (75%)]\tLoss: 0.340714\tGrad Norm: 1.609030\tLR: 0.030000\n",
      "Train Epoch: 957 [167936/194182 (85%)]\tLoss: 0.325191\tGrad Norm: 1.200073\tLR: 0.030000\n",
      "Train Epoch: 957 [188416/194182 (96%)]\tLoss: 0.325976\tGrad Norm: 1.018041\tLR: 0.030000\n",
      "Train set: Average loss: 0.3309\n",
      "Test set: Average loss: 0.2407, Average MAE: 0.3413\n",
      "Train Epoch: 958 [4096/194182 (2%)]\tLoss: 0.332352\tGrad Norm: 1.276983\tLR: 0.030000\n",
      "Train Epoch: 958 [24576/194182 (12%)]\tLoss: 0.335898\tGrad Norm: 1.317455\tLR: 0.030000\n",
      "Train Epoch: 958 [45056/194182 (23%)]\tLoss: 0.343547\tGrad Norm: 1.679516\tLR: 0.030000\n",
      "Train Epoch: 958 [65536/194182 (33%)]\tLoss: 0.327347\tGrad Norm: 1.269629\tLR: 0.030000\n",
      "Train Epoch: 958 [86016/194182 (44%)]\tLoss: 0.329492\tGrad Norm: 1.053614\tLR: 0.030000\n",
      "Train Epoch: 958 [106496/194182 (54%)]\tLoss: 0.320426\tGrad Norm: 1.015400\tLR: 0.030000\n",
      "Train Epoch: 958 [126976/194182 (65%)]\tLoss: 0.330895\tGrad Norm: 1.215787\tLR: 0.030000\n",
      "Train Epoch: 958 [147456/194182 (75%)]\tLoss: 0.329659\tGrad Norm: 1.422092\tLR: 0.030000\n",
      "Train Epoch: 958 [167936/194182 (85%)]\tLoss: 0.327733\tGrad Norm: 1.166743\tLR: 0.030000\n",
      "Train Epoch: 958 [188416/194182 (96%)]\tLoss: 0.323518\tGrad Norm: 1.218256\tLR: 0.030000\n",
      "Train set: Average loss: 0.3301\n",
      "Test set: Average loss: 0.2442, Average MAE: 0.3516\n",
      "Train Epoch: 959 [4096/194182 (2%)]\tLoss: 0.322287\tGrad Norm: 1.609037\tLR: 0.030000\n",
      "Train Epoch: 959 [24576/194182 (12%)]\tLoss: 0.329882\tGrad Norm: 1.355705\tLR: 0.030000\n",
      "Train Epoch: 959 [45056/194182 (23%)]\tLoss: 0.328614\tGrad Norm: 1.061608\tLR: 0.030000\n",
      "Train Epoch: 959 [65536/194182 (33%)]\tLoss: 0.331960\tGrad Norm: 1.346633\tLR: 0.030000\n",
      "Train Epoch: 959 [86016/194182 (44%)]\tLoss: 0.344172\tGrad Norm: 1.749334\tLR: 0.030000\n",
      "Train Epoch: 959 [106496/194182 (54%)]\tLoss: 0.339227\tGrad Norm: 1.722878\tLR: 0.030000\n",
      "Train Epoch: 959 [126976/194182 (65%)]\tLoss: 0.332310\tGrad Norm: 1.485735\tLR: 0.030000\n",
      "Train Epoch: 959 [147456/194182 (75%)]\tLoss: 0.328896\tGrad Norm: 1.467441\tLR: 0.030000\n",
      "Train Epoch: 959 [167936/194182 (85%)]\tLoss: 0.330334\tGrad Norm: 1.359332\tLR: 0.030000\n",
      "Train Epoch: 959 [188416/194182 (96%)]\tLoss: 0.333967\tGrad Norm: 1.609825\tLR: 0.030000\n",
      "Train set: Average loss: 0.3320\n",
      "Test set: Average loss: 0.2499, Average MAE: 0.3559\n",
      "Train Epoch: 960 [4096/194182 (2%)]\tLoss: 0.334854\tGrad Norm: 1.638859\tLR: 0.030000\n",
      "Train Epoch: 960 [24576/194182 (12%)]\tLoss: 0.334284\tGrad Norm: 1.400325\tLR: 0.030000\n",
      "Train Epoch: 960 [45056/194182 (23%)]\tLoss: 0.334069\tGrad Norm: 1.220453\tLR: 0.030000\n",
      "Train Epoch: 960 [65536/194182 (33%)]\tLoss: 0.329948\tGrad Norm: 1.297108\tLR: 0.030000\n",
      "Train Epoch: 960 [86016/194182 (44%)]\tLoss: 0.324419\tGrad Norm: 1.297325\tLR: 0.030000\n",
      "Train Epoch: 960 [106496/194182 (54%)]\tLoss: 0.324582\tGrad Norm: 1.004190\tLR: 0.030000\n",
      "Train Epoch: 960 [126976/194182 (65%)]\tLoss: 0.322008\tGrad Norm: 0.783882\tLR: 0.030000\n",
      "Train Epoch: 960 [147456/194182 (75%)]\tLoss: 0.330625\tGrad Norm: 1.133357\tLR: 0.030000\n",
      "Train Epoch: 960 [167936/194182 (85%)]\tLoss: 0.331462\tGrad Norm: 1.460231\tLR: 0.030000\n",
      "Train Epoch: 960 [188416/194182 (96%)]\tLoss: 0.337473\tGrad Norm: 1.625347\tLR: 0.030000\n",
      "Train set: Average loss: 0.3295\n",
      "Test set: Average loss: 0.2480, Average MAE: 0.3573\n",
      "Epoch 960: Mean reward = 0.063 +/- 0.068\n",
      "Train Epoch: 961 [4096/194182 (2%)]\tLoss: 0.331972\tGrad Norm: 1.537092\tLR: 0.030000\n",
      "Train Epoch: 961 [24576/194182 (12%)]\tLoss: 0.328601\tGrad Norm: 1.269592\tLR: 0.030000\n",
      "Train Epoch: 961 [45056/194182 (23%)]\tLoss: 0.328929\tGrad Norm: 1.453606\tLR: 0.030000\n",
      "Train Epoch: 961 [65536/194182 (33%)]\tLoss: 0.330118\tGrad Norm: 1.211699\tLR: 0.030000\n",
      "Train Epoch: 961 [86016/194182 (44%)]\tLoss: 0.321608\tGrad Norm: 1.353680\tLR: 0.030000\n",
      "Train Epoch: 961 [106496/194182 (54%)]\tLoss: 0.328487\tGrad Norm: 1.578675\tLR: 0.030000\n",
      "Train Epoch: 961 [126976/194182 (65%)]\tLoss: 0.336135\tGrad Norm: 1.883834\tLR: 0.030000\n",
      "Train Epoch: 961 [147456/194182 (75%)]\tLoss: 0.335337\tGrad Norm: 1.628422\tLR: 0.030000\n",
      "Train Epoch: 961 [167936/194182 (85%)]\tLoss: 0.323980\tGrad Norm: 1.493165\tLR: 0.030000\n",
      "Train Epoch: 961 [188416/194182 (96%)]\tLoss: 0.338014\tGrad Norm: 1.612866\tLR: 0.030000\n",
      "Train set: Average loss: 0.3320\n",
      "Test set: Average loss: 0.2449, Average MAE: 0.3549\n",
      "Train Epoch: 962 [4096/194182 (2%)]\tLoss: 0.332332\tGrad Norm: 1.464374\tLR: 0.030000\n",
      "Train Epoch: 962 [24576/194182 (12%)]\tLoss: 0.328962\tGrad Norm: 1.180418\tLR: 0.030000\n",
      "Train Epoch: 962 [45056/194182 (23%)]\tLoss: 0.327835\tGrad Norm: 1.336470\tLR: 0.030000\n",
      "Train Epoch: 962 [65536/194182 (33%)]\tLoss: 0.332334\tGrad Norm: 1.297345\tLR: 0.030000\n",
      "Train Epoch: 962 [86016/194182 (44%)]\tLoss: 0.325450\tGrad Norm: 1.513688\tLR: 0.030000\n",
      "Train Epoch: 962 [106496/194182 (54%)]\tLoss: 0.334866\tGrad Norm: 1.438538\tLR: 0.030000\n",
      "Train Epoch: 962 [126976/194182 (65%)]\tLoss: 0.336420\tGrad Norm: 1.521288\tLR: 0.030000\n",
      "Train Epoch: 962 [147456/194182 (75%)]\tLoss: 0.344912\tGrad Norm: 1.504813\tLR: 0.030000\n",
      "Train Epoch: 962 [167936/194182 (85%)]\tLoss: 0.330871\tGrad Norm: 1.134014\tLR: 0.030000\n",
      "Train Epoch: 962 [188416/194182 (96%)]\tLoss: 0.337456\tGrad Norm: 1.671610\tLR: 0.030000\n",
      "Train set: Average loss: 0.3309\n",
      "Test set: Average loss: 0.2449, Average MAE: 0.3569\n",
      "Train Epoch: 963 [4096/194182 (2%)]\tLoss: 0.331036\tGrad Norm: 1.481272\tLR: 0.030000\n",
      "Train Epoch: 963 [24576/194182 (12%)]\tLoss: 0.331514\tGrad Norm: 1.409000\tLR: 0.030000\n",
      "Train Epoch: 963 [45056/194182 (23%)]\tLoss: 0.333195\tGrad Norm: 1.032843\tLR: 0.030000\n",
      "Train Epoch: 963 [65536/194182 (33%)]\tLoss: 0.329160\tGrad Norm: 1.272083\tLR: 0.030000\n",
      "Train Epoch: 963 [86016/194182 (44%)]\tLoss: 0.326671\tGrad Norm: 1.244116\tLR: 0.030000\n",
      "Train Epoch: 963 [106496/194182 (54%)]\tLoss: 0.324245\tGrad Norm: 1.120122\tLR: 0.030000\n",
      "Train Epoch: 963 [126976/194182 (65%)]\tLoss: 0.327874\tGrad Norm: 1.143460\tLR: 0.030000\n",
      "Train Epoch: 963 [147456/194182 (75%)]\tLoss: 0.329466\tGrad Norm: 1.040787\tLR: 0.030000\n",
      "Train Epoch: 963 [167936/194182 (85%)]\tLoss: 0.328914\tGrad Norm: 1.134097\tLR: 0.030000\n",
      "Train Epoch: 963 [188416/194182 (96%)]\tLoss: 0.335184\tGrad Norm: 1.453560\tLR: 0.030000\n",
      "Train set: Average loss: 0.3276\n",
      "Test set: Average loss: 0.2447, Average MAE: 0.3550\n",
      "Train Epoch: 964 [4096/194182 (2%)]\tLoss: 0.324801\tGrad Norm: 1.578355\tLR: 0.030000\n",
      "Train Epoch: 964 [24576/194182 (12%)]\tLoss: 0.327684\tGrad Norm: 1.335418\tLR: 0.030000\n",
      "Train Epoch: 964 [45056/194182 (23%)]\tLoss: 0.337209\tGrad Norm: 1.708003\tLR: 0.030000\n",
      "Train Epoch: 964 [65536/194182 (33%)]\tLoss: 0.334856\tGrad Norm: 1.490778\tLR: 0.030000\n",
      "Train Epoch: 964 [86016/194182 (44%)]\tLoss: 0.335195\tGrad Norm: 1.612774\tLR: 0.030000\n",
      "Train Epoch: 964 [106496/194182 (54%)]\tLoss: 0.328091\tGrad Norm: 1.327250\tLR: 0.030000\n",
      "Train Epoch: 964 [126976/194182 (65%)]\tLoss: 0.318839\tGrad Norm: 1.026230\tLR: 0.030000\n",
      "Train Epoch: 964 [147456/194182 (75%)]\tLoss: 0.329835\tGrad Norm: 0.944243\tLR: 0.030000\n",
      "Train Epoch: 964 [167936/194182 (85%)]\tLoss: 0.333510\tGrad Norm: 1.268796\tLR: 0.030000\n",
      "Train Epoch: 964 [188416/194182 (96%)]\tLoss: 0.336814\tGrad Norm: 1.524618\tLR: 0.030000\n",
      "Train set: Average loss: 0.3298\n",
      "Test set: Average loss: 0.2431, Average MAE: 0.3475\n",
      "Train Epoch: 965 [4096/194182 (2%)]\tLoss: 0.335973\tGrad Norm: 1.356995\tLR: 0.030000\n",
      "Train Epoch: 965 [24576/194182 (12%)]\tLoss: 0.334238\tGrad Norm: 1.446369\tLR: 0.030000\n",
      "Train Epoch: 965 [45056/194182 (23%)]\tLoss: 0.328492\tGrad Norm: 1.252233\tLR: 0.030000\n",
      "Train Epoch: 965 [65536/194182 (33%)]\tLoss: 0.322411\tGrad Norm: 1.059416\tLR: 0.030000\n",
      "Train Epoch: 965 [86016/194182 (44%)]\tLoss: 0.325593\tGrad Norm: 1.388090\tLR: 0.030000\n",
      "Train Epoch: 965 [106496/194182 (54%)]\tLoss: 0.334088\tGrad Norm: 1.515598\tLR: 0.030000\n",
      "Train Epoch: 965 [126976/194182 (65%)]\tLoss: 0.320461\tGrad Norm: 1.458675\tLR: 0.030000\n",
      "Train Epoch: 965 [147456/194182 (75%)]\tLoss: 0.332656\tGrad Norm: 1.473807\tLR: 0.030000\n",
      "Train Epoch: 965 [167936/194182 (85%)]\tLoss: 0.331547\tGrad Norm: 1.374313\tLR: 0.030000\n",
      "Train Epoch: 965 [188416/194182 (96%)]\tLoss: 0.330307\tGrad Norm: 1.444047\tLR: 0.030000\n",
      "Train set: Average loss: 0.3299\n",
      "Test set: Average loss: 0.2437, Average MAE: 0.3538\n",
      "Epoch 965: Mean reward = 0.049 +/- 0.027\n",
      "Train Epoch: 966 [4096/194182 (2%)]\tLoss: 0.327461\tGrad Norm: 1.487462\tLR: 0.030000\n",
      "Train Epoch: 966 [24576/194182 (12%)]\tLoss: 0.336929\tGrad Norm: 1.715806\tLR: 0.030000\n",
      "Train Epoch: 966 [45056/194182 (23%)]\tLoss: 0.330947\tGrad Norm: 1.484760\tLR: 0.030000\n",
      "Train Epoch: 966 [65536/194182 (33%)]\tLoss: 0.327810\tGrad Norm: 1.224671\tLR: 0.030000\n",
      "Train Epoch: 966 [86016/194182 (44%)]\tLoss: 0.327912\tGrad Norm: 1.306012\tLR: 0.030000\n",
      "Train Epoch: 966 [106496/194182 (54%)]\tLoss: 0.330069\tGrad Norm: 1.257597\tLR: 0.030000\n",
      "Train Epoch: 966 [126976/194182 (65%)]\tLoss: 0.320796\tGrad Norm: 1.047419\tLR: 0.030000\n",
      "Train Epoch: 966 [147456/194182 (75%)]\tLoss: 0.330564\tGrad Norm: 1.434836\tLR: 0.030000\n",
      "Train Epoch: 966 [167936/194182 (85%)]\tLoss: 0.335160\tGrad Norm: 1.380337\tLR: 0.030000\n",
      "Train Epoch: 966 [188416/194182 (96%)]\tLoss: 0.327275\tGrad Norm: 1.186112\tLR: 0.030000\n",
      "Train set: Average loss: 0.3293\n",
      "Test set: Average loss: 0.2407, Average MAE: 0.3325\n",
      "Train Epoch: 967 [4096/194182 (2%)]\tLoss: 0.324231\tGrad Norm: 1.234187\tLR: 0.030000\n",
      "Train Epoch: 967 [24576/194182 (12%)]\tLoss: 0.319532\tGrad Norm: 0.998699\tLR: 0.030000\n",
      "Train Epoch: 967 [45056/194182 (23%)]\tLoss: 0.325159\tGrad Norm: 1.104001\tLR: 0.030000\n",
      "Train Epoch: 967 [65536/194182 (33%)]\tLoss: 0.326167\tGrad Norm: 1.359657\tLR: 0.030000\n",
      "Train Epoch: 967 [86016/194182 (44%)]\tLoss: 0.323467\tGrad Norm: 1.334113\tLR: 0.030000\n",
      "Train Epoch: 967 [106496/194182 (54%)]\tLoss: 0.333060\tGrad Norm: 1.093036\tLR: 0.030000\n",
      "Train Epoch: 967 [126976/194182 (65%)]\tLoss: 0.335457\tGrad Norm: 1.212576\tLR: 0.030000\n",
      "Train Epoch: 967 [147456/194182 (75%)]\tLoss: 0.327860\tGrad Norm: 1.425087\tLR: 0.030000\n",
      "Train Epoch: 967 [167936/194182 (85%)]\tLoss: 0.332288\tGrad Norm: 1.662831\tLR: 0.030000\n",
      "Train Epoch: 967 [188416/194182 (96%)]\tLoss: 0.331426\tGrad Norm: 1.503335\tLR: 0.030000\n",
      "Train set: Average loss: 0.3277\n",
      "Test set: Average loss: 0.2453, Average MAE: 0.3517\n",
      "Train Epoch: 968 [4096/194182 (2%)]\tLoss: 0.331376\tGrad Norm: 1.432953\tLR: 0.030000\n",
      "Train Epoch: 968 [24576/194182 (12%)]\tLoss: 0.328091\tGrad Norm: 1.344660\tLR: 0.030000\n",
      "Train Epoch: 968 [45056/194182 (23%)]\tLoss: 0.328533\tGrad Norm: 1.511895\tLR: 0.030000\n",
      "Train Epoch: 968 [65536/194182 (33%)]\tLoss: 0.322532\tGrad Norm: 1.372196\tLR: 0.030000\n",
      "Train Epoch: 968 [86016/194182 (44%)]\tLoss: 0.337112\tGrad Norm: 1.495615\tLR: 0.030000\n",
      "Train Epoch: 968 [106496/194182 (54%)]\tLoss: 0.330983\tGrad Norm: 1.347152\tLR: 0.030000\n",
      "Train Epoch: 968 [126976/194182 (65%)]\tLoss: 0.331383\tGrad Norm: 1.529340\tLR: 0.030000\n",
      "Train Epoch: 968 [147456/194182 (75%)]\tLoss: 0.332844\tGrad Norm: 1.583485\tLR: 0.030000\n",
      "Train Epoch: 968 [167936/194182 (85%)]\tLoss: 0.319053\tGrad Norm: 1.182682\tLR: 0.030000\n",
      "Train Epoch: 968 [188416/194182 (96%)]\tLoss: 0.341000\tGrad Norm: 1.838452\tLR: 0.030000\n",
      "Train set: Average loss: 0.3298\n",
      "Test set: Average loss: 0.2439, Average MAE: 0.3560\n",
      "Train Epoch: 969 [4096/194182 (2%)]\tLoss: 0.327578\tGrad Norm: 1.333292\tLR: 0.030000\n",
      "Train Epoch: 969 [24576/194182 (12%)]\tLoss: 0.321405\tGrad Norm: 1.158309\tLR: 0.030000\n",
      "Train Epoch: 969 [45056/194182 (23%)]\tLoss: 0.326458\tGrad Norm: 1.343663\tLR: 0.030000\n",
      "Train Epoch: 969 [65536/194182 (33%)]\tLoss: 0.335918\tGrad Norm: 1.592737\tLR: 0.030000\n",
      "Train Epoch: 969 [86016/194182 (44%)]\tLoss: 0.332934\tGrad Norm: 1.272932\tLR: 0.030000\n",
      "Train Epoch: 969 [106496/194182 (54%)]\tLoss: 0.328697\tGrad Norm: 1.346930\tLR: 0.030000\n",
      "Train Epoch: 969 [126976/194182 (65%)]\tLoss: 0.324200\tGrad Norm: 1.508247\tLR: 0.030000\n",
      "Train Epoch: 969 [147456/194182 (75%)]\tLoss: 0.313559\tGrad Norm: 1.229778\tLR: 0.030000\n",
      "Train Epoch: 969 [167936/194182 (85%)]\tLoss: 0.326101\tGrad Norm: 1.257816\tLR: 0.030000\n",
      "Train Epoch: 969 [188416/194182 (96%)]\tLoss: 0.319906\tGrad Norm: 1.128855\tLR: 0.030000\n",
      "Train set: Average loss: 0.3280\n",
      "Test set: Average loss: 0.2415, Average MAE: 0.3441\n",
      "Train Epoch: 970 [4096/194182 (2%)]\tLoss: 0.327196\tGrad Norm: 1.230848\tLR: 0.030000\n",
      "Train Epoch: 970 [24576/194182 (12%)]\tLoss: 0.329052\tGrad Norm: 1.084361\tLR: 0.030000\n",
      "Train Epoch: 970 [45056/194182 (23%)]\tLoss: 0.323028\tGrad Norm: 1.088992\tLR: 0.030000\n",
      "Train Epoch: 970 [65536/194182 (33%)]\tLoss: 0.324564\tGrad Norm: 1.244082\tLR: 0.030000\n",
      "Train Epoch: 970 [86016/194182 (44%)]\tLoss: 0.324743\tGrad Norm: 1.180776\tLR: 0.030000\n",
      "Train Epoch: 970 [106496/194182 (54%)]\tLoss: 0.331647\tGrad Norm: 1.445601\tLR: 0.030000\n",
      "Train Epoch: 970 [126976/194182 (65%)]\tLoss: 0.323461\tGrad Norm: 1.014123\tLR: 0.030000\n",
      "Train Epoch: 970 [147456/194182 (75%)]\tLoss: 0.331843\tGrad Norm: 1.358371\tLR: 0.030000\n",
      "Train Epoch: 970 [167936/194182 (85%)]\tLoss: 0.323057\tGrad Norm: 1.177999\tLR: 0.030000\n",
      "Train Epoch: 970 [188416/194182 (96%)]\tLoss: 0.323508\tGrad Norm: 1.093083\tLR: 0.030000\n",
      "Train set: Average loss: 0.3262\n",
      "Test set: Average loss: 0.2383, Average MAE: 0.3327\n",
      "Epoch 970: Mean reward = 0.062 +/- 0.080\n",
      "Train Epoch: 971 [4096/194182 (2%)]\tLoss: 0.320880\tGrad Norm: 1.231248\tLR: 0.030000\n",
      "Train Epoch: 971 [24576/194182 (12%)]\tLoss: 0.337934\tGrad Norm: 1.776665\tLR: 0.030000\n",
      "Train Epoch: 971 [45056/194182 (23%)]\tLoss: 0.332190\tGrad Norm: 1.824941\tLR: 0.030000\n",
      "Train Epoch: 971 [65536/194182 (33%)]\tLoss: 0.332205\tGrad Norm: 1.458447\tLR: 0.030000\n",
      "Train Epoch: 971 [86016/194182 (44%)]\tLoss: 0.315317\tGrad Norm: 1.229090\tLR: 0.030000\n",
      "Train Epoch: 971 [106496/194182 (54%)]\tLoss: 0.337856\tGrad Norm: 1.638120\tLR: 0.030000\n",
      "Train Epoch: 971 [126976/194182 (65%)]\tLoss: 0.319702\tGrad Norm: 1.069403\tLR: 0.030000\n",
      "Train Epoch: 971 [147456/194182 (75%)]\tLoss: 0.325429\tGrad Norm: 0.987063\tLR: 0.030000\n",
      "Train Epoch: 971 [167936/194182 (85%)]\tLoss: 0.325602\tGrad Norm: 1.324322\tLR: 0.030000\n",
      "Train Epoch: 971 [188416/194182 (96%)]\tLoss: 0.329367\tGrad Norm: 1.544611\tLR: 0.030000\n",
      "Train set: Average loss: 0.3279\n",
      "Test set: Average loss: 0.2397, Average MAE: 0.3387\n",
      "Train Epoch: 972 [4096/194182 (2%)]\tLoss: 0.320550\tGrad Norm: 1.076958\tLR: 0.030000\n",
      "Train Epoch: 972 [24576/194182 (12%)]\tLoss: 0.337741\tGrad Norm: 1.090984\tLR: 0.030000\n",
      "Train Epoch: 972 [45056/194182 (23%)]\tLoss: 0.324640\tGrad Norm: 1.320866\tLR: 0.030000\n",
      "Train Epoch: 972 [65536/194182 (33%)]\tLoss: 0.315855\tGrad Norm: 1.318427\tLR: 0.030000\n",
      "Train Epoch: 972 [86016/194182 (44%)]\tLoss: 0.313654\tGrad Norm: 1.127809\tLR: 0.030000\n",
      "Train Epoch: 972 [106496/194182 (54%)]\tLoss: 0.325619\tGrad Norm: 1.474288\tLR: 0.030000\n",
      "Train Epoch: 972 [126976/194182 (65%)]\tLoss: 0.331665\tGrad Norm: 1.259028\tLR: 0.030000\n",
      "Train Epoch: 972 [147456/194182 (75%)]\tLoss: 0.337349\tGrad Norm: 1.596292\tLR: 0.030000\n",
      "Train Epoch: 972 [167936/194182 (85%)]\tLoss: 0.324227\tGrad Norm: 1.389396\tLR: 0.030000\n",
      "Train Epoch: 972 [188416/194182 (96%)]\tLoss: 0.329006\tGrad Norm: 1.340383\tLR: 0.030000\n",
      "Train set: Average loss: 0.3267\n",
      "Test set: Average loss: 0.2437, Average MAE: 0.3528\n",
      "Train Epoch: 973 [4096/194182 (2%)]\tLoss: 0.325122\tGrad Norm: 1.428173\tLR: 0.030000\n",
      "Train Epoch: 973 [24576/194182 (12%)]\tLoss: 0.337493\tGrad Norm: 2.388625\tLR: 0.030000\n",
      "Train Epoch: 973 [45056/194182 (23%)]\tLoss: 0.332012\tGrad Norm: 1.619903\tLR: 0.030000\n",
      "Train Epoch: 973 [65536/194182 (33%)]\tLoss: 0.327327\tGrad Norm: 1.456885\tLR: 0.030000\n",
      "Train Epoch: 973 [86016/194182 (44%)]\tLoss: 0.316194\tGrad Norm: 1.030997\tLR: 0.030000\n",
      "Train Epoch: 973 [106496/194182 (54%)]\tLoss: 0.333644\tGrad Norm: 1.402915\tLR: 0.030000\n",
      "Train Epoch: 973 [126976/194182 (65%)]\tLoss: 0.334322\tGrad Norm: 1.670685\tLR: 0.030000\n",
      "Train Epoch: 973 [147456/194182 (75%)]\tLoss: 0.331623\tGrad Norm: 1.596915\tLR: 0.030000\n",
      "Train Epoch: 973 [167936/194182 (85%)]\tLoss: 0.319870\tGrad Norm: 1.003200\tLR: 0.030000\n",
      "Train Epoch: 973 [188416/194182 (96%)]\tLoss: 0.328962\tGrad Norm: 1.411226\tLR: 0.030000\n",
      "Train set: Average loss: 0.3290\n",
      "Test set: Average loss: 0.2483, Average MAE: 0.3415\n",
      "Train Epoch: 974 [4096/194182 (2%)]\tLoss: 0.332547\tGrad Norm: 1.711717\tLR: 0.030000\n",
      "Train Epoch: 974 [24576/194182 (12%)]\tLoss: 0.326883\tGrad Norm: 1.151118\tLR: 0.030000\n",
      "Train Epoch: 974 [45056/194182 (23%)]\tLoss: 0.328121\tGrad Norm: 1.314254\tLR: 0.030000\n",
      "Train Epoch: 974 [65536/194182 (33%)]\tLoss: 0.334089\tGrad Norm: 1.504162\tLR: 0.030000\n",
      "Train Epoch: 974 [86016/194182 (44%)]\tLoss: 0.324034\tGrad Norm: 1.217162\tLR: 0.030000\n",
      "Train Epoch: 974 [106496/194182 (54%)]\tLoss: 0.331316\tGrad Norm: 1.530954\tLR: 0.030000\n",
      "Train Epoch: 974 [126976/194182 (65%)]\tLoss: 0.327629\tGrad Norm: 1.073612\tLR: 0.030000\n",
      "Train Epoch: 974 [147456/194182 (75%)]\tLoss: 0.323546\tGrad Norm: 1.090597\tLR: 0.030000\n",
      "Train Epoch: 974 [167936/194182 (85%)]\tLoss: 0.328377\tGrad Norm: 1.373875\tLR: 0.030000\n",
      "Train Epoch: 974 [188416/194182 (96%)]\tLoss: 0.337746\tGrad Norm: 1.838667\tLR: 0.030000\n",
      "Train set: Average loss: 0.3269\n",
      "Test set: Average loss: 0.2453, Average MAE: 0.3552\n",
      "Train Epoch: 975 [4096/194182 (2%)]\tLoss: 0.327337\tGrad Norm: 1.472411\tLR: 0.030000\n",
      "Train Epoch: 975 [24576/194182 (12%)]\tLoss: 0.325312\tGrad Norm: 1.547634\tLR: 0.030000\n",
      "Train Epoch: 975 [45056/194182 (23%)]\tLoss: 0.329201\tGrad Norm: 1.686037\tLR: 0.030000\n",
      "Train Epoch: 975 [65536/194182 (33%)]\tLoss: 0.342471\tGrad Norm: 1.881744\tLR: 0.030000\n",
      "Train Epoch: 975 [86016/194182 (44%)]\tLoss: 0.327733\tGrad Norm: 1.319926\tLR: 0.030000\n",
      "Train Epoch: 975 [106496/194182 (54%)]\tLoss: 0.321611\tGrad Norm: 1.120335\tLR: 0.030000\n",
      "Train Epoch: 975 [126976/194182 (65%)]\tLoss: 0.319165\tGrad Norm: 0.871044\tLR: 0.030000\n",
      "Train Epoch: 975 [147456/194182 (75%)]\tLoss: 0.329066\tGrad Norm: 1.139877\tLR: 0.030000\n",
      "Train Epoch: 975 [167936/194182 (85%)]\tLoss: 0.324457\tGrad Norm: 1.187618\tLR: 0.030000\n",
      "Train Epoch: 975 [188416/194182 (96%)]\tLoss: 0.327546\tGrad Norm: 1.287971\tLR: 0.030000\n",
      "Train set: Average loss: 0.3274\n",
      "Test set: Average loss: 0.2443, Average MAE: 0.3462\n",
      "Epoch 975: Mean reward = 0.050 +/- 0.046\n",
      "Train Epoch: 976 [4096/194182 (2%)]\tLoss: 0.329178\tGrad Norm: 1.355221\tLR: 0.030000\n",
      "Train Epoch: 976 [24576/194182 (12%)]\tLoss: 0.323345\tGrad Norm: 1.185372\tLR: 0.030000\n",
      "Train Epoch: 976 [45056/194182 (23%)]\tLoss: 0.329678\tGrad Norm: 1.145064\tLR: 0.030000\n",
      "Train Epoch: 976 [65536/194182 (33%)]\tLoss: 0.335005\tGrad Norm: 1.327914\tLR: 0.030000\n",
      "Train Epoch: 976 [86016/194182 (44%)]\tLoss: 0.326147\tGrad Norm: 1.436057\tLR: 0.030000\n",
      "Train Epoch: 976 [106496/194182 (54%)]\tLoss: 0.326751\tGrad Norm: 1.583331\tLR: 0.030000\n",
      "Train Epoch: 976 [126976/194182 (65%)]\tLoss: 0.330525\tGrad Norm: 1.439479\tLR: 0.030000\n",
      "Train Epoch: 976 [147456/194182 (75%)]\tLoss: 0.327925\tGrad Norm: 1.290948\tLR: 0.030000\n",
      "Train Epoch: 976 [167936/194182 (85%)]\tLoss: 0.319458\tGrad Norm: 1.171972\tLR: 0.030000\n",
      "Train Epoch: 976 [188416/194182 (96%)]\tLoss: 0.325536\tGrad Norm: 1.470468\tLR: 0.030000\n",
      "Train set: Average loss: 0.3265\n",
      "Test set: Average loss: 0.2438, Average MAE: 0.3530\n",
      "Train Epoch: 977 [4096/194182 (2%)]\tLoss: 0.327547\tGrad Norm: 1.358581\tLR: 0.030000\n",
      "Train Epoch: 977 [24576/194182 (12%)]\tLoss: 0.326810\tGrad Norm: 1.371922\tLR: 0.030000\n",
      "Train Epoch: 977 [45056/194182 (23%)]\tLoss: 0.329864\tGrad Norm: 1.552308\tLR: 0.030000\n",
      "Train Epoch: 977 [65536/194182 (33%)]\tLoss: 0.333406\tGrad Norm: 1.655304\tLR: 0.030000\n",
      "Train Epoch: 977 [86016/194182 (44%)]\tLoss: 0.328753\tGrad Norm: 1.707737\tLR: 0.030000\n",
      "Train Epoch: 977 [106496/194182 (54%)]\tLoss: 0.331778\tGrad Norm: 1.640737\tLR: 0.030000\n",
      "Train Epoch: 977 [126976/194182 (65%)]\tLoss: 0.325597\tGrad Norm: 1.029204\tLR: 0.030000\n",
      "Train Epoch: 977 [147456/194182 (75%)]\tLoss: 0.330133\tGrad Norm: 1.390819\tLR: 0.030000\n",
      "Train Epoch: 977 [167936/194182 (85%)]\tLoss: 0.328520\tGrad Norm: 1.627634\tLR: 0.030000\n",
      "Train Epoch: 977 [188416/194182 (96%)]\tLoss: 0.325507\tGrad Norm: 1.290708\tLR: 0.030000\n",
      "Train set: Average loss: 0.3282\n",
      "Test set: Average loss: 0.2474, Average MAE: 0.3531\n",
      "Train Epoch: 978 [4096/194182 (2%)]\tLoss: 0.335189\tGrad Norm: 1.447575\tLR: 0.030000\n",
      "Train Epoch: 978 [24576/194182 (12%)]\tLoss: 0.322642\tGrad Norm: 1.133882\tLR: 0.030000\n",
      "Train Epoch: 978 [45056/194182 (23%)]\tLoss: 0.322118\tGrad Norm: 1.217096\tLR: 0.030000\n",
      "Train Epoch: 978 [65536/194182 (33%)]\tLoss: 0.325497\tGrad Norm: 1.286841\tLR: 0.030000\n",
      "Train Epoch: 978 [86016/194182 (44%)]\tLoss: 0.317067\tGrad Norm: 1.158139\tLR: 0.030000\n",
      "Train Epoch: 978 [106496/194182 (54%)]\tLoss: 0.317707\tGrad Norm: 1.024725\tLR: 0.030000\n",
      "Train Epoch: 978 [126976/194182 (65%)]\tLoss: 0.326608\tGrad Norm: 1.497593\tLR: 0.030000\n",
      "Train Epoch: 978 [147456/194182 (75%)]\tLoss: 0.324508\tGrad Norm: 1.533208\tLR: 0.030000\n",
      "Train Epoch: 978 [167936/194182 (85%)]\tLoss: 0.325341\tGrad Norm: 1.471087\tLR: 0.030000\n",
      "Train Epoch: 978 [188416/194182 (96%)]\tLoss: 0.322618\tGrad Norm: 1.400026\tLR: 0.030000\n",
      "Train set: Average loss: 0.3256\n",
      "Test set: Average loss: 0.2418, Average MAE: 0.3321\n",
      "Train Epoch: 979 [4096/194182 (2%)]\tLoss: 0.334536\tGrad Norm: 1.437565\tLR: 0.030000\n",
      "Train Epoch: 979 [24576/194182 (12%)]\tLoss: 0.329237\tGrad Norm: 1.227256\tLR: 0.030000\n",
      "Train Epoch: 979 [45056/194182 (23%)]\tLoss: 0.333084\tGrad Norm: 1.728788\tLR: 0.030000\n",
      "Train Epoch: 979 [65536/194182 (33%)]\tLoss: 0.327547\tGrad Norm: 1.658578\tLR: 0.030000\n",
      "Train Epoch: 979 [86016/194182 (44%)]\tLoss: 0.319853\tGrad Norm: 1.233585\tLR: 0.030000\n",
      "Train Epoch: 979 [106496/194182 (54%)]\tLoss: 0.328789\tGrad Norm: 1.462567\tLR: 0.030000\n",
      "Train Epoch: 979 [126976/194182 (65%)]\tLoss: 0.327627\tGrad Norm: 1.229914\tLR: 0.030000\n",
      "Train Epoch: 979 [147456/194182 (75%)]\tLoss: 0.324388\tGrad Norm: 1.350111\tLR: 0.030000\n",
      "Train Epoch: 979 [167936/194182 (85%)]\tLoss: 0.331742\tGrad Norm: 1.489857\tLR: 0.030000\n",
      "Train Epoch: 979 [188416/194182 (96%)]\tLoss: 0.329440\tGrad Norm: 1.734175\tLR: 0.030000\n",
      "Train set: Average loss: 0.3277\n",
      "Test set: Average loss: 0.2440, Average MAE: 0.3532\n",
      "Train Epoch: 980 [4096/194182 (2%)]\tLoss: 0.328987\tGrad Norm: 1.468201\tLR: 0.030000\n",
      "Train Epoch: 980 [24576/194182 (12%)]\tLoss: 0.324344\tGrad Norm: 1.413285\tLR: 0.030000\n",
      "Train Epoch: 980 [45056/194182 (23%)]\tLoss: 0.320228\tGrad Norm: 1.085148\tLR: 0.030000\n",
      "Train Epoch: 980 [65536/194182 (33%)]\tLoss: 0.322018\tGrad Norm: 1.314906\tLR: 0.030000\n",
      "Train Epoch: 980 [86016/194182 (44%)]\tLoss: 0.322517\tGrad Norm: 1.105336\tLR: 0.030000\n",
      "Train Epoch: 980 [106496/194182 (54%)]\tLoss: 0.323205\tGrad Norm: 1.368030\tLR: 0.030000\n",
      "Train Epoch: 980 [126976/194182 (65%)]\tLoss: 0.325710\tGrad Norm: 1.408924\tLR: 0.030000\n",
      "Train Epoch: 980 [147456/194182 (75%)]\tLoss: 0.322915\tGrad Norm: 1.471961\tLR: 0.030000\n",
      "Train Epoch: 980 [167936/194182 (85%)]\tLoss: 0.323937\tGrad Norm: 1.388786\tLR: 0.030000\n",
      "Train Epoch: 980 [188416/194182 (96%)]\tLoss: 0.329952\tGrad Norm: 1.404917\tLR: 0.030000\n",
      "Train set: Average loss: 0.3251\n",
      "Test set: Average loss: 0.2497, Average MAE: 0.3572\n",
      "Epoch 980: Mean reward = 0.050 +/- 0.047\n",
      "Train Epoch: 981 [4096/194182 (2%)]\tLoss: 0.332798\tGrad Norm: 1.644819\tLR: 0.030000\n",
      "Train Epoch: 981 [24576/194182 (12%)]\tLoss: 0.324283\tGrad Norm: 1.506784\tLR: 0.030000\n",
      "Train Epoch: 981 [45056/194182 (23%)]\tLoss: 0.332655\tGrad Norm: 1.502150\tLR: 0.030000\n",
      "Train Epoch: 981 [65536/194182 (33%)]\tLoss: 0.322387\tGrad Norm: 1.219491\tLR: 0.030000\n",
      "Train Epoch: 981 [86016/194182 (44%)]\tLoss: 0.317597\tGrad Norm: 0.991141\tLR: 0.030000\n",
      "Train Epoch: 981 [106496/194182 (54%)]\tLoss: 0.324860\tGrad Norm: 1.247888\tLR: 0.030000\n",
      "Train Epoch: 981 [126976/194182 (65%)]\tLoss: 0.327006\tGrad Norm: 1.201151\tLR: 0.030000\n",
      "Train Epoch: 981 [147456/194182 (75%)]\tLoss: 0.327505\tGrad Norm: 1.542957\tLR: 0.030000\n",
      "Train Epoch: 981 [167936/194182 (85%)]\tLoss: 0.331483\tGrad Norm: 1.358878\tLR: 0.030000\n",
      "Train Epoch: 981 [188416/194182 (96%)]\tLoss: 0.319985\tGrad Norm: 1.453963\tLR: 0.030000\n",
      "Train set: Average loss: 0.3259\n",
      "Test set: Average loss: 0.2455, Average MAE: 0.3528\n",
      "Train Epoch: 982 [4096/194182 (2%)]\tLoss: 0.333578\tGrad Norm: 1.392450\tLR: 0.030000\n",
      "Train Epoch: 982 [24576/194182 (12%)]\tLoss: 0.335183\tGrad Norm: 1.944227\tLR: 0.030000\n",
      "Train Epoch: 982 [45056/194182 (23%)]\tLoss: 0.335540\tGrad Norm: 1.704279\tLR: 0.030000\n",
      "Train Epoch: 982 [65536/194182 (33%)]\tLoss: 0.335131\tGrad Norm: 1.826943\tLR: 0.030000\n",
      "Train Epoch: 982 [86016/194182 (44%)]\tLoss: 0.318147\tGrad Norm: 1.161250\tLR: 0.030000\n",
      "Train Epoch: 982 [106496/194182 (54%)]\tLoss: 0.328138\tGrad Norm: 1.385185\tLR: 0.030000\n",
      "Train Epoch: 982 [126976/194182 (65%)]\tLoss: 0.326960\tGrad Norm: 1.468617\tLR: 0.030000\n",
      "Train Epoch: 982 [147456/194182 (75%)]\tLoss: 0.332846\tGrad Norm: 1.546803\tLR: 0.030000\n",
      "Train Epoch: 982 [167936/194182 (85%)]\tLoss: 0.321966\tGrad Norm: 1.376215\tLR: 0.030000\n",
      "Train Epoch: 982 [188416/194182 (96%)]\tLoss: 0.329628\tGrad Norm: 1.261909\tLR: 0.030000\n",
      "Train set: Average loss: 0.3281\n",
      "Test set: Average loss: 0.2435, Average MAE: 0.3474\n",
      "Train Epoch: 983 [4096/194182 (2%)]\tLoss: 0.330753\tGrad Norm: 1.379699\tLR: 0.030000\n",
      "Train Epoch: 983 [24576/194182 (12%)]\tLoss: 0.317691\tGrad Norm: 1.271049\tLR: 0.030000\n",
      "Train Epoch: 983 [45056/194182 (23%)]\tLoss: 0.330775\tGrad Norm: 1.535084\tLR: 0.030000\n",
      "Train Epoch: 983 [65536/194182 (33%)]\tLoss: 0.317951\tGrad Norm: 1.184712\tLR: 0.030000\n",
      "Train Epoch: 983 [86016/194182 (44%)]\tLoss: 0.333590\tGrad Norm: 1.521970\tLR: 0.030000\n",
      "Train Epoch: 983 [106496/194182 (54%)]\tLoss: 0.325154\tGrad Norm: 1.447916\tLR: 0.030000\n",
      "Train Epoch: 983 [126976/194182 (65%)]\tLoss: 0.313882\tGrad Norm: 1.201876\tLR: 0.030000\n",
      "Train Epoch: 983 [147456/194182 (75%)]\tLoss: 0.324479\tGrad Norm: 1.396250\tLR: 0.030000\n",
      "Train Epoch: 983 [167936/194182 (85%)]\tLoss: 0.330765\tGrad Norm: 1.653763\tLR: 0.030000\n",
      "Train Epoch: 983 [188416/194182 (96%)]\tLoss: 0.331257\tGrad Norm: 1.191316\tLR: 0.030000\n",
      "Train set: Average loss: 0.3253\n",
      "Test set: Average loss: 0.2399, Average MAE: 0.3405\n",
      "Train Epoch: 984 [4096/194182 (2%)]\tLoss: 0.330116\tGrad Norm: 1.210351\tLR: 0.030000\n",
      "Train Epoch: 984 [24576/194182 (12%)]\tLoss: 0.324146\tGrad Norm: 1.417073\tLR: 0.030000\n",
      "Train Epoch: 984 [45056/194182 (23%)]\tLoss: 0.331039\tGrad Norm: 1.290310\tLR: 0.030000\n",
      "Train Epoch: 984 [65536/194182 (33%)]\tLoss: 0.326726\tGrad Norm: 1.323055\tLR: 0.030000\n",
      "Train Epoch: 984 [86016/194182 (44%)]\tLoss: 0.326474\tGrad Norm: 1.365989\tLR: 0.030000\n",
      "Train Epoch: 984 [106496/194182 (54%)]\tLoss: 0.335329\tGrad Norm: 1.344918\tLR: 0.030000\n",
      "Train Epoch: 984 [126976/194182 (65%)]\tLoss: 0.333567\tGrad Norm: 1.435141\tLR: 0.030000\n",
      "Train Epoch: 984 [147456/194182 (75%)]\tLoss: 0.327231\tGrad Norm: 1.264152\tLR: 0.030000\n",
      "Train Epoch: 984 [167936/194182 (85%)]\tLoss: 0.324224\tGrad Norm: 1.583530\tLR: 0.030000\n",
      "Train Epoch: 984 [188416/194182 (96%)]\tLoss: 0.315720\tGrad Norm: 1.003259\tLR: 0.030000\n",
      "Train set: Average loss: 0.3249\n",
      "Test set: Average loss: 0.2407, Average MAE: 0.3419\n",
      "Train Epoch: 985 [4096/194182 (2%)]\tLoss: 0.324795\tGrad Norm: 1.203019\tLR: 0.030000\n",
      "Train Epoch: 985 [24576/194182 (12%)]\tLoss: 0.326450\tGrad Norm: 1.359382\tLR: 0.030000\n",
      "Train Epoch: 985 [45056/194182 (23%)]\tLoss: 0.318391\tGrad Norm: 1.234648\tLR: 0.030000\n",
      "Train Epoch: 985 [65536/194182 (33%)]\tLoss: 0.334745\tGrad Norm: 1.624900\tLR: 0.030000\n",
      "Train Epoch: 985 [86016/194182 (44%)]\tLoss: 0.334018\tGrad Norm: 1.731657\tLR: 0.030000\n",
      "Train Epoch: 985 [106496/194182 (54%)]\tLoss: 0.318489\tGrad Norm: 0.985257\tLR: 0.030000\n",
      "Train Epoch: 985 [126976/194182 (65%)]\tLoss: 0.322002\tGrad Norm: 1.037771\tLR: 0.030000\n",
      "Train Epoch: 985 [147456/194182 (75%)]\tLoss: 0.323403\tGrad Norm: 1.530619\tLR: 0.030000\n",
      "Train Epoch: 985 [167936/194182 (85%)]\tLoss: 0.330804\tGrad Norm: 1.584744\tLR: 0.030000\n",
      "Train Epoch: 985 [188416/194182 (96%)]\tLoss: 0.320871\tGrad Norm: 1.204998\tLR: 0.030000\n",
      "Train set: Average loss: 0.3254\n",
      "Test set: Average loss: 0.2413, Average MAE: 0.3475\n",
      "Epoch 985: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 986 [4096/194182 (2%)]\tLoss: 0.320535\tGrad Norm: 1.173945\tLR: 0.030000\n",
      "Train Epoch: 986 [24576/194182 (12%)]\tLoss: 0.316733\tGrad Norm: 0.873730\tLR: 0.030000\n",
      "Train Epoch: 986 [45056/194182 (23%)]\tLoss: 0.327262\tGrad Norm: 1.394205\tLR: 0.030000\n",
      "Train Epoch: 986 [65536/194182 (33%)]\tLoss: 0.327295\tGrad Norm: 1.630930\tLR: 0.030000\n",
      "Train Epoch: 986 [86016/194182 (44%)]\tLoss: 0.322581\tGrad Norm: 1.260913\tLR: 0.030000\n",
      "Train Epoch: 986 [106496/194182 (54%)]\tLoss: 0.329471\tGrad Norm: 1.167862\tLR: 0.030000\n",
      "Train Epoch: 986 [126976/194182 (65%)]\tLoss: 0.321380\tGrad Norm: 1.242010\tLR: 0.030000\n",
      "Train Epoch: 986 [147456/194182 (75%)]\tLoss: 0.327115\tGrad Norm: 1.279102\tLR: 0.030000\n",
      "Train Epoch: 986 [167936/194182 (85%)]\tLoss: 0.330923\tGrad Norm: 1.327547\tLR: 0.030000\n",
      "Train Epoch: 986 [188416/194182 (96%)]\tLoss: 0.323050\tGrad Norm: 1.372455\tLR: 0.030000\n",
      "Train set: Average loss: 0.3243\n",
      "Test set: Average loss: 0.2465, Average MAE: 0.3544\n",
      "Train Epoch: 987 [4096/194182 (2%)]\tLoss: 0.325954\tGrad Norm: 1.538054\tLR: 0.030000\n",
      "Train Epoch: 987 [24576/194182 (12%)]\tLoss: 0.335204\tGrad Norm: 1.724367\tLR: 0.030000\n",
      "Train Epoch: 987 [45056/194182 (23%)]\tLoss: 0.330529\tGrad Norm: 1.505339\tLR: 0.030000\n",
      "Train Epoch: 987 [65536/194182 (33%)]\tLoss: 0.320269\tGrad Norm: 1.031352\tLR: 0.030000\n",
      "Train Epoch: 987 [86016/194182 (44%)]\tLoss: 0.320038\tGrad Norm: 1.181549\tLR: 0.030000\n",
      "Train Epoch: 987 [106496/194182 (54%)]\tLoss: 0.329987\tGrad Norm: 1.399262\tLR: 0.030000\n",
      "Train Epoch: 987 [126976/194182 (65%)]\tLoss: 0.329945\tGrad Norm: 1.379846\tLR: 0.030000\n",
      "Train Epoch: 987 [147456/194182 (75%)]\tLoss: 0.323213\tGrad Norm: 1.525645\tLR: 0.030000\n",
      "Train Epoch: 987 [167936/194182 (85%)]\tLoss: 0.327805\tGrad Norm: 1.179115\tLR: 0.030000\n",
      "Train Epoch: 987 [188416/194182 (96%)]\tLoss: 0.315182\tGrad Norm: 0.946485\tLR: 0.030000\n",
      "Train set: Average loss: 0.3235\n",
      "Test set: Average loss: 0.2394, Average MAE: 0.3358\n",
      "Train Epoch: 988 [4096/194182 (2%)]\tLoss: 0.319158\tGrad Norm: 1.158385\tLR: 0.030000\n",
      "Train Epoch: 988 [24576/194182 (12%)]\tLoss: 0.323405\tGrad Norm: 1.007684\tLR: 0.030000\n",
      "Train Epoch: 988 [45056/194182 (23%)]\tLoss: 0.325284\tGrad Norm: 1.513299\tLR: 0.030000\n",
      "Train Epoch: 988 [65536/194182 (33%)]\tLoss: 0.330584\tGrad Norm: 1.341690\tLR: 0.030000\n",
      "Train Epoch: 988 [86016/194182 (44%)]\tLoss: 0.320954\tGrad Norm: 1.432878\tLR: 0.030000\n",
      "Train Epoch: 988 [106496/194182 (54%)]\tLoss: 0.329352\tGrad Norm: 1.461577\tLR: 0.030000\n",
      "Train Epoch: 988 [126976/194182 (65%)]\tLoss: 0.321587\tGrad Norm: 1.038040\tLR: 0.030000\n",
      "Train Epoch: 988 [147456/194182 (75%)]\tLoss: 0.323721\tGrad Norm: 1.123479\tLR: 0.030000\n",
      "Train Epoch: 988 [167936/194182 (85%)]\tLoss: 0.313517\tGrad Norm: 0.976348\tLR: 0.030000\n",
      "Train Epoch: 988 [188416/194182 (96%)]\tLoss: 0.318803\tGrad Norm: 1.448482\tLR: 0.030000\n",
      "Train set: Average loss: 0.3220\n",
      "Test set: Average loss: 0.2420, Average MAE: 0.3434\n",
      "Train Epoch: 989 [4096/194182 (2%)]\tLoss: 0.323912\tGrad Norm: 1.369144\tLR: 0.030000\n",
      "Train Epoch: 989 [24576/194182 (12%)]\tLoss: 0.323098\tGrad Norm: 1.504732\tLR: 0.030000\n",
      "Train Epoch: 989 [45056/194182 (23%)]\tLoss: 0.322486\tGrad Norm: 1.166042\tLR: 0.030000\n",
      "Train Epoch: 989 [65536/194182 (33%)]\tLoss: 0.328282\tGrad Norm: 1.368431\tLR: 0.030000\n",
      "Train Epoch: 989 [86016/194182 (44%)]\tLoss: 0.326351\tGrad Norm: 1.452871\tLR: 0.030000\n",
      "Train Epoch: 989 [106496/194182 (54%)]\tLoss: 0.321453\tGrad Norm: 1.178096\tLR: 0.030000\n",
      "Train Epoch: 989 [126976/194182 (65%)]\tLoss: 0.330980\tGrad Norm: 1.427092\tLR: 0.030000\n",
      "Train Epoch: 989 [147456/194182 (75%)]\tLoss: 0.319623\tGrad Norm: 1.493817\tLR: 0.030000\n",
      "Train Epoch: 989 [167936/194182 (85%)]\tLoss: 0.324898\tGrad Norm: 1.220975\tLR: 0.030000\n",
      "Train Epoch: 989 [188416/194182 (96%)]\tLoss: 0.330207\tGrad Norm: 1.494819\tLR: 0.030000\n",
      "Train set: Average loss: 0.3241\n",
      "Test set: Average loss: 0.2445, Average MAE: 0.3310\n",
      "Train Epoch: 990 [4096/194182 (2%)]\tLoss: 0.329712\tGrad Norm: 1.658424\tLR: 0.030000\n",
      "Train Epoch: 990 [24576/194182 (12%)]\tLoss: 0.343314\tGrad Norm: 2.206775\tLR: 0.030000\n",
      "Train Epoch: 990 [45056/194182 (23%)]\tLoss: 0.327106\tGrad Norm: 1.539552\tLR: 0.030000\n",
      "Train Epoch: 990 [65536/194182 (33%)]\tLoss: 0.331047\tGrad Norm: 1.523087\tLR: 0.030000\n",
      "Train Epoch: 990 [86016/194182 (44%)]\tLoss: 0.325591\tGrad Norm: 1.259315\tLR: 0.030000\n",
      "Train Epoch: 990 [106496/194182 (54%)]\tLoss: 0.317142\tGrad Norm: 1.180756\tLR: 0.030000\n",
      "Train Epoch: 990 [126976/194182 (65%)]\tLoss: 0.327513\tGrad Norm: 1.422631\tLR: 0.030000\n",
      "Train Epoch: 990 [147456/194182 (75%)]\tLoss: 0.330199\tGrad Norm: 1.577837\tLR: 0.030000\n",
      "Train Epoch: 990 [167936/194182 (85%)]\tLoss: 0.320212\tGrad Norm: 1.332063\tLR: 0.030000\n",
      "Train Epoch: 990 [188416/194182 (96%)]\tLoss: 0.319286\tGrad Norm: 1.091062\tLR: 0.030000\n",
      "Train set: Average loss: 0.3255\n",
      "Test set: Average loss: 0.2459, Average MAE: 0.3451\n",
      "Epoch 990: Mean reward = 0.046 +/- 0.001\n",
      "Train Epoch: 991 [4096/194182 (2%)]\tLoss: 0.327425\tGrad Norm: 1.508647\tLR: 0.030000\n",
      "Train Epoch: 991 [24576/194182 (12%)]\tLoss: 0.333444\tGrad Norm: 1.655435\tLR: 0.030000\n",
      "Train Epoch: 991 [45056/194182 (23%)]\tLoss: 0.315262\tGrad Norm: 1.016906\tLR: 0.030000\n",
      "Train Epoch: 991 [65536/194182 (33%)]\tLoss: 0.323039\tGrad Norm: 1.198400\tLR: 0.030000\n",
      "Train Epoch: 991 [86016/194182 (44%)]\tLoss: 0.316290\tGrad Norm: 1.253430\tLR: 0.030000\n",
      "Train Epoch: 991 [106496/194182 (54%)]\tLoss: 0.321675\tGrad Norm: 1.184841\tLR: 0.030000\n",
      "Train Epoch: 991 [126976/194182 (65%)]\tLoss: 0.323637\tGrad Norm: 1.251684\tLR: 0.030000\n",
      "Train Epoch: 991 [147456/194182 (75%)]\tLoss: 0.328966\tGrad Norm: 1.626705\tLR: 0.030000\n",
      "Train Epoch: 991 [167936/194182 (85%)]\tLoss: 0.323172\tGrad Norm: 1.410221\tLR: 0.030000\n",
      "Train Epoch: 991 [188416/194182 (96%)]\tLoss: 0.308816\tGrad Norm: 1.086603\tLR: 0.030000\n",
      "Train set: Average loss: 0.3229\n",
      "Test set: Average loss: 0.2410, Average MAE: 0.3469\n",
      "Train Epoch: 992 [4096/194182 (2%)]\tLoss: 0.326204\tGrad Norm: 1.147361\tLR: 0.030000\n",
      "Train Epoch: 992 [24576/194182 (12%)]\tLoss: 0.318785\tGrad Norm: 1.482290\tLR: 0.030000\n",
      "Train Epoch: 992 [45056/194182 (23%)]\tLoss: 0.329695\tGrad Norm: 1.833526\tLR: 0.030000\n",
      "Train Epoch: 992 [65536/194182 (33%)]\tLoss: 0.321819\tGrad Norm: 1.196651\tLR: 0.030000\n",
      "Train Epoch: 992 [86016/194182 (44%)]\tLoss: 0.317709\tGrad Norm: 1.036142\tLR: 0.030000\n",
      "Train Epoch: 992 [106496/194182 (54%)]\tLoss: 0.318984\tGrad Norm: 1.105466\tLR: 0.030000\n",
      "Train Epoch: 992 [126976/194182 (65%)]\tLoss: 0.323656\tGrad Norm: 1.079721\tLR: 0.030000\n",
      "Train Epoch: 992 [147456/194182 (75%)]\tLoss: 0.330802\tGrad Norm: 1.159934\tLR: 0.030000\n",
      "Train Epoch: 992 [167936/194182 (85%)]\tLoss: 0.334421\tGrad Norm: 1.737919\tLR: 0.030000\n",
      "Train Epoch: 992 [188416/194182 (96%)]\tLoss: 0.326050\tGrad Norm: 1.347029\tLR: 0.030000\n",
      "Train set: Average loss: 0.3228\n",
      "Test set: Average loss: 0.2403, Average MAE: 0.3439\n",
      "Train Epoch: 993 [4096/194182 (2%)]\tLoss: 0.316928\tGrad Norm: 1.004226\tLR: 0.030000\n",
      "Train Epoch: 993 [24576/194182 (12%)]\tLoss: 0.318585\tGrad Norm: 1.138235\tLR: 0.030000\n",
      "Train Epoch: 993 [45056/194182 (23%)]\tLoss: 0.313063\tGrad Norm: 0.979662\tLR: 0.030000\n",
      "Train Epoch: 993 [65536/194182 (33%)]\tLoss: 0.321969\tGrad Norm: 1.438961\tLR: 0.030000\n",
      "Train Epoch: 993 [86016/194182 (44%)]\tLoss: 0.316024\tGrad Norm: 1.340316\tLR: 0.030000\n",
      "Train Epoch: 993 [106496/194182 (54%)]\tLoss: 0.325752\tGrad Norm: 1.298182\tLR: 0.030000\n",
      "Train Epoch: 993 [126976/194182 (65%)]\tLoss: 0.318065\tGrad Norm: 1.367658\tLR: 0.030000\n",
      "Train Epoch: 993 [147456/194182 (75%)]\tLoss: 0.325690\tGrad Norm: 1.150776\tLR: 0.030000\n",
      "Train Epoch: 993 [167936/194182 (85%)]\tLoss: 0.312397\tGrad Norm: 1.133418\tLR: 0.030000\n",
      "Train Epoch: 993 [188416/194182 (96%)]\tLoss: 0.325971\tGrad Norm: 1.232550\tLR: 0.030000\n",
      "Train set: Average loss: 0.3207\n",
      "Test set: Average loss: 0.2431, Average MAE: 0.3489\n",
      "Train Epoch: 994 [4096/194182 (2%)]\tLoss: 0.327610\tGrad Norm: 1.367002\tLR: 0.030000\n",
      "Train Epoch: 994 [24576/194182 (12%)]\tLoss: 0.330689\tGrad Norm: 1.602822\tLR: 0.030000\n",
      "Train Epoch: 994 [45056/194182 (23%)]\tLoss: 0.327893\tGrad Norm: 1.251436\tLR: 0.030000\n",
      "Train Epoch: 994 [65536/194182 (33%)]\tLoss: 0.317389\tGrad Norm: 1.121797\tLR: 0.030000\n",
      "Train Epoch: 994 [86016/194182 (44%)]\tLoss: 0.319982\tGrad Norm: 1.401133\tLR: 0.030000\n",
      "Train Epoch: 994 [106496/194182 (54%)]\tLoss: 0.329756\tGrad Norm: 1.446897\tLR: 0.030000\n",
      "Train Epoch: 994 [126976/194182 (65%)]\tLoss: 0.318689\tGrad Norm: 0.959528\tLR: 0.030000\n",
      "Train Epoch: 994 [147456/194182 (75%)]\tLoss: 0.316629\tGrad Norm: 1.175031\tLR: 0.030000\n",
      "Train Epoch: 994 [167936/194182 (85%)]\tLoss: 0.326168\tGrad Norm: 1.685696\tLR: 0.030000\n",
      "Train Epoch: 994 [188416/194182 (96%)]\tLoss: 0.325439\tGrad Norm: 1.762391\tLR: 0.030000\n",
      "Train set: Average loss: 0.3235\n",
      "Test set: Average loss: 0.2445, Average MAE: 0.3321\n",
      "Train Epoch: 995 [4096/194182 (2%)]\tLoss: 0.330061\tGrad Norm: 1.568827\tLR: 0.030000\n",
      "Train Epoch: 995 [24576/194182 (12%)]\tLoss: 0.322819\tGrad Norm: 1.588342\tLR: 0.030000\n",
      "Train Epoch: 995 [45056/194182 (23%)]\tLoss: 0.339423\tGrad Norm: 2.092247\tLR: 0.030000\n",
      "Train Epoch: 995 [65536/194182 (33%)]\tLoss: 0.331565\tGrad Norm: 1.467076\tLR: 0.030000\n",
      "Train Epoch: 995 [86016/194182 (44%)]\tLoss: 0.325916\tGrad Norm: 1.516744\tLR: 0.030000\n",
      "Train Epoch: 995 [106496/194182 (54%)]\tLoss: 0.320352\tGrad Norm: 1.482929\tLR: 0.030000\n",
      "Train Epoch: 995 [126976/194182 (65%)]\tLoss: 0.324124\tGrad Norm: 1.397236\tLR: 0.030000\n",
      "Train Epoch: 995 [147456/194182 (75%)]\tLoss: 0.323647\tGrad Norm: 1.309224\tLR: 0.030000\n",
      "Train Epoch: 995 [167936/194182 (85%)]\tLoss: 0.323707\tGrad Norm: 1.342052\tLR: 0.030000\n",
      "Train Epoch: 995 [188416/194182 (96%)]\tLoss: 0.318664\tGrad Norm: 1.379973\tLR: 0.030000\n",
      "Train set: Average loss: 0.3255\n",
      "Test set: Average loss: 0.2464, Average MAE: 0.3374\n",
      "Epoch 995: Mean reward = 0.051 +/- 0.038\n",
      "Train Epoch: 996 [4096/194182 (2%)]\tLoss: 0.330486\tGrad Norm: 1.578598\tLR: 0.030000\n",
      "Train Epoch: 996 [24576/194182 (12%)]\tLoss: 0.316186\tGrad Norm: 1.016702\tLR: 0.030000\n",
      "Train Epoch: 996 [45056/194182 (23%)]\tLoss: 0.322623\tGrad Norm: 1.133366\tLR: 0.030000\n",
      "Train Epoch: 996 [65536/194182 (33%)]\tLoss: 0.314009\tGrad Norm: 1.446747\tLR: 0.030000\n",
      "Train Epoch: 996 [86016/194182 (44%)]\tLoss: 0.323122\tGrad Norm: 1.342135\tLR: 0.030000\n",
      "Train Epoch: 996 [106496/194182 (54%)]\tLoss: 0.329477\tGrad Norm: 1.772374\tLR: 0.030000\n",
      "Train Epoch: 996 [126976/194182 (65%)]\tLoss: 0.330452\tGrad Norm: 1.614691\tLR: 0.030000\n",
      "Train Epoch: 996 [147456/194182 (75%)]\tLoss: 0.330374\tGrad Norm: 1.749215\tLR: 0.030000\n",
      "Train Epoch: 996 [167936/194182 (85%)]\tLoss: 0.325352\tGrad Norm: 1.568030\tLR: 0.030000\n",
      "Train Epoch: 996 [188416/194182 (96%)]\tLoss: 0.322076\tGrad Norm: 1.208585\tLR: 0.030000\n",
      "Train set: Average loss: 0.3236\n",
      "Test set: Average loss: 0.2444, Average MAE: 0.3322\n",
      "Train Epoch: 997 [4096/194182 (2%)]\tLoss: 0.324981\tGrad Norm: 1.494794\tLR: 0.030000\n",
      "Train Epoch: 997 [24576/194182 (12%)]\tLoss: 0.319820\tGrad Norm: 1.200571\tLR: 0.030000\n",
      "Train Epoch: 997 [45056/194182 (23%)]\tLoss: 0.325410\tGrad Norm: 1.420248\tLR: 0.030000\n",
      "Train Epoch: 997 [65536/194182 (33%)]\tLoss: 0.324136\tGrad Norm: 1.434623\tLR: 0.030000\n",
      "Train Epoch: 997 [86016/194182 (44%)]\tLoss: 0.322926\tGrad Norm: 1.315320\tLR: 0.030000\n",
      "Train Epoch: 997 [106496/194182 (54%)]\tLoss: 0.327839\tGrad Norm: 1.434341\tLR: 0.030000\n",
      "Train Epoch: 997 [126976/194182 (65%)]\tLoss: 0.326094\tGrad Norm: 1.516451\tLR: 0.030000\n",
      "Train Epoch: 997 [147456/194182 (75%)]\tLoss: 0.325331\tGrad Norm: 1.511337\tLR: 0.030000\n",
      "Train Epoch: 997 [167936/194182 (85%)]\tLoss: 0.320976\tGrad Norm: 1.530887\tLR: 0.030000\n",
      "Train Epoch: 997 [188416/194182 (96%)]\tLoss: 0.320803\tGrad Norm: 1.587856\tLR: 0.030000\n",
      "Train set: Average loss: 0.3237\n",
      "Test set: Average loss: 0.2422, Average MAE: 0.3503\n",
      "Train Epoch: 998 [4096/194182 (2%)]\tLoss: 0.330204\tGrad Norm: 1.421849\tLR: 0.030000\n",
      "Train Epoch: 998 [24576/194182 (12%)]\tLoss: 0.328127\tGrad Norm: 1.273990\tLR: 0.030000\n",
      "Train Epoch: 998 [45056/194182 (23%)]\tLoss: 0.318449\tGrad Norm: 0.990063\tLR: 0.030000\n",
      "Train Epoch: 998 [65536/194182 (33%)]\tLoss: 0.309937\tGrad Norm: 0.791983\tLR: 0.030000\n",
      "Train Epoch: 998 [86016/194182 (44%)]\tLoss: 0.318470\tGrad Norm: 1.342049\tLR: 0.030000\n",
      "Train Epoch: 998 [106496/194182 (54%)]\tLoss: 0.323082\tGrad Norm: 1.302872\tLR: 0.030000\n",
      "Train Epoch: 998 [126976/194182 (65%)]\tLoss: 0.317205\tGrad Norm: 1.206840\tLR: 0.030000\n",
      "Train Epoch: 998 [147456/194182 (75%)]\tLoss: 0.325476\tGrad Norm: 1.436184\tLR: 0.030000\n",
      "Train Epoch: 998 [167936/194182 (85%)]\tLoss: 0.334527\tGrad Norm: 1.363690\tLR: 0.030000\n",
      "Train Epoch: 998 [188416/194182 (96%)]\tLoss: 0.323304\tGrad Norm: 1.621598\tLR: 0.030000\n",
      "Train set: Average loss: 0.3211\n",
      "Test set: Average loss: 0.2523, Average MAE: 0.3648\n",
      "Train Epoch: 999 [4096/194182 (2%)]\tLoss: 0.324775\tGrad Norm: 1.897553\tLR: 0.030000\n",
      "Train Epoch: 999 [24576/194182 (12%)]\tLoss: 0.326496\tGrad Norm: 1.887205\tLR: 0.030000\n",
      "Train Epoch: 999 [45056/194182 (23%)]\tLoss: 0.330000\tGrad Norm: 1.373438\tLR: 0.030000\n",
      "Train Epoch: 999 [65536/194182 (33%)]\tLoss: 0.320522\tGrad Norm: 1.241415\tLR: 0.030000\n",
      "Train Epoch: 999 [86016/194182 (44%)]\tLoss: 0.320793\tGrad Norm: 1.498723\tLR: 0.030000\n",
      "Train Epoch: 999 [106496/194182 (54%)]\tLoss: 0.320009\tGrad Norm: 1.546551\tLR: 0.030000\n",
      "Train Epoch: 999 [126976/194182 (65%)]\tLoss: 0.325732\tGrad Norm: 1.339147\tLR: 0.030000\n",
      "Train Epoch: 999 [147456/194182 (75%)]\tLoss: 0.313841\tGrad Norm: 1.360236\tLR: 0.030000\n",
      "Train Epoch: 999 [167936/194182 (85%)]\tLoss: 0.331428\tGrad Norm: 1.712988\tLR: 0.030000\n",
      "Train Epoch: 999 [188416/194182 (96%)]\tLoss: 0.335196\tGrad Norm: 1.588329\tLR: 0.030000\n",
      "Train set: Average loss: 0.3237\n",
      "Test set: Average loss: 0.2466, Average MAE: 0.3474\n",
      "Train Epoch: 1000 [4096/194182 (2%)]\tLoss: 0.326256\tGrad Norm: 1.699903\tLR: 0.030000\n",
      "Train Epoch: 1000 [24576/194182 (12%)]\tLoss: 0.326478\tGrad Norm: 1.500750\tLR: 0.030000\n",
      "Train Epoch: 1000 [45056/194182 (23%)]\tLoss: 0.319247\tGrad Norm: 1.210268\tLR: 0.030000\n",
      "Train Epoch: 1000 [65536/194182 (33%)]\tLoss: 0.317047\tGrad Norm: 1.310534\tLR: 0.030000\n",
      "Train Epoch: 1000 [86016/194182 (44%)]\tLoss: 0.327018\tGrad Norm: 1.682760\tLR: 0.030000\n",
      "Train Epoch: 1000 [106496/194182 (54%)]\tLoss: 0.330998\tGrad Norm: 1.525106\tLR: 0.030000\n",
      "Train Epoch: 1000 [126976/194182 (65%)]\tLoss: 0.327508\tGrad Norm: 1.578988\tLR: 0.030000\n",
      "Train Epoch: 1000 [147456/194182 (75%)]\tLoss: 0.322433\tGrad Norm: 1.229382\tLR: 0.030000\n",
      "Train Epoch: 1000 [167936/194182 (85%)]\tLoss: 0.311072\tGrad Norm: 0.918831\tLR: 0.030000\n",
      "Train Epoch: 1000 [188416/194182 (96%)]\tLoss: 0.315839\tGrad Norm: 1.175603\tLR: 0.030000\n",
      "Train set: Average loss: 0.3223\n",
      "Test set: Average loss: 0.2373, Average MAE: 0.3354\n",
      "Epoch 1000: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 1001 [4096/194182 (2%)]\tLoss: 0.316384\tGrad Norm: 0.973549\tLR: 0.030000\n",
      "Train Epoch: 1001 [24576/194182 (12%)]\tLoss: 0.322423\tGrad Norm: 1.113102\tLR: 0.030000\n",
      "Train Epoch: 1001 [45056/194182 (23%)]\tLoss: 0.322221\tGrad Norm: 1.491501\tLR: 0.030000\n",
      "Train Epoch: 1001 [65536/194182 (33%)]\tLoss: 0.326860\tGrad Norm: 1.522221\tLR: 0.030000\n",
      "Train Epoch: 1001 [86016/194182 (44%)]\tLoss: 0.312102\tGrad Norm: 1.255914\tLR: 0.030000\n",
      "Train Epoch: 1001 [106496/194182 (54%)]\tLoss: 0.321875\tGrad Norm: 1.359867\tLR: 0.030000\n",
      "Train Epoch: 1001 [126976/194182 (65%)]\tLoss: 0.326049\tGrad Norm: 1.317522\tLR: 0.030000\n",
      "Train Epoch: 1001 [147456/194182 (75%)]\tLoss: 0.320277\tGrad Norm: 1.185555\tLR: 0.030000\n",
      "Train Epoch: 1001 [167936/194182 (85%)]\tLoss: 0.322770\tGrad Norm: 1.860402\tLR: 0.030000\n",
      "Train Epoch: 1001 [188416/194182 (96%)]\tLoss: 0.329679\tGrad Norm: 1.701101\tLR: 0.030000\n",
      "Train set: Average loss: 0.3220\n",
      "Test set: Average loss: 0.2487, Average MAE: 0.3561\n",
      "Train Epoch: 1002 [4096/194182 (2%)]\tLoss: 0.324323\tGrad Norm: 1.575934\tLR: 0.030000\n",
      "Train Epoch: 1002 [24576/194182 (12%)]\tLoss: 0.315890\tGrad Norm: 1.331203\tLR: 0.030000\n",
      "Train Epoch: 1002 [45056/194182 (23%)]\tLoss: 0.319456\tGrad Norm: 1.430580\tLR: 0.030000\n",
      "Train Epoch: 1002 [65536/194182 (33%)]\tLoss: 0.313298\tGrad Norm: 1.192438\tLR: 0.030000\n",
      "Train Epoch: 1002 [86016/194182 (44%)]\tLoss: 0.321122\tGrad Norm: 1.269121\tLR: 0.030000\n",
      "Train Epoch: 1002 [106496/194182 (54%)]\tLoss: 0.318333\tGrad Norm: 1.396781\tLR: 0.030000\n",
      "Train Epoch: 1002 [126976/194182 (65%)]\tLoss: 0.325513\tGrad Norm: 1.429079\tLR: 0.030000\n",
      "Train Epoch: 1002 [147456/194182 (75%)]\tLoss: 0.320735\tGrad Norm: 1.422927\tLR: 0.030000\n",
      "Train Epoch: 1002 [167936/194182 (85%)]\tLoss: 0.315582\tGrad Norm: 1.056551\tLR: 0.030000\n",
      "Train Epoch: 1002 [188416/194182 (96%)]\tLoss: 0.314498\tGrad Norm: 1.159310\tLR: 0.030000\n",
      "Train set: Average loss: 0.3215\n",
      "Test set: Average loss: 0.2430, Average MAE: 0.3507\n",
      "Train Epoch: 1003 [4096/194182 (2%)]\tLoss: 0.325444\tGrad Norm: 1.334645\tLR: 0.030000\n",
      "Train Epoch: 1003 [24576/194182 (12%)]\tLoss: 0.320841\tGrad Norm: 1.333708\tLR: 0.030000\n",
      "Train Epoch: 1003 [45056/194182 (23%)]\tLoss: 0.316627\tGrad Norm: 1.330091\tLR: 0.030000\n",
      "Train Epoch: 1003 [65536/194182 (33%)]\tLoss: 0.320027\tGrad Norm: 1.308117\tLR: 0.030000\n",
      "Train Epoch: 1003 [86016/194182 (44%)]\tLoss: 0.322701\tGrad Norm: 1.485273\tLR: 0.030000\n",
      "Train Epoch: 1003 [106496/194182 (54%)]\tLoss: 0.318056\tGrad Norm: 1.403210\tLR: 0.030000\n",
      "Train Epoch: 1003 [126976/194182 (65%)]\tLoss: 0.319623\tGrad Norm: 1.339493\tLR: 0.030000\n",
      "Train Epoch: 1003 [147456/194182 (75%)]\tLoss: 0.326571\tGrad Norm: 1.404045\tLR: 0.030000\n",
      "Train Epoch: 1003 [167936/194182 (85%)]\tLoss: 0.322646\tGrad Norm: 1.600146\tLR: 0.030000\n",
      "Train Epoch: 1003 [188416/194182 (96%)]\tLoss: 0.326392\tGrad Norm: 1.765792\tLR: 0.030000\n",
      "Train set: Average loss: 0.3221\n",
      "Test set: Average loss: 0.2556, Average MAE: 0.3667\n",
      "Train Epoch: 1004 [4096/194182 (2%)]\tLoss: 0.327868\tGrad Norm: 1.887981\tLR: 0.030000\n",
      "Train Epoch: 1004 [24576/194182 (12%)]\tLoss: 0.323230\tGrad Norm: 1.438466\tLR: 0.030000\n",
      "Train Epoch: 1004 [45056/194182 (23%)]\tLoss: 0.325090\tGrad Norm: 1.427101\tLR: 0.030000\n",
      "Train Epoch: 1004 [65536/194182 (33%)]\tLoss: 0.305853\tGrad Norm: 0.986019\tLR: 0.030000\n",
      "Train Epoch: 1004 [86016/194182 (44%)]\tLoss: 0.316915\tGrad Norm: 0.942339\tLR: 0.030000\n",
      "Train Epoch: 1004 [106496/194182 (54%)]\tLoss: 0.317251\tGrad Norm: 1.251096\tLR: 0.030000\n",
      "Train Epoch: 1004 [126976/194182 (65%)]\tLoss: 0.322816\tGrad Norm: 1.340630\tLR: 0.030000\n",
      "Train Epoch: 1004 [147456/194182 (75%)]\tLoss: 0.313471\tGrad Norm: 1.180885\tLR: 0.030000\n",
      "Train Epoch: 1004 [167936/194182 (85%)]\tLoss: 0.322996\tGrad Norm: 1.505256\tLR: 0.030000\n",
      "Train Epoch: 1004 [188416/194182 (96%)]\tLoss: 0.326041\tGrad Norm: 1.447618\tLR: 0.030000\n",
      "Train set: Average loss: 0.3199\n",
      "Test set: Average loss: 0.2428, Average MAE: 0.3370\n",
      "Train Epoch: 1005 [4096/194182 (2%)]\tLoss: 0.332215\tGrad Norm: 1.702011\tLR: 0.030000\n",
      "Train Epoch: 1005 [24576/194182 (12%)]\tLoss: 0.318459\tGrad Norm: 1.362026\tLR: 0.030000\n",
      "Train Epoch: 1005 [45056/194182 (23%)]\tLoss: 0.324426\tGrad Norm: 1.729027\tLR: 0.030000\n",
      "Train Epoch: 1005 [65536/194182 (33%)]\tLoss: 0.324667\tGrad Norm: 1.483285\tLR: 0.030000\n",
      "Train Epoch: 1005 [86016/194182 (44%)]\tLoss: 0.330759\tGrad Norm: 1.609994\tLR: 0.030000\n",
      "Train Epoch: 1005 [106496/194182 (54%)]\tLoss: 0.321733\tGrad Norm: 1.468893\tLR: 0.030000\n",
      "Train Epoch: 1005 [126976/194182 (65%)]\tLoss: 0.310771\tGrad Norm: 0.724271\tLR: 0.030000\n",
      "Train Epoch: 1005 [147456/194182 (75%)]\tLoss: 0.304690\tGrad Norm: 0.589002\tLR: 0.030000\n",
      "Train Epoch: 1005 [167936/194182 (85%)]\tLoss: 0.323394\tGrad Norm: 1.083361\tLR: 0.030000\n",
      "Train Epoch: 1005 [188416/194182 (96%)]\tLoss: 0.322930\tGrad Norm: 1.577812\tLR: 0.030000\n",
      "Train set: Average loss: 0.3203\n",
      "Test set: Average loss: 0.2492, Average MAE: 0.3537\n",
      "Epoch 1005: Mean reward = 0.049 +/- 0.043\n",
      "Train Epoch: 1006 [4096/194182 (2%)]\tLoss: 0.323444\tGrad Norm: 1.618629\tLR: 0.030000\n",
      "Train Epoch: 1006 [24576/194182 (12%)]\tLoss: 0.325022\tGrad Norm: 1.446863\tLR: 0.030000\n",
      "Train Epoch: 1006 [45056/194182 (23%)]\tLoss: 0.322302\tGrad Norm: 1.557316\tLR: 0.030000\n",
      "Train Epoch: 1006 [65536/194182 (33%)]\tLoss: 0.330019\tGrad Norm: 1.474207\tLR: 0.030000\n",
      "Train Epoch: 1006 [86016/194182 (44%)]\tLoss: 0.315143\tGrad Norm: 1.020170\tLR: 0.030000\n",
      "Train Epoch: 1006 [106496/194182 (54%)]\tLoss: 0.316598\tGrad Norm: 1.076296\tLR: 0.030000\n",
      "Train Epoch: 1006 [126976/194182 (65%)]\tLoss: 0.319937\tGrad Norm: 1.238716\tLR: 0.030000\n",
      "Train Epoch: 1006 [147456/194182 (75%)]\tLoss: 0.318961\tGrad Norm: 1.233721\tLR: 0.030000\n",
      "Train Epoch: 1006 [167936/194182 (85%)]\tLoss: 0.319003\tGrad Norm: 1.432393\tLR: 0.030000\n",
      "Train Epoch: 1006 [188416/194182 (96%)]\tLoss: 0.320263\tGrad Norm: 1.464242\tLR: 0.030000\n",
      "Train set: Average loss: 0.3209\n",
      "Test set: Average loss: 0.2452, Average MAE: 0.3449\n",
      "Train Epoch: 1007 [4096/194182 (2%)]\tLoss: 0.327206\tGrad Norm: 1.396082\tLR: 0.030000\n",
      "Train Epoch: 1007 [24576/194182 (12%)]\tLoss: 0.321453\tGrad Norm: 1.356179\tLR: 0.030000\n",
      "Train Epoch: 1007 [45056/194182 (23%)]\tLoss: 0.316598\tGrad Norm: 1.468678\tLR: 0.030000\n",
      "Train Epoch: 1007 [65536/194182 (33%)]\tLoss: 0.318803\tGrad Norm: 1.418072\tLR: 0.030000\n",
      "Train Epoch: 1007 [86016/194182 (44%)]\tLoss: 0.313037\tGrad Norm: 1.290497\tLR: 0.030000\n",
      "Train Epoch: 1007 [106496/194182 (54%)]\tLoss: 0.314459\tGrad Norm: 1.518690\tLR: 0.030000\n",
      "Train Epoch: 1007 [126976/194182 (65%)]\tLoss: 0.314443\tGrad Norm: 1.488622\tLR: 0.030000\n",
      "Train Epoch: 1007 [147456/194182 (75%)]\tLoss: 0.326337\tGrad Norm: 1.712109\tLR: 0.030000\n",
      "Train Epoch: 1007 [167936/194182 (85%)]\tLoss: 0.315370\tGrad Norm: 1.429685\tLR: 0.030000\n",
      "Train Epoch: 1007 [188416/194182 (96%)]\tLoss: 0.319064\tGrad Norm: 1.132213\tLR: 0.030000\n",
      "Train set: Average loss: 0.3216\n",
      "Test set: Average loss: 0.2398, Average MAE: 0.3388\n",
      "Train Epoch: 1008 [4096/194182 (2%)]\tLoss: 0.317016\tGrad Norm: 1.106480\tLR: 0.030000\n",
      "Train Epoch: 1008 [24576/194182 (12%)]\tLoss: 0.313450\tGrad Norm: 0.869817\tLR: 0.030000\n",
      "Train Epoch: 1008 [45056/194182 (23%)]\tLoss: 0.326263\tGrad Norm: 1.384046\tLR: 0.030000\n",
      "Train Epoch: 1008 [65536/194182 (33%)]\tLoss: 0.318205\tGrad Norm: 1.205474\tLR: 0.030000\n",
      "Train Epoch: 1008 [86016/194182 (44%)]\tLoss: 0.311488\tGrad Norm: 1.136050\tLR: 0.030000\n",
      "Train Epoch: 1008 [106496/194182 (54%)]\tLoss: 0.312918\tGrad Norm: 1.402182\tLR: 0.030000\n",
      "Train Epoch: 1008 [126976/194182 (65%)]\tLoss: 0.318503\tGrad Norm: 1.362122\tLR: 0.030000\n",
      "Train Epoch: 1008 [147456/194182 (75%)]\tLoss: 0.325841\tGrad Norm: 1.160180\tLR: 0.030000\n",
      "Train Epoch: 1008 [167936/194182 (85%)]\tLoss: 0.319641\tGrad Norm: 1.690000\tLR: 0.030000\n",
      "Train Epoch: 1008 [188416/194182 (96%)]\tLoss: 0.316862\tGrad Norm: 1.328834\tLR: 0.030000\n",
      "Train set: Average loss: 0.3191\n",
      "Test set: Average loss: 0.2459, Average MAE: 0.3340\n",
      "Train Epoch: 1009 [4096/194182 (2%)]\tLoss: 0.320202\tGrad Norm: 1.528404\tLR: 0.030000\n",
      "Train Epoch: 1009 [24576/194182 (12%)]\tLoss: 0.318752\tGrad Norm: 1.271831\tLR: 0.030000\n",
      "Train Epoch: 1009 [45056/194182 (23%)]\tLoss: 0.323089\tGrad Norm: 1.328702\tLR: 0.030000\n",
      "Train Epoch: 1009 [65536/194182 (33%)]\tLoss: 0.315505\tGrad Norm: 1.308838\tLR: 0.030000\n",
      "Train Epoch: 1009 [86016/194182 (44%)]\tLoss: 0.320286\tGrad Norm: 1.481148\tLR: 0.030000\n",
      "Train Epoch: 1009 [106496/194182 (54%)]\tLoss: 0.323323\tGrad Norm: 1.140713\tLR: 0.030000\n",
      "Train Epoch: 1009 [126976/194182 (65%)]\tLoss: 0.326840\tGrad Norm: 1.782900\tLR: 0.030000\n",
      "Train Epoch: 1009 [147456/194182 (75%)]\tLoss: 0.319474\tGrad Norm: 1.418510\tLR: 0.030000\n",
      "Train Epoch: 1009 [167936/194182 (85%)]\tLoss: 0.324091\tGrad Norm: 1.493983\tLR: 0.030000\n",
      "Train Epoch: 1009 [188416/194182 (96%)]\tLoss: 0.326298\tGrad Norm: 1.443953\tLR: 0.030000\n",
      "Train set: Average loss: 0.3202\n",
      "Test set: Average loss: 0.2390, Average MAE: 0.3324\n",
      "Train Epoch: 1010 [4096/194182 (2%)]\tLoss: 0.318156\tGrad Norm: 1.101030\tLR: 0.030000\n",
      "Train Epoch: 1010 [24576/194182 (12%)]\tLoss: 0.322606\tGrad Norm: 1.382562\tLR: 0.030000\n",
      "Train Epoch: 1010 [45056/194182 (23%)]\tLoss: 0.321844\tGrad Norm: 1.462643\tLR: 0.030000\n",
      "Train Epoch: 1010 [65536/194182 (33%)]\tLoss: 0.326385\tGrad Norm: 1.716970\tLR: 0.030000\n",
      "Train Epoch: 1010 [86016/194182 (44%)]\tLoss: 0.309097\tGrad Norm: 0.991990\tLR: 0.030000\n",
      "Train Epoch: 1010 [106496/194182 (54%)]\tLoss: 0.317714\tGrad Norm: 1.153370\tLR: 0.030000\n",
      "Train Epoch: 1010 [126976/194182 (65%)]\tLoss: 0.314483\tGrad Norm: 1.171281\tLR: 0.030000\n",
      "Train Epoch: 1010 [147456/194182 (75%)]\tLoss: 0.329090\tGrad Norm: 1.640575\tLR: 0.030000\n",
      "Train Epoch: 1010 [167936/194182 (85%)]\tLoss: 0.324692\tGrad Norm: 1.181265\tLR: 0.030000\n",
      "Train Epoch: 1010 [188416/194182 (96%)]\tLoss: 0.318425\tGrad Norm: 1.773551\tLR: 0.030000\n",
      "Train set: Average loss: 0.3197\n",
      "Test set: Average loss: 0.2401, Average MAE: 0.3311\n",
      "Epoch 1010: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 1011 [4096/194182 (2%)]\tLoss: 0.314831\tGrad Norm: 1.385632\tLR: 0.030000\n",
      "Train Epoch: 1011 [24576/194182 (12%)]\tLoss: 0.315709\tGrad Norm: 1.432744\tLR: 0.030000\n",
      "Train Epoch: 1011 [45056/194182 (23%)]\tLoss: 0.324475\tGrad Norm: 1.729793\tLR: 0.030000\n",
      "Train Epoch: 1011 [65536/194182 (33%)]\tLoss: 0.322502\tGrad Norm: 1.506185\tLR: 0.030000\n",
      "Train Epoch: 1011 [86016/194182 (44%)]\tLoss: 0.319926\tGrad Norm: 1.177439\tLR: 0.030000\n",
      "Train Epoch: 1011 [106496/194182 (54%)]\tLoss: 0.313821\tGrad Norm: 1.155793\tLR: 0.030000\n",
      "Train Epoch: 1011 [126976/194182 (65%)]\tLoss: 0.320246\tGrad Norm: 1.321395\tLR: 0.030000\n",
      "Train Epoch: 1011 [147456/194182 (75%)]\tLoss: 0.318215\tGrad Norm: 1.554423\tLR: 0.030000\n",
      "Train Epoch: 1011 [167936/194182 (85%)]\tLoss: 0.324406\tGrad Norm: 1.509347\tLR: 0.030000\n",
      "Train Epoch: 1011 [188416/194182 (96%)]\tLoss: 0.312742\tGrad Norm: 1.269865\tLR: 0.030000\n",
      "Train set: Average loss: 0.3195\n",
      "Test set: Average loss: 0.2391, Average MAE: 0.3345\n",
      "Train Epoch: 1012 [4096/194182 (2%)]\tLoss: 0.316543\tGrad Norm: 1.109741\tLR: 0.030000\n",
      "Train Epoch: 1012 [24576/194182 (12%)]\tLoss: 0.313977\tGrad Norm: 0.981181\tLR: 0.030000\n",
      "Train Epoch: 1012 [45056/194182 (23%)]\tLoss: 0.313522\tGrad Norm: 1.298109\tLR: 0.030000\n",
      "Train Epoch: 1012 [65536/194182 (33%)]\tLoss: 0.316932\tGrad Norm: 1.423769\tLR: 0.030000\n",
      "Train Epoch: 1012 [86016/194182 (44%)]\tLoss: 0.321707\tGrad Norm: 1.456482\tLR: 0.030000\n",
      "Train Epoch: 1012 [106496/194182 (54%)]\tLoss: 0.320626\tGrad Norm: 1.323011\tLR: 0.030000\n",
      "Train Epoch: 1012 [126976/194182 (65%)]\tLoss: 0.315194\tGrad Norm: 1.305686\tLR: 0.030000\n",
      "Train Epoch: 1012 [147456/194182 (75%)]\tLoss: 0.317504\tGrad Norm: 1.307008\tLR: 0.030000\n",
      "Train Epoch: 1012 [167936/194182 (85%)]\tLoss: 0.321378\tGrad Norm: 1.075701\tLR: 0.030000\n",
      "Train Epoch: 1012 [188416/194182 (96%)]\tLoss: 0.321980\tGrad Norm: 1.597268\tLR: 0.030000\n",
      "Train set: Average loss: 0.3189\n",
      "Test set: Average loss: 0.2446, Average MAE: 0.3518\n",
      "Train Epoch: 1013 [4096/194182 (2%)]\tLoss: 0.319086\tGrad Norm: 1.403122\tLR: 0.030000\n",
      "Train Epoch: 1013 [24576/194182 (12%)]\tLoss: 0.329496\tGrad Norm: 1.799334\tLR: 0.030000\n",
      "Train Epoch: 1013 [45056/194182 (23%)]\tLoss: 0.317393\tGrad Norm: 1.588702\tLR: 0.030000\n",
      "Train Epoch: 1013 [65536/194182 (33%)]\tLoss: 0.315260\tGrad Norm: 1.364110\tLR: 0.030000\n",
      "Train Epoch: 1013 [86016/194182 (44%)]\tLoss: 0.318671\tGrad Norm: 1.384489\tLR: 0.030000\n",
      "Train Epoch: 1013 [106496/194182 (54%)]\tLoss: 0.316098\tGrad Norm: 1.260149\tLR: 0.030000\n",
      "Train Epoch: 1013 [126976/194182 (65%)]\tLoss: 0.328213\tGrad Norm: 1.560629\tLR: 0.030000\n",
      "Train Epoch: 1013 [147456/194182 (75%)]\tLoss: 0.318296\tGrad Norm: 1.437417\tLR: 0.030000\n",
      "Train Epoch: 1013 [167936/194182 (85%)]\tLoss: 0.317624\tGrad Norm: 1.065762\tLR: 0.030000\n",
      "Train Epoch: 1013 [188416/194182 (96%)]\tLoss: 0.313953\tGrad Norm: 1.350321\tLR: 0.030000\n",
      "Train set: Average loss: 0.3197\n",
      "Test set: Average loss: 0.2402, Average MAE: 0.3449\n",
      "Train Epoch: 1014 [4096/194182 (2%)]\tLoss: 0.308137\tGrad Norm: 1.358610\tLR: 0.030000\n",
      "Train Epoch: 1014 [24576/194182 (12%)]\tLoss: 0.316066\tGrad Norm: 1.428341\tLR: 0.030000\n",
      "Train Epoch: 1014 [45056/194182 (23%)]\tLoss: 0.324267\tGrad Norm: 1.499027\tLR: 0.030000\n",
      "Train Epoch: 1014 [65536/194182 (33%)]\tLoss: 0.309195\tGrad Norm: 1.288118\tLR: 0.030000\n",
      "Train Epoch: 1014 [86016/194182 (44%)]\tLoss: 0.320342\tGrad Norm: 1.686185\tLR: 0.030000\n",
      "Train Epoch: 1014 [106496/194182 (54%)]\tLoss: 0.321834\tGrad Norm: 1.366696\tLR: 0.030000\n",
      "Train Epoch: 1014 [126976/194182 (65%)]\tLoss: 0.306346\tGrad Norm: 1.145888\tLR: 0.030000\n",
      "Train Epoch: 1014 [147456/194182 (75%)]\tLoss: 0.318199\tGrad Norm: 1.414506\tLR: 0.030000\n",
      "Train Epoch: 1014 [167936/194182 (85%)]\tLoss: 0.318645\tGrad Norm: 1.483145\tLR: 0.030000\n",
      "Train Epoch: 1014 [188416/194182 (96%)]\tLoss: 0.317632\tGrad Norm: 1.212168\tLR: 0.030000\n",
      "Train set: Average loss: 0.3191\n",
      "Test set: Average loss: 0.2404, Average MAE: 0.3338\n",
      "Train Epoch: 1015 [4096/194182 (2%)]\tLoss: 0.315129\tGrad Norm: 1.106735\tLR: 0.030000\n",
      "Train Epoch: 1015 [24576/194182 (12%)]\tLoss: 0.308454\tGrad Norm: 0.906877\tLR: 0.030000\n",
      "Train Epoch: 1015 [45056/194182 (23%)]\tLoss: 0.319861\tGrad Norm: 1.258605\tLR: 0.030000\n",
      "Train Epoch: 1015 [65536/194182 (33%)]\tLoss: 0.317381\tGrad Norm: 1.215479\tLR: 0.030000\n",
      "Train Epoch: 1015 [86016/194182 (44%)]\tLoss: 0.319878\tGrad Norm: 1.721783\tLR: 0.030000\n",
      "Train Epoch: 1015 [106496/194182 (54%)]\tLoss: 0.320925\tGrad Norm: 1.546715\tLR: 0.030000\n",
      "Train Epoch: 1015 [126976/194182 (65%)]\tLoss: 0.325599\tGrad Norm: 1.587636\tLR: 0.030000\n",
      "Train Epoch: 1015 [147456/194182 (75%)]\tLoss: 0.320083\tGrad Norm: 1.308185\tLR: 0.030000\n",
      "Train Epoch: 1015 [167936/194182 (85%)]\tLoss: 0.310648\tGrad Norm: 1.117057\tLR: 0.030000\n",
      "Train Epoch: 1015 [188416/194182 (96%)]\tLoss: 0.319126\tGrad Norm: 1.170898\tLR: 0.030000\n",
      "Train set: Average loss: 0.3173\n",
      "Test set: Average loss: 0.2439, Average MAE: 0.3475\n",
      "Epoch 1015: Mean reward = 0.037 +/- 0.019\n",
      "Train Epoch: 1016 [4096/194182 (2%)]\tLoss: 0.327225\tGrad Norm: 1.343395\tLR: 0.030000\n",
      "Train Epoch: 1016 [24576/194182 (12%)]\tLoss: 0.313597\tGrad Norm: 1.446925\tLR: 0.030000\n",
      "Train Epoch: 1016 [45056/194182 (23%)]\tLoss: 0.314681\tGrad Norm: 1.462315\tLR: 0.030000\n",
      "Train Epoch: 1016 [65536/194182 (33%)]\tLoss: 0.329223\tGrad Norm: 1.857504\tLR: 0.030000\n",
      "Train Epoch: 1016 [86016/194182 (44%)]\tLoss: 0.328310\tGrad Norm: 1.823108\tLR: 0.030000\n",
      "Train Epoch: 1016 [106496/194182 (54%)]\tLoss: 0.326773\tGrad Norm: 1.475305\tLR: 0.030000\n",
      "Train Epoch: 1016 [126976/194182 (65%)]\tLoss: 0.311599\tGrad Norm: 1.486433\tLR: 0.030000\n",
      "Train Epoch: 1016 [147456/194182 (75%)]\tLoss: 0.314573\tGrad Norm: 1.300033\tLR: 0.030000\n",
      "Train Epoch: 1016 [167936/194182 (85%)]\tLoss: 0.309485\tGrad Norm: 0.902584\tLR: 0.030000\n",
      "Train Epoch: 1016 [188416/194182 (96%)]\tLoss: 0.312372\tGrad Norm: 1.021635\tLR: 0.030000\n",
      "Train set: Average loss: 0.3184\n",
      "Test set: Average loss: 0.2395, Average MAE: 0.3447\n",
      "Train Epoch: 1017 [4096/194182 (2%)]\tLoss: 0.317914\tGrad Norm: 1.330217\tLR: 0.030000\n",
      "Train Epoch: 1017 [24576/194182 (12%)]\tLoss: 0.312984\tGrad Norm: 1.314803\tLR: 0.030000\n",
      "Train Epoch: 1017 [45056/194182 (23%)]\tLoss: 0.311729\tGrad Norm: 1.459199\tLR: 0.030000\n",
      "Train Epoch: 1017 [65536/194182 (33%)]\tLoss: 0.328166\tGrad Norm: 1.718849\tLR: 0.030000\n",
      "Train Epoch: 1017 [86016/194182 (44%)]\tLoss: 0.320906\tGrad Norm: 1.353856\tLR: 0.030000\n",
      "Train Epoch: 1017 [106496/194182 (54%)]\tLoss: 0.311084\tGrad Norm: 1.226317\tLR: 0.030000\n",
      "Train Epoch: 1017 [126976/194182 (65%)]\tLoss: 0.315049\tGrad Norm: 1.303688\tLR: 0.030000\n",
      "Train Epoch: 1017 [147456/194182 (75%)]\tLoss: 0.316154\tGrad Norm: 1.479936\tLR: 0.030000\n",
      "Train Epoch: 1017 [167936/194182 (85%)]\tLoss: 0.322963\tGrad Norm: 1.174954\tLR: 0.030000\n",
      "Train Epoch: 1017 [188416/194182 (96%)]\tLoss: 0.319127\tGrad Norm: 1.485920\tLR: 0.030000\n",
      "Train set: Average loss: 0.3186\n",
      "Test set: Average loss: 0.2424, Average MAE: 0.3383\n",
      "Train Epoch: 1018 [4096/194182 (2%)]\tLoss: 0.309948\tGrad Norm: 1.269731\tLR: 0.030000\n",
      "Train Epoch: 1018 [24576/194182 (12%)]\tLoss: 0.318109\tGrad Norm: 1.452204\tLR: 0.030000\n",
      "Train Epoch: 1018 [45056/194182 (23%)]\tLoss: 0.321105\tGrad Norm: 1.331467\tLR: 0.030000\n",
      "Train Epoch: 1018 [65536/194182 (33%)]\tLoss: 0.311937\tGrad Norm: 1.468412\tLR: 0.030000\n",
      "Train Epoch: 1018 [86016/194182 (44%)]\tLoss: 0.321870\tGrad Norm: 1.477848\tLR: 0.030000\n",
      "Train Epoch: 1018 [106496/194182 (54%)]\tLoss: 0.322343\tGrad Norm: 1.363839\tLR: 0.030000\n",
      "Train Epoch: 1018 [126976/194182 (65%)]\tLoss: 0.313601\tGrad Norm: 1.399315\tLR: 0.030000\n",
      "Train Epoch: 1018 [147456/194182 (75%)]\tLoss: 0.318951\tGrad Norm: 1.086897\tLR: 0.030000\n",
      "Train Epoch: 1018 [167936/194182 (85%)]\tLoss: 0.307679\tGrad Norm: 1.044122\tLR: 0.030000\n",
      "Train Epoch: 1018 [188416/194182 (96%)]\tLoss: 0.308720\tGrad Norm: 0.919815\tLR: 0.030000\n",
      "Train set: Average loss: 0.3172\n",
      "Test set: Average loss: 0.2487, Average MAE: 0.3594\n",
      "Train Epoch: 1019 [4096/194182 (2%)]\tLoss: 0.324185\tGrad Norm: 1.799245\tLR: 0.030000\n",
      "Train Epoch: 1019 [24576/194182 (12%)]\tLoss: 0.327806\tGrad Norm: 1.869689\tLR: 0.030000\n",
      "Train Epoch: 1019 [45056/194182 (23%)]\tLoss: 0.323834\tGrad Norm: 1.443091\tLR: 0.030000\n",
      "Train Epoch: 1019 [65536/194182 (33%)]\tLoss: 0.319621\tGrad Norm: 1.330415\tLR: 0.030000\n",
      "Train Epoch: 1019 [86016/194182 (44%)]\tLoss: 0.323427\tGrad Norm: 1.613747\tLR: 0.030000\n",
      "Train Epoch: 1019 [106496/194182 (54%)]\tLoss: 0.320629\tGrad Norm: 1.397811\tLR: 0.030000\n",
      "Train Epoch: 1019 [126976/194182 (65%)]\tLoss: 0.322943\tGrad Norm: 1.580268\tLR: 0.030000\n",
      "Train Epoch: 1019 [147456/194182 (75%)]\tLoss: 0.320862\tGrad Norm: 1.701598\tLR: 0.030000\n",
      "Train Epoch: 1019 [167936/194182 (85%)]\tLoss: 0.309840\tGrad Norm: 0.922424\tLR: 0.030000\n",
      "Train Epoch: 1019 [188416/194182 (96%)]\tLoss: 0.312405\tGrad Norm: 1.290833\tLR: 0.030000\n",
      "Train set: Average loss: 0.3195\n",
      "Test set: Average loss: 0.2406, Average MAE: 0.3324\n",
      "Train Epoch: 1020 [4096/194182 (2%)]\tLoss: 0.318141\tGrad Norm: 1.573991\tLR: 0.030000\n",
      "Train Epoch: 1020 [24576/194182 (12%)]\tLoss: 0.314681\tGrad Norm: 1.320974\tLR: 0.030000\n",
      "Train Epoch: 1020 [45056/194182 (23%)]\tLoss: 0.308044\tGrad Norm: 1.079940\tLR: 0.030000\n",
      "Train Epoch: 1020 [65536/194182 (33%)]\tLoss: 0.310133\tGrad Norm: 1.101063\tLR: 0.030000\n",
      "Train Epoch: 1020 [86016/194182 (44%)]\tLoss: 0.317623\tGrad Norm: 1.375832\tLR: 0.030000\n",
      "Train Epoch: 1020 [106496/194182 (54%)]\tLoss: 0.312637\tGrad Norm: 1.165814\tLR: 0.030000\n",
      "Train Epoch: 1020 [126976/194182 (65%)]\tLoss: 0.317121\tGrad Norm: 1.446933\tLR: 0.030000\n",
      "Train Epoch: 1020 [147456/194182 (75%)]\tLoss: 0.321691\tGrad Norm: 1.465835\tLR: 0.030000\n",
      "Train Epoch: 1020 [167936/194182 (85%)]\tLoss: 0.317508\tGrad Norm: 1.479690\tLR: 0.030000\n",
      "Train Epoch: 1020 [188416/194182 (96%)]\tLoss: 0.320605\tGrad Norm: 1.290712\tLR: 0.030000\n",
      "Train set: Average loss: 0.3164\n",
      "Test set: Average loss: 0.2436, Average MAE: 0.3460\n",
      "Epoch 1020: Mean reward = 0.049 +/- 0.044\n",
      "Train Epoch: 1021 [4096/194182 (2%)]\tLoss: 0.315982\tGrad Norm: 1.306949\tLR: 0.030000\n",
      "Train Epoch: 1021 [24576/194182 (12%)]\tLoss: 0.311766\tGrad Norm: 1.293492\tLR: 0.030000\n",
      "Train Epoch: 1021 [45056/194182 (23%)]\tLoss: 0.312437\tGrad Norm: 1.185904\tLR: 0.030000\n",
      "Train Epoch: 1021 [65536/194182 (33%)]\tLoss: 0.320551\tGrad Norm: 1.619917\tLR: 0.030000\n",
      "Train Epoch: 1021 [86016/194182 (44%)]\tLoss: 0.321348\tGrad Norm: 1.472507\tLR: 0.030000\n",
      "Train Epoch: 1021 [106496/194182 (54%)]\tLoss: 0.307690\tGrad Norm: 1.149646\tLR: 0.030000\n",
      "Train Epoch: 1021 [126976/194182 (65%)]\tLoss: 0.322376\tGrad Norm: 1.587938\tLR: 0.030000\n",
      "Train Epoch: 1021 [147456/194182 (75%)]\tLoss: 0.318333\tGrad Norm: 1.363372\tLR: 0.030000\n",
      "Train Epoch: 1021 [167936/194182 (85%)]\tLoss: 0.315239\tGrad Norm: 1.029648\tLR: 0.030000\n",
      "Train Epoch: 1021 [188416/194182 (96%)]\tLoss: 0.320019\tGrad Norm: 1.446288\tLR: 0.030000\n",
      "Train set: Average loss: 0.3167\n",
      "Test set: Average loss: 0.2456, Average MAE: 0.3530\n",
      "Train Epoch: 1022 [4096/194182 (2%)]\tLoss: 0.322827\tGrad Norm: 1.568350\tLR: 0.030000\n",
      "Train Epoch: 1022 [24576/194182 (12%)]\tLoss: 0.318133\tGrad Norm: 1.437472\tLR: 0.030000\n",
      "Train Epoch: 1022 [45056/194182 (23%)]\tLoss: 0.319117\tGrad Norm: 1.425816\tLR: 0.030000\n",
      "Train Epoch: 1022 [65536/194182 (33%)]\tLoss: 0.315021\tGrad Norm: 1.356684\tLR: 0.030000\n",
      "Train Epoch: 1022 [86016/194182 (44%)]\tLoss: 0.324639\tGrad Norm: 1.221604\tLR: 0.030000\n",
      "Train Epoch: 1022 [106496/194182 (54%)]\tLoss: 0.314978\tGrad Norm: 1.605461\tLR: 0.030000\n",
      "Train Epoch: 1022 [126976/194182 (65%)]\tLoss: 0.325803\tGrad Norm: 1.444446\tLR: 0.030000\n",
      "Train Epoch: 1022 [147456/194182 (75%)]\tLoss: 0.314742\tGrad Norm: 1.358731\tLR: 0.030000\n",
      "Train Epoch: 1022 [167936/194182 (85%)]\tLoss: 0.316105\tGrad Norm: 1.603078\tLR: 0.030000\n",
      "Train Epoch: 1022 [188416/194182 (96%)]\tLoss: 0.328281\tGrad Norm: 1.439313\tLR: 0.030000\n",
      "Train set: Average loss: 0.3186\n",
      "Test set: Average loss: 0.2416, Average MAE: 0.3366\n",
      "Train Epoch: 1023 [4096/194182 (2%)]\tLoss: 0.317166\tGrad Norm: 1.252851\tLR: 0.030000\n",
      "Train Epoch: 1023 [24576/194182 (12%)]\tLoss: 0.321453\tGrad Norm: 1.594780\tLR: 0.030000\n",
      "Train Epoch: 1023 [45056/194182 (23%)]\tLoss: 0.314339\tGrad Norm: 1.609800\tLR: 0.030000\n",
      "Train Epoch: 1023 [65536/194182 (33%)]\tLoss: 0.315956\tGrad Norm: 1.139649\tLR: 0.030000\n",
      "Train Epoch: 1023 [86016/194182 (44%)]\tLoss: 0.311279\tGrad Norm: 1.053663\tLR: 0.030000\n",
      "Train Epoch: 1023 [106496/194182 (54%)]\tLoss: 0.315280\tGrad Norm: 1.289303\tLR: 0.030000\n",
      "Train Epoch: 1023 [126976/194182 (65%)]\tLoss: 0.313748\tGrad Norm: 1.245984\tLR: 0.030000\n",
      "Train Epoch: 1023 [147456/194182 (75%)]\tLoss: 0.329693\tGrad Norm: 1.534439\tLR: 0.030000\n",
      "Train Epoch: 1023 [167936/194182 (85%)]\tLoss: 0.318236\tGrad Norm: 1.566907\tLR: 0.030000\n",
      "Train Epoch: 1023 [188416/194182 (96%)]\tLoss: 0.321775\tGrad Norm: 1.886530\tLR: 0.030000\n",
      "Train set: Average loss: 0.3174\n",
      "Test set: Average loss: 0.2485, Average MAE: 0.3310\n",
      "Train Epoch: 1024 [4096/194182 (2%)]\tLoss: 0.328742\tGrad Norm: 1.740394\tLR: 0.030000\n",
      "Train Epoch: 1024 [24576/194182 (12%)]\tLoss: 0.310423\tGrad Norm: 1.325365\tLR: 0.030000\n",
      "Train Epoch: 1024 [45056/194182 (23%)]\tLoss: 0.312253\tGrad Norm: 1.398813\tLR: 0.030000\n",
      "Train Epoch: 1024 [65536/194182 (33%)]\tLoss: 0.327267\tGrad Norm: 1.563232\tLR: 0.030000\n",
      "Train Epoch: 1024 [86016/194182 (44%)]\tLoss: 0.313283\tGrad Norm: 1.484976\tLR: 0.030000\n",
      "Train Epoch: 1024 [106496/194182 (54%)]\tLoss: 0.314012\tGrad Norm: 1.480179\tLR: 0.030000\n",
      "Train Epoch: 1024 [126976/194182 (65%)]\tLoss: 0.313194\tGrad Norm: 1.151806\tLR: 0.030000\n",
      "Train Epoch: 1024 [147456/194182 (75%)]\tLoss: 0.305525\tGrad Norm: 1.044363\tLR: 0.030000\n",
      "Train Epoch: 1024 [167936/194182 (85%)]\tLoss: 0.320662\tGrad Norm: 1.348498\tLR: 0.030000\n",
      "Train Epoch: 1024 [188416/194182 (96%)]\tLoss: 0.312574\tGrad Norm: 1.278237\tLR: 0.030000\n",
      "Train set: Average loss: 0.3165\n",
      "Test set: Average loss: 0.2389, Average MAE: 0.3358\n",
      "Train Epoch: 1025 [4096/194182 (2%)]\tLoss: 0.313356\tGrad Norm: 1.147759\tLR: 0.030000\n",
      "Train Epoch: 1025 [24576/194182 (12%)]\tLoss: 0.324993\tGrad Norm: 1.461930\tLR: 0.030000\n",
      "Train Epoch: 1025 [45056/194182 (23%)]\tLoss: 0.310524\tGrad Norm: 1.191375\tLR: 0.030000\n",
      "Train Epoch: 1025 [65536/194182 (33%)]\tLoss: 0.314794\tGrad Norm: 1.508763\tLR: 0.030000\n",
      "Train Epoch: 1025 [86016/194182 (44%)]\tLoss: 0.317625\tGrad Norm: 1.267562\tLR: 0.030000\n",
      "Train Epoch: 1025 [106496/194182 (54%)]\tLoss: 0.323579\tGrad Norm: 1.579136\tLR: 0.030000\n",
      "Train Epoch: 1025 [126976/194182 (65%)]\tLoss: 0.324474\tGrad Norm: 1.916855\tLR: 0.030000\n",
      "Train Epoch: 1025 [147456/194182 (75%)]\tLoss: 0.325361\tGrad Norm: 1.790536\tLR: 0.030000\n",
      "Train Epoch: 1025 [167936/194182 (85%)]\tLoss: 0.316247\tGrad Norm: 1.346332\tLR: 0.030000\n",
      "Train Epoch: 1025 [188416/194182 (96%)]\tLoss: 0.317158\tGrad Norm: 1.706812\tLR: 0.030000\n",
      "Train set: Average loss: 0.3181\n",
      "Test set: Average loss: 0.2499, Average MAE: 0.3394\n",
      "Epoch 1025: Mean reward = 0.065 +/- 0.063\n",
      "Train Epoch: 1026 [4096/194182 (2%)]\tLoss: 0.323872\tGrad Norm: 1.647182\tLR: 0.030000\n",
      "Train Epoch: 1026 [24576/194182 (12%)]\tLoss: 0.321152\tGrad Norm: 1.397739\tLR: 0.030000\n",
      "Train Epoch: 1026 [45056/194182 (23%)]\tLoss: 0.318665\tGrad Norm: 1.329272\tLR: 0.030000\n",
      "Train Epoch: 1026 [65536/194182 (33%)]\tLoss: 0.308287\tGrad Norm: 1.119913\tLR: 0.030000\n",
      "Train Epoch: 1026 [86016/194182 (44%)]\tLoss: 0.312283\tGrad Norm: 1.463302\tLR: 0.030000\n",
      "Train Epoch: 1026 [106496/194182 (54%)]\tLoss: 0.315448\tGrad Norm: 1.438433\tLR: 0.030000\n",
      "Train Epoch: 1026 [126976/194182 (65%)]\tLoss: 0.324332\tGrad Norm: 1.600047\tLR: 0.030000\n",
      "Train Epoch: 1026 [147456/194182 (75%)]\tLoss: 0.307321\tGrad Norm: 1.034638\tLR: 0.030000\n",
      "Train Epoch: 1026 [167936/194182 (85%)]\tLoss: 0.304782\tGrad Norm: 0.809869\tLR: 0.030000\n",
      "Train Epoch: 1026 [188416/194182 (96%)]\tLoss: 0.324761\tGrad Norm: 1.624717\tLR: 0.030000\n",
      "Train set: Average loss: 0.3159\n",
      "Test set: Average loss: 0.2448, Average MAE: 0.3492\n",
      "Train Epoch: 1027 [4096/194182 (2%)]\tLoss: 0.322345\tGrad Norm: 1.293168\tLR: 0.030000\n",
      "Train Epoch: 1027 [24576/194182 (12%)]\tLoss: 0.317223\tGrad Norm: 1.222188\tLR: 0.030000\n",
      "Train Epoch: 1027 [45056/194182 (23%)]\tLoss: 0.316497\tGrad Norm: 1.504639\tLR: 0.030000\n",
      "Train Epoch: 1027 [65536/194182 (33%)]\tLoss: 0.315415\tGrad Norm: 1.732068\tLR: 0.030000\n",
      "Train Epoch: 1027 [86016/194182 (44%)]\tLoss: 0.323825\tGrad Norm: 1.690529\tLR: 0.030000\n",
      "Train Epoch: 1027 [106496/194182 (54%)]\tLoss: 0.314356\tGrad Norm: 1.592357\tLR: 0.030000\n",
      "Train Epoch: 1027 [126976/194182 (65%)]\tLoss: 0.319157\tGrad Norm: 1.498994\tLR: 0.030000\n",
      "Train Epoch: 1027 [147456/194182 (75%)]\tLoss: 0.312890\tGrad Norm: 1.307851\tLR: 0.030000\n",
      "Train Epoch: 1027 [167936/194182 (85%)]\tLoss: 0.316770\tGrad Norm: 0.995413\tLR: 0.030000\n",
      "Train Epoch: 1027 [188416/194182 (96%)]\tLoss: 0.315656\tGrad Norm: 1.409477\tLR: 0.030000\n",
      "Train set: Average loss: 0.3161\n",
      "Test set: Average loss: 0.2450, Average MAE: 0.3509\n",
      "Train Epoch: 1028 [4096/194182 (2%)]\tLoss: 0.321550\tGrad Norm: 1.544179\tLR: 0.030000\n",
      "Train Epoch: 1028 [24576/194182 (12%)]\tLoss: 0.310814\tGrad Norm: 1.166419\tLR: 0.030000\n",
      "Train Epoch: 1028 [45056/194182 (23%)]\tLoss: 0.322115\tGrad Norm: 1.505725\tLR: 0.030000\n",
      "Train Epoch: 1028 [65536/194182 (33%)]\tLoss: 0.321738\tGrad Norm: 1.597642\tLR: 0.030000\n",
      "Train Epoch: 1028 [86016/194182 (44%)]\tLoss: 0.315899\tGrad Norm: 1.595580\tLR: 0.030000\n",
      "Train Epoch: 1028 [106496/194182 (54%)]\tLoss: 0.310554\tGrad Norm: 1.175995\tLR: 0.030000\n",
      "Train Epoch: 1028 [126976/194182 (65%)]\tLoss: 0.305054\tGrad Norm: 1.148223\tLR: 0.030000\n",
      "Train Epoch: 1028 [147456/194182 (75%)]\tLoss: 0.306927\tGrad Norm: 1.447192\tLR: 0.030000\n",
      "Train Epoch: 1028 [167936/194182 (85%)]\tLoss: 0.314156\tGrad Norm: 1.314017\tLR: 0.030000\n",
      "Train Epoch: 1028 [188416/194182 (96%)]\tLoss: 0.321620\tGrad Norm: 1.510970\tLR: 0.030000\n",
      "Train set: Average loss: 0.3160\n",
      "Test set: Average loss: 0.2443, Average MAE: 0.3323\n",
      "Train Epoch: 1029 [4096/194182 (2%)]\tLoss: 0.321190\tGrad Norm: 1.466512\tLR: 0.030000\n",
      "Train Epoch: 1029 [24576/194182 (12%)]\tLoss: 0.313521\tGrad Norm: 1.208490\tLR: 0.030000\n",
      "Train Epoch: 1029 [45056/194182 (23%)]\tLoss: 0.321944\tGrad Norm: 1.464987\tLR: 0.030000\n",
      "Train Epoch: 1029 [65536/194182 (33%)]\tLoss: 0.319230\tGrad Norm: 1.526924\tLR: 0.030000\n",
      "Train Epoch: 1029 [86016/194182 (44%)]\tLoss: 0.316586\tGrad Norm: 1.240152\tLR: 0.030000\n",
      "Train Epoch: 1029 [106496/194182 (54%)]\tLoss: 0.313054\tGrad Norm: 1.209180\tLR: 0.030000\n",
      "Train Epoch: 1029 [126976/194182 (65%)]\tLoss: 0.314041\tGrad Norm: 1.418561\tLR: 0.030000\n",
      "Train Epoch: 1029 [147456/194182 (75%)]\tLoss: 0.319215\tGrad Norm: 1.606433\tLR: 0.030000\n",
      "Train Epoch: 1029 [167936/194182 (85%)]\tLoss: 0.325873\tGrad Norm: 1.833748\tLR: 0.030000\n",
      "Train Epoch: 1029 [188416/194182 (96%)]\tLoss: 0.319226\tGrad Norm: 1.439986\tLR: 0.030000\n",
      "Train set: Average loss: 0.3167\n",
      "Test set: Average loss: 0.2398, Average MAE: 0.3443\n",
      "Train Epoch: 1030 [4096/194182 (2%)]\tLoss: 0.312422\tGrad Norm: 1.365660\tLR: 0.030000\n",
      "Train Epoch: 1030 [24576/194182 (12%)]\tLoss: 0.304389\tGrad Norm: 0.788064\tLR: 0.030000\n",
      "Train Epoch: 1030 [45056/194182 (23%)]\tLoss: 0.303649\tGrad Norm: 1.171042\tLR: 0.030000\n",
      "Train Epoch: 1030 [65536/194182 (33%)]\tLoss: 0.304548\tGrad Norm: 1.196356\tLR: 0.030000\n",
      "Train Epoch: 1030 [86016/194182 (44%)]\tLoss: 0.320427\tGrad Norm: 1.438958\tLR: 0.030000\n",
      "Train Epoch: 1030 [106496/194182 (54%)]\tLoss: 0.314343\tGrad Norm: 1.359776\tLR: 0.030000\n",
      "Train Epoch: 1030 [126976/194182 (65%)]\tLoss: 0.313300\tGrad Norm: 1.290036\tLR: 0.030000\n",
      "Train Epoch: 1030 [147456/194182 (75%)]\tLoss: 0.316410\tGrad Norm: 1.415817\tLR: 0.030000\n",
      "Train Epoch: 1030 [167936/194182 (85%)]\tLoss: 0.319549\tGrad Norm: 1.623575\tLR: 0.030000\n",
      "Train Epoch: 1030 [188416/194182 (96%)]\tLoss: 0.320506\tGrad Norm: 1.575224\tLR: 0.030000\n",
      "Train set: Average loss: 0.3149\n",
      "Test set: Average loss: 0.2481, Average MAE: 0.3598\n",
      "Epoch 1030: Mean reward = 0.046 +/- 0.001\n",
      "Train Epoch: 1031 [4096/194182 (2%)]\tLoss: 0.319909\tGrad Norm: 1.618993\tLR: 0.030000\n",
      "Train Epoch: 1031 [24576/194182 (12%)]\tLoss: 0.312052\tGrad Norm: 1.115136\tLR: 0.030000\n",
      "Train Epoch: 1031 [45056/194182 (23%)]\tLoss: 0.314241\tGrad Norm: 1.697222\tLR: 0.030000\n",
      "Train Epoch: 1031 [65536/194182 (33%)]\tLoss: 0.316556\tGrad Norm: 1.581057\tLR: 0.030000\n",
      "Train Epoch: 1031 [86016/194182 (44%)]\tLoss: 0.313808\tGrad Norm: 1.332315\tLR: 0.030000\n",
      "Train Epoch: 1031 [106496/194182 (54%)]\tLoss: 0.314072\tGrad Norm: 1.297169\tLR: 0.030000\n",
      "Train Epoch: 1031 [126976/194182 (65%)]\tLoss: 0.314208\tGrad Norm: 1.044436\tLR: 0.030000\n",
      "Train Epoch: 1031 [147456/194182 (75%)]\tLoss: 0.311345\tGrad Norm: 1.171068\tLR: 0.030000\n",
      "Train Epoch: 1031 [167936/194182 (85%)]\tLoss: 0.306013\tGrad Norm: 1.119440\tLR: 0.030000\n",
      "Train Epoch: 1031 [188416/194182 (96%)]\tLoss: 0.315625\tGrad Norm: 1.405556\tLR: 0.030000\n",
      "Train set: Average loss: 0.3152\n",
      "Test set: Average loss: 0.2438, Average MAE: 0.3496\n",
      "Train Epoch: 1032 [4096/194182 (2%)]\tLoss: 0.312696\tGrad Norm: 1.290448\tLR: 0.030000\n",
      "Train Epoch: 1032 [24576/194182 (12%)]\tLoss: 0.306740\tGrad Norm: 1.287741\tLR: 0.030000\n",
      "Train Epoch: 1032 [45056/194182 (23%)]\tLoss: 0.322894\tGrad Norm: 1.453868\tLR: 0.030000\n",
      "Train Epoch: 1032 [65536/194182 (33%)]\tLoss: 0.315714\tGrad Norm: 1.182687\tLR: 0.030000\n",
      "Train Epoch: 1032 [86016/194182 (44%)]\tLoss: 0.318286\tGrad Norm: 1.444528\tLR: 0.030000\n",
      "Train Epoch: 1032 [106496/194182 (54%)]\tLoss: 0.318232\tGrad Norm: 1.431664\tLR: 0.030000\n",
      "Train Epoch: 1032 [126976/194182 (65%)]\tLoss: 0.319104\tGrad Norm: 1.767300\tLR: 0.030000\n",
      "Train Epoch: 1032 [147456/194182 (75%)]\tLoss: 0.313639\tGrad Norm: 1.428447\tLR: 0.030000\n",
      "Train Epoch: 1032 [167936/194182 (85%)]\tLoss: 0.321731\tGrad Norm: 1.696466\tLR: 0.030000\n",
      "Train Epoch: 1032 [188416/194182 (96%)]\tLoss: 0.315336\tGrad Norm: 1.399523\tLR: 0.030000\n",
      "Train set: Average loss: 0.3156\n",
      "Test set: Average loss: 0.2415, Average MAE: 0.3330\n",
      "Train Epoch: 1033 [4096/194182 (2%)]\tLoss: 0.314571\tGrad Norm: 1.276300\tLR: 0.030000\n",
      "Train Epoch: 1033 [24576/194182 (12%)]\tLoss: 0.310259\tGrad Norm: 1.414332\tLR: 0.030000\n",
      "Train Epoch: 1033 [45056/194182 (23%)]\tLoss: 0.308313\tGrad Norm: 1.245095\tLR: 0.030000\n",
      "Train Epoch: 1033 [65536/194182 (33%)]\tLoss: 0.315666\tGrad Norm: 1.435921\tLR: 0.030000\n",
      "Train Epoch: 1033 [86016/194182 (44%)]\tLoss: 0.324724\tGrad Norm: 1.673805\tLR: 0.030000\n",
      "Train Epoch: 1033 [106496/194182 (54%)]\tLoss: 0.309198\tGrad Norm: 1.440221\tLR: 0.030000\n",
      "Train Epoch: 1033 [126976/194182 (65%)]\tLoss: 0.312641\tGrad Norm: 1.411369\tLR: 0.030000\n",
      "Train Epoch: 1033 [147456/194182 (75%)]\tLoss: 0.317515\tGrad Norm: 1.450326\tLR: 0.030000\n",
      "Train Epoch: 1033 [167936/194182 (85%)]\tLoss: 0.316177\tGrad Norm: 1.211862\tLR: 0.030000\n",
      "Train Epoch: 1033 [188416/194182 (96%)]\tLoss: 0.315813\tGrad Norm: 1.553333\tLR: 0.030000\n",
      "Train set: Average loss: 0.3155\n",
      "Test set: Average loss: 0.2478, Average MAE: 0.3464\n",
      "Train Epoch: 1034 [4096/194182 (2%)]\tLoss: 0.316368\tGrad Norm: 1.640596\tLR: 0.030000\n",
      "Train Epoch: 1034 [24576/194182 (12%)]\tLoss: 0.310515\tGrad Norm: 1.311422\tLR: 0.030000\n",
      "Train Epoch: 1034 [45056/194182 (23%)]\tLoss: 0.310040\tGrad Norm: 1.067173\tLR: 0.030000\n",
      "Train Epoch: 1034 [65536/194182 (33%)]\tLoss: 0.311486\tGrad Norm: 1.110436\tLR: 0.030000\n",
      "Train Epoch: 1034 [86016/194182 (44%)]\tLoss: 0.306926\tGrad Norm: 1.020213\tLR: 0.030000\n",
      "Train Epoch: 1034 [106496/194182 (54%)]\tLoss: 0.310128\tGrad Norm: 1.289058\tLR: 0.030000\n",
      "Train Epoch: 1034 [126976/194182 (65%)]\tLoss: 0.317066\tGrad Norm: 1.713444\tLR: 0.030000\n",
      "Train Epoch: 1034 [147456/194182 (75%)]\tLoss: 0.306824\tGrad Norm: 1.396947\tLR: 0.030000\n",
      "Train Epoch: 1034 [167936/194182 (85%)]\tLoss: 0.316657\tGrad Norm: 1.382161\tLR: 0.030000\n",
      "Train Epoch: 1034 [188416/194182 (96%)]\tLoss: 0.312349\tGrad Norm: 1.329431\tLR: 0.030000\n",
      "Train set: Average loss: 0.3143\n",
      "Test set: Average loss: 0.2463, Average MAE: 0.3556\n",
      "Train Epoch: 1035 [4096/194182 (2%)]\tLoss: 0.321104\tGrad Norm: 1.392023\tLR: 0.030000\n",
      "Train Epoch: 1035 [24576/194182 (12%)]\tLoss: 0.321907\tGrad Norm: 1.604700\tLR: 0.030000\n",
      "Train Epoch: 1035 [45056/194182 (23%)]\tLoss: 0.324895\tGrad Norm: 1.848500\tLR: 0.030000\n",
      "Train Epoch: 1035 [65536/194182 (33%)]\tLoss: 0.314400\tGrad Norm: 1.542092\tLR: 0.030000\n",
      "Train Epoch: 1035 [86016/194182 (44%)]\tLoss: 0.312523\tGrad Norm: 1.441957\tLR: 0.030000\n",
      "Train Epoch: 1035 [106496/194182 (54%)]\tLoss: 0.312931\tGrad Norm: 1.587656\tLR: 0.030000\n",
      "Train Epoch: 1035 [126976/194182 (65%)]\tLoss: 0.316412\tGrad Norm: 1.360894\tLR: 0.030000\n",
      "Train Epoch: 1035 [147456/194182 (75%)]\tLoss: 0.312910\tGrad Norm: 1.273932\tLR: 0.030000\n",
      "Train Epoch: 1035 [167936/194182 (85%)]\tLoss: 0.318210\tGrad Norm: 1.292239\tLR: 0.030000\n",
      "Train Epoch: 1035 [188416/194182 (96%)]\tLoss: 0.312798\tGrad Norm: 1.241591\tLR: 0.030000\n",
      "Train set: Average loss: 0.3151\n",
      "Test set: Average loss: 0.2401, Average MAE: 0.3458\n",
      "Epoch 1035: Mean reward = 0.084 +/- 0.088\n",
      "Train Epoch: 1036 [4096/194182 (2%)]\tLoss: 0.303318\tGrad Norm: 1.099958\tLR: 0.030000\n",
      "Train Epoch: 1036 [24576/194182 (12%)]\tLoss: 0.310968\tGrad Norm: 1.419332\tLR: 0.030000\n",
      "Train Epoch: 1036 [45056/194182 (23%)]\tLoss: 0.314862\tGrad Norm: 1.008644\tLR: 0.030000\n",
      "Train Epoch: 1036 [65536/194182 (33%)]\tLoss: 0.315013\tGrad Norm: 1.373274\tLR: 0.030000\n",
      "Train Epoch: 1036 [86016/194182 (44%)]\tLoss: 0.309973\tGrad Norm: 1.605610\tLR: 0.030000\n",
      "Train Epoch: 1036 [106496/194182 (54%)]\tLoss: 0.310253\tGrad Norm: 1.366446\tLR: 0.030000\n",
      "Train Epoch: 1036 [126976/194182 (65%)]\tLoss: 0.315537\tGrad Norm: 1.643429\tLR: 0.030000\n",
      "Train Epoch: 1036 [147456/194182 (75%)]\tLoss: 0.326830\tGrad Norm: 2.005435\tLR: 0.030000\n",
      "Train Epoch: 1036 [167936/194182 (85%)]\tLoss: 0.321806\tGrad Norm: 1.391139\tLR: 0.030000\n",
      "Train Epoch: 1036 [188416/194182 (96%)]\tLoss: 0.310784\tGrad Norm: 0.925118\tLR: 0.030000\n",
      "Train set: Average loss: 0.3147\n",
      "Test set: Average loss: 0.2414, Average MAE: 0.3483\n",
      "Train Epoch: 1037 [4096/194182 (2%)]\tLoss: 0.311528\tGrad Norm: 1.383025\tLR: 0.030000\n",
      "Train Epoch: 1037 [24576/194182 (12%)]\tLoss: 0.314406\tGrad Norm: 1.661686\tLR: 0.030000\n",
      "Train Epoch: 1037 [45056/194182 (23%)]\tLoss: 0.313600\tGrad Norm: 1.207012\tLR: 0.030000\n",
      "Train Epoch: 1037 [65536/194182 (33%)]\tLoss: 0.313585\tGrad Norm: 1.088969\tLR: 0.030000\n",
      "Train Epoch: 1037 [86016/194182 (44%)]\tLoss: 0.314165\tGrad Norm: 1.695504\tLR: 0.030000\n",
      "Train Epoch: 1037 [106496/194182 (54%)]\tLoss: 0.320287\tGrad Norm: 1.472828\tLR: 0.030000\n",
      "Train Epoch: 1037 [126976/194182 (65%)]\tLoss: 0.309300\tGrad Norm: 1.180318\tLR: 0.030000\n",
      "Train Epoch: 1037 [147456/194182 (75%)]\tLoss: 0.315482\tGrad Norm: 1.229425\tLR: 0.030000\n",
      "Train Epoch: 1037 [167936/194182 (85%)]\tLoss: 0.317355\tGrad Norm: 1.575108\tLR: 0.030000\n",
      "Train Epoch: 1037 [188416/194182 (96%)]\tLoss: 0.324580\tGrad Norm: 1.720846\tLR: 0.030000\n",
      "Train set: Average loss: 0.3153\n",
      "Test set: Average loss: 0.2461, Average MAE: 0.3383\n",
      "Train Epoch: 1038 [4096/194182 (2%)]\tLoss: 0.311982\tGrad Norm: 1.504863\tLR: 0.030000\n",
      "Train Epoch: 1038 [24576/194182 (12%)]\tLoss: 0.303178\tGrad Norm: 1.146411\tLR: 0.030000\n",
      "Train Epoch: 1038 [45056/194182 (23%)]\tLoss: 0.308345\tGrad Norm: 1.165507\tLR: 0.030000\n",
      "Train Epoch: 1038 [65536/194182 (33%)]\tLoss: 0.312715\tGrad Norm: 1.350736\tLR: 0.030000\n",
      "Train Epoch: 1038 [86016/194182 (44%)]\tLoss: 0.319942\tGrad Norm: 1.491857\tLR: 0.030000\n",
      "Train Epoch: 1038 [106496/194182 (54%)]\tLoss: 0.322422\tGrad Norm: 1.556003\tLR: 0.030000\n",
      "Train Epoch: 1038 [126976/194182 (65%)]\tLoss: 0.307791\tGrad Norm: 1.194395\tLR: 0.030000\n",
      "Train Epoch: 1038 [147456/194182 (75%)]\tLoss: 0.311826\tGrad Norm: 1.215492\tLR: 0.030000\n",
      "Train Epoch: 1038 [167936/194182 (85%)]\tLoss: 0.318412\tGrad Norm: 1.698890\tLR: 0.030000\n",
      "Train Epoch: 1038 [188416/194182 (96%)]\tLoss: 0.321305\tGrad Norm: 1.828995\tLR: 0.030000\n",
      "Train set: Average loss: 0.3143\n",
      "Test set: Average loss: 0.2436, Average MAE: 0.3537\n",
      "Train Epoch: 1039 [4096/194182 (2%)]\tLoss: 0.315662\tGrad Norm: 1.294461\tLR: 0.030000\n",
      "Train Epoch: 1039 [24576/194182 (12%)]\tLoss: 0.309627\tGrad Norm: 1.426813\tLR: 0.030000\n",
      "Train Epoch: 1039 [45056/194182 (23%)]\tLoss: 0.314916\tGrad Norm: 0.958630\tLR: 0.030000\n",
      "Train Epoch: 1039 [65536/194182 (33%)]\tLoss: 0.310296\tGrad Norm: 0.811718\tLR: 0.030000\n",
      "Train Epoch: 1039 [86016/194182 (44%)]\tLoss: 0.307619\tGrad Norm: 1.416341\tLR: 0.030000\n",
      "Train Epoch: 1039 [106496/194182 (54%)]\tLoss: 0.313897\tGrad Norm: 1.393593\tLR: 0.030000\n",
      "Train Epoch: 1039 [126976/194182 (65%)]\tLoss: 0.314087\tGrad Norm: 1.146601\tLR: 0.030000\n",
      "Train Epoch: 1039 [147456/194182 (75%)]\tLoss: 0.311287\tGrad Norm: 1.305755\tLR: 0.030000\n",
      "Train Epoch: 1039 [167936/194182 (85%)]\tLoss: 0.311491\tGrad Norm: 1.323990\tLR: 0.030000\n",
      "Train Epoch: 1039 [188416/194182 (96%)]\tLoss: 0.318979\tGrad Norm: 1.517138\tLR: 0.030000\n",
      "Train set: Average loss: 0.3113\n",
      "Test set: Average loss: 0.2495, Average MAE: 0.3407\n",
      "Train Epoch: 1040 [4096/194182 (2%)]\tLoss: 0.319197\tGrad Norm: 1.693950\tLR: 0.030000\n",
      "Train Epoch: 1040 [24576/194182 (12%)]\tLoss: 0.315436\tGrad Norm: 1.575751\tLR: 0.030000\n",
      "Train Epoch: 1040 [45056/194182 (23%)]\tLoss: 0.318808\tGrad Norm: 1.496075\tLR: 0.030000\n",
      "Train Epoch: 1040 [65536/194182 (33%)]\tLoss: 0.313159\tGrad Norm: 1.165990\tLR: 0.030000\n",
      "Train Epoch: 1040 [86016/194182 (44%)]\tLoss: 0.309746\tGrad Norm: 1.295102\tLR: 0.030000\n",
      "Train Epoch: 1040 [106496/194182 (54%)]\tLoss: 0.317187\tGrad Norm: 1.766623\tLR: 0.030000\n",
      "Train Epoch: 1040 [126976/194182 (65%)]\tLoss: 0.315545\tGrad Norm: 1.420866\tLR: 0.030000\n",
      "Train Epoch: 1040 [147456/194182 (75%)]\tLoss: 0.318637\tGrad Norm: 1.552331\tLR: 0.030000\n",
      "Train Epoch: 1040 [167936/194182 (85%)]\tLoss: 0.315893\tGrad Norm: 1.690296\tLR: 0.030000\n",
      "Train Epoch: 1040 [188416/194182 (96%)]\tLoss: 0.314462\tGrad Norm: 1.694823\tLR: 0.030000\n",
      "Train set: Average loss: 0.3145\n",
      "Test set: Average loss: 0.2419, Average MAE: 0.3377\n",
      "Epoch 1040: Mean reward = 0.046 +/- 0.001\n",
      "Train Epoch: 1041 [4096/194182 (2%)]\tLoss: 0.313026\tGrad Norm: 1.329785\tLR: 0.030000\n",
      "Train Epoch: 1041 [24576/194182 (12%)]\tLoss: 0.315672\tGrad Norm: 1.322066\tLR: 0.030000\n",
      "Train Epoch: 1041 [45056/194182 (23%)]\tLoss: 0.311808\tGrad Norm: 1.306215\tLR: 0.030000\n",
      "Train Epoch: 1041 [65536/194182 (33%)]\tLoss: 0.308627\tGrad Norm: 1.324564\tLR: 0.030000\n",
      "Train Epoch: 1041 [86016/194182 (44%)]\tLoss: 0.309940\tGrad Norm: 1.357791\tLR: 0.030000\n",
      "Train Epoch: 1041 [106496/194182 (54%)]\tLoss: 0.310435\tGrad Norm: 1.386013\tLR: 0.030000\n",
      "Train Epoch: 1041 [126976/194182 (65%)]\tLoss: 0.310212\tGrad Norm: 1.168095\tLR: 0.030000\n",
      "Train Epoch: 1041 [147456/194182 (75%)]\tLoss: 0.311407\tGrad Norm: 1.263355\tLR: 0.030000\n",
      "Train Epoch: 1041 [167936/194182 (85%)]\tLoss: 0.319928\tGrad Norm: 1.642835\tLR: 0.030000\n",
      "Train Epoch: 1041 [188416/194182 (96%)]\tLoss: 0.316777\tGrad Norm: 1.438463\tLR: 0.030000\n",
      "Train set: Average loss: 0.3134\n",
      "Test set: Average loss: 0.2446, Average MAE: 0.3328\n",
      "Train Epoch: 1042 [4096/194182 (2%)]\tLoss: 0.313763\tGrad Norm: 2.198084\tLR: 0.030000\n",
      "Train Epoch: 1042 [24576/194182 (12%)]\tLoss: 0.320608\tGrad Norm: 1.555193\tLR: 0.030000\n",
      "Train Epoch: 1042 [45056/194182 (23%)]\tLoss: 0.319817\tGrad Norm: 1.641397\tLR: 0.030000\n",
      "Train Epoch: 1042 [65536/194182 (33%)]\tLoss: 0.311163\tGrad Norm: 1.254702\tLR: 0.030000\n",
      "Train Epoch: 1042 [86016/194182 (44%)]\tLoss: 0.310859\tGrad Norm: 1.181220\tLR: 0.030000\n",
      "Train Epoch: 1042 [106496/194182 (54%)]\tLoss: 0.306844\tGrad Norm: 1.059646\tLR: 0.030000\n",
      "Train Epoch: 1042 [126976/194182 (65%)]\tLoss: 0.315667\tGrad Norm: 1.260376\tLR: 0.030000\n",
      "Train Epoch: 1042 [147456/194182 (75%)]\tLoss: 0.304131\tGrad Norm: 1.227446\tLR: 0.030000\n",
      "Train Epoch: 1042 [167936/194182 (85%)]\tLoss: 0.313740\tGrad Norm: 1.100999\tLR: 0.030000\n",
      "Train Epoch: 1042 [188416/194182 (96%)]\tLoss: 0.312414\tGrad Norm: 1.165659\tLR: 0.030000\n",
      "Train set: Average loss: 0.3124\n",
      "Test set: Average loss: 0.2412, Average MAE: 0.3446\n",
      "Train Epoch: 1043 [4096/194182 (2%)]\tLoss: 0.308468\tGrad Norm: 1.206345\tLR: 0.030000\n",
      "Train Epoch: 1043 [24576/194182 (12%)]\tLoss: 0.314120\tGrad Norm: 1.241214\tLR: 0.030000\n",
      "Train Epoch: 1043 [45056/194182 (23%)]\tLoss: 0.307952\tGrad Norm: 1.271202\tLR: 0.030000\n",
      "Train Epoch: 1043 [65536/194182 (33%)]\tLoss: 0.311965\tGrad Norm: 1.310626\tLR: 0.030000\n",
      "Train Epoch: 1043 [86016/194182 (44%)]\tLoss: 0.309351\tGrad Norm: 1.234354\tLR: 0.030000\n",
      "Train Epoch: 1043 [106496/194182 (54%)]\tLoss: 0.308858\tGrad Norm: 1.377894\tLR: 0.030000\n",
      "Train Epoch: 1043 [126976/194182 (65%)]\tLoss: 0.308156\tGrad Norm: 1.220743\tLR: 0.030000\n",
      "Train Epoch: 1043 [147456/194182 (75%)]\tLoss: 0.307806\tGrad Norm: 0.987477\tLR: 0.030000\n",
      "Train Epoch: 1043 [167936/194182 (85%)]\tLoss: 0.313828\tGrad Norm: 1.242767\tLR: 0.030000\n",
      "Train Epoch: 1043 [188416/194182 (96%)]\tLoss: 0.328112\tGrad Norm: 2.197254\tLR: 0.030000\n",
      "Train set: Average loss: 0.3123\n",
      "Test set: Average loss: 0.2522, Average MAE: 0.3635\n",
      "Train Epoch: 1044 [4096/194182 (2%)]\tLoss: 0.319028\tGrad Norm: 1.871458\tLR: 0.030000\n",
      "Train Epoch: 1044 [24576/194182 (12%)]\tLoss: 0.314603\tGrad Norm: 1.300283\tLR: 0.030000\n",
      "Train Epoch: 1044 [45056/194182 (23%)]\tLoss: 0.313170\tGrad Norm: 1.312794\tLR: 0.030000\n",
      "Train Epoch: 1044 [65536/194182 (33%)]\tLoss: 0.309055\tGrad Norm: 0.924614\tLR: 0.030000\n",
      "Train Epoch: 1044 [86016/194182 (44%)]\tLoss: 0.309204\tGrad Norm: 1.093414\tLR: 0.030000\n",
      "Train Epoch: 1044 [106496/194182 (54%)]\tLoss: 0.308782\tGrad Norm: 1.413587\tLR: 0.030000\n",
      "Train Epoch: 1044 [126976/194182 (65%)]\tLoss: 0.308369\tGrad Norm: 1.106970\tLR: 0.030000\n",
      "Train Epoch: 1044 [147456/194182 (75%)]\tLoss: 0.309882\tGrad Norm: 1.305204\tLR: 0.030000\n",
      "Train Epoch: 1044 [167936/194182 (85%)]\tLoss: 0.316387\tGrad Norm: 1.286855\tLR: 0.030000\n",
      "Train Epoch: 1044 [188416/194182 (96%)]\tLoss: 0.315223\tGrad Norm: 1.255224\tLR: 0.030000\n",
      "Train set: Average loss: 0.3112\n",
      "Test set: Average loss: 0.2482, Average MAE: 0.3515\n",
      "Train Epoch: 1045 [4096/194182 (2%)]\tLoss: 0.322660\tGrad Norm: 1.822024\tLR: 0.030000\n",
      "Train Epoch: 1045 [24576/194182 (12%)]\tLoss: 0.307757\tGrad Norm: 1.424315\tLR: 0.030000\n",
      "Train Epoch: 1045 [45056/194182 (23%)]\tLoss: 0.319417\tGrad Norm: 1.351959\tLR: 0.030000\n",
      "Train Epoch: 1045 [65536/194182 (33%)]\tLoss: 0.320074\tGrad Norm: 1.887199\tLR: 0.030000\n",
      "Train Epoch: 1045 [86016/194182 (44%)]\tLoss: 0.329108\tGrad Norm: 1.517175\tLR: 0.030000\n",
      "Train Epoch: 1045 [106496/194182 (54%)]\tLoss: 0.308675\tGrad Norm: 1.357608\tLR: 0.030000\n",
      "Train Epoch: 1045 [126976/194182 (65%)]\tLoss: 0.313221\tGrad Norm: 1.245585\tLR: 0.030000\n",
      "Train Epoch: 1045 [147456/194182 (75%)]\tLoss: 0.309845\tGrad Norm: 1.482819\tLR: 0.030000\n",
      "Train Epoch: 1045 [167936/194182 (85%)]\tLoss: 0.315866\tGrad Norm: 1.743409\tLR: 0.030000\n",
      "Train Epoch: 1045 [188416/194182 (96%)]\tLoss: 0.322466\tGrad Norm: 1.807724\tLR: 0.030000\n",
      "Train set: Average loss: 0.3147\n",
      "Test set: Average loss: 0.2448, Average MAE: 0.3320\n",
      "Epoch 1045: Mean reward = 0.060 +/- 0.076\n",
      "Train Epoch: 1046 [4096/194182 (2%)]\tLoss: 0.320371\tGrad Norm: 1.760085\tLR: 0.030000\n",
      "Train Epoch: 1046 [24576/194182 (12%)]\tLoss: 0.313604\tGrad Norm: 1.580595\tLR: 0.030000\n",
      "Train Epoch: 1046 [45056/194182 (23%)]\tLoss: 0.305581\tGrad Norm: 1.180661\tLR: 0.030000\n",
      "Train Epoch: 1046 [65536/194182 (33%)]\tLoss: 0.309856\tGrad Norm: 1.296983\tLR: 0.030000\n",
      "Train Epoch: 1046 [86016/194182 (44%)]\tLoss: 0.315533\tGrad Norm: 1.379390\tLR: 0.030000\n",
      "Train Epoch: 1046 [106496/194182 (54%)]\tLoss: 0.325588\tGrad Norm: 1.833575\tLR: 0.030000\n",
      "Train Epoch: 1046 [126976/194182 (65%)]\tLoss: 0.317978\tGrad Norm: 1.555207\tLR: 0.030000\n",
      "Train Epoch: 1046 [147456/194182 (75%)]\tLoss: 0.309839\tGrad Norm: 1.734001\tLR: 0.030000\n",
      "Train Epoch: 1046 [167936/194182 (85%)]\tLoss: 0.310742\tGrad Norm: 1.341795\tLR: 0.030000\n",
      "Train Epoch: 1046 [188416/194182 (96%)]\tLoss: 0.309404\tGrad Norm: 1.148503\tLR: 0.030000\n",
      "Train set: Average loss: 0.3126\n",
      "Test set: Average loss: 0.2438, Average MAE: 0.3484\n",
      "Train Epoch: 1047 [4096/194182 (2%)]\tLoss: 0.314462\tGrad Norm: 1.325504\tLR: 0.030000\n",
      "Train Epoch: 1047 [24576/194182 (12%)]\tLoss: 0.310311\tGrad Norm: 1.443007\tLR: 0.030000\n",
      "Train Epoch: 1047 [45056/194182 (23%)]\tLoss: 0.313885\tGrad Norm: 1.233605\tLR: 0.030000\n",
      "Train Epoch: 1047 [65536/194182 (33%)]\tLoss: 0.306836\tGrad Norm: 1.163322\tLR: 0.030000\n",
      "Train Epoch: 1047 [86016/194182 (44%)]\tLoss: 0.300529\tGrad Norm: 1.040003\tLR: 0.030000\n",
      "Train Epoch: 1047 [106496/194182 (54%)]\tLoss: 0.315368\tGrad Norm: 1.407886\tLR: 0.030000\n",
      "Train Epoch: 1047 [126976/194182 (65%)]\tLoss: 0.309042\tGrad Norm: 1.266794\tLR: 0.030000\n",
      "Train Epoch: 1047 [147456/194182 (75%)]\tLoss: 0.319426\tGrad Norm: 1.687995\tLR: 0.030000\n",
      "Train Epoch: 1047 [167936/194182 (85%)]\tLoss: 0.318963\tGrad Norm: 1.586717\tLR: 0.030000\n",
      "Train Epoch: 1047 [188416/194182 (96%)]\tLoss: 0.310801\tGrad Norm: 1.217984\tLR: 0.030000\n",
      "Train set: Average loss: 0.3119\n",
      "Test set: Average loss: 0.2468, Average MAE: 0.3588\n",
      "Train Epoch: 1048 [4096/194182 (2%)]\tLoss: 0.314021\tGrad Norm: 1.602059\tLR: 0.030000\n",
      "Train Epoch: 1048 [24576/194182 (12%)]\tLoss: 0.321241\tGrad Norm: 1.899447\tLR: 0.030000\n",
      "Train Epoch: 1048 [45056/194182 (23%)]\tLoss: 0.317130\tGrad Norm: 1.760063\tLR: 0.030000\n",
      "Train Epoch: 1048 [65536/194182 (33%)]\tLoss: 0.312658\tGrad Norm: 1.515247\tLR: 0.030000\n",
      "Train Epoch: 1048 [86016/194182 (44%)]\tLoss: 0.315217\tGrad Norm: 1.904122\tLR: 0.030000\n",
      "Train Epoch: 1048 [106496/194182 (54%)]\tLoss: 0.317210\tGrad Norm: 1.506141\tLR: 0.030000\n",
      "Train Epoch: 1048 [126976/194182 (65%)]\tLoss: 0.310124\tGrad Norm: 1.334833\tLR: 0.030000\n",
      "Train Epoch: 1048 [147456/194182 (75%)]\tLoss: 0.309820\tGrad Norm: 1.291639\tLR: 0.030000\n",
      "Train Epoch: 1048 [167936/194182 (85%)]\tLoss: 0.304663\tGrad Norm: 1.059471\tLR: 0.030000\n",
      "Train Epoch: 1048 [188416/194182 (96%)]\tLoss: 0.307865\tGrad Norm: 1.396329\tLR: 0.030000\n",
      "Train set: Average loss: 0.3141\n",
      "Test set: Average loss: 0.2466, Average MAE: 0.3386\n",
      "Train Epoch: 1049 [4096/194182 (2%)]\tLoss: 0.316630\tGrad Norm: 1.505959\tLR: 0.030000\n",
      "Train Epoch: 1049 [24576/194182 (12%)]\tLoss: 0.313491\tGrad Norm: 1.411516\tLR: 0.030000\n",
      "Train Epoch: 1049 [45056/194182 (23%)]\tLoss: 0.314813\tGrad Norm: 1.497655\tLR: 0.030000\n",
      "Train Epoch: 1049 [65536/194182 (33%)]\tLoss: 0.311088\tGrad Norm: 1.175943\tLR: 0.030000\n",
      "Train Epoch: 1049 [86016/194182 (44%)]\tLoss: 0.312559\tGrad Norm: 1.277619\tLR: 0.030000\n",
      "Train Epoch: 1049 [106496/194182 (54%)]\tLoss: 0.315206\tGrad Norm: 1.359246\tLR: 0.030000\n",
      "Train Epoch: 1049 [126976/194182 (65%)]\tLoss: 0.312868\tGrad Norm: 1.482640\tLR: 0.030000\n",
      "Train Epoch: 1049 [147456/194182 (75%)]\tLoss: 0.324524\tGrad Norm: 1.854537\tLR: 0.030000\n",
      "Train Epoch: 1049 [167936/194182 (85%)]\tLoss: 0.308175\tGrad Norm: 1.613610\tLR: 0.030000\n",
      "Train Epoch: 1049 [188416/194182 (96%)]\tLoss: 0.317384\tGrad Norm: 1.623964\tLR: 0.030000\n",
      "Train set: Average loss: 0.3129\n",
      "Test set: Average loss: 0.2454, Average MAE: 0.3412\n",
      "Train Epoch: 1050 [4096/194182 (2%)]\tLoss: 0.313568\tGrad Norm: 1.433792\tLR: 0.030000\n",
      "Train Epoch: 1050 [24576/194182 (12%)]\tLoss: 0.312995\tGrad Norm: 1.243909\tLR: 0.030000\n",
      "Train Epoch: 1050 [45056/194182 (23%)]\tLoss: 0.315428\tGrad Norm: 1.190306\tLR: 0.030000\n",
      "Train Epoch: 1050 [65536/194182 (33%)]\tLoss: 0.300298\tGrad Norm: 1.262456\tLR: 0.030000\n",
      "Train Epoch: 1050 [86016/194182 (44%)]\tLoss: 0.312262\tGrad Norm: 1.126959\tLR: 0.030000\n",
      "Train Epoch: 1050 [106496/194182 (54%)]\tLoss: 0.315173\tGrad Norm: 1.315786\tLR: 0.030000\n",
      "Train Epoch: 1050 [126976/194182 (65%)]\tLoss: 0.309188\tGrad Norm: 1.164795\tLR: 0.030000\n",
      "Train Epoch: 1050 [147456/194182 (75%)]\tLoss: 0.313450\tGrad Norm: 1.198047\tLR: 0.030000\n",
      "Train Epoch: 1050 [167936/194182 (85%)]\tLoss: 0.308185\tGrad Norm: 1.185170\tLR: 0.030000\n",
      "Train Epoch: 1050 [188416/194182 (96%)]\tLoss: 0.314376\tGrad Norm: 1.512525\tLR: 0.030000\n",
      "Train set: Average loss: 0.3098\n",
      "Test set: Average loss: 0.2469, Average MAE: 0.3564\n",
      "Epoch 1050: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 1051 [4096/194182 (2%)]\tLoss: 0.314357\tGrad Norm: 1.475472\tLR: 0.030000\n",
      "Train Epoch: 1051 [24576/194182 (12%)]\tLoss: 0.316469\tGrad Norm: 1.674334\tLR: 0.030000\n",
      "Train Epoch: 1051 [45056/194182 (23%)]\tLoss: 0.306574\tGrad Norm: 1.116155\tLR: 0.030000\n",
      "Train Epoch: 1051 [65536/194182 (33%)]\tLoss: 0.311940\tGrad Norm: 1.704252\tLR: 0.030000\n",
      "Train Epoch: 1051 [86016/194182 (44%)]\tLoss: 0.311058\tGrad Norm: 1.541940\tLR: 0.030000\n",
      "Train Epoch: 1051 [106496/194182 (54%)]\tLoss: 0.308138\tGrad Norm: 1.171626\tLR: 0.030000\n",
      "Train Epoch: 1051 [126976/194182 (65%)]\tLoss: 0.311179\tGrad Norm: 1.267033\tLR: 0.030000\n",
      "Train Epoch: 1051 [147456/194182 (75%)]\tLoss: 0.305530\tGrad Norm: 1.401018\tLR: 0.030000\n",
      "Train Epoch: 1051 [167936/194182 (85%)]\tLoss: 0.310338\tGrad Norm: 1.362372\tLR: 0.030000\n",
      "Train Epoch: 1051 [188416/194182 (96%)]\tLoss: 0.309579\tGrad Norm: 1.361208\tLR: 0.030000\n",
      "Train set: Average loss: 0.3115\n",
      "Test set: Average loss: 0.2441, Average MAE: 0.3447\n",
      "Train Epoch: 1052 [4096/194182 (2%)]\tLoss: 0.309355\tGrad Norm: 1.425999\tLR: 0.030000\n",
      "Train Epoch: 1052 [24576/194182 (12%)]\tLoss: 0.305604\tGrad Norm: 1.204610\tLR: 0.030000\n",
      "Train Epoch: 1052 [45056/194182 (23%)]\tLoss: 0.306712\tGrad Norm: 1.211325\tLR: 0.030000\n",
      "Train Epoch: 1052 [65536/194182 (33%)]\tLoss: 0.305212\tGrad Norm: 1.305349\tLR: 0.030000\n",
      "Train Epoch: 1052 [86016/194182 (44%)]\tLoss: 0.314988\tGrad Norm: 1.367643\tLR: 0.030000\n",
      "Train Epoch: 1052 [106496/194182 (54%)]\tLoss: 0.318029\tGrad Norm: 1.650839\tLR: 0.030000\n",
      "Train Epoch: 1052 [126976/194182 (65%)]\tLoss: 0.307856\tGrad Norm: 1.475820\tLR: 0.030000\n",
      "Train Epoch: 1052 [147456/194182 (75%)]\tLoss: 0.308993\tGrad Norm: 1.351536\tLR: 0.030000\n",
      "Train Epoch: 1052 [167936/194182 (85%)]\tLoss: 0.307133\tGrad Norm: 1.564684\tLR: 0.030000\n",
      "Train Epoch: 1052 [188416/194182 (96%)]\tLoss: 0.307821\tGrad Norm: 1.205992\tLR: 0.030000\n",
      "Train set: Average loss: 0.3105\n",
      "Test set: Average loss: 0.2429, Average MAE: 0.3392\n",
      "Train Epoch: 1053 [4096/194182 (2%)]\tLoss: 0.306020\tGrad Norm: 1.327787\tLR: 0.030000\n",
      "Train Epoch: 1053 [24576/194182 (12%)]\tLoss: 0.301269\tGrad Norm: 1.326854\tLR: 0.030000\n",
      "Train Epoch: 1053 [45056/194182 (23%)]\tLoss: 0.313378\tGrad Norm: 1.817580\tLR: 0.030000\n",
      "Train Epoch: 1053 [65536/194182 (33%)]\tLoss: 0.315131\tGrad Norm: 1.649690\tLR: 0.030000\n",
      "Train Epoch: 1053 [86016/194182 (44%)]\tLoss: 0.309693\tGrad Norm: 1.529366\tLR: 0.030000\n",
      "Train Epoch: 1053 [106496/194182 (54%)]\tLoss: 0.299697\tGrad Norm: 0.908013\tLR: 0.030000\n",
      "Train Epoch: 1053 [126976/194182 (65%)]\tLoss: 0.310884\tGrad Norm: 1.259713\tLR: 0.030000\n",
      "Train Epoch: 1053 [147456/194182 (75%)]\tLoss: 0.313481\tGrad Norm: 1.397642\tLR: 0.030000\n",
      "Train Epoch: 1053 [167936/194182 (85%)]\tLoss: 0.317835\tGrad Norm: 1.510249\tLR: 0.030000\n",
      "Train Epoch: 1053 [188416/194182 (96%)]\tLoss: 0.315689\tGrad Norm: 1.381881\tLR: 0.030000\n",
      "Train set: Average loss: 0.3110\n",
      "Test set: Average loss: 0.2406, Average MAE: 0.3378\n",
      "Train Epoch: 1054 [4096/194182 (2%)]\tLoss: 0.313012\tGrad Norm: 1.290610\tLR: 0.030000\n",
      "Train Epoch: 1054 [24576/194182 (12%)]\tLoss: 0.310895\tGrad Norm: 1.653880\tLR: 0.030000\n",
      "Train Epoch: 1054 [45056/194182 (23%)]\tLoss: 0.305883\tGrad Norm: 1.137936\tLR: 0.030000\n",
      "Train Epoch: 1054 [65536/194182 (33%)]\tLoss: 0.304018\tGrad Norm: 1.661979\tLR: 0.030000\n",
      "Train Epoch: 1054 [86016/194182 (44%)]\tLoss: 0.310796\tGrad Norm: 1.376995\tLR: 0.030000\n",
      "Train Epoch: 1054 [106496/194182 (54%)]\tLoss: 0.305013\tGrad Norm: 1.380465\tLR: 0.030000\n",
      "Train Epoch: 1054 [126976/194182 (65%)]\tLoss: 0.306876\tGrad Norm: 1.292886\tLR: 0.030000\n",
      "Train Epoch: 1054 [147456/194182 (75%)]\tLoss: 0.311924\tGrad Norm: 1.441978\tLR: 0.030000\n",
      "Train Epoch: 1054 [167936/194182 (85%)]\tLoss: 0.312786\tGrad Norm: 1.457633\tLR: 0.030000\n",
      "Train Epoch: 1054 [188416/194182 (96%)]\tLoss: 0.310085\tGrad Norm: 1.183480\tLR: 0.030000\n",
      "Train set: Average loss: 0.3103\n",
      "Test set: Average loss: 0.2454, Average MAE: 0.3464\n",
      "Train Epoch: 1055 [4096/194182 (2%)]\tLoss: 0.310680\tGrad Norm: 1.369862\tLR: 0.030000\n",
      "Train Epoch: 1055 [24576/194182 (12%)]\tLoss: 0.318312\tGrad Norm: 1.483926\tLR: 0.030000\n",
      "Train Epoch: 1055 [45056/194182 (23%)]\tLoss: 0.316922\tGrad Norm: 1.671323\tLR: 0.030000\n",
      "Train Epoch: 1055 [65536/194182 (33%)]\tLoss: 0.312812\tGrad Norm: 1.762357\tLR: 0.030000\n",
      "Train Epoch: 1055 [86016/194182 (44%)]\tLoss: 0.321200\tGrad Norm: 1.846019\tLR: 0.030000\n",
      "Train Epoch: 1055 [106496/194182 (54%)]\tLoss: 0.314578\tGrad Norm: 1.639604\tLR: 0.030000\n",
      "Train Epoch: 1055 [126976/194182 (65%)]\tLoss: 0.310298\tGrad Norm: 1.068611\tLR: 0.030000\n",
      "Train Epoch: 1055 [147456/194182 (75%)]\tLoss: 0.302342\tGrad Norm: 1.121968\tLR: 0.030000\n",
      "Train Epoch: 1055 [167936/194182 (85%)]\tLoss: 0.302522\tGrad Norm: 1.009770\tLR: 0.030000\n",
      "Train Epoch: 1055 [188416/194182 (96%)]\tLoss: 0.306166\tGrad Norm: 1.097905\tLR: 0.030000\n",
      "Train set: Average loss: 0.3105\n",
      "Test set: Average loss: 0.2407, Average MAE: 0.3420\n",
      "Epoch 1055: Mean reward = 0.052 +/- 0.020\n",
      "Train Epoch: 1056 [4096/194182 (2%)]\tLoss: 0.305693\tGrad Norm: 1.201770\tLR: 0.030000\n",
      "Train Epoch: 1056 [24576/194182 (12%)]\tLoss: 0.311509\tGrad Norm: 1.272493\tLR: 0.030000\n",
      "Train Epoch: 1056 [45056/194182 (23%)]\tLoss: 0.306068\tGrad Norm: 1.370761\tLR: 0.030000\n",
      "Train Epoch: 1056 [65536/194182 (33%)]\tLoss: 0.316237\tGrad Norm: 1.410795\tLR: 0.030000\n",
      "Train Epoch: 1056 [86016/194182 (44%)]\tLoss: 0.312867\tGrad Norm: 1.549529\tLR: 0.030000\n",
      "Train Epoch: 1056 [106496/194182 (54%)]\tLoss: 0.308328\tGrad Norm: 1.454650\tLR: 0.030000\n",
      "Train Epoch: 1056 [126976/194182 (65%)]\tLoss: 0.297936\tGrad Norm: 1.111841\tLR: 0.030000\n",
      "Train Epoch: 1056 [147456/194182 (75%)]\tLoss: 0.316542\tGrad Norm: 1.459071\tLR: 0.030000\n",
      "Train Epoch: 1056 [167936/194182 (85%)]\tLoss: 0.315213\tGrad Norm: 1.481792\tLR: 0.030000\n",
      "Train Epoch: 1056 [188416/194182 (96%)]\tLoss: 0.312246\tGrad Norm: 1.540544\tLR: 0.030000\n",
      "Train set: Average loss: 0.3096\n",
      "Test set: Average loss: 0.2414, Average MAE: 0.3302\n",
      "Train Epoch: 1057 [4096/194182 (2%)]\tLoss: 0.307859\tGrad Norm: 1.466400\tLR: 0.030000\n",
      "Train Epoch: 1057 [24576/194182 (12%)]\tLoss: 0.320131\tGrad Norm: 1.732765\tLR: 0.030000\n",
      "Train Epoch: 1057 [45056/194182 (23%)]\tLoss: 0.312375\tGrad Norm: 1.609850\tLR: 0.030000\n",
      "Train Epoch: 1057 [65536/194182 (33%)]\tLoss: 0.321227\tGrad Norm: 1.608224\tLR: 0.030000\n",
      "Train Epoch: 1057 [86016/194182 (44%)]\tLoss: 0.311203\tGrad Norm: 1.488874\tLR: 0.030000\n",
      "Train Epoch: 1057 [106496/194182 (54%)]\tLoss: 0.302059\tGrad Norm: 0.783016\tLR: 0.030000\n",
      "Train Epoch: 1057 [126976/194182 (65%)]\tLoss: 0.308068\tGrad Norm: 1.347352\tLR: 0.030000\n",
      "Train Epoch: 1057 [147456/194182 (75%)]\tLoss: 0.311175\tGrad Norm: 1.309770\tLR: 0.030000\n",
      "Train Epoch: 1057 [167936/194182 (85%)]\tLoss: 0.314430\tGrad Norm: 1.718946\tLR: 0.030000\n",
      "Train Epoch: 1057 [188416/194182 (96%)]\tLoss: 0.310065\tGrad Norm: 1.552530\tLR: 0.030000\n",
      "Train set: Average loss: 0.3113\n",
      "Test set: Average loss: 0.2466, Average MAE: 0.3364\n",
      "Train Epoch: 1058 [4096/194182 (2%)]\tLoss: 0.313129\tGrad Norm: 1.634088\tLR: 0.030000\n",
      "Train Epoch: 1058 [24576/194182 (12%)]\tLoss: 0.316058\tGrad Norm: 1.911929\tLR: 0.030000\n",
      "Train Epoch: 1058 [45056/194182 (23%)]\tLoss: 0.308624\tGrad Norm: 1.494406\tLR: 0.030000\n",
      "Train Epoch: 1058 [65536/194182 (33%)]\tLoss: 0.311369\tGrad Norm: 1.336844\tLR: 0.030000\n",
      "Train Epoch: 1058 [86016/194182 (44%)]\tLoss: 0.318976\tGrad Norm: 1.559951\tLR: 0.030000\n",
      "Train Epoch: 1058 [106496/194182 (54%)]\tLoss: 0.315978\tGrad Norm: 1.584429\tLR: 0.030000\n",
      "Train Epoch: 1058 [126976/194182 (65%)]\tLoss: 0.315522\tGrad Norm: 1.340154\tLR: 0.030000\n",
      "Train Epoch: 1058 [147456/194182 (75%)]\tLoss: 0.302353\tGrad Norm: 1.277482\tLR: 0.030000\n",
      "Train Epoch: 1058 [167936/194182 (85%)]\tLoss: 0.309502\tGrad Norm: 1.308470\tLR: 0.030000\n",
      "Train Epoch: 1058 [188416/194182 (96%)]\tLoss: 0.303801\tGrad Norm: 1.117407\tLR: 0.030000\n",
      "Train set: Average loss: 0.3107\n",
      "Test set: Average loss: 0.2373, Average MAE: 0.3349\n",
      "Train Epoch: 1059 [4096/194182 (2%)]\tLoss: 0.298795\tGrad Norm: 0.962919\tLR: 0.030000\n",
      "Train Epoch: 1059 [24576/194182 (12%)]\tLoss: 0.300929\tGrad Norm: 1.132896\tLR: 0.030000\n",
      "Train Epoch: 1059 [45056/194182 (23%)]\tLoss: 0.311309\tGrad Norm: 1.284547\tLR: 0.030000\n",
      "Train Epoch: 1059 [65536/194182 (33%)]\tLoss: 0.305975\tGrad Norm: 1.040887\tLR: 0.030000\n",
      "Train Epoch: 1059 [86016/194182 (44%)]\tLoss: 0.312914\tGrad Norm: 1.289142\tLR: 0.030000\n",
      "Train Epoch: 1059 [106496/194182 (54%)]\tLoss: 0.300922\tGrad Norm: 1.406659\tLR: 0.030000\n",
      "Train Epoch: 1059 [126976/194182 (65%)]\tLoss: 0.313676\tGrad Norm: 1.650097\tLR: 0.030000\n",
      "Train Epoch: 1059 [147456/194182 (75%)]\tLoss: 0.308790\tGrad Norm: 1.505610\tLR: 0.030000\n",
      "Train Epoch: 1059 [167936/194182 (85%)]\tLoss: 0.309256\tGrad Norm: 1.355167\tLR: 0.030000\n",
      "Train Epoch: 1059 [188416/194182 (96%)]\tLoss: 0.317721\tGrad Norm: 1.753950\tLR: 0.030000\n",
      "Train set: Average loss: 0.3092\n",
      "Test set: Average loss: 0.2587, Average MAE: 0.3697\n",
      "Train Epoch: 1060 [4096/194182 (2%)]\tLoss: 0.329412\tGrad Norm: 2.062753\tLR: 0.030000\n",
      "Train Epoch: 1060 [24576/194182 (12%)]\tLoss: 0.314175\tGrad Norm: 1.591180\tLR: 0.030000\n",
      "Train Epoch: 1060 [45056/194182 (23%)]\tLoss: 0.320290\tGrad Norm: 1.823710\tLR: 0.030000\n",
      "Train Epoch: 1060 [65536/194182 (33%)]\tLoss: 0.306278\tGrad Norm: 1.213107\tLR: 0.030000\n",
      "Train Epoch: 1060 [86016/194182 (44%)]\tLoss: 0.319199\tGrad Norm: 1.387471\tLR: 0.030000\n",
      "Train Epoch: 1060 [106496/194182 (54%)]\tLoss: 0.311701\tGrad Norm: 1.398280\tLR: 0.030000\n",
      "Train Epoch: 1060 [126976/194182 (65%)]\tLoss: 0.304017\tGrad Norm: 1.017838\tLR: 0.030000\n",
      "Train Epoch: 1060 [147456/194182 (75%)]\tLoss: 0.299200\tGrad Norm: 1.052341\tLR: 0.030000\n",
      "Train Epoch: 1060 [167936/194182 (85%)]\tLoss: 0.305357\tGrad Norm: 1.006126\tLR: 0.030000\n",
      "Train Epoch: 1060 [188416/194182 (96%)]\tLoss: 0.304940\tGrad Norm: 1.404526\tLR: 0.030000\n",
      "Train set: Average loss: 0.3091\n",
      "Test set: Average loss: 0.2458, Average MAE: 0.3531\n",
      "Epoch 1060: Mean reward = 0.057 +/- 0.067\n",
      "Train Epoch: 1061 [4096/194182 (2%)]\tLoss: 0.311194\tGrad Norm: 1.579943\tLR: 0.030000\n",
      "Train Epoch: 1061 [24576/194182 (12%)]\tLoss: 0.304835\tGrad Norm: 1.438366\tLR: 0.030000\n",
      "Train Epoch: 1061 [45056/194182 (23%)]\tLoss: 0.305754\tGrad Norm: 1.259166\tLR: 0.030000\n",
      "Train Epoch: 1061 [65536/194182 (33%)]\tLoss: 0.310979\tGrad Norm: 1.565699\tLR: 0.030000\n",
      "Train Epoch: 1061 [86016/194182 (44%)]\tLoss: 0.312045\tGrad Norm: 1.532227\tLR: 0.030000\n",
      "Train Epoch: 1061 [106496/194182 (54%)]\tLoss: 0.315908\tGrad Norm: 1.580349\tLR: 0.030000\n",
      "Train Epoch: 1061 [126976/194182 (65%)]\tLoss: 0.312929\tGrad Norm: 1.387116\tLR: 0.030000\n",
      "Train Epoch: 1061 [147456/194182 (75%)]\tLoss: 0.303262\tGrad Norm: 1.264127\tLR: 0.030000\n",
      "Train Epoch: 1061 [167936/194182 (85%)]\tLoss: 0.315534\tGrad Norm: 1.991809\tLR: 0.030000\n",
      "Train Epoch: 1061 [188416/194182 (96%)]\tLoss: 0.305033\tGrad Norm: 1.377860\tLR: 0.030000\n",
      "Train set: Average loss: 0.3105\n",
      "Test set: Average loss: 0.2384, Average MAE: 0.3417\n",
      "Train Epoch: 1062 [4096/194182 (2%)]\tLoss: 0.303062\tGrad Norm: 0.919790\tLR: 0.030000\n",
      "Train Epoch: 1062 [24576/194182 (12%)]\tLoss: 0.299068\tGrad Norm: 1.141722\tLR: 0.030000\n",
      "Train Epoch: 1062 [45056/194182 (23%)]\tLoss: 0.312466\tGrad Norm: 1.480354\tLR: 0.030000\n",
      "Train Epoch: 1062 [65536/194182 (33%)]\tLoss: 0.317933\tGrad Norm: 1.763886\tLR: 0.030000\n",
      "Train Epoch: 1062 [86016/194182 (44%)]\tLoss: 0.316336\tGrad Norm: 1.567410\tLR: 0.030000\n",
      "Train Epoch: 1062 [106496/194182 (54%)]\tLoss: 0.315637\tGrad Norm: 1.587594\tLR: 0.030000\n",
      "Train Epoch: 1062 [126976/194182 (65%)]\tLoss: 0.317076\tGrad Norm: 1.304084\tLR: 0.030000\n",
      "Train Epoch: 1062 [147456/194182 (75%)]\tLoss: 0.313452\tGrad Norm: 1.417751\tLR: 0.030000\n",
      "Train Epoch: 1062 [167936/194182 (85%)]\tLoss: 0.298926\tGrad Norm: 1.100501\tLR: 0.030000\n",
      "Train Epoch: 1062 [188416/194182 (96%)]\tLoss: 0.305305\tGrad Norm: 1.138803\tLR: 0.030000\n",
      "Train set: Average loss: 0.3097\n",
      "Test set: Average loss: 0.2413, Average MAE: 0.3453\n",
      "Train Epoch: 1063 [4096/194182 (2%)]\tLoss: 0.306826\tGrad Norm: 1.158256\tLR: 0.030000\n",
      "Train Epoch: 1063 [24576/194182 (12%)]\tLoss: 0.305657\tGrad Norm: 1.508289\tLR: 0.030000\n",
      "Train Epoch: 1063 [45056/194182 (23%)]\tLoss: 0.308673\tGrad Norm: 1.589808\tLR: 0.030000\n",
      "Train Epoch: 1063 [65536/194182 (33%)]\tLoss: 0.300433\tGrad Norm: 1.125959\tLR: 0.030000\n",
      "Train Epoch: 1063 [86016/194182 (44%)]\tLoss: 0.305280\tGrad Norm: 1.394057\tLR: 0.030000\n",
      "Train Epoch: 1063 [106496/194182 (54%)]\tLoss: 0.301789\tGrad Norm: 1.164140\tLR: 0.030000\n",
      "Train Epoch: 1063 [126976/194182 (65%)]\tLoss: 0.307329\tGrad Norm: 1.320870\tLR: 0.030000\n",
      "Train Epoch: 1063 [147456/194182 (75%)]\tLoss: 0.311476\tGrad Norm: 1.245155\tLR: 0.030000\n",
      "Train Epoch: 1063 [167936/194182 (85%)]\tLoss: 0.304878\tGrad Norm: 1.193343\tLR: 0.030000\n",
      "Train Epoch: 1063 [188416/194182 (96%)]\tLoss: 0.309614\tGrad Norm: 1.474990\tLR: 0.030000\n",
      "Train set: Average loss: 0.3079\n",
      "Test set: Average loss: 0.2436, Average MAE: 0.3467\n",
      "Train Epoch: 1064 [4096/194182 (2%)]\tLoss: 0.303736\tGrad Norm: 1.222571\tLR: 0.030000\n",
      "Train Epoch: 1064 [24576/194182 (12%)]\tLoss: 0.311744\tGrad Norm: 1.283962\tLR: 0.030000\n",
      "Train Epoch: 1064 [45056/194182 (23%)]\tLoss: 0.310315\tGrad Norm: 1.347680\tLR: 0.030000\n",
      "Train Epoch: 1064 [65536/194182 (33%)]\tLoss: 0.307430\tGrad Norm: 1.509304\tLR: 0.030000\n",
      "Train Epoch: 1064 [86016/194182 (44%)]\tLoss: 0.310513\tGrad Norm: 1.262148\tLR: 0.030000\n",
      "Train Epoch: 1064 [106496/194182 (54%)]\tLoss: 0.309626\tGrad Norm: 1.578761\tLR: 0.030000\n",
      "Train Epoch: 1064 [126976/194182 (65%)]\tLoss: 0.309797\tGrad Norm: 1.215894\tLR: 0.030000\n",
      "Train Epoch: 1064 [147456/194182 (75%)]\tLoss: 0.315458\tGrad Norm: 1.680457\tLR: 0.030000\n",
      "Train Epoch: 1064 [167936/194182 (85%)]\tLoss: 0.300241\tGrad Norm: 1.221320\tLR: 0.030000\n",
      "Train Epoch: 1064 [188416/194182 (96%)]\tLoss: 0.308031\tGrad Norm: 1.144409\tLR: 0.030000\n",
      "Train set: Average loss: 0.3077\n",
      "Test set: Average loss: 0.2392, Average MAE: 0.3351\n",
      "Train Epoch: 1065 [4096/194182 (2%)]\tLoss: 0.310938\tGrad Norm: 1.135779\tLR: 0.030000\n",
      "Train Epoch: 1065 [24576/194182 (12%)]\tLoss: 0.304902\tGrad Norm: 1.329707\tLR: 0.030000\n",
      "Train Epoch: 1065 [45056/194182 (23%)]\tLoss: 0.300578\tGrad Norm: 1.294760\tLR: 0.030000\n",
      "Train Epoch: 1065 [65536/194182 (33%)]\tLoss: 0.311264\tGrad Norm: 1.511859\tLR: 0.030000\n",
      "Train Epoch: 1065 [86016/194182 (44%)]\tLoss: 0.319101\tGrad Norm: 1.940540\tLR: 0.030000\n",
      "Train Epoch: 1065 [106496/194182 (54%)]\tLoss: 0.316192\tGrad Norm: 1.739091\tLR: 0.030000\n",
      "Train Epoch: 1065 [126976/194182 (65%)]\tLoss: 0.309048\tGrad Norm: 1.487683\tLR: 0.030000\n",
      "Train Epoch: 1065 [147456/194182 (75%)]\tLoss: 0.312913\tGrad Norm: 1.664723\tLR: 0.030000\n",
      "Train Epoch: 1065 [167936/194182 (85%)]\tLoss: 0.305779\tGrad Norm: 1.409321\tLR: 0.030000\n",
      "Train Epoch: 1065 [188416/194182 (96%)]\tLoss: 0.307692\tGrad Norm: 1.170231\tLR: 0.030000\n",
      "Train set: Average loss: 0.3093\n",
      "Test set: Average loss: 0.2430, Average MAE: 0.3341\n",
      "Epoch 1065: Mean reward = 0.042 +/- 0.014\n",
      "Train Epoch: 1066 [4096/194182 (2%)]\tLoss: 0.306470\tGrad Norm: 1.444144\tLR: 0.030000\n",
      "Train Epoch: 1066 [24576/194182 (12%)]\tLoss: 0.313755\tGrad Norm: 1.475498\tLR: 0.030000\n",
      "Train Epoch: 1066 [45056/194182 (23%)]\tLoss: 0.307216\tGrad Norm: 1.340077\tLR: 0.030000\n",
      "Train Epoch: 1066 [65536/194182 (33%)]\tLoss: 0.309107\tGrad Norm: 1.329907\tLR: 0.030000\n",
      "Train Epoch: 1066 [86016/194182 (44%)]\tLoss: 0.310105\tGrad Norm: 1.554468\tLR: 0.030000\n",
      "Train Epoch: 1066 [106496/194182 (54%)]\tLoss: 0.309078\tGrad Norm: 1.252315\tLR: 0.030000\n",
      "Train Epoch: 1066 [126976/194182 (65%)]\tLoss: 0.299156\tGrad Norm: 1.129259\tLR: 0.030000\n",
      "Train Epoch: 1066 [147456/194182 (75%)]\tLoss: 0.307829\tGrad Norm: 1.239756\tLR: 0.030000\n",
      "Train Epoch: 1066 [167936/194182 (85%)]\tLoss: 0.306652\tGrad Norm: 1.406587\tLR: 0.030000\n",
      "Train Epoch: 1066 [188416/194182 (96%)]\tLoss: 0.311040\tGrad Norm: 1.656494\tLR: 0.030000\n",
      "Train set: Average loss: 0.3082\n",
      "Test set: Average loss: 0.2468, Average MAE: 0.3531\n",
      "Train Epoch: 1067 [4096/194182 (2%)]\tLoss: 0.315697\tGrad Norm: 1.448257\tLR: 0.030000\n",
      "Train Epoch: 1067 [24576/194182 (12%)]\tLoss: 0.299454\tGrad Norm: 1.328924\tLR: 0.030000\n",
      "Train Epoch: 1067 [45056/194182 (23%)]\tLoss: 0.307872\tGrad Norm: 1.575910\tLR: 0.030000\n",
      "Train Epoch: 1067 [65536/194182 (33%)]\tLoss: 0.303071\tGrad Norm: 1.507372\tLR: 0.030000\n",
      "Train Epoch: 1067 [86016/194182 (44%)]\tLoss: 0.321076\tGrad Norm: 1.734692\tLR: 0.030000\n",
      "Train Epoch: 1067 [106496/194182 (54%)]\tLoss: 0.310406\tGrad Norm: 1.831350\tLR: 0.030000\n",
      "Train Epoch: 1067 [126976/194182 (65%)]\tLoss: 0.303773\tGrad Norm: 1.420230\tLR: 0.030000\n",
      "Train Epoch: 1067 [147456/194182 (75%)]\tLoss: 0.306736\tGrad Norm: 1.291518\tLR: 0.030000\n",
      "Train Epoch: 1067 [167936/194182 (85%)]\tLoss: 0.313150\tGrad Norm: 1.141037\tLR: 0.030000\n",
      "Train Epoch: 1067 [188416/194182 (96%)]\tLoss: 0.309814\tGrad Norm: 1.333747\tLR: 0.030000\n",
      "Train set: Average loss: 0.3091\n",
      "Test set: Average loss: 0.2469, Average MAE: 0.3542\n",
      "Train Epoch: 1068 [4096/194182 (2%)]\tLoss: 0.310542\tGrad Norm: 1.482655\tLR: 0.030000\n",
      "Train Epoch: 1068 [24576/194182 (12%)]\tLoss: 0.309604\tGrad Norm: 1.413003\tLR: 0.030000\n",
      "Train Epoch: 1068 [45056/194182 (23%)]\tLoss: 0.318050\tGrad Norm: 1.698856\tLR: 0.030000\n",
      "Train Epoch: 1068 [65536/194182 (33%)]\tLoss: 0.322981\tGrad Norm: 1.870465\tLR: 0.030000\n",
      "Train Epoch: 1068 [86016/194182 (44%)]\tLoss: 0.309732\tGrad Norm: 1.606006\tLR: 0.030000\n",
      "Train Epoch: 1068 [106496/194182 (54%)]\tLoss: 0.300972\tGrad Norm: 1.087182\tLR: 0.030000\n",
      "Train Epoch: 1068 [126976/194182 (65%)]\tLoss: 0.308872\tGrad Norm: 1.600970\tLR: 0.030000\n",
      "Train Epoch: 1068 [147456/194182 (75%)]\tLoss: 0.310017\tGrad Norm: 1.478955\tLR: 0.030000\n",
      "Train Epoch: 1068 [167936/194182 (85%)]\tLoss: 0.313236\tGrad Norm: 1.442222\tLR: 0.030000\n",
      "Train Epoch: 1068 [188416/194182 (96%)]\tLoss: 0.309473\tGrad Norm: 1.429950\tLR: 0.030000\n",
      "Train set: Average loss: 0.3093\n",
      "Test set: Average loss: 0.2477, Average MAE: 0.3558\n",
      "Train Epoch: 1069 [4096/194182 (2%)]\tLoss: 0.312003\tGrad Norm: 1.667850\tLR: 0.030000\n",
      "Train Epoch: 1069 [24576/194182 (12%)]\tLoss: 0.317873\tGrad Norm: 2.099678\tLR: 0.030000\n",
      "Train Epoch: 1069 [45056/194182 (23%)]\tLoss: 0.305188\tGrad Norm: 1.374835\tLR: 0.030000\n",
      "Train Epoch: 1069 [65536/194182 (33%)]\tLoss: 0.304736\tGrad Norm: 1.145145\tLR: 0.030000\n",
      "Train Epoch: 1069 [86016/194182 (44%)]\tLoss: 0.314358\tGrad Norm: 1.724867\tLR: 0.030000\n",
      "Train Epoch: 1069 [106496/194182 (54%)]\tLoss: 0.305953\tGrad Norm: 1.564406\tLR: 0.030000\n",
      "Train Epoch: 1069 [126976/194182 (65%)]\tLoss: 0.311698\tGrad Norm: 1.468749\tLR: 0.030000\n",
      "Train Epoch: 1069 [147456/194182 (75%)]\tLoss: 0.301590\tGrad Norm: 1.019633\tLR: 0.030000\n",
      "Train Epoch: 1069 [167936/194182 (85%)]\tLoss: 0.302728\tGrad Norm: 1.228347\tLR: 0.030000\n",
      "Train Epoch: 1069 [188416/194182 (96%)]\tLoss: 0.303959\tGrad Norm: 1.305785\tLR: 0.030000\n",
      "Train set: Average loss: 0.3079\n",
      "Test set: Average loss: 0.2422, Average MAE: 0.3508\n",
      "Train Epoch: 1070 [4096/194182 (2%)]\tLoss: 0.306330\tGrad Norm: 1.274384\tLR: 0.030000\n",
      "Train Epoch: 1070 [24576/194182 (12%)]\tLoss: 0.310739\tGrad Norm: 1.376352\tLR: 0.030000\n",
      "Train Epoch: 1070 [45056/194182 (23%)]\tLoss: 0.309836\tGrad Norm: 1.242903\tLR: 0.030000\n",
      "Train Epoch: 1070 [65536/194182 (33%)]\tLoss: 0.309358\tGrad Norm: 1.284864\tLR: 0.030000\n",
      "Train Epoch: 1070 [86016/194182 (44%)]\tLoss: 0.306674\tGrad Norm: 1.640018\tLR: 0.030000\n",
      "Train Epoch: 1070 [106496/194182 (54%)]\tLoss: 0.316525\tGrad Norm: 1.425886\tLR: 0.030000\n",
      "Train Epoch: 1070 [126976/194182 (65%)]\tLoss: 0.322241\tGrad Norm: 1.967948\tLR: 0.030000\n",
      "Train Epoch: 1070 [147456/194182 (75%)]\tLoss: 0.308140\tGrad Norm: 1.833602\tLR: 0.030000\n",
      "Train Epoch: 1070 [167936/194182 (85%)]\tLoss: 0.308263\tGrad Norm: 1.527848\tLR: 0.030000\n",
      "Train Epoch: 1070 [188416/194182 (96%)]\tLoss: 0.309559\tGrad Norm: 1.445996\tLR: 0.030000\n",
      "Train set: Average loss: 0.3091\n",
      "Test set: Average loss: 0.2424, Average MAE: 0.3324\n",
      "Epoch 1070: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 1071 [4096/194182 (2%)]\tLoss: 0.312889\tGrad Norm: 1.490199\tLR: 0.030000\n",
      "Train Epoch: 1071 [24576/194182 (12%)]\tLoss: 0.303361\tGrad Norm: 1.202063\tLR: 0.030000\n",
      "Train Epoch: 1071 [45056/194182 (23%)]\tLoss: 0.303187\tGrad Norm: 1.193694\tLR: 0.030000\n",
      "Train Epoch: 1071 [65536/194182 (33%)]\tLoss: 0.298934\tGrad Norm: 1.150256\tLR: 0.030000\n",
      "Train Epoch: 1071 [86016/194182 (44%)]\tLoss: 0.294737\tGrad Norm: 1.099707\tLR: 0.030000\n",
      "Train Epoch: 1071 [106496/194182 (54%)]\tLoss: 0.312378\tGrad Norm: 1.509398\tLR: 0.030000\n",
      "Train Epoch: 1071 [126976/194182 (65%)]\tLoss: 0.306389\tGrad Norm: 1.634883\tLR: 0.030000\n",
      "Train Epoch: 1071 [147456/194182 (75%)]\tLoss: 0.307968\tGrad Norm: 1.484895\tLR: 0.030000\n",
      "Train Epoch: 1071 [167936/194182 (85%)]\tLoss: 0.313173\tGrad Norm: 1.631011\tLR: 0.030000\n",
      "Train Epoch: 1071 [188416/194182 (96%)]\tLoss: 0.302293\tGrad Norm: 1.336405\tLR: 0.030000\n",
      "Train set: Average loss: 0.3064\n",
      "Test set: Average loss: 0.2432, Average MAE: 0.3353\n",
      "Train Epoch: 1072 [4096/194182 (2%)]\tLoss: 0.303376\tGrad Norm: 1.267234\tLR: 0.030000\n",
      "Train Epoch: 1072 [24576/194182 (12%)]\tLoss: 0.294757\tGrad Norm: 0.968716\tLR: 0.030000\n",
      "Train Epoch: 1072 [45056/194182 (23%)]\tLoss: 0.300418\tGrad Norm: 1.400283\tLR: 0.030000\n",
      "Train Epoch: 1072 [65536/194182 (33%)]\tLoss: 0.314024\tGrad Norm: 1.802972\tLR: 0.030000\n",
      "Train Epoch: 1072 [86016/194182 (44%)]\tLoss: 0.301687\tGrad Norm: 1.701614\tLR: 0.030000\n",
      "Train Epoch: 1072 [106496/194182 (54%)]\tLoss: 0.307464\tGrad Norm: 1.510359\tLR: 0.030000\n",
      "Train Epoch: 1072 [126976/194182 (65%)]\tLoss: 0.308279\tGrad Norm: 1.528608\tLR: 0.030000\n",
      "Train Epoch: 1072 [147456/194182 (75%)]\tLoss: 0.319227\tGrad Norm: 1.682178\tLR: 0.030000\n",
      "Train Epoch: 1072 [167936/194182 (85%)]\tLoss: 0.310398\tGrad Norm: 1.297416\tLR: 0.030000\n",
      "Train Epoch: 1072 [188416/194182 (96%)]\tLoss: 0.307128\tGrad Norm: 1.292934\tLR: 0.030000\n",
      "Train set: Average loss: 0.3079\n",
      "Test set: Average loss: 0.2403, Average MAE: 0.3467\n",
      "Train Epoch: 1073 [4096/194182 (2%)]\tLoss: 0.311234\tGrad Norm: 1.321328\tLR: 0.030000\n",
      "Train Epoch: 1073 [24576/194182 (12%)]\tLoss: 0.306629\tGrad Norm: 1.562321\tLR: 0.030000\n",
      "Train Epoch: 1073 [45056/194182 (23%)]\tLoss: 0.303535\tGrad Norm: 1.279113\tLR: 0.030000\n",
      "Train Epoch: 1073 [65536/194182 (33%)]\tLoss: 0.312105\tGrad Norm: 1.350000\tLR: 0.030000\n",
      "Train Epoch: 1073 [86016/194182 (44%)]\tLoss: 0.313266\tGrad Norm: 1.457723\tLR: 0.030000\n",
      "Train Epoch: 1073 [106496/194182 (54%)]\tLoss: 0.308517\tGrad Norm: 1.341682\tLR: 0.030000\n",
      "Train Epoch: 1073 [126976/194182 (65%)]\tLoss: 0.314540\tGrad Norm: 1.738263\tLR: 0.030000\n",
      "Train Epoch: 1073 [147456/194182 (75%)]\tLoss: 0.312153\tGrad Norm: 1.201953\tLR: 0.030000\n",
      "Train Epoch: 1073 [167936/194182 (85%)]\tLoss: 0.298499\tGrad Norm: 1.390404\tLR: 0.030000\n",
      "Train Epoch: 1073 [188416/194182 (96%)]\tLoss: 0.318320\tGrad Norm: 1.473968\tLR: 0.030000\n",
      "Train set: Average loss: 0.3079\n",
      "Test set: Average loss: 0.2520, Average MAE: 0.3406\n",
      "Train Epoch: 1074 [4096/194182 (2%)]\tLoss: 0.325960\tGrad Norm: 2.072299\tLR: 0.030000\n",
      "Train Epoch: 1074 [24576/194182 (12%)]\tLoss: 0.302363\tGrad Norm: 1.240591\tLR: 0.030000\n",
      "Train Epoch: 1074 [45056/194182 (23%)]\tLoss: 0.313019\tGrad Norm: 1.223625\tLR: 0.030000\n",
      "Train Epoch: 1074 [65536/194182 (33%)]\tLoss: 0.314490\tGrad Norm: 1.481114\tLR: 0.030000\n",
      "Train Epoch: 1074 [86016/194182 (44%)]\tLoss: 0.309050\tGrad Norm: 1.546035\tLR: 0.030000\n",
      "Train Epoch: 1074 [106496/194182 (54%)]\tLoss: 0.304743\tGrad Norm: 1.270072\tLR: 0.030000\n",
      "Train Epoch: 1074 [126976/194182 (65%)]\tLoss: 0.308154\tGrad Norm: 1.324252\tLR: 0.030000\n",
      "Train Epoch: 1074 [147456/194182 (75%)]\tLoss: 0.311051\tGrad Norm: 1.406582\tLR: 0.030000\n",
      "Train Epoch: 1074 [167936/194182 (85%)]\tLoss: 0.303083\tGrad Norm: 1.208932\tLR: 0.030000\n",
      "Train Epoch: 1074 [188416/194182 (96%)]\tLoss: 0.307889\tGrad Norm: 1.423298\tLR: 0.030000\n",
      "Train set: Average loss: 0.3073\n",
      "Test set: Average loss: 0.2477, Average MAE: 0.3530\n",
      "Train Epoch: 1075 [4096/194182 (2%)]\tLoss: 0.314114\tGrad Norm: 1.550497\tLR: 0.030000\n",
      "Train Epoch: 1075 [24576/194182 (12%)]\tLoss: 0.306508\tGrad Norm: 1.513610\tLR: 0.030000\n",
      "Train Epoch: 1075 [45056/194182 (23%)]\tLoss: 0.300363\tGrad Norm: 1.383304\tLR: 0.030000\n",
      "Train Epoch: 1075 [65536/194182 (33%)]\tLoss: 0.309586\tGrad Norm: 1.592135\tLR: 0.030000\n",
      "Train Epoch: 1075 [86016/194182 (44%)]\tLoss: 0.309537\tGrad Norm: 1.593863\tLR: 0.030000\n",
      "Train Epoch: 1075 [106496/194182 (54%)]\tLoss: 0.308465\tGrad Norm: 1.539565\tLR: 0.030000\n",
      "Train Epoch: 1075 [126976/194182 (65%)]\tLoss: 0.307576\tGrad Norm: 1.524101\tLR: 0.030000\n",
      "Train Epoch: 1075 [147456/194182 (75%)]\tLoss: 0.301604\tGrad Norm: 1.596714\tLR: 0.030000\n",
      "Train Epoch: 1075 [167936/194182 (85%)]\tLoss: 0.299463\tGrad Norm: 1.001625\tLR: 0.030000\n",
      "Train Epoch: 1075 [188416/194182 (96%)]\tLoss: 0.296201\tGrad Norm: 1.118890\tLR: 0.030000\n",
      "Train set: Average loss: 0.3064\n",
      "Test set: Average loss: 0.2442, Average MAE: 0.3463\n",
      "Epoch 1075: Mean reward = 0.074 +/- 0.082\n",
      "Train Epoch: 1076 [4096/194182 (2%)]\tLoss: 0.299828\tGrad Norm: 1.365056\tLR: 0.030000\n",
      "Train Epoch: 1076 [24576/194182 (12%)]\tLoss: 0.307531\tGrad Norm: 1.457507\tLR: 0.030000\n",
      "Train Epoch: 1076 [45056/194182 (23%)]\tLoss: 0.306014\tGrad Norm: 1.411568\tLR: 0.030000\n",
      "Train Epoch: 1076 [65536/194182 (33%)]\tLoss: 0.310106\tGrad Norm: 1.507216\tLR: 0.030000\n",
      "Train Epoch: 1076 [86016/194182 (44%)]\tLoss: 0.302261\tGrad Norm: 1.198591\tLR: 0.030000\n",
      "Train Epoch: 1076 [106496/194182 (54%)]\tLoss: 0.305135\tGrad Norm: 1.307850\tLR: 0.030000\n",
      "Train Epoch: 1076 [126976/194182 (65%)]\tLoss: 0.302986\tGrad Norm: 1.539287\tLR: 0.030000\n",
      "Train Epoch: 1076 [147456/194182 (75%)]\tLoss: 0.301016\tGrad Norm: 1.431294\tLR: 0.030000\n",
      "Train Epoch: 1076 [167936/194182 (85%)]\tLoss: 0.317493\tGrad Norm: 1.825188\tLR: 0.030000\n",
      "Train Epoch: 1076 [188416/194182 (96%)]\tLoss: 0.318735\tGrad Norm: 2.021815\tLR: 0.030000\n",
      "Train set: Average loss: 0.3075\n",
      "Test set: Average loss: 0.2481, Average MAE: 0.3599\n",
      "Train Epoch: 1077 [4096/194182 (2%)]\tLoss: 0.311040\tGrad Norm: 1.574074\tLR: 0.030000\n",
      "Train Epoch: 1077 [24576/194182 (12%)]\tLoss: 0.311036\tGrad Norm: 1.263181\tLR: 0.030000\n",
      "Train Epoch: 1077 [45056/194182 (23%)]\tLoss: 0.300774\tGrad Norm: 1.125297\tLR: 0.030000\n",
      "Train Epoch: 1077 [65536/194182 (33%)]\tLoss: 0.310837\tGrad Norm: 1.292980\tLR: 0.030000\n",
      "Train Epoch: 1077 [86016/194182 (44%)]\tLoss: 0.309699\tGrad Norm: 1.592468\tLR: 0.030000\n",
      "Train Epoch: 1077 [106496/194182 (54%)]\tLoss: 0.304806\tGrad Norm: 1.289511\tLR: 0.030000\n",
      "Train Epoch: 1077 [126976/194182 (65%)]\tLoss: 0.302294\tGrad Norm: 1.132049\tLR: 0.030000\n",
      "Train Epoch: 1077 [147456/194182 (75%)]\tLoss: 0.307135\tGrad Norm: 1.416978\tLR: 0.030000\n",
      "Train Epoch: 1077 [167936/194182 (85%)]\tLoss: 0.303463\tGrad Norm: 1.282940\tLR: 0.030000\n",
      "Train Epoch: 1077 [188416/194182 (96%)]\tLoss: 0.306677\tGrad Norm: 1.460468\tLR: 0.030000\n",
      "Train set: Average loss: 0.3055\n",
      "Test set: Average loss: 0.2404, Average MAE: 0.3307\n",
      "Train Epoch: 1078 [4096/194182 (2%)]\tLoss: 0.296278\tGrad Norm: 1.332347\tLR: 0.030000\n",
      "Train Epoch: 1078 [24576/194182 (12%)]\tLoss: 0.306681\tGrad Norm: 1.654245\tLR: 0.030000\n",
      "Train Epoch: 1078 [45056/194182 (23%)]\tLoss: 0.313097\tGrad Norm: 1.553758\tLR: 0.030000\n",
      "Train Epoch: 1078 [65536/194182 (33%)]\tLoss: 0.307175\tGrad Norm: 1.619653\tLR: 0.030000\n",
      "Train Epoch: 1078 [86016/194182 (44%)]\tLoss: 0.308904\tGrad Norm: 1.558904\tLR: 0.030000\n",
      "Train Epoch: 1078 [106496/194182 (54%)]\tLoss: 0.300229\tGrad Norm: 1.190492\tLR: 0.030000\n",
      "Train Epoch: 1078 [126976/194182 (65%)]\tLoss: 0.290841\tGrad Norm: 1.081873\tLR: 0.030000\n",
      "Train Epoch: 1078 [147456/194182 (75%)]\tLoss: 0.303753\tGrad Norm: 1.234135\tLR: 0.030000\n",
      "Train Epoch: 1078 [167936/194182 (85%)]\tLoss: 0.307447\tGrad Norm: 1.607684\tLR: 0.030000\n",
      "Train Epoch: 1078 [188416/194182 (96%)]\tLoss: 0.306264\tGrad Norm: 1.403946\tLR: 0.030000\n",
      "Train set: Average loss: 0.3064\n",
      "Test set: Average loss: 0.2417, Average MAE: 0.3309\n",
      "Train Epoch: 1079 [4096/194182 (2%)]\tLoss: 0.304622\tGrad Norm: 1.437089\tLR: 0.030000\n",
      "Train Epoch: 1079 [24576/194182 (12%)]\tLoss: 0.312221\tGrad Norm: 1.709528\tLR: 0.030000\n",
      "Train Epoch: 1079 [45056/194182 (23%)]\tLoss: 0.315807\tGrad Norm: 1.541954\tLR: 0.030000\n",
      "Train Epoch: 1079 [65536/194182 (33%)]\tLoss: 0.304134\tGrad Norm: 1.441806\tLR: 0.030000\n",
      "Train Epoch: 1079 [86016/194182 (44%)]\tLoss: 0.306873\tGrad Norm: 1.372272\tLR: 0.030000\n",
      "Train Epoch: 1079 [106496/194182 (54%)]\tLoss: 0.299473\tGrad Norm: 1.373354\tLR: 0.030000\n",
      "Train Epoch: 1079 [126976/194182 (65%)]\tLoss: 0.316317\tGrad Norm: 1.656883\tLR: 0.030000\n",
      "Train Epoch: 1079 [147456/194182 (75%)]\tLoss: 0.298343\tGrad Norm: 1.537118\tLR: 0.030000\n",
      "Train Epoch: 1079 [167936/194182 (85%)]\tLoss: 0.296164\tGrad Norm: 1.203657\tLR: 0.030000\n",
      "Train Epoch: 1079 [188416/194182 (96%)]\tLoss: 0.319448\tGrad Norm: 1.856074\tLR: 0.030000\n",
      "Train set: Average loss: 0.3077\n",
      "Test set: Average loss: 0.2479, Average MAE: 0.3387\n",
      "Train Epoch: 1080 [4096/194182 (2%)]\tLoss: 0.304521\tGrad Norm: 1.563869\tLR: 0.030000\n",
      "Train Epoch: 1080 [24576/194182 (12%)]\tLoss: 0.300819\tGrad Norm: 1.265134\tLR: 0.030000\n",
      "Train Epoch: 1080 [45056/194182 (23%)]\tLoss: 0.297775\tGrad Norm: 1.384189\tLR: 0.030000\n",
      "Train Epoch: 1080 [65536/194182 (33%)]\tLoss: 0.310424\tGrad Norm: 1.379933\tLR: 0.030000\n",
      "Train Epoch: 1080 [86016/194182 (44%)]\tLoss: 0.314009\tGrad Norm: 1.789170\tLR: 0.030000\n",
      "Train Epoch: 1080 [106496/194182 (54%)]\tLoss: 0.307101\tGrad Norm: 1.186559\tLR: 0.030000\n",
      "Train Epoch: 1080 [126976/194182 (65%)]\tLoss: 0.304277\tGrad Norm: 1.352127\tLR: 0.030000\n",
      "Train Epoch: 1080 [147456/194182 (75%)]\tLoss: 0.301555\tGrad Norm: 1.397777\tLR: 0.030000\n",
      "Train Epoch: 1080 [167936/194182 (85%)]\tLoss: 0.292576\tGrad Norm: 0.695879\tLR: 0.030000\n",
      "Train Epoch: 1080 [188416/194182 (96%)]\tLoss: 0.302681\tGrad Norm: 0.851447\tLR: 0.030000\n",
      "Train set: Average loss: 0.3037\n",
      "Test set: Average loss: 0.2391, Average MAE: 0.3381\n",
      "Epoch 1080: Mean reward = 0.052 +/- 0.035\n",
      "Train Epoch: 1081 [4096/194182 (2%)]\tLoss: 0.304512\tGrad Norm: 1.028491\tLR: 0.030000\n",
      "Train Epoch: 1081 [24576/194182 (12%)]\tLoss: 0.300077\tGrad Norm: 1.286764\tLR: 0.030000\n",
      "Train Epoch: 1081 [45056/194182 (23%)]\tLoss: 0.303984\tGrad Norm: 1.253292\tLR: 0.030000\n",
      "Train Epoch: 1081 [65536/194182 (33%)]\tLoss: 0.302891\tGrad Norm: 1.197324\tLR: 0.030000\n",
      "Train Epoch: 1081 [86016/194182 (44%)]\tLoss: 0.307964\tGrad Norm: 1.268094\tLR: 0.030000\n",
      "Train Epoch: 1081 [106496/194182 (54%)]\tLoss: 0.309354\tGrad Norm: 1.670669\tLR: 0.030000\n",
      "Train Epoch: 1081 [126976/194182 (65%)]\tLoss: 0.304560\tGrad Norm: 1.128005\tLR: 0.030000\n",
      "Train Epoch: 1081 [147456/194182 (75%)]\tLoss: 0.310867\tGrad Norm: 1.546006\tLR: 0.030000\n",
      "Train Epoch: 1081 [167936/194182 (85%)]\tLoss: 0.313763\tGrad Norm: 1.589487\tLR: 0.030000\n",
      "Train Epoch: 1081 [188416/194182 (96%)]\tLoss: 0.301084\tGrad Norm: 1.431394\tLR: 0.030000\n",
      "Train set: Average loss: 0.3051\n",
      "Test set: Average loss: 0.2412, Average MAE: 0.3485\n",
      "Train Epoch: 1082 [4096/194182 (2%)]\tLoss: 0.302331\tGrad Norm: 1.286871\tLR: 0.030000\n",
      "Train Epoch: 1082 [24576/194182 (12%)]\tLoss: 0.305749\tGrad Norm: 1.653803\tLR: 0.030000\n",
      "Train Epoch: 1082 [45056/194182 (23%)]\tLoss: 0.310605\tGrad Norm: 1.775310\tLR: 0.030000\n",
      "Train Epoch: 1082 [65536/194182 (33%)]\tLoss: 0.308185\tGrad Norm: 1.683883\tLR: 0.030000\n",
      "Train Epoch: 1082 [86016/194182 (44%)]\tLoss: 0.314739\tGrad Norm: 1.835557\tLR: 0.030000\n",
      "Train Epoch: 1082 [106496/194182 (54%)]\tLoss: 0.317771\tGrad Norm: 1.873782\tLR: 0.030000\n",
      "Train Epoch: 1082 [126976/194182 (65%)]\tLoss: 0.303395\tGrad Norm: 1.250633\tLR: 0.030000\n",
      "Train Epoch: 1082 [147456/194182 (75%)]\tLoss: 0.299239\tGrad Norm: 0.995102\tLR: 0.030000\n",
      "Train Epoch: 1082 [167936/194182 (85%)]\tLoss: 0.310164\tGrad Norm: 1.728825\tLR: 0.030000\n",
      "Train Epoch: 1082 [188416/194182 (96%)]\tLoss: 0.313955\tGrad Norm: 1.814519\tLR: 0.030000\n",
      "Train set: Average loss: 0.3078\n",
      "Test set: Average loss: 0.2486, Average MAE: 0.3549\n",
      "Train Epoch: 1083 [4096/194182 (2%)]\tLoss: 0.306625\tGrad Norm: 1.536922\tLR: 0.030000\n",
      "Train Epoch: 1083 [24576/194182 (12%)]\tLoss: 0.303486\tGrad Norm: 1.178431\tLR: 0.030000\n",
      "Train Epoch: 1083 [45056/194182 (23%)]\tLoss: 0.302908\tGrad Norm: 1.370032\tLR: 0.030000\n",
      "Train Epoch: 1083 [65536/194182 (33%)]\tLoss: 0.310837\tGrad Norm: 1.361728\tLR: 0.030000\n",
      "Train Epoch: 1083 [86016/194182 (44%)]\tLoss: 0.309704\tGrad Norm: 1.460012\tLR: 0.030000\n",
      "Train Epoch: 1083 [106496/194182 (54%)]\tLoss: 0.301435\tGrad Norm: 1.271318\tLR: 0.030000\n",
      "Train Epoch: 1083 [126976/194182 (65%)]\tLoss: 0.302491\tGrad Norm: 1.326773\tLR: 0.030000\n",
      "Train Epoch: 1083 [147456/194182 (75%)]\tLoss: 0.304328\tGrad Norm: 1.443592\tLR: 0.030000\n",
      "Train Epoch: 1083 [167936/194182 (85%)]\tLoss: 0.305358\tGrad Norm: 1.475594\tLR: 0.030000\n",
      "Train Epoch: 1083 [188416/194182 (96%)]\tLoss: 0.311673\tGrad Norm: 1.658264\tLR: 0.030000\n",
      "Train set: Average loss: 0.3056\n",
      "Test set: Average loss: 0.2513, Average MAE: 0.3637\n",
      "Train Epoch: 1084 [4096/194182 (2%)]\tLoss: 0.315631\tGrad Norm: 1.711071\tLR: 0.030000\n",
      "Train Epoch: 1084 [24576/194182 (12%)]\tLoss: 0.310443\tGrad Norm: 1.365343\tLR: 0.030000\n",
      "Train Epoch: 1084 [45056/194182 (23%)]\tLoss: 0.308531\tGrad Norm: 1.572158\tLR: 0.030000\n",
      "Train Epoch: 1084 [65536/194182 (33%)]\tLoss: 0.308656\tGrad Norm: 1.559587\tLR: 0.030000\n",
      "Train Epoch: 1084 [86016/194182 (44%)]\tLoss: 0.302275\tGrad Norm: 1.301660\tLR: 0.030000\n",
      "Train Epoch: 1084 [106496/194182 (54%)]\tLoss: 0.304603\tGrad Norm: 1.218080\tLR: 0.030000\n",
      "Train Epoch: 1084 [126976/194182 (65%)]\tLoss: 0.298763\tGrad Norm: 1.161728\tLR: 0.030000\n",
      "Train Epoch: 1084 [147456/194182 (75%)]\tLoss: 0.301649\tGrad Norm: 1.493387\tLR: 0.030000\n",
      "Train Epoch: 1084 [167936/194182 (85%)]\tLoss: 0.309784\tGrad Norm: 1.448213\tLR: 0.030000\n",
      "Train Epoch: 1084 [188416/194182 (96%)]\tLoss: 0.304178\tGrad Norm: 1.198643\tLR: 0.030000\n",
      "Train set: Average loss: 0.3040\n",
      "Test set: Average loss: 0.2421, Average MAE: 0.3465\n",
      "Train Epoch: 1085 [4096/194182 (2%)]\tLoss: 0.301524\tGrad Norm: 1.213106\tLR: 0.030000\n",
      "Train Epoch: 1085 [24576/194182 (12%)]\tLoss: 0.303218\tGrad Norm: 1.426051\tLR: 0.030000\n",
      "Train Epoch: 1085 [45056/194182 (23%)]\tLoss: 0.311584\tGrad Norm: 1.257418\tLR: 0.030000\n",
      "Train Epoch: 1085 [65536/194182 (33%)]\tLoss: 0.304170\tGrad Norm: 1.578764\tLR: 0.030000\n",
      "Train Epoch: 1085 [86016/194182 (44%)]\tLoss: 0.306533\tGrad Norm: 1.553924\tLR: 0.030000\n",
      "Train Epoch: 1085 [106496/194182 (54%)]\tLoss: 0.302794\tGrad Norm: 1.124115\tLR: 0.030000\n",
      "Train Epoch: 1085 [126976/194182 (65%)]\tLoss: 0.297502\tGrad Norm: 0.654821\tLR: 0.030000\n",
      "Train Epoch: 1085 [147456/194182 (75%)]\tLoss: 0.304021\tGrad Norm: 1.017304\tLR: 0.030000\n",
      "Train Epoch: 1085 [167936/194182 (85%)]\tLoss: 0.310469\tGrad Norm: 1.326986\tLR: 0.030000\n",
      "Train Epoch: 1085 [188416/194182 (96%)]\tLoss: 0.304517\tGrad Norm: 1.583015\tLR: 0.030000\n",
      "Train set: Average loss: 0.3033\n",
      "Test set: Average loss: 0.2434, Average MAE: 0.3297\n",
      "Epoch 1085: Mean reward = 0.061 +/- 0.056\n",
      "Train Epoch: 1086 [4096/194182 (2%)]\tLoss: 0.301858\tGrad Norm: 1.520323\tLR: 0.030000\n",
      "Train Epoch: 1086 [24576/194182 (12%)]\tLoss: 0.306670\tGrad Norm: 1.591650\tLR: 0.030000\n",
      "Train Epoch: 1086 [45056/194182 (23%)]\tLoss: 0.310542\tGrad Norm: 1.682702\tLR: 0.030000\n",
      "Train Epoch: 1086 [65536/194182 (33%)]\tLoss: 0.301658\tGrad Norm: 1.329542\tLR: 0.030000\n",
      "Train Epoch: 1086 [86016/194182 (44%)]\tLoss: 0.299049\tGrad Norm: 1.260952\tLR: 0.030000\n",
      "Train Epoch: 1086 [106496/194182 (54%)]\tLoss: 0.303502\tGrad Norm: 1.139413\tLR: 0.030000\n",
      "Train Epoch: 1086 [126976/194182 (65%)]\tLoss: 0.300413\tGrad Norm: 1.547302\tLR: 0.030000\n",
      "Train Epoch: 1086 [147456/194182 (75%)]\tLoss: 0.316297\tGrad Norm: 1.665723\tLR: 0.030000\n",
      "Train Epoch: 1086 [167936/194182 (85%)]\tLoss: 0.307051\tGrad Norm: 1.627793\tLR: 0.030000\n",
      "Train Epoch: 1086 [188416/194182 (96%)]\tLoss: 0.298446\tGrad Norm: 1.264956\tLR: 0.030000\n",
      "Train set: Average loss: 0.3052\n",
      "Test set: Average loss: 0.2450, Average MAE: 0.3415\n",
      "Train Epoch: 1087 [4096/194182 (2%)]\tLoss: 0.298988\tGrad Norm: 1.449699\tLR: 0.030000\n",
      "Train Epoch: 1087 [24576/194182 (12%)]\tLoss: 0.304908\tGrad Norm: 1.449547\tLR: 0.030000\n",
      "Train Epoch: 1087 [45056/194182 (23%)]\tLoss: 0.308728\tGrad Norm: 1.419807\tLR: 0.030000\n",
      "Train Epoch: 1087 [65536/194182 (33%)]\tLoss: 0.309702\tGrad Norm: 1.545418\tLR: 0.030000\n",
      "Train Epoch: 1087 [86016/194182 (44%)]\tLoss: 0.305673\tGrad Norm: 1.445256\tLR: 0.030000\n",
      "Train Epoch: 1087 [106496/194182 (54%)]\tLoss: 0.311629\tGrad Norm: 1.442869\tLR: 0.030000\n",
      "Train Epoch: 1087 [126976/194182 (65%)]\tLoss: 0.302191\tGrad Norm: 1.378087\tLR: 0.030000\n",
      "Train Epoch: 1087 [147456/194182 (75%)]\tLoss: 0.304376\tGrad Norm: 1.478193\tLR: 0.030000\n",
      "Train Epoch: 1087 [167936/194182 (85%)]\tLoss: 0.314866\tGrad Norm: 1.901545\tLR: 0.030000\n",
      "Train Epoch: 1087 [188416/194182 (96%)]\tLoss: 0.302468\tGrad Norm: 1.347865\tLR: 0.030000\n",
      "Train set: Average loss: 0.3054\n",
      "Test set: Average loss: 0.2436, Average MAE: 0.3527\n",
      "Train Epoch: 1088 [4096/194182 (2%)]\tLoss: 0.296964\tGrad Norm: 1.321278\tLR: 0.030000\n",
      "Train Epoch: 1088 [24576/194182 (12%)]\tLoss: 0.319015\tGrad Norm: 1.852827\tLR: 0.030000\n",
      "Train Epoch: 1088 [45056/194182 (23%)]\tLoss: 0.302755\tGrad Norm: 1.516073\tLR: 0.030000\n",
      "Train Epoch: 1088 [65536/194182 (33%)]\tLoss: 0.296812\tGrad Norm: 1.092347\tLR: 0.030000\n",
      "Train Epoch: 1088 [86016/194182 (44%)]\tLoss: 0.299263\tGrad Norm: 1.447303\tLR: 0.030000\n",
      "Train Epoch: 1088 [106496/194182 (54%)]\tLoss: 0.302024\tGrad Norm: 1.406399\tLR: 0.030000\n",
      "Train Epoch: 1088 [126976/194182 (65%)]\tLoss: 0.304356\tGrad Norm: 1.427787\tLR: 0.030000\n",
      "Train Epoch: 1088 [147456/194182 (75%)]\tLoss: 0.300955\tGrad Norm: 1.295694\tLR: 0.030000\n",
      "Train Epoch: 1088 [167936/194182 (85%)]\tLoss: 0.306631\tGrad Norm: 1.375578\tLR: 0.030000\n",
      "Train Epoch: 1088 [188416/194182 (96%)]\tLoss: 0.305951\tGrad Norm: 1.224187\tLR: 0.030000\n",
      "Train set: Average loss: 0.3041\n",
      "Test set: Average loss: 0.2428, Average MAE: 0.3469\n",
      "Train Epoch: 1089 [4096/194182 (2%)]\tLoss: 0.295706\tGrad Norm: 1.391859\tLR: 0.030000\n",
      "Train Epoch: 1089 [24576/194182 (12%)]\tLoss: 0.295288\tGrad Norm: 0.777260\tLR: 0.030000\n",
      "Train Epoch: 1089 [45056/194182 (23%)]\tLoss: 0.305289\tGrad Norm: 1.547638\tLR: 0.030000\n",
      "Train Epoch: 1089 [65536/194182 (33%)]\tLoss: 0.295841\tGrad Norm: 1.482667\tLR: 0.030000\n",
      "Train Epoch: 1089 [86016/194182 (44%)]\tLoss: 0.303320\tGrad Norm: 1.348480\tLR: 0.030000\n",
      "Train Epoch: 1089 [106496/194182 (54%)]\tLoss: 0.301765\tGrad Norm: 1.230765\tLR: 0.030000\n",
      "Train Epoch: 1089 [126976/194182 (65%)]\tLoss: 0.300196\tGrad Norm: 1.127808\tLR: 0.030000\n",
      "Train Epoch: 1089 [147456/194182 (75%)]\tLoss: 0.300902\tGrad Norm: 1.287897\tLR: 0.030000\n",
      "Train Epoch: 1089 [167936/194182 (85%)]\tLoss: 0.305540\tGrad Norm: 1.379549\tLR: 0.030000\n",
      "Train Epoch: 1089 [188416/194182 (96%)]\tLoss: 0.306513\tGrad Norm: 1.473480\tLR: 0.030000\n",
      "Train set: Average loss: 0.3018\n",
      "Test set: Average loss: 0.2461, Average MAE: 0.3519\n",
      "Train Epoch: 1090 [4096/194182 (2%)]\tLoss: 0.297173\tGrad Norm: 1.522529\tLR: 0.030000\n",
      "Train Epoch: 1090 [24576/194182 (12%)]\tLoss: 0.309587\tGrad Norm: 1.631955\tLR: 0.030000\n",
      "Train Epoch: 1090 [45056/194182 (23%)]\tLoss: 0.312081\tGrad Norm: 1.855417\tLR: 0.030000\n",
      "Train Epoch: 1090 [65536/194182 (33%)]\tLoss: 0.311714\tGrad Norm: 1.725417\tLR: 0.030000\n",
      "Train Epoch: 1090 [86016/194182 (44%)]\tLoss: 0.306775\tGrad Norm: 1.640074\tLR: 0.030000\n",
      "Train Epoch: 1090 [106496/194182 (54%)]\tLoss: 0.310133\tGrad Norm: 1.300387\tLR: 0.030000\n",
      "Train Epoch: 1090 [126976/194182 (65%)]\tLoss: 0.299632\tGrad Norm: 1.202288\tLR: 0.030000\n",
      "Train Epoch: 1090 [147456/194182 (75%)]\tLoss: 0.298950\tGrad Norm: 1.510640\tLR: 0.030000\n",
      "Train Epoch: 1090 [167936/194182 (85%)]\tLoss: 0.304220\tGrad Norm: 1.687798\tLR: 0.030000\n",
      "Train Epoch: 1090 [188416/194182 (96%)]\tLoss: 0.308326\tGrad Norm: 1.624220\tLR: 0.030000\n",
      "Train set: Average loss: 0.3064\n",
      "Test set: Average loss: 0.2482, Average MAE: 0.3558\n",
      "Epoch 1090: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 1091 [4096/194182 (2%)]\tLoss: 0.306292\tGrad Norm: 1.541983\tLR: 0.030000\n",
      "Train Epoch: 1091 [24576/194182 (12%)]\tLoss: 0.309525\tGrad Norm: 1.415017\tLR: 0.030000\n",
      "Train Epoch: 1091 [45056/194182 (23%)]\tLoss: 0.296835\tGrad Norm: 1.341575\tLR: 0.030000\n",
      "Train Epoch: 1091 [65536/194182 (33%)]\tLoss: 0.300580\tGrad Norm: 1.485676\tLR: 0.030000\n",
      "Train Epoch: 1091 [86016/194182 (44%)]\tLoss: 0.299391\tGrad Norm: 1.184885\tLR: 0.030000\n",
      "Train Epoch: 1091 [106496/194182 (54%)]\tLoss: 0.311357\tGrad Norm: 2.410968\tLR: 0.030000\n",
      "Train Epoch: 1091 [126976/194182 (65%)]\tLoss: 0.311495\tGrad Norm: 1.651732\tLR: 0.030000\n",
      "Train Epoch: 1091 [147456/194182 (75%)]\tLoss: 0.302915\tGrad Norm: 1.156714\tLR: 0.030000\n",
      "Train Epoch: 1091 [167936/194182 (85%)]\tLoss: 0.301461\tGrad Norm: 1.248274\tLR: 0.030000\n",
      "Train Epoch: 1091 [188416/194182 (96%)]\tLoss: 0.307712\tGrad Norm: 1.562297\tLR: 0.030000\n",
      "Train set: Average loss: 0.3035\n",
      "Test set: Average loss: 0.2484, Average MAE: 0.3390\n",
      "Train Epoch: 1092 [4096/194182 (2%)]\tLoss: 0.309864\tGrad Norm: 1.798306\tLR: 0.030000\n",
      "Train Epoch: 1092 [24576/194182 (12%)]\tLoss: 0.307237\tGrad Norm: 1.860462\tLR: 0.030000\n",
      "Train Epoch: 1092 [45056/194182 (23%)]\tLoss: 0.306915\tGrad Norm: 1.658613\tLR: 0.030000\n",
      "Train Epoch: 1092 [65536/194182 (33%)]\tLoss: 0.301884\tGrad Norm: 1.390624\tLR: 0.030000\n",
      "Train Epoch: 1092 [86016/194182 (44%)]\tLoss: 0.313607\tGrad Norm: 1.688793\tLR: 0.030000\n",
      "Train Epoch: 1092 [106496/194182 (54%)]\tLoss: 0.307239\tGrad Norm: 1.500712\tLR: 0.030000\n",
      "Train Epoch: 1092 [126976/194182 (65%)]\tLoss: 0.292915\tGrad Norm: 1.091767\tLR: 0.030000\n",
      "Train Epoch: 1092 [147456/194182 (75%)]\tLoss: 0.298418\tGrad Norm: 1.064197\tLR: 0.030000\n",
      "Train Epoch: 1092 [167936/194182 (85%)]\tLoss: 0.296318\tGrad Norm: 1.251445\tLR: 0.030000\n",
      "Train Epoch: 1092 [188416/194182 (96%)]\tLoss: 0.305189\tGrad Norm: 1.329248\tLR: 0.030000\n",
      "Train set: Average loss: 0.3043\n",
      "Test set: Average loss: 0.2528, Average MAE: 0.3396\n",
      "Train Epoch: 1093 [4096/194182 (2%)]\tLoss: 0.315456\tGrad Norm: 2.161318\tLR: 0.030000\n",
      "Train Epoch: 1093 [24576/194182 (12%)]\tLoss: 0.307003\tGrad Norm: 1.198272\tLR: 0.030000\n",
      "Train Epoch: 1093 [45056/194182 (23%)]\tLoss: 0.298886\tGrad Norm: 1.282386\tLR: 0.030000\n",
      "Train Epoch: 1093 [65536/194182 (33%)]\tLoss: 0.309024\tGrad Norm: 1.507176\tLR: 0.030000\n",
      "Train Epoch: 1093 [86016/194182 (44%)]\tLoss: 0.310250\tGrad Norm: 1.708091\tLR: 0.030000\n",
      "Train Epoch: 1093 [106496/194182 (54%)]\tLoss: 0.299256\tGrad Norm: 1.380487\tLR: 0.030000\n",
      "Train Epoch: 1093 [126976/194182 (65%)]\tLoss: 0.303057\tGrad Norm: 1.430158\tLR: 0.030000\n",
      "Train Epoch: 1093 [147456/194182 (75%)]\tLoss: 0.296697\tGrad Norm: 1.099059\tLR: 0.030000\n",
      "Train Epoch: 1093 [167936/194182 (85%)]\tLoss: 0.308265\tGrad Norm: 1.276312\tLR: 0.030000\n",
      "Train Epoch: 1093 [188416/194182 (96%)]\tLoss: 0.305717\tGrad Norm: 1.535994\tLR: 0.030000\n",
      "Train set: Average loss: 0.3041\n",
      "Test set: Average loss: 0.2403, Average MAE: 0.3354\n",
      "Train Epoch: 1094 [4096/194182 (2%)]\tLoss: 0.295898\tGrad Norm: 1.311931\tLR: 0.030000\n",
      "Train Epoch: 1094 [24576/194182 (12%)]\tLoss: 0.299414\tGrad Norm: 1.173382\tLR: 0.030000\n",
      "Train Epoch: 1094 [45056/194182 (23%)]\tLoss: 0.301530\tGrad Norm: 1.552953\tLR: 0.030000\n",
      "Train Epoch: 1094 [65536/194182 (33%)]\tLoss: 0.307260\tGrad Norm: 1.380404\tLR: 0.030000\n",
      "Train Epoch: 1094 [86016/194182 (44%)]\tLoss: 0.306259\tGrad Norm: 1.731647\tLR: 0.030000\n",
      "Train Epoch: 1094 [106496/194182 (54%)]\tLoss: 0.301589\tGrad Norm: 1.534013\tLR: 0.030000\n",
      "Train Epoch: 1094 [126976/194182 (65%)]\tLoss: 0.297968\tGrad Norm: 1.509520\tLR: 0.030000\n",
      "Train Epoch: 1094 [147456/194182 (75%)]\tLoss: 0.302707\tGrad Norm: 1.524427\tLR: 0.030000\n",
      "Train Epoch: 1094 [167936/194182 (85%)]\tLoss: 0.302611\tGrad Norm: 1.379028\tLR: 0.030000\n",
      "Train Epoch: 1094 [188416/194182 (96%)]\tLoss: 0.302588\tGrad Norm: 1.446547\tLR: 0.030000\n",
      "Train set: Average loss: 0.3034\n",
      "Test set: Average loss: 0.2431, Average MAE: 0.3326\n",
      "Train Epoch: 1095 [4096/194182 (2%)]\tLoss: 0.299895\tGrad Norm: 1.345407\tLR: 0.030000\n",
      "Train Epoch: 1095 [24576/194182 (12%)]\tLoss: 0.303639\tGrad Norm: 1.328440\tLR: 0.030000\n",
      "Train Epoch: 1095 [45056/194182 (23%)]\tLoss: 0.307420\tGrad Norm: 1.748025\tLR: 0.030000\n",
      "Train Epoch: 1095 [65536/194182 (33%)]\tLoss: 0.298478\tGrad Norm: 1.278601\tLR: 0.030000\n",
      "Train Epoch: 1095 [86016/194182 (44%)]\tLoss: 0.303559\tGrad Norm: 1.155252\tLR: 0.030000\n",
      "Train Epoch: 1095 [106496/194182 (54%)]\tLoss: 0.298662\tGrad Norm: 1.434914\tLR: 0.030000\n",
      "Train Epoch: 1095 [126976/194182 (65%)]\tLoss: 0.310664\tGrad Norm: 1.430457\tLR: 0.030000\n",
      "Train Epoch: 1095 [147456/194182 (75%)]\tLoss: 0.309743\tGrad Norm: 1.736596\tLR: 0.030000\n",
      "Train Epoch: 1095 [167936/194182 (85%)]\tLoss: 0.299585\tGrad Norm: 1.203043\tLR: 0.030000\n",
      "Train Epoch: 1095 [188416/194182 (96%)]\tLoss: 0.307362\tGrad Norm: 1.656439\tLR: 0.030000\n",
      "Train set: Average loss: 0.3038\n",
      "Test set: Average loss: 0.2478, Average MAE: 0.3565\n",
      "Epoch 1095: Mean reward = 0.054 +/- 0.030\n",
      "Train Epoch: 1096 [4096/194182 (2%)]\tLoss: 0.299941\tGrad Norm: 1.619945\tLR: 0.030000\n",
      "Train Epoch: 1096 [24576/194182 (12%)]\tLoss: 0.294928\tGrad Norm: 1.187843\tLR: 0.030000\n",
      "Train Epoch: 1096 [45056/194182 (23%)]\tLoss: 0.296658\tGrad Norm: 1.257567\tLR: 0.030000\n",
      "Train Epoch: 1096 [65536/194182 (33%)]\tLoss: 0.305450\tGrad Norm: 1.088462\tLR: 0.030000\n",
      "Train Epoch: 1096 [86016/194182 (44%)]\tLoss: 0.298997\tGrad Norm: 1.270923\tLR: 0.030000\n",
      "Train Epoch: 1096 [106496/194182 (54%)]\tLoss: 0.305835\tGrad Norm: 1.602035\tLR: 0.030000\n",
      "Train Epoch: 1096 [126976/194182 (65%)]\tLoss: 0.301963\tGrad Norm: 1.170658\tLR: 0.030000\n",
      "Train Epoch: 1096 [147456/194182 (75%)]\tLoss: 0.300089\tGrad Norm: 1.359594\tLR: 0.030000\n",
      "Train Epoch: 1096 [167936/194182 (85%)]\tLoss: 0.305023\tGrad Norm: 1.704966\tLR: 0.030000\n",
      "Train Epoch: 1096 [188416/194182 (96%)]\tLoss: 0.306738\tGrad Norm: 1.566132\tLR: 0.030000\n",
      "Train set: Average loss: 0.3017\n",
      "Test set: Average loss: 0.2412, Average MAE: 0.3465\n",
      "Train Epoch: 1097 [4096/194182 (2%)]\tLoss: 0.292385\tGrad Norm: 1.282552\tLR: 0.030000\n",
      "Train Epoch: 1097 [24576/194182 (12%)]\tLoss: 0.305311\tGrad Norm: 1.733901\tLR: 0.030000\n",
      "Train Epoch: 1097 [45056/194182 (23%)]\tLoss: 0.307224\tGrad Norm: 1.696535\tLR: 0.030000\n",
      "Train Epoch: 1097 [65536/194182 (33%)]\tLoss: 0.306843\tGrad Norm: 1.620845\tLR: 0.030000\n",
      "Train Epoch: 1097 [86016/194182 (44%)]\tLoss: 0.306883\tGrad Norm: 1.688669\tLR: 0.030000\n",
      "Train Epoch: 1097 [106496/194182 (54%)]\tLoss: 0.302219\tGrad Norm: 1.523800\tLR: 0.030000\n",
      "Train Epoch: 1097 [126976/194182 (65%)]\tLoss: 0.301939\tGrad Norm: 1.266950\tLR: 0.030000\n",
      "Train Epoch: 1097 [147456/194182 (75%)]\tLoss: 0.299823\tGrad Norm: 1.097904\tLR: 0.030000\n",
      "Train Epoch: 1097 [167936/194182 (85%)]\tLoss: 0.303667\tGrad Norm: 1.134768\tLR: 0.030000\n",
      "Train Epoch: 1097 [188416/194182 (96%)]\tLoss: 0.302428\tGrad Norm: 1.055086\tLR: 0.030000\n",
      "Train set: Average loss: 0.3026\n",
      "Test set: Average loss: 0.2393, Average MAE: 0.3346\n",
      "Train Epoch: 1098 [4096/194182 (2%)]\tLoss: 0.302590\tGrad Norm: 1.187944\tLR: 0.030000\n",
      "Train Epoch: 1098 [24576/194182 (12%)]\tLoss: 0.301953\tGrad Norm: 1.485483\tLR: 0.030000\n",
      "Train Epoch: 1098 [45056/194182 (23%)]\tLoss: 0.311925\tGrad Norm: 1.654549\tLR: 0.030000\n",
      "Train Epoch: 1098 [65536/194182 (33%)]\tLoss: 0.307708\tGrad Norm: 1.606703\tLR: 0.030000\n",
      "Train Epoch: 1098 [86016/194182 (44%)]\tLoss: 0.302132\tGrad Norm: 1.374313\tLR: 0.030000\n",
      "Train Epoch: 1098 [106496/194182 (54%)]\tLoss: 0.308682\tGrad Norm: 1.495670\tLR: 0.030000\n",
      "Train Epoch: 1098 [126976/194182 (65%)]\tLoss: 0.309831\tGrad Norm: 1.452019\tLR: 0.030000\n",
      "Train Epoch: 1098 [147456/194182 (75%)]\tLoss: 0.310995\tGrad Norm: 1.886944\tLR: 0.030000\n",
      "Train Epoch: 1098 [167936/194182 (85%)]\tLoss: 0.305642\tGrad Norm: 1.392334\tLR: 0.030000\n",
      "Train Epoch: 1098 [188416/194182 (96%)]\tLoss: 0.305414\tGrad Norm: 1.405627\tLR: 0.030000\n",
      "Train set: Average loss: 0.3041\n",
      "Test set: Average loss: 0.2424, Average MAE: 0.3447\n",
      "Train Epoch: 1099 [4096/194182 (2%)]\tLoss: 0.297232\tGrad Norm: 1.277182\tLR: 0.030000\n",
      "Train Epoch: 1099 [24576/194182 (12%)]\tLoss: 0.297157\tGrad Norm: 1.108022\tLR: 0.030000\n",
      "Train Epoch: 1099 [45056/194182 (23%)]\tLoss: 0.300541\tGrad Norm: 1.269088\tLR: 0.030000\n",
      "Train Epoch: 1099 [65536/194182 (33%)]\tLoss: 0.303747\tGrad Norm: 1.337711\tLR: 0.030000\n",
      "Train Epoch: 1099 [86016/194182 (44%)]\tLoss: 0.296875\tGrad Norm: 1.652628\tLR: 0.030000\n",
      "Train Epoch: 1099 [106496/194182 (54%)]\tLoss: 0.319124\tGrad Norm: 2.063637\tLR: 0.030000\n",
      "Train Epoch: 1099 [126976/194182 (65%)]\tLoss: 0.298063\tGrad Norm: 1.444369\tLR: 0.030000\n",
      "Train Epoch: 1099 [147456/194182 (75%)]\tLoss: 0.305680\tGrad Norm: 1.429675\tLR: 0.030000\n",
      "Train Epoch: 1099 [167936/194182 (85%)]\tLoss: 0.305690\tGrad Norm: 1.516563\tLR: 0.030000\n",
      "Train Epoch: 1099 [188416/194182 (96%)]\tLoss: 0.295905\tGrad Norm: 1.234962\tLR: 0.030000\n",
      "Train set: Average loss: 0.3023\n",
      "Test set: Average loss: 0.2441, Average MAE: 0.3412\n",
      "Train Epoch: 1100 [4096/194182 (2%)]\tLoss: 0.300046\tGrad Norm: 1.395261\tLR: 0.030000\n",
      "Train Epoch: 1100 [24576/194182 (12%)]\tLoss: 0.297244\tGrad Norm: 1.433956\tLR: 0.030000\n",
      "Train Epoch: 1100 [45056/194182 (23%)]\tLoss: 0.305564\tGrad Norm: 1.411336\tLR: 0.030000\n",
      "Train Epoch: 1100 [65536/194182 (33%)]\tLoss: 0.305328\tGrad Norm: 1.643781\tLR: 0.030000\n",
      "Train Epoch: 1100 [86016/194182 (44%)]\tLoss: 0.310042\tGrad Norm: 1.451994\tLR: 0.030000\n",
      "Train Epoch: 1100 [106496/194182 (54%)]\tLoss: 0.304043\tGrad Norm: 1.325623\tLR: 0.030000\n",
      "Train Epoch: 1100 [126976/194182 (65%)]\tLoss: 0.301563\tGrad Norm: 1.387212\tLR: 0.030000\n",
      "Train Epoch: 1100 [147456/194182 (75%)]\tLoss: 0.297202\tGrad Norm: 1.667696\tLR: 0.030000\n",
      "Train Epoch: 1100 [167936/194182 (85%)]\tLoss: 0.299611\tGrad Norm: 1.155940\tLR: 0.030000\n",
      "Train Epoch: 1100 [188416/194182 (96%)]\tLoss: 0.303449\tGrad Norm: 1.323045\tLR: 0.030000\n",
      "Train set: Average loss: 0.3016\n",
      "Test set: Average loss: 0.2469, Average MAE: 0.3465\n",
      "Epoch 1100: Mean reward = 0.067 +/- 0.074\n",
      "Train Epoch: 1101 [4096/194182 (2%)]\tLoss: 0.300846\tGrad Norm: 1.550677\tLR: 0.030000\n",
      "Train Epoch: 1101 [24576/194182 (12%)]\tLoss: 0.308158\tGrad Norm: 1.612888\tLR: 0.030000\n",
      "Train Epoch: 1101 [45056/194182 (23%)]\tLoss: 0.297020\tGrad Norm: 1.185159\tLR: 0.030000\n",
      "Train Epoch: 1101 [65536/194182 (33%)]\tLoss: 0.298173\tGrad Norm: 1.412074\tLR: 0.030000\n",
      "Train Epoch: 1101 [86016/194182 (44%)]\tLoss: 0.304487\tGrad Norm: 1.647697\tLR: 0.030000\n",
      "Train Epoch: 1101 [106496/194182 (54%)]\tLoss: 0.301816\tGrad Norm: 1.323719\tLR: 0.030000\n",
      "Train Epoch: 1101 [126976/194182 (65%)]\tLoss: 0.298351\tGrad Norm: 1.328579\tLR: 0.030000\n",
      "Train Epoch: 1101 [147456/194182 (75%)]\tLoss: 0.310144\tGrad Norm: 1.432224\tLR: 0.030000\n",
      "Train Epoch: 1101 [167936/194182 (85%)]\tLoss: 0.303091\tGrad Norm: 1.358258\tLR: 0.030000\n",
      "Train Epoch: 1101 [188416/194182 (96%)]\tLoss: 0.309135\tGrad Norm: 2.233175\tLR: 0.030000\n",
      "Train set: Average loss: 0.3025\n",
      "Test set: Average loss: 0.2462, Average MAE: 0.3557\n",
      "Train Epoch: 1102 [4096/194182 (2%)]\tLoss: 0.307641\tGrad Norm: 1.510756\tLR: 0.030000\n",
      "Train Epoch: 1102 [24576/194182 (12%)]\tLoss: 0.301645\tGrad Norm: 1.401229\tLR: 0.030000\n",
      "Train Epoch: 1102 [45056/194182 (23%)]\tLoss: 0.292427\tGrad Norm: 1.023722\tLR: 0.030000\n",
      "Train Epoch: 1102 [65536/194182 (33%)]\tLoss: 0.295862\tGrad Norm: 0.939826\tLR: 0.030000\n",
      "Train Epoch: 1102 [86016/194182 (44%)]\tLoss: 0.293225\tGrad Norm: 1.360780\tLR: 0.030000\n",
      "Train Epoch: 1102 [106496/194182 (54%)]\tLoss: 0.303848\tGrad Norm: 1.644248\tLR: 0.030000\n",
      "Train Epoch: 1102 [126976/194182 (65%)]\tLoss: 0.312409\tGrad Norm: 1.418853\tLR: 0.030000\n",
      "Train Epoch: 1102 [147456/194182 (75%)]\tLoss: 0.299564\tGrad Norm: 1.407939\tLR: 0.030000\n",
      "Train Epoch: 1102 [167936/194182 (85%)]\tLoss: 0.302304\tGrad Norm: 1.389924\tLR: 0.030000\n",
      "Train Epoch: 1102 [188416/194182 (96%)]\tLoss: 0.302056\tGrad Norm: 1.549694\tLR: 0.030000\n",
      "Train set: Average loss: 0.3017\n",
      "Test set: Average loss: 0.2416, Average MAE: 0.3321\n",
      "Train Epoch: 1103 [4096/194182 (2%)]\tLoss: 0.304230\tGrad Norm: 1.251584\tLR: 0.030000\n",
      "Train Epoch: 1103 [24576/194182 (12%)]\tLoss: 0.310732\tGrad Norm: 1.631376\tLR: 0.030000\n",
      "Train Epoch: 1103 [45056/194182 (23%)]\tLoss: 0.296840\tGrad Norm: 1.319012\tLR: 0.030000\n",
      "Train Epoch: 1103 [65536/194182 (33%)]\tLoss: 0.303227\tGrad Norm: 1.651392\tLR: 0.030000\n",
      "Train Epoch: 1103 [86016/194182 (44%)]\tLoss: 0.298355\tGrad Norm: 1.363919\tLR: 0.030000\n",
      "Train Epoch: 1103 [106496/194182 (54%)]\tLoss: 0.299982\tGrad Norm: 1.083142\tLR: 0.030000\n",
      "Train Epoch: 1103 [126976/194182 (65%)]\tLoss: 0.295823\tGrad Norm: 1.095900\tLR: 0.030000\n",
      "Train Epoch: 1103 [147456/194182 (75%)]\tLoss: 0.303820\tGrad Norm: 1.698468\tLR: 0.030000\n",
      "Train Epoch: 1103 [167936/194182 (85%)]\tLoss: 0.292230\tGrad Norm: 1.315439\tLR: 0.030000\n",
      "Train Epoch: 1103 [188416/194182 (96%)]\tLoss: 0.304921\tGrad Norm: 1.363373\tLR: 0.030000\n",
      "Train set: Average loss: 0.3008\n",
      "Test set: Average loss: 0.2481, Average MAE: 0.3534\n",
      "Train Epoch: 1104 [4096/194182 (2%)]\tLoss: 0.300709\tGrad Norm: 1.584817\tLR: 0.030000\n",
      "Train Epoch: 1104 [24576/194182 (12%)]\tLoss: 0.297349\tGrad Norm: 1.144850\tLR: 0.030000\n",
      "Train Epoch: 1104 [45056/194182 (23%)]\tLoss: 0.306461\tGrad Norm: 1.616046\tLR: 0.030000\n",
      "Train Epoch: 1104 [65536/194182 (33%)]\tLoss: 0.299665\tGrad Norm: 1.468451\tLR: 0.030000\n",
      "Train Epoch: 1104 [86016/194182 (44%)]\tLoss: 0.300560\tGrad Norm: 1.569382\tLR: 0.030000\n",
      "Train Epoch: 1104 [106496/194182 (54%)]\tLoss: 0.305885\tGrad Norm: 1.748511\tLR: 0.030000\n",
      "Train Epoch: 1104 [126976/194182 (65%)]\tLoss: 0.309624\tGrad Norm: 1.898736\tLR: 0.030000\n",
      "Train Epoch: 1104 [147456/194182 (75%)]\tLoss: 0.299717\tGrad Norm: 1.492516\tLR: 0.030000\n",
      "Train Epoch: 1104 [167936/194182 (85%)]\tLoss: 0.305201\tGrad Norm: 1.709851\tLR: 0.030000\n",
      "Train Epoch: 1104 [188416/194182 (96%)]\tLoss: 0.304173\tGrad Norm: 1.400921\tLR: 0.030000\n",
      "Train set: Average loss: 0.3039\n",
      "Test set: Average loss: 0.2444, Average MAE: 0.3487\n",
      "Train Epoch: 1105 [4096/194182 (2%)]\tLoss: 0.302053\tGrad Norm: 1.361485\tLR: 0.030000\n",
      "Train Epoch: 1105 [24576/194182 (12%)]\tLoss: 0.300918\tGrad Norm: 1.282477\tLR: 0.030000\n",
      "Train Epoch: 1105 [45056/194182 (23%)]\tLoss: 0.301390\tGrad Norm: 1.333509\tLR: 0.030000\n",
      "Train Epoch: 1105 [65536/194182 (33%)]\tLoss: 0.301720\tGrad Norm: 1.561675\tLR: 0.030000\n",
      "Train Epoch: 1105 [86016/194182 (44%)]\tLoss: 0.306200\tGrad Norm: 1.548105\tLR: 0.030000\n",
      "Train Epoch: 1105 [106496/194182 (54%)]\tLoss: 0.297685\tGrad Norm: 1.704191\tLR: 0.030000\n",
      "Train Epoch: 1105 [126976/194182 (65%)]\tLoss: 0.302748\tGrad Norm: 1.543455\tLR: 0.030000\n",
      "Train Epoch: 1105 [147456/194182 (75%)]\tLoss: 0.301439\tGrad Norm: 1.268870\tLR: 0.030000\n",
      "Train Epoch: 1105 [167936/194182 (85%)]\tLoss: 0.303789\tGrad Norm: 1.584774\tLR: 0.030000\n",
      "Train Epoch: 1105 [188416/194182 (96%)]\tLoss: 0.294170\tGrad Norm: 1.178470\tLR: 0.030000\n",
      "Train set: Average loss: 0.3010\n",
      "Test set: Average loss: 0.2467, Average MAE: 0.3407\n",
      "Epoch 1105: Mean reward = 0.055 +/- 0.044\n",
      "Train Epoch: 1106 [4096/194182 (2%)]\tLoss: 0.303195\tGrad Norm: 1.577801\tLR: 0.030000\n",
      "Train Epoch: 1106 [24576/194182 (12%)]\tLoss: 0.303164\tGrad Norm: 1.479212\tLR: 0.030000\n",
      "Train Epoch: 1106 [45056/194182 (23%)]\tLoss: 0.303931\tGrad Norm: 1.563590\tLR: 0.030000\n",
      "Train Epoch: 1106 [65536/194182 (33%)]\tLoss: 0.301867\tGrad Norm: 1.409485\tLR: 0.030000\n",
      "Train Epoch: 1106 [86016/194182 (44%)]\tLoss: 0.295200\tGrad Norm: 1.301249\tLR: 0.030000\n",
      "Train Epoch: 1106 [106496/194182 (54%)]\tLoss: 0.292102\tGrad Norm: 1.396173\tLR: 0.030000\n",
      "Train Epoch: 1106 [126976/194182 (65%)]\tLoss: 0.296194\tGrad Norm: 1.596247\tLR: 0.030000\n",
      "Train Epoch: 1106 [147456/194182 (75%)]\tLoss: 0.298347\tGrad Norm: 1.295749\tLR: 0.030000\n",
      "Train Epoch: 1106 [167936/194182 (85%)]\tLoss: 0.296142\tGrad Norm: 1.188217\tLR: 0.030000\n",
      "Train Epoch: 1106 [188416/194182 (96%)]\tLoss: 0.306499\tGrad Norm: 1.829776\tLR: 0.030000\n",
      "Train set: Average loss: 0.3013\n",
      "Test set: Average loss: 0.2448, Average MAE: 0.3328\n",
      "Train Epoch: 1107 [4096/194182 (2%)]\tLoss: 0.308321\tGrad Norm: 1.516738\tLR: 0.030000\n",
      "Train Epoch: 1107 [24576/194182 (12%)]\tLoss: 0.300809\tGrad Norm: 1.401051\tLR: 0.030000\n",
      "Train Epoch: 1107 [45056/194182 (23%)]\tLoss: 0.305104\tGrad Norm: 1.635460\tLR: 0.030000\n",
      "Train Epoch: 1107 [65536/194182 (33%)]\tLoss: 0.302026\tGrad Norm: 1.391637\tLR: 0.030000\n",
      "Train Epoch: 1107 [86016/194182 (44%)]\tLoss: 0.305567\tGrad Norm: 1.432133\tLR: 0.030000\n",
      "Train Epoch: 1107 [106496/194182 (54%)]\tLoss: 0.300198\tGrad Norm: 1.146027\tLR: 0.030000\n",
      "Train Epoch: 1107 [126976/194182 (65%)]\tLoss: 0.305485\tGrad Norm: 1.709668\tLR: 0.030000\n",
      "Train Epoch: 1107 [147456/194182 (75%)]\tLoss: 0.305373\tGrad Norm: 1.419397\tLR: 0.030000\n",
      "Train Epoch: 1107 [167936/194182 (85%)]\tLoss: 0.303173\tGrad Norm: 1.553198\tLR: 0.030000\n",
      "Train Epoch: 1107 [188416/194182 (96%)]\tLoss: 0.292850\tGrad Norm: 1.258600\tLR: 0.030000\n",
      "Train set: Average loss: 0.3014\n",
      "Test set: Average loss: 0.2409, Average MAE: 0.3449\n",
      "Train Epoch: 1108 [4096/194182 (2%)]\tLoss: 0.297839\tGrad Norm: 1.136611\tLR: 0.030000\n",
      "Train Epoch: 1108 [24576/194182 (12%)]\tLoss: 0.299340\tGrad Norm: 1.843039\tLR: 0.030000\n",
      "Train Epoch: 1108 [45056/194182 (23%)]\tLoss: 0.305293\tGrad Norm: 1.585165\tLR: 0.030000\n",
      "Train Epoch: 1108 [65536/194182 (33%)]\tLoss: 0.299642\tGrad Norm: 1.366096\tLR: 0.030000\n",
      "Train Epoch: 1108 [86016/194182 (44%)]\tLoss: 0.297940\tGrad Norm: 1.445800\tLR: 0.030000\n",
      "Train Epoch: 1108 [106496/194182 (54%)]\tLoss: 0.305305\tGrad Norm: 1.427102\tLR: 0.030000\n",
      "Train Epoch: 1108 [126976/194182 (65%)]\tLoss: 0.298521\tGrad Norm: 1.381279\tLR: 0.030000\n",
      "Train Epoch: 1108 [147456/194182 (75%)]\tLoss: 0.302603\tGrad Norm: 1.692351\tLR: 0.030000\n",
      "Train Epoch: 1108 [167936/194182 (85%)]\tLoss: 0.302370\tGrad Norm: 1.288188\tLR: 0.030000\n",
      "Train Epoch: 1108 [188416/194182 (96%)]\tLoss: 0.302135\tGrad Norm: 1.311385\tLR: 0.030000\n",
      "Train set: Average loss: 0.3011\n",
      "Test set: Average loss: 0.2495, Average MAE: 0.3544\n",
      "Train Epoch: 1109 [4096/194182 (2%)]\tLoss: 0.297606\tGrad Norm: 1.624248\tLR: 0.030000\n",
      "Train Epoch: 1109 [24576/194182 (12%)]\tLoss: 0.299525\tGrad Norm: 1.257233\tLR: 0.030000\n",
      "Train Epoch: 1109 [45056/194182 (23%)]\tLoss: 0.305904\tGrad Norm: 1.351194\tLR: 0.030000\n",
      "Train Epoch: 1109 [65536/194182 (33%)]\tLoss: 0.300918\tGrad Norm: 1.342062\tLR: 0.030000\n",
      "Train Epoch: 1109 [86016/194182 (44%)]\tLoss: 0.298413\tGrad Norm: 1.293868\tLR: 0.030000\n",
      "Train Epoch: 1109 [106496/194182 (54%)]\tLoss: 0.297948\tGrad Norm: 1.205387\tLR: 0.030000\n",
      "Train Epoch: 1109 [126976/194182 (65%)]\tLoss: 0.301315\tGrad Norm: 1.350998\tLR: 0.030000\n",
      "Train Epoch: 1109 [147456/194182 (75%)]\tLoss: 0.307363\tGrad Norm: 1.472998\tLR: 0.030000\n",
      "Train Epoch: 1109 [167936/194182 (85%)]\tLoss: 0.309629\tGrad Norm: 1.790170\tLR: 0.030000\n",
      "Train Epoch: 1109 [188416/194182 (96%)]\tLoss: 0.300615\tGrad Norm: 1.666946\tLR: 0.030000\n",
      "Train set: Average loss: 0.3010\n",
      "Test set: Average loss: 0.2521, Average MAE: 0.3347\n",
      "Train Epoch: 1110 [4096/194182 (2%)]\tLoss: 0.318307\tGrad Norm: 2.071947\tLR: 0.030000\n",
      "Train Epoch: 1110 [24576/194182 (12%)]\tLoss: 0.299202\tGrad Norm: 1.479703\tLR: 0.030000\n",
      "Train Epoch: 1110 [45056/194182 (23%)]\tLoss: 0.294009\tGrad Norm: 0.854841\tLR: 0.030000\n",
      "Train Epoch: 1110 [65536/194182 (33%)]\tLoss: 0.294776\tGrad Norm: 1.057994\tLR: 0.030000\n",
      "Train Epoch: 1110 [86016/194182 (44%)]\tLoss: 0.312569\tGrad Norm: 1.850531\tLR: 0.030000\n",
      "Train Epoch: 1110 [106496/194182 (54%)]\tLoss: 0.307217\tGrad Norm: 1.905589\tLR: 0.030000\n",
      "Train Epoch: 1110 [126976/194182 (65%)]\tLoss: 0.294240\tGrad Norm: 1.229077\tLR: 0.030000\n",
      "Train Epoch: 1110 [147456/194182 (75%)]\tLoss: 0.296380\tGrad Norm: 1.338403\tLR: 0.030000\n",
      "Train Epoch: 1110 [167936/194182 (85%)]\tLoss: 0.301787\tGrad Norm: 1.483966\tLR: 0.030000\n",
      "Train Epoch: 1110 [188416/194182 (96%)]\tLoss: 0.303429\tGrad Norm: 1.687847\tLR: 0.030000\n",
      "Train set: Average loss: 0.3005\n",
      "Test set: Average loss: 0.2520, Average MAE: 0.3375\n",
      "Epoch 1110: Mean reward = 0.055 +/- 0.045\n",
      "Train Epoch: 1111 [4096/194182 (2%)]\tLoss: 0.315224\tGrad Norm: 1.966166\tLR: 0.030000\n",
      "Train Epoch: 1111 [24576/194182 (12%)]\tLoss: 0.290108\tGrad Norm: 1.102989\tLR: 0.030000\n",
      "Train Epoch: 1111 [45056/194182 (23%)]\tLoss: 0.301952\tGrad Norm: 1.408326\tLR: 0.030000\n",
      "Train Epoch: 1111 [65536/194182 (33%)]\tLoss: 0.305248\tGrad Norm: 1.540420\tLR: 0.030000\n",
      "Train Epoch: 1111 [86016/194182 (44%)]\tLoss: 0.305357\tGrad Norm: 1.333747\tLR: 0.030000\n",
      "Train Epoch: 1111 [106496/194182 (54%)]\tLoss: 0.299349\tGrad Norm: 1.309747\tLR: 0.030000\n",
      "Train Epoch: 1111 [126976/194182 (65%)]\tLoss: 0.300264\tGrad Norm: 1.343754\tLR: 0.030000\n",
      "Train Epoch: 1111 [147456/194182 (75%)]\tLoss: 0.300746\tGrad Norm: 1.510562\tLR: 0.030000\n",
      "Train Epoch: 1111 [167936/194182 (85%)]\tLoss: 0.307025\tGrad Norm: 2.088781\tLR: 0.030000\n",
      "Train Epoch: 1111 [188416/194182 (96%)]\tLoss: 0.291073\tGrad Norm: 0.905144\tLR: 0.030000\n",
      "Train set: Average loss: 0.2999\n",
      "Test set: Average loss: 0.2397, Average MAE: 0.3400\n",
      "Train Epoch: 1112 [4096/194182 (2%)]\tLoss: 0.291618\tGrad Norm: 1.029645\tLR: 0.030000\n",
      "Train Epoch: 1112 [24576/194182 (12%)]\tLoss: 0.304563\tGrad Norm: 1.547212\tLR: 0.030000\n",
      "Train Epoch: 1112 [45056/194182 (23%)]\tLoss: 0.300813\tGrad Norm: 1.415935\tLR: 0.030000\n",
      "Train Epoch: 1112 [65536/194182 (33%)]\tLoss: 0.291888\tGrad Norm: 0.908659\tLR: 0.030000\n",
      "Train Epoch: 1112 [86016/194182 (44%)]\tLoss: 0.305971\tGrad Norm: 1.417229\tLR: 0.030000\n",
      "Train Epoch: 1112 [106496/194182 (54%)]\tLoss: 0.292430\tGrad Norm: 1.169422\tLR: 0.030000\n",
      "Train Epoch: 1112 [126976/194182 (65%)]\tLoss: 0.297415\tGrad Norm: 1.151221\tLR: 0.030000\n",
      "Train Epoch: 1112 [147456/194182 (75%)]\tLoss: 0.292194\tGrad Norm: 1.369116\tLR: 0.030000\n",
      "Train Epoch: 1112 [167936/194182 (85%)]\tLoss: 0.298463\tGrad Norm: 1.285198\tLR: 0.030000\n",
      "Train Epoch: 1112 [188416/194182 (96%)]\tLoss: 0.304946\tGrad Norm: 1.648972\tLR: 0.030000\n",
      "Train set: Average loss: 0.2983\n",
      "Test set: Average loss: 0.2471, Average MAE: 0.3439\n",
      "Train Epoch: 1113 [4096/194182 (2%)]\tLoss: 0.297867\tGrad Norm: 1.592036\tLR: 0.030000\n",
      "Train Epoch: 1113 [24576/194182 (12%)]\tLoss: 0.302714\tGrad Norm: 1.410422\tLR: 0.030000\n",
      "Train Epoch: 1113 [45056/194182 (23%)]\tLoss: 0.305280\tGrad Norm: 1.630032\tLR: 0.030000\n",
      "Train Epoch: 1113 [65536/194182 (33%)]\tLoss: 0.308069\tGrad Norm: 1.787330\tLR: 0.030000\n",
      "Train Epoch: 1113 [86016/194182 (44%)]\tLoss: 0.293438\tGrad Norm: 1.464503\tLR: 0.030000\n",
      "Train Epoch: 1113 [106496/194182 (54%)]\tLoss: 0.306765\tGrad Norm: 1.577565\tLR: 0.030000\n",
      "Train Epoch: 1113 [126976/194182 (65%)]\tLoss: 0.306075\tGrad Norm: 1.593297\tLR: 0.030000\n",
      "Train Epoch: 1113 [147456/194182 (75%)]\tLoss: 0.293715\tGrad Norm: 1.226996\tLR: 0.030000\n",
      "Train Epoch: 1113 [167936/194182 (85%)]\tLoss: 0.302142\tGrad Norm: 1.767085\tLR: 0.030000\n",
      "Train Epoch: 1113 [188416/194182 (96%)]\tLoss: 0.288776\tGrad Norm: 1.217656\tLR: 0.030000\n",
      "Train set: Average loss: 0.3008\n",
      "Test set: Average loss: 0.2411, Average MAE: 0.3353\n",
      "Train Epoch: 1114 [4096/194182 (2%)]\tLoss: 0.294845\tGrad Norm: 1.902260\tLR: 0.030000\n",
      "Train Epoch: 1114 [24576/194182 (12%)]\tLoss: 0.305821\tGrad Norm: 1.256087\tLR: 0.030000\n",
      "Train Epoch: 1114 [45056/194182 (23%)]\tLoss: 0.303517\tGrad Norm: 1.526962\tLR: 0.030000\n",
      "Train Epoch: 1114 [65536/194182 (33%)]\tLoss: 0.298831\tGrad Norm: 1.397501\tLR: 0.030000\n",
      "Train Epoch: 1114 [86016/194182 (44%)]\tLoss: 0.292635\tGrad Norm: 1.141886\tLR: 0.030000\n",
      "Train Epoch: 1114 [106496/194182 (54%)]\tLoss: 0.300413\tGrad Norm: 1.386874\tLR: 0.030000\n",
      "Train Epoch: 1114 [126976/194182 (65%)]\tLoss: 0.307312\tGrad Norm: 1.341730\tLR: 0.030000\n",
      "Train Epoch: 1114 [147456/194182 (75%)]\tLoss: 0.301980\tGrad Norm: 1.397090\tLR: 0.030000\n",
      "Train Epoch: 1114 [167936/194182 (85%)]\tLoss: 0.297312\tGrad Norm: 1.515152\tLR: 0.030000\n",
      "Train Epoch: 1114 [188416/194182 (96%)]\tLoss: 0.299433\tGrad Norm: 1.601793\tLR: 0.030000\n",
      "Train set: Average loss: 0.2989\n",
      "Test set: Average loss: 0.2442, Average MAE: 0.3523\n",
      "Train Epoch: 1115 [4096/194182 (2%)]\tLoss: 0.299582\tGrad Norm: 1.595721\tLR: 0.030000\n",
      "Train Epoch: 1115 [24576/194182 (12%)]\tLoss: 0.294758\tGrad Norm: 1.107859\tLR: 0.030000\n",
      "Train Epoch: 1115 [45056/194182 (23%)]\tLoss: 0.301031\tGrad Norm: 1.398326\tLR: 0.030000\n",
      "Train Epoch: 1115 [65536/194182 (33%)]\tLoss: 0.303958\tGrad Norm: 1.783964\tLR: 0.030000\n",
      "Train Epoch: 1115 [86016/194182 (44%)]\tLoss: 0.305683\tGrad Norm: 1.614419\tLR: 0.030000\n",
      "Train Epoch: 1115 [106496/194182 (54%)]\tLoss: 0.304019\tGrad Norm: 1.904923\tLR: 0.030000\n",
      "Train Epoch: 1115 [126976/194182 (65%)]\tLoss: 0.305102\tGrad Norm: 1.655582\tLR: 0.030000\n",
      "Train Epoch: 1115 [147456/194182 (75%)]\tLoss: 0.296308\tGrad Norm: 1.357889\tLR: 0.030000\n",
      "Train Epoch: 1115 [167936/194182 (85%)]\tLoss: 0.292459\tGrad Norm: 1.395199\tLR: 0.030000\n",
      "Train Epoch: 1115 [188416/194182 (96%)]\tLoss: 0.302354\tGrad Norm: 1.492469\tLR: 0.030000\n",
      "Train set: Average loss: 0.3014\n",
      "Test set: Average loss: 0.2459, Average MAE: 0.3493\n",
      "Epoch 1115: Mean reward = 0.042 +/- 0.014\n",
      "Train Epoch: 1116 [4096/194182 (2%)]\tLoss: 0.294103\tGrad Norm: 1.478420\tLR: 0.030000\n",
      "Train Epoch: 1116 [24576/194182 (12%)]\tLoss: 0.298960\tGrad Norm: 1.358299\tLR: 0.030000\n",
      "Train Epoch: 1116 [45056/194182 (23%)]\tLoss: 0.301805\tGrad Norm: 1.448453\tLR: 0.030000\n",
      "Train Epoch: 1116 [65536/194182 (33%)]\tLoss: 0.298912\tGrad Norm: 1.370342\tLR: 0.030000\n",
      "Train Epoch: 1116 [86016/194182 (44%)]\tLoss: 0.295333\tGrad Norm: 1.443901\tLR: 0.030000\n",
      "Train Epoch: 1116 [106496/194182 (54%)]\tLoss: 0.295037\tGrad Norm: 1.536119\tLR: 0.030000\n",
      "Train Epoch: 1116 [126976/194182 (65%)]\tLoss: 0.295686\tGrad Norm: 1.466090\tLR: 0.030000\n",
      "Train Epoch: 1116 [147456/194182 (75%)]\tLoss: 0.300056\tGrad Norm: 1.325146\tLR: 0.030000\n",
      "Train Epoch: 1116 [167936/194182 (85%)]\tLoss: 0.297311\tGrad Norm: 1.310350\tLR: 0.030000\n",
      "Train Epoch: 1116 [188416/194182 (96%)]\tLoss: 0.296990\tGrad Norm: 1.414762\tLR: 0.030000\n",
      "Train set: Average loss: 0.2990\n",
      "Test set: Average loss: 0.2422, Average MAE: 0.3419\n",
      "Train Epoch: 1117 [4096/194182 (2%)]\tLoss: 0.297855\tGrad Norm: 1.276778\tLR: 0.030000\n",
      "Train Epoch: 1117 [24576/194182 (12%)]\tLoss: 0.292836\tGrad Norm: 0.801131\tLR: 0.030000\n",
      "Train Epoch: 1117 [45056/194182 (23%)]\tLoss: 0.297330\tGrad Norm: 1.275696\tLR: 0.030000\n",
      "Train Epoch: 1117 [65536/194182 (33%)]\tLoss: 0.295831\tGrad Norm: 1.415539\tLR: 0.030000\n",
      "Train Epoch: 1117 [86016/194182 (44%)]\tLoss: 0.296725\tGrad Norm: 1.213597\tLR: 0.030000\n",
      "Train Epoch: 1117 [106496/194182 (54%)]\tLoss: 0.300548\tGrad Norm: 1.585529\tLR: 0.030000\n",
      "Train Epoch: 1117 [126976/194182 (65%)]\tLoss: 0.302392\tGrad Norm: 1.859081\tLR: 0.030000\n",
      "Train Epoch: 1117 [147456/194182 (75%)]\tLoss: 0.304430\tGrad Norm: 1.829452\tLR: 0.030000\n",
      "Train Epoch: 1117 [167936/194182 (85%)]\tLoss: 0.299831\tGrad Norm: 1.679285\tLR: 0.030000\n",
      "Train Epoch: 1117 [188416/194182 (96%)]\tLoss: 0.295862\tGrad Norm: 1.205628\tLR: 0.030000\n",
      "Train set: Average loss: 0.2992\n",
      "Test set: Average loss: 0.2399, Average MAE: 0.3293\n",
      "Train Epoch: 1118 [4096/194182 (2%)]\tLoss: 0.297090\tGrad Norm: 1.246946\tLR: 0.030000\n",
      "Train Epoch: 1118 [24576/194182 (12%)]\tLoss: 0.302147\tGrad Norm: 1.488755\tLR: 0.030000\n",
      "Train Epoch: 1118 [45056/194182 (23%)]\tLoss: 0.288410\tGrad Norm: 1.198256\tLR: 0.030000\n",
      "Train Epoch: 1118 [65536/194182 (33%)]\tLoss: 0.302621\tGrad Norm: 1.573609\tLR: 0.030000\n",
      "Train Epoch: 1118 [86016/194182 (44%)]\tLoss: 0.302179\tGrad Norm: 1.467007\tLR: 0.030000\n",
      "Train Epoch: 1118 [106496/194182 (54%)]\tLoss: 0.301475\tGrad Norm: 1.329167\tLR: 0.030000\n",
      "Train Epoch: 1118 [126976/194182 (65%)]\tLoss: 0.288884\tGrad Norm: 1.090751\tLR: 0.030000\n",
      "Train Epoch: 1118 [147456/194182 (75%)]\tLoss: 0.301124\tGrad Norm: 1.274013\tLR: 0.030000\n",
      "Train Epoch: 1118 [167936/194182 (85%)]\tLoss: 0.299753\tGrad Norm: 1.255688\tLR: 0.030000\n",
      "Train Epoch: 1118 [188416/194182 (96%)]\tLoss: 0.303158\tGrad Norm: 1.481391\tLR: 0.030000\n",
      "Train set: Average loss: 0.2975\n",
      "Test set: Average loss: 0.2416, Average MAE: 0.3454\n",
      "Train Epoch: 1119 [4096/194182 (2%)]\tLoss: 0.292003\tGrad Norm: 1.169391\tLR: 0.030000\n",
      "Train Epoch: 1119 [24576/194182 (12%)]\tLoss: 0.306999\tGrad Norm: 1.644019\tLR: 0.030000\n",
      "Train Epoch: 1119 [45056/194182 (23%)]\tLoss: 0.297365\tGrad Norm: 1.631544\tLR: 0.030000\n",
      "Train Epoch: 1119 [65536/194182 (33%)]\tLoss: 0.298614\tGrad Norm: 1.717641\tLR: 0.030000\n",
      "Train Epoch: 1119 [86016/194182 (44%)]\tLoss: 0.296472\tGrad Norm: 1.640779\tLR: 0.030000\n",
      "Train Epoch: 1119 [106496/194182 (54%)]\tLoss: 0.300928\tGrad Norm: 1.064891\tLR: 0.030000\n",
      "Train Epoch: 1119 [126976/194182 (65%)]\tLoss: 0.300373\tGrad Norm: 1.361717\tLR: 0.030000\n",
      "Train Epoch: 1119 [147456/194182 (75%)]\tLoss: 0.293301\tGrad Norm: 1.118399\tLR: 0.030000\n",
      "Train Epoch: 1119 [167936/194182 (85%)]\tLoss: 0.298569\tGrad Norm: 1.205172\tLR: 0.030000\n",
      "Train Epoch: 1119 [188416/194182 (96%)]\tLoss: 0.298556\tGrad Norm: 1.379466\tLR: 0.030000\n",
      "Train set: Average loss: 0.2979\n",
      "Test set: Average loss: 0.2455, Average MAE: 0.3445\n",
      "Train Epoch: 1120 [4096/194182 (2%)]\tLoss: 0.298843\tGrad Norm: 1.553845\tLR: 0.030000\n",
      "Train Epoch: 1120 [24576/194182 (12%)]\tLoss: 0.295760\tGrad Norm: 1.448514\tLR: 0.030000\n",
      "Train Epoch: 1120 [45056/194182 (23%)]\tLoss: 0.287375\tGrad Norm: 1.241781\tLR: 0.030000\n",
      "Train Epoch: 1120 [65536/194182 (33%)]\tLoss: 0.300190\tGrad Norm: 1.800157\tLR: 0.030000\n",
      "Train Epoch: 1120 [86016/194182 (44%)]\tLoss: 0.304587\tGrad Norm: 1.497305\tLR: 0.030000\n",
      "Train Epoch: 1120 [106496/194182 (54%)]\tLoss: 0.300586\tGrad Norm: 1.540243\tLR: 0.030000\n",
      "Train Epoch: 1120 [126976/194182 (65%)]\tLoss: 0.298899\tGrad Norm: 1.118893\tLR: 0.030000\n",
      "Train Epoch: 1120 [147456/194182 (75%)]\tLoss: 0.301418\tGrad Norm: 1.327953\tLR: 0.030000\n",
      "Train Epoch: 1120 [167936/194182 (85%)]\tLoss: 0.306170\tGrad Norm: 1.713491\tLR: 0.030000\n",
      "Train Epoch: 1120 [188416/194182 (96%)]\tLoss: 0.294399\tGrad Norm: 1.731761\tLR: 0.030000\n",
      "Train set: Average loss: 0.2993\n",
      "Test set: Average loss: 0.2520, Average MAE: 0.3598\n",
      "Epoch 1120: Mean reward = 0.055 +/- 0.045\n",
      "Train Epoch: 1121 [4096/194182 (2%)]\tLoss: 0.303070\tGrad Norm: 1.849891\tLR: 0.030000\n",
      "Train Epoch: 1121 [24576/194182 (12%)]\tLoss: 0.292593\tGrad Norm: 0.923187\tLR: 0.030000\n",
      "Train Epoch: 1121 [45056/194182 (23%)]\tLoss: 0.297552\tGrad Norm: 1.438168\tLR: 0.030000\n",
      "Train Epoch: 1121 [65536/194182 (33%)]\tLoss: 0.295110\tGrad Norm: 1.539582\tLR: 0.030000\n",
      "Train Epoch: 1121 [86016/194182 (44%)]\tLoss: 0.289855\tGrad Norm: 1.309429\tLR: 0.030000\n",
      "Train Epoch: 1121 [106496/194182 (54%)]\tLoss: 0.299383\tGrad Norm: 1.250183\tLR: 0.030000\n",
      "Train Epoch: 1121 [126976/194182 (65%)]\tLoss: 0.301662\tGrad Norm: 1.145920\tLR: 0.030000\n",
      "Train Epoch: 1121 [147456/194182 (75%)]\tLoss: 0.302805\tGrad Norm: 1.280803\tLR: 0.030000\n",
      "Train Epoch: 1121 [167936/194182 (85%)]\tLoss: 0.299746\tGrad Norm: 1.401506\tLR: 0.030000\n",
      "Train Epoch: 1121 [188416/194182 (96%)]\tLoss: 0.297823\tGrad Norm: 1.278803\tLR: 0.030000\n",
      "Train set: Average loss: 0.2965\n",
      "Test set: Average loss: 0.2468, Average MAE: 0.3330\n",
      "Train Epoch: 1122 [4096/194182 (2%)]\tLoss: 0.296995\tGrad Norm: 1.604734\tLR: 0.030000\n",
      "Train Epoch: 1122 [24576/194182 (12%)]\tLoss: 0.300826\tGrad Norm: 1.389501\tLR: 0.030000\n",
      "Train Epoch: 1122 [45056/194182 (23%)]\tLoss: 0.299192\tGrad Norm: 1.684871\tLR: 0.030000\n",
      "Train Epoch: 1122 [65536/194182 (33%)]\tLoss: 0.301197\tGrad Norm: 1.699563\tLR: 0.030000\n",
      "Train Epoch: 1122 [86016/194182 (44%)]\tLoss: 0.304055\tGrad Norm: 1.730055\tLR: 0.030000\n",
      "Train Epoch: 1122 [106496/194182 (54%)]\tLoss: 0.301732\tGrad Norm: 1.602994\tLR: 0.030000\n",
      "Train Epoch: 1122 [126976/194182 (65%)]\tLoss: 0.298534\tGrad Norm: 1.468760\tLR: 0.030000\n",
      "Train Epoch: 1122 [147456/194182 (75%)]\tLoss: 0.306776\tGrad Norm: 1.548283\tLR: 0.030000\n",
      "Train Epoch: 1122 [167936/194182 (85%)]\tLoss: 0.292149\tGrad Norm: 1.210907\tLR: 0.030000\n",
      "Train Epoch: 1122 [188416/194182 (96%)]\tLoss: 0.298765\tGrad Norm: 1.484562\tLR: 0.030000\n",
      "Train set: Average loss: 0.2983\n",
      "Test set: Average loss: 0.2507, Average MAE: 0.3366\n",
      "Train Epoch: 1123 [4096/194182 (2%)]\tLoss: 0.307856\tGrad Norm: 1.874689\tLR: 0.030000\n",
      "Train Epoch: 1123 [24576/194182 (12%)]\tLoss: 0.303050\tGrad Norm: 1.763743\tLR: 0.030000\n",
      "Train Epoch: 1123 [45056/194182 (23%)]\tLoss: 0.297004\tGrad Norm: 1.491740\tLR: 0.030000\n",
      "Train Epoch: 1123 [65536/194182 (33%)]\tLoss: 0.296984\tGrad Norm: 1.349151\tLR: 0.030000\n",
      "Train Epoch: 1123 [86016/194182 (44%)]\tLoss: 0.295989\tGrad Norm: 1.254041\tLR: 0.030000\n",
      "Train Epoch: 1123 [106496/194182 (54%)]\tLoss: 0.295618\tGrad Norm: 1.153927\tLR: 0.030000\n",
      "Train Epoch: 1123 [126976/194182 (65%)]\tLoss: 0.305335\tGrad Norm: 1.544057\tLR: 0.030000\n",
      "Train Epoch: 1123 [147456/194182 (75%)]\tLoss: 0.301954\tGrad Norm: 1.553147\tLR: 0.030000\n",
      "Train Epoch: 1123 [167936/194182 (85%)]\tLoss: 0.293243\tGrad Norm: 1.375218\tLR: 0.030000\n",
      "Train Epoch: 1123 [188416/194182 (96%)]\tLoss: 0.290431\tGrad Norm: 1.169628\tLR: 0.030000\n",
      "Train set: Average loss: 0.2984\n",
      "Test set: Average loss: 0.2457, Average MAE: 0.3538\n",
      "Train Epoch: 1124 [4096/194182 (2%)]\tLoss: 0.296437\tGrad Norm: 1.539526\tLR: 0.030000\n",
      "Train Epoch: 1124 [24576/194182 (12%)]\tLoss: 0.299119\tGrad Norm: 1.735082\tLR: 0.030000\n",
      "Train Epoch: 1124 [45056/194182 (23%)]\tLoss: 0.297693\tGrad Norm: 1.567364\tLR: 0.030000\n",
      "Train Epoch: 1124 [65536/194182 (33%)]\tLoss: 0.293493\tGrad Norm: 1.229917\tLR: 0.030000\n",
      "Train Epoch: 1124 [86016/194182 (44%)]\tLoss: 0.307061\tGrad Norm: 1.614512\tLR: 0.030000\n",
      "Train Epoch: 1124 [106496/194182 (54%)]\tLoss: 0.303161\tGrad Norm: 1.730128\tLR: 0.030000\n",
      "Train Epoch: 1124 [126976/194182 (65%)]\tLoss: 0.306457\tGrad Norm: 1.795632\tLR: 0.030000\n",
      "Train Epoch: 1124 [147456/194182 (75%)]\tLoss: 0.294885\tGrad Norm: 1.549712\tLR: 0.030000\n",
      "Train Epoch: 1124 [167936/194182 (85%)]\tLoss: 0.303926\tGrad Norm: 1.685341\tLR: 0.030000\n",
      "Train Epoch: 1124 [188416/194182 (96%)]\tLoss: 0.296178\tGrad Norm: 1.390318\tLR: 0.030000\n",
      "Train set: Average loss: 0.2991\n",
      "Test set: Average loss: 0.2481, Average MAE: 0.3548\n",
      "Train Epoch: 1125 [4096/194182 (2%)]\tLoss: 0.296528\tGrad Norm: 1.651651\tLR: 0.030000\n",
      "Train Epoch: 1125 [24576/194182 (12%)]\tLoss: 0.299206\tGrad Norm: 1.674821\tLR: 0.030000\n",
      "Train Epoch: 1125 [45056/194182 (23%)]\tLoss: 0.297886\tGrad Norm: 1.190383\tLR: 0.030000\n",
      "Train Epoch: 1125 [65536/194182 (33%)]\tLoss: 0.294357\tGrad Norm: 1.114587\tLR: 0.030000\n",
      "Train Epoch: 1125 [86016/194182 (44%)]\tLoss: 0.299757\tGrad Norm: 1.518369\tLR: 0.030000\n",
      "Train Epoch: 1125 [106496/194182 (54%)]\tLoss: 0.303917\tGrad Norm: 1.717675\tLR: 0.030000\n",
      "Train Epoch: 1125 [126976/194182 (65%)]\tLoss: 0.294415\tGrad Norm: 1.331511\tLR: 0.030000\n",
      "Train Epoch: 1125 [147456/194182 (75%)]\tLoss: 0.296513\tGrad Norm: 1.024058\tLR: 0.030000\n",
      "Train Epoch: 1125 [167936/194182 (85%)]\tLoss: 0.295411\tGrad Norm: 1.140889\tLR: 0.030000\n",
      "Train Epoch: 1125 [188416/194182 (96%)]\tLoss: 0.297252\tGrad Norm: 1.508225\tLR: 0.030000\n",
      "Train set: Average loss: 0.2969\n",
      "Test set: Average loss: 0.2537, Average MAE: 0.3562\n",
      "Epoch 1125: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 1126 [4096/194182 (2%)]\tLoss: 0.309115\tGrad Norm: 1.866465\tLR: 0.030000\n",
      "Train Epoch: 1126 [24576/194182 (12%)]\tLoss: 0.293789\tGrad Norm: 1.214216\tLR: 0.030000\n",
      "Train Epoch: 1126 [45056/194182 (23%)]\tLoss: 0.297749\tGrad Norm: 1.205243\tLR: 0.030000\n",
      "Train Epoch: 1126 [65536/194182 (33%)]\tLoss: 0.289276\tGrad Norm: 1.202742\tLR: 0.030000\n",
      "Train Epoch: 1126 [86016/194182 (44%)]\tLoss: 0.297791\tGrad Norm: 1.246856\tLR: 0.030000\n",
      "Train Epoch: 1126 [106496/194182 (54%)]\tLoss: 0.292448\tGrad Norm: 1.172862\tLR: 0.030000\n",
      "Train Epoch: 1126 [126976/194182 (65%)]\tLoss: 0.297370\tGrad Norm: 1.558413\tLR: 0.030000\n",
      "Train Epoch: 1126 [147456/194182 (75%)]\tLoss: 0.290121\tGrad Norm: 1.550742\tLR: 0.030000\n",
      "Train Epoch: 1126 [167936/194182 (85%)]\tLoss: 0.296845\tGrad Norm: 1.461563\tLR: 0.030000\n",
      "Train Epoch: 1126 [188416/194182 (96%)]\tLoss: 0.301707\tGrad Norm: 1.754445\tLR: 0.030000\n",
      "Train set: Average loss: 0.2968\n",
      "Test set: Average loss: 0.2449, Average MAE: 0.3520\n",
      "Train Epoch: 1127 [4096/194182 (2%)]\tLoss: 0.298525\tGrad Norm: 1.434969\tLR: 0.030000\n",
      "Train Epoch: 1127 [24576/194182 (12%)]\tLoss: 0.297966\tGrad Norm: 1.411930\tLR: 0.030000\n",
      "Train Epoch: 1127 [45056/194182 (23%)]\tLoss: 0.296069\tGrad Norm: 1.235917\tLR: 0.030000\n",
      "Train Epoch: 1127 [65536/194182 (33%)]\tLoss: 0.290454\tGrad Norm: 1.170658\tLR: 0.030000\n",
      "Train Epoch: 1127 [86016/194182 (44%)]\tLoss: 0.291796\tGrad Norm: 1.455818\tLR: 0.030000\n",
      "Train Epoch: 1127 [106496/194182 (54%)]\tLoss: 0.301368\tGrad Norm: 1.640910\tLR: 0.030000\n",
      "Train Epoch: 1127 [126976/194182 (65%)]\tLoss: 0.293159\tGrad Norm: 1.754696\tLR: 0.030000\n",
      "Train Epoch: 1127 [147456/194182 (75%)]\tLoss: 0.305118\tGrad Norm: 1.812633\tLR: 0.030000\n",
      "Train Epoch: 1127 [167936/194182 (85%)]\tLoss: 0.290815\tGrad Norm: 1.665129\tLR: 0.030000\n",
      "Train Epoch: 1127 [188416/194182 (96%)]\tLoss: 0.296472\tGrad Norm: 1.498994\tLR: 0.030000\n",
      "Train set: Average loss: 0.2981\n",
      "Test set: Average loss: 0.2384, Average MAE: 0.3337\n",
      "Train Epoch: 1128 [4096/194182 (2%)]\tLoss: 0.285264\tGrad Norm: 1.090944\tLR: 0.030000\n",
      "Train Epoch: 1128 [24576/194182 (12%)]\tLoss: 0.295582\tGrad Norm: 1.494018\tLR: 0.030000\n",
      "Train Epoch: 1128 [45056/194182 (23%)]\tLoss: 0.288973\tGrad Norm: 1.148152\tLR: 0.030000\n",
      "Train Epoch: 1128 [65536/194182 (33%)]\tLoss: 0.296653\tGrad Norm: 1.470034\tLR: 0.030000\n",
      "Train Epoch: 1128 [86016/194182 (44%)]\tLoss: 0.290056\tGrad Norm: 1.189747\tLR: 0.030000\n",
      "Train Epoch: 1128 [106496/194182 (54%)]\tLoss: 0.296953\tGrad Norm: 1.248959\tLR: 0.030000\n",
      "Train Epoch: 1128 [126976/194182 (65%)]\tLoss: 0.291558\tGrad Norm: 1.485641\tLR: 0.030000\n",
      "Train Epoch: 1128 [147456/194182 (75%)]\tLoss: 0.304650\tGrad Norm: 1.865022\tLR: 0.030000\n",
      "Train Epoch: 1128 [167936/194182 (85%)]\tLoss: 0.299793\tGrad Norm: 1.471785\tLR: 0.030000\n",
      "Train Epoch: 1128 [188416/194182 (96%)]\tLoss: 0.311237\tGrad Norm: 1.824163\tLR: 0.030000\n",
      "Train set: Average loss: 0.2967\n",
      "Test set: Average loss: 0.2480, Average MAE: 0.3595\n",
      "Train Epoch: 1129 [4096/194182 (2%)]\tLoss: 0.297017\tGrad Norm: 1.721190\tLR: 0.030000\n",
      "Train Epoch: 1129 [24576/194182 (12%)]\tLoss: 0.295549\tGrad Norm: 1.355903\tLR: 0.030000\n",
      "Train Epoch: 1129 [45056/194182 (23%)]\tLoss: 0.298961\tGrad Norm: 1.631323\tLR: 0.030000\n",
      "Train Epoch: 1129 [65536/194182 (33%)]\tLoss: 0.295495\tGrad Norm: 1.578766\tLR: 0.030000\n",
      "Train Epoch: 1129 [86016/194182 (44%)]\tLoss: 0.294983\tGrad Norm: 1.408073\tLR: 0.030000\n",
      "Train Epoch: 1129 [106496/194182 (54%)]\tLoss: 0.295399\tGrad Norm: 1.379180\tLR: 0.030000\n",
      "Train Epoch: 1129 [126976/194182 (65%)]\tLoss: 0.300457\tGrad Norm: 1.635656\tLR: 0.030000\n",
      "Train Epoch: 1129 [147456/194182 (75%)]\tLoss: 0.288374\tGrad Norm: 1.516238\tLR: 0.030000\n",
      "Train Epoch: 1129 [167936/194182 (85%)]\tLoss: 0.293698\tGrad Norm: 1.458592\tLR: 0.030000\n",
      "Train Epoch: 1129 [188416/194182 (96%)]\tLoss: 0.295321\tGrad Norm: 1.248383\tLR: 0.030000\n",
      "Train set: Average loss: 0.2967\n",
      "Test set: Average loss: 0.2415, Average MAE: 0.3328\n",
      "Train Epoch: 1130 [4096/194182 (2%)]\tLoss: 0.287411\tGrad Norm: 1.253810\tLR: 0.030000\n",
      "Train Epoch: 1130 [24576/194182 (12%)]\tLoss: 0.289706\tGrad Norm: 1.210759\tLR: 0.030000\n",
      "Train Epoch: 1130 [45056/194182 (23%)]\tLoss: 0.287603\tGrad Norm: 1.063336\tLR: 0.030000\n",
      "Train Epoch: 1130 [65536/194182 (33%)]\tLoss: 0.299439\tGrad Norm: 1.424027\tLR: 0.030000\n",
      "Train Epoch: 1130 [86016/194182 (44%)]\tLoss: 0.299336\tGrad Norm: 1.613921\tLR: 0.030000\n",
      "Train Epoch: 1130 [106496/194182 (54%)]\tLoss: 0.303961\tGrad Norm: 2.001444\tLR: 0.030000\n",
      "Train Epoch: 1130 [126976/194182 (65%)]\tLoss: 0.305386\tGrad Norm: 1.669864\tLR: 0.030000\n",
      "Train Epoch: 1130 [147456/194182 (75%)]\tLoss: 0.288098\tGrad Norm: 1.415857\tLR: 0.030000\n",
      "Train Epoch: 1130 [167936/194182 (85%)]\tLoss: 0.304726\tGrad Norm: 1.864752\tLR: 0.030000\n",
      "Train Epoch: 1130 [188416/194182 (96%)]\tLoss: 0.293863\tGrad Norm: 1.308784\tLR: 0.030000\n",
      "Train set: Average loss: 0.2971\n",
      "Test set: Average loss: 0.2419, Average MAE: 0.3337\n",
      "Epoch 1130: Mean reward = 0.056 +/- 0.046\n",
      "Train Epoch: 1131 [4096/194182 (2%)]\tLoss: 0.281680\tGrad Norm: 1.150539\tLR: 0.030000\n",
      "Train Epoch: 1131 [24576/194182 (12%)]\tLoss: 0.297222\tGrad Norm: 1.030076\tLR: 0.030000\n",
      "Train Epoch: 1131 [45056/194182 (23%)]\tLoss: 0.294053\tGrad Norm: 1.335578\tLR: 0.030000\n",
      "Train Epoch: 1131 [65536/194182 (33%)]\tLoss: 0.296197\tGrad Norm: 1.314462\tLR: 0.030000\n",
      "Train Epoch: 1131 [86016/194182 (44%)]\tLoss: 0.305715\tGrad Norm: 1.589554\tLR: 0.030000\n",
      "Train Epoch: 1131 [106496/194182 (54%)]\tLoss: 0.293241\tGrad Norm: 1.223573\tLR: 0.030000\n",
      "Train Epoch: 1131 [126976/194182 (65%)]\tLoss: 0.293246\tGrad Norm: 1.650876\tLR: 0.030000\n",
      "Train Epoch: 1131 [147456/194182 (75%)]\tLoss: 0.294878\tGrad Norm: 1.642779\tLR: 0.030000\n",
      "Train Epoch: 1131 [167936/194182 (85%)]\tLoss: 0.304967\tGrad Norm: 1.390446\tLR: 0.030000\n",
      "Train Epoch: 1131 [188416/194182 (96%)]\tLoss: 0.294499\tGrad Norm: 1.296596\tLR: 0.030000\n",
      "Train set: Average loss: 0.2956\n",
      "Test set: Average loss: 0.2475, Average MAE: 0.3374\n",
      "Train Epoch: 1132 [4096/194182 (2%)]\tLoss: 0.300267\tGrad Norm: 1.772670\tLR: 0.030000\n",
      "Train Epoch: 1132 [24576/194182 (12%)]\tLoss: 0.297156\tGrad Norm: 1.349646\tLR: 0.030000\n",
      "Train Epoch: 1132 [45056/194182 (23%)]\tLoss: 0.303856\tGrad Norm: 1.503648\tLR: 0.030000\n",
      "Train Epoch: 1132 [65536/194182 (33%)]\tLoss: 0.291265\tGrad Norm: 1.413327\tLR: 0.030000\n",
      "Train Epoch: 1132 [86016/194182 (44%)]\tLoss: 0.301844\tGrad Norm: 1.732366\tLR: 0.030000\n",
      "Train Epoch: 1132 [106496/194182 (54%)]\tLoss: 0.300170\tGrad Norm: 1.749532\tLR: 0.030000\n",
      "Train Epoch: 1132 [126976/194182 (65%)]\tLoss: 0.293083\tGrad Norm: 1.581581\tLR: 0.030000\n",
      "Train Epoch: 1132 [147456/194182 (75%)]\tLoss: 0.292968\tGrad Norm: 1.428704\tLR: 0.030000\n",
      "Train Epoch: 1132 [167936/194182 (85%)]\tLoss: 0.297481\tGrad Norm: 1.648424\tLR: 0.030000\n",
      "Train Epoch: 1132 [188416/194182 (96%)]\tLoss: 0.291199\tGrad Norm: 1.348670\tLR: 0.030000\n",
      "Train set: Average loss: 0.2967\n",
      "Test set: Average loss: 0.2422, Average MAE: 0.3309\n",
      "Train Epoch: 1133 [4096/194182 (2%)]\tLoss: 0.286250\tGrad Norm: 1.259990\tLR: 0.030000\n",
      "Train Epoch: 1133 [24576/194182 (12%)]\tLoss: 0.295547\tGrad Norm: 1.208744\tLR: 0.030000\n",
      "Train Epoch: 1133 [45056/194182 (23%)]\tLoss: 0.296359\tGrad Norm: 1.727812\tLR: 0.030000\n",
      "Train Epoch: 1133 [65536/194182 (33%)]\tLoss: 0.299781\tGrad Norm: 1.898976\tLR: 0.030000\n",
      "Train Epoch: 1133 [86016/194182 (44%)]\tLoss: 0.300054\tGrad Norm: 1.746724\tLR: 0.030000\n",
      "Train Epoch: 1133 [106496/194182 (54%)]\tLoss: 0.291320\tGrad Norm: 1.359034\tLR: 0.030000\n",
      "Train Epoch: 1133 [126976/194182 (65%)]\tLoss: 0.299031\tGrad Norm: 1.624954\tLR: 0.030000\n",
      "Train Epoch: 1133 [147456/194182 (75%)]\tLoss: 0.289792\tGrad Norm: 1.243084\tLR: 0.030000\n",
      "Train Epoch: 1133 [167936/194182 (85%)]\tLoss: 0.301450\tGrad Norm: 1.568487\tLR: 0.030000\n",
      "Train Epoch: 1133 [188416/194182 (96%)]\tLoss: 0.294196\tGrad Norm: 1.416380\tLR: 0.030000\n",
      "Train set: Average loss: 0.2975\n",
      "Test set: Average loss: 0.2461, Average MAE: 0.3437\n",
      "Train Epoch: 1134 [4096/194182 (2%)]\tLoss: 0.292917\tGrad Norm: 1.599619\tLR: 0.030000\n",
      "Train Epoch: 1134 [24576/194182 (12%)]\tLoss: 0.299062\tGrad Norm: 1.477727\tLR: 0.030000\n",
      "Train Epoch: 1134 [45056/194182 (23%)]\tLoss: 0.294193\tGrad Norm: 1.326494\tLR: 0.030000\n",
      "Train Epoch: 1134 [65536/194182 (33%)]\tLoss: 0.299167\tGrad Norm: 1.630283\tLR: 0.030000\n",
      "Train Epoch: 1134 [86016/194182 (44%)]\tLoss: 0.286420\tGrad Norm: 1.329640\tLR: 0.030000\n",
      "Train Epoch: 1134 [106496/194182 (54%)]\tLoss: 0.293648\tGrad Norm: 1.212220\tLR: 0.030000\n",
      "Train Epoch: 1134 [126976/194182 (65%)]\tLoss: 0.291933\tGrad Norm: 1.214647\tLR: 0.030000\n",
      "Train Epoch: 1134 [147456/194182 (75%)]\tLoss: 0.295706\tGrad Norm: 1.722979\tLR: 0.030000\n",
      "Train Epoch: 1134 [167936/194182 (85%)]\tLoss: 0.296147\tGrad Norm: 1.474641\tLR: 0.030000\n",
      "Train Epoch: 1134 [188416/194182 (96%)]\tLoss: 0.296420\tGrad Norm: 1.414623\tLR: 0.030000\n",
      "Train set: Average loss: 0.2957\n",
      "Test set: Average loss: 0.2412, Average MAE: 0.3466\n",
      "Train Epoch: 1135 [4096/194182 (2%)]\tLoss: 0.296774\tGrad Norm: 1.149299\tLR: 0.030000\n",
      "Train Epoch: 1135 [24576/194182 (12%)]\tLoss: 0.297967\tGrad Norm: 1.401469\tLR: 0.030000\n",
      "Train Epoch: 1135 [45056/194182 (23%)]\tLoss: 0.293666\tGrad Norm: 1.565726\tLR: 0.030000\n",
      "Train Epoch: 1135 [65536/194182 (33%)]\tLoss: 0.293056\tGrad Norm: 1.370975\tLR: 0.030000\n",
      "Train Epoch: 1135 [86016/194182 (44%)]\tLoss: 0.297653\tGrad Norm: 1.572108\tLR: 0.030000\n",
      "Train Epoch: 1135 [106496/194182 (54%)]\tLoss: 0.293622\tGrad Norm: 1.206652\tLR: 0.030000\n",
      "Train Epoch: 1135 [126976/194182 (65%)]\tLoss: 0.297337\tGrad Norm: 1.921733\tLR: 0.030000\n",
      "Train Epoch: 1135 [147456/194182 (75%)]\tLoss: 0.290996\tGrad Norm: 1.476469\tLR: 0.030000\n",
      "Train Epoch: 1135 [167936/194182 (85%)]\tLoss: 0.302426\tGrad Norm: 1.913641\tLR: 0.030000\n",
      "Train Epoch: 1135 [188416/194182 (96%)]\tLoss: 0.302936\tGrad Norm: 1.483145\tLR: 0.030000\n",
      "Train set: Average loss: 0.2972\n",
      "Test set: Average loss: 0.2502, Average MAE: 0.3511\n",
      "Epoch 1135: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 1136 [4096/194182 (2%)]\tLoss: 0.295858\tGrad Norm: 1.665391\tLR: 0.030000\n",
      "Train Epoch: 1136 [24576/194182 (12%)]\tLoss: 0.284715\tGrad Norm: 1.166974\tLR: 0.030000\n",
      "Train Epoch: 1136 [45056/194182 (23%)]\tLoss: 0.297443\tGrad Norm: 1.478604\tLR: 0.030000\n",
      "Train Epoch: 1136 [65536/194182 (33%)]\tLoss: 0.297046\tGrad Norm: 1.694676\tLR: 0.030000\n",
      "Train Epoch: 1136 [86016/194182 (44%)]\tLoss: 0.301088\tGrad Norm: 1.694187\tLR: 0.030000\n",
      "Train Epoch: 1136 [106496/194182 (54%)]\tLoss: 0.297188\tGrad Norm: 1.483138\tLR: 0.030000\n",
      "Train Epoch: 1136 [126976/194182 (65%)]\tLoss: 0.280322\tGrad Norm: 0.869854\tLR: 0.030000\n",
      "Train Epoch: 1136 [147456/194182 (75%)]\tLoss: 0.282652\tGrad Norm: 1.176146\tLR: 0.030000\n",
      "Train Epoch: 1136 [167936/194182 (85%)]\tLoss: 0.291194\tGrad Norm: 0.856472\tLR: 0.030000\n",
      "Train Epoch: 1136 [188416/194182 (96%)]\tLoss: 0.299718\tGrad Norm: 1.771711\tLR: 0.030000\n",
      "Train set: Average loss: 0.2938\n",
      "Test set: Average loss: 0.2464, Average MAE: 0.3375\n",
      "Train Epoch: 1137 [4096/194182 (2%)]\tLoss: 0.297450\tGrad Norm: 1.465442\tLR: 0.030000\n",
      "Train Epoch: 1137 [24576/194182 (12%)]\tLoss: 0.298047\tGrad Norm: 1.511307\tLR: 0.030000\n",
      "Train Epoch: 1137 [45056/194182 (23%)]\tLoss: 0.291583\tGrad Norm: 1.205075\tLR: 0.030000\n",
      "Train Epoch: 1137 [65536/194182 (33%)]\tLoss: 0.298298\tGrad Norm: 1.129321\tLR: 0.030000\n",
      "Train Epoch: 1137 [86016/194182 (44%)]\tLoss: 0.289766\tGrad Norm: 1.410298\tLR: 0.030000\n",
      "Train Epoch: 1137 [106496/194182 (54%)]\tLoss: 0.295508\tGrad Norm: 1.160961\tLR: 0.030000\n",
      "Train Epoch: 1137 [126976/194182 (65%)]\tLoss: 0.295663\tGrad Norm: 1.726289\tLR: 0.030000\n",
      "Train Epoch: 1137 [147456/194182 (75%)]\tLoss: 0.295367\tGrad Norm: 1.396514\tLR: 0.030000\n",
      "Train Epoch: 1137 [167936/194182 (85%)]\tLoss: 0.303053\tGrad Norm: 1.788589\tLR: 0.030000\n",
      "Train Epoch: 1137 [188416/194182 (96%)]\tLoss: 0.302834\tGrad Norm: 1.920374\tLR: 0.030000\n",
      "Train set: Average loss: 0.2955\n",
      "Test set: Average loss: 0.2517, Average MAE: 0.3598\n",
      "Train Epoch: 1138 [4096/194182 (2%)]\tLoss: 0.297101\tGrad Norm: 1.762790\tLR: 0.030000\n",
      "Train Epoch: 1138 [24576/194182 (12%)]\tLoss: 0.286636\tGrad Norm: 1.731063\tLR: 0.030000\n",
      "Train Epoch: 1138 [45056/194182 (23%)]\tLoss: 0.295635\tGrad Norm: 1.345045\tLR: 0.030000\n",
      "Train Epoch: 1138 [65536/194182 (33%)]\tLoss: 0.294866\tGrad Norm: 1.256190\tLR: 0.030000\n",
      "Train Epoch: 1138 [86016/194182 (44%)]\tLoss: 0.290927\tGrad Norm: 1.261453\tLR: 0.030000\n",
      "Train Epoch: 1138 [106496/194182 (54%)]\tLoss: 0.296539\tGrad Norm: 1.326415\tLR: 0.030000\n",
      "Train Epoch: 1138 [126976/194182 (65%)]\tLoss: 0.297189\tGrad Norm: 1.455238\tLR: 0.030000\n",
      "Train Epoch: 1138 [147456/194182 (75%)]\tLoss: 0.288809\tGrad Norm: 1.082460\tLR: 0.030000\n",
      "Train Epoch: 1138 [167936/194182 (85%)]\tLoss: 0.300447\tGrad Norm: 1.672926\tLR: 0.030000\n",
      "Train Epoch: 1138 [188416/194182 (96%)]\tLoss: 0.295502\tGrad Norm: 1.216169\tLR: 0.030000\n",
      "Train set: Average loss: 0.2949\n",
      "Test set: Average loss: 0.2405, Average MAE: 0.3421\n",
      "Train Epoch: 1139 [4096/194182 (2%)]\tLoss: 0.284975\tGrad Norm: 1.107943\tLR: 0.030000\n",
      "Train Epoch: 1139 [24576/194182 (12%)]\tLoss: 0.292795\tGrad Norm: 1.331430\tLR: 0.030000\n",
      "Train Epoch: 1139 [45056/194182 (23%)]\tLoss: 0.290898\tGrad Norm: 1.238518\tLR: 0.030000\n",
      "Train Epoch: 1139 [65536/194182 (33%)]\tLoss: 0.306415\tGrad Norm: 1.573412\tLR: 0.030000\n",
      "Train Epoch: 1139 [86016/194182 (44%)]\tLoss: 0.303558\tGrad Norm: 1.590860\tLR: 0.030000\n",
      "Train Epoch: 1139 [106496/194182 (54%)]\tLoss: 0.291896\tGrad Norm: 1.348107\tLR: 0.030000\n",
      "Train Epoch: 1139 [126976/194182 (65%)]\tLoss: 0.291217\tGrad Norm: 1.286462\tLR: 0.030000\n",
      "Train Epoch: 1139 [147456/194182 (75%)]\tLoss: 0.296618\tGrad Norm: 1.474809\tLR: 0.030000\n",
      "Train Epoch: 1139 [167936/194182 (85%)]\tLoss: 0.297215\tGrad Norm: 1.953947\tLR: 0.030000\n",
      "Train Epoch: 1139 [188416/194182 (96%)]\tLoss: 0.289999\tGrad Norm: 1.381751\tLR: 0.030000\n",
      "Train set: Average loss: 0.2945\n",
      "Test set: Average loss: 0.2417, Average MAE: 0.3478\n",
      "Train Epoch: 1140 [4096/194182 (2%)]\tLoss: 0.285937\tGrad Norm: 1.228178\tLR: 0.030000\n",
      "Train Epoch: 1140 [24576/194182 (12%)]\tLoss: 0.291672\tGrad Norm: 1.299900\tLR: 0.030000\n",
      "Train Epoch: 1140 [45056/194182 (23%)]\tLoss: 0.292940\tGrad Norm: 1.440209\tLR: 0.030000\n",
      "Train Epoch: 1140 [65536/194182 (33%)]\tLoss: 0.297901\tGrad Norm: 1.901862\tLR: 0.030000\n",
      "Train Epoch: 1140 [86016/194182 (44%)]\tLoss: 0.291933\tGrad Norm: 1.299739\tLR: 0.030000\n",
      "Train Epoch: 1140 [106496/194182 (54%)]\tLoss: 0.294862\tGrad Norm: 1.256132\tLR: 0.030000\n",
      "Train Epoch: 1140 [126976/194182 (65%)]\tLoss: 0.294733\tGrad Norm: 1.414128\tLR: 0.030000\n",
      "Train Epoch: 1140 [147456/194182 (75%)]\tLoss: 0.296848\tGrad Norm: 1.507915\tLR: 0.030000\n",
      "Train Epoch: 1140 [167936/194182 (85%)]\tLoss: 0.294674\tGrad Norm: 1.547803\tLR: 0.030000\n",
      "Train Epoch: 1140 [188416/194182 (96%)]\tLoss: 0.283741\tGrad Norm: 1.157941\tLR: 0.030000\n",
      "Train set: Average loss: 0.2952\n",
      "Test set: Average loss: 0.2471, Average MAE: 0.3471\n",
      "Epoch 1140: Mean reward = 0.081 +/- 0.087\n",
      "Train Epoch: 1141 [4096/194182 (2%)]\tLoss: 0.302585\tGrad Norm: 1.511193\tLR: 0.030000\n",
      "Train Epoch: 1141 [24576/194182 (12%)]\tLoss: 0.294647\tGrad Norm: 1.230804\tLR: 0.030000\n",
      "Train Epoch: 1141 [45056/194182 (23%)]\tLoss: 0.290082\tGrad Norm: 1.276909\tLR: 0.030000\n",
      "Train Epoch: 1141 [65536/194182 (33%)]\tLoss: 0.293478\tGrad Norm: 1.162322\tLR: 0.030000\n",
      "Train Epoch: 1141 [86016/194182 (44%)]\tLoss: 0.285690\tGrad Norm: 1.134949\tLR: 0.030000\n",
      "Train Epoch: 1141 [106496/194182 (54%)]\tLoss: 0.295584\tGrad Norm: 1.436874\tLR: 0.030000\n",
      "Train Epoch: 1141 [126976/194182 (65%)]\tLoss: 0.296522\tGrad Norm: 1.479915\tLR: 0.030000\n",
      "Train Epoch: 1141 [147456/194182 (75%)]\tLoss: 0.301036\tGrad Norm: 1.766679\tLR: 0.030000\n",
      "Train Epoch: 1141 [167936/194182 (85%)]\tLoss: 0.293981\tGrad Norm: 1.965260\tLR: 0.030000\n",
      "Train Epoch: 1141 [188416/194182 (96%)]\tLoss: 0.292342\tGrad Norm: 1.689445\tLR: 0.030000\n",
      "Train set: Average loss: 0.2949\n",
      "Test set: Average loss: 0.2504, Average MAE: 0.3318\n",
      "Train Epoch: 1142 [4096/194182 (2%)]\tLoss: 0.313762\tGrad Norm: 2.051568\tLR: 0.030000\n",
      "Train Epoch: 1142 [24576/194182 (12%)]\tLoss: 0.294794\tGrad Norm: 1.532191\tLR: 0.030000\n",
      "Train Epoch: 1142 [45056/194182 (23%)]\tLoss: 0.292002\tGrad Norm: 1.397367\tLR: 0.030000\n",
      "Train Epoch: 1142 [65536/194182 (33%)]\tLoss: 0.293958\tGrad Norm: 1.848668\tLR: 0.030000\n",
      "Train Epoch: 1142 [86016/194182 (44%)]\tLoss: 0.295145\tGrad Norm: 1.750627\tLR: 0.030000\n",
      "Train Epoch: 1142 [106496/194182 (54%)]\tLoss: 0.290070\tGrad Norm: 1.555671\tLR: 0.030000\n",
      "Train Epoch: 1142 [126976/194182 (65%)]\tLoss: 0.297699\tGrad Norm: 1.529822\tLR: 0.030000\n",
      "Train Epoch: 1142 [147456/194182 (75%)]\tLoss: 0.296133\tGrad Norm: 1.697023\tLR: 0.030000\n",
      "Train Epoch: 1142 [167936/194182 (85%)]\tLoss: 0.294402\tGrad Norm: 1.299188\tLR: 0.030000\n",
      "Train Epoch: 1142 [188416/194182 (96%)]\tLoss: 0.288119\tGrad Norm: 1.306136\tLR: 0.030000\n",
      "Train set: Average loss: 0.2955\n",
      "Test set: Average loss: 0.2464, Average MAE: 0.3449\n",
      "Train Epoch: 1143 [4096/194182 (2%)]\tLoss: 0.299140\tGrad Norm: 1.485747\tLR: 0.030000\n",
      "Train Epoch: 1143 [24576/194182 (12%)]\tLoss: 0.294884\tGrad Norm: 1.394894\tLR: 0.030000\n",
      "Train Epoch: 1143 [45056/194182 (23%)]\tLoss: 0.288578\tGrad Norm: 1.205551\tLR: 0.030000\n",
      "Train Epoch: 1143 [65536/194182 (33%)]\tLoss: 0.296245\tGrad Norm: 1.541497\tLR: 0.030000\n",
      "Train Epoch: 1143 [86016/194182 (44%)]\tLoss: 0.290441\tGrad Norm: 1.419140\tLR: 0.030000\n",
      "Train Epoch: 1143 [106496/194182 (54%)]\tLoss: 0.291010\tGrad Norm: 1.405949\tLR: 0.030000\n",
      "Train Epoch: 1143 [126976/194182 (65%)]\tLoss: 0.295505\tGrad Norm: 1.530261\tLR: 0.030000\n",
      "Train Epoch: 1143 [147456/194182 (75%)]\tLoss: 0.294826\tGrad Norm: 1.402688\tLR: 0.030000\n",
      "Train Epoch: 1143 [167936/194182 (85%)]\tLoss: 0.292802\tGrad Norm: 1.277400\tLR: 0.030000\n",
      "Train Epoch: 1143 [188416/194182 (96%)]\tLoss: 0.294670\tGrad Norm: 1.231690\tLR: 0.030000\n",
      "Train set: Average loss: 0.2939\n",
      "Test set: Average loss: 0.2494, Average MAE: 0.3581\n",
      "Train Epoch: 1144 [4096/194182 (2%)]\tLoss: 0.299501\tGrad Norm: 1.718449\tLR: 0.030000\n",
      "Train Epoch: 1144 [24576/194182 (12%)]\tLoss: 0.290033\tGrad Norm: 1.447295\tLR: 0.030000\n",
      "Train Epoch: 1144 [45056/194182 (23%)]\tLoss: 0.294889\tGrad Norm: 1.766611\tLR: 0.030000\n",
      "Train Epoch: 1144 [65536/194182 (33%)]\tLoss: 0.299183\tGrad Norm: 1.666261\tLR: 0.030000\n",
      "Train Epoch: 1144 [86016/194182 (44%)]\tLoss: 0.295789\tGrad Norm: 1.283454\tLR: 0.030000\n",
      "Train Epoch: 1144 [106496/194182 (54%)]\tLoss: 0.289059\tGrad Norm: 1.004094\tLR: 0.030000\n",
      "Train Epoch: 1144 [126976/194182 (65%)]\tLoss: 0.280948\tGrad Norm: 0.953637\tLR: 0.030000\n",
      "Train Epoch: 1144 [147456/194182 (75%)]\tLoss: 0.298220\tGrad Norm: 1.865438\tLR: 0.030000\n",
      "Train Epoch: 1144 [167936/194182 (85%)]\tLoss: 0.299186\tGrad Norm: 1.674114\tLR: 0.030000\n",
      "Train Epoch: 1144 [188416/194182 (96%)]\tLoss: 0.308452\tGrad Norm: 1.768640\tLR: 0.030000\n",
      "Train set: Average loss: 0.2949\n",
      "Test set: Average loss: 0.2494, Average MAE: 0.3563\n",
      "Train Epoch: 1145 [4096/194182 (2%)]\tLoss: 0.291156\tGrad Norm: 1.572756\tLR: 0.030000\n",
      "Train Epoch: 1145 [24576/194182 (12%)]\tLoss: 0.295691\tGrad Norm: 1.628849\tLR: 0.030000\n",
      "Train Epoch: 1145 [45056/194182 (23%)]\tLoss: 0.292420\tGrad Norm: 1.436811\tLR: 0.030000\n",
      "Train Epoch: 1145 [65536/194182 (33%)]\tLoss: 0.299202\tGrad Norm: 1.638112\tLR: 0.030000\n",
      "Train Epoch: 1145 [86016/194182 (44%)]\tLoss: 0.294389\tGrad Norm: 1.153837\tLR: 0.030000\n",
      "Train Epoch: 1145 [106496/194182 (54%)]\tLoss: 0.286254\tGrad Norm: 1.179089\tLR: 0.030000\n",
      "Train Epoch: 1145 [126976/194182 (65%)]\tLoss: 0.294973\tGrad Norm: 1.525513\tLR: 0.030000\n",
      "Train Epoch: 1145 [147456/194182 (75%)]\tLoss: 0.293218\tGrad Norm: 1.349918\tLR: 0.030000\n",
      "Train Epoch: 1145 [167936/194182 (85%)]\tLoss: 0.304137\tGrad Norm: 1.487336\tLR: 0.030000\n",
      "Train Epoch: 1145 [188416/194182 (96%)]\tLoss: 0.302750\tGrad Norm: 1.987389\tLR: 0.030000\n",
      "Train set: Average loss: 0.2946\n",
      "Test set: Average loss: 0.2435, Average MAE: 0.3364\n",
      "Epoch 1145: Mean reward = 0.063 +/- 0.049\n",
      "Train Epoch: 1146 [4096/194182 (2%)]\tLoss: 0.295643\tGrad Norm: 1.429701\tLR: 0.030000\n",
      "Train Epoch: 1146 [24576/194182 (12%)]\tLoss: 0.292578\tGrad Norm: 1.186204\tLR: 0.030000\n",
      "Train Epoch: 1146 [45056/194182 (23%)]\tLoss: 0.283984\tGrad Norm: 1.180040\tLR: 0.030000\n",
      "Train Epoch: 1146 [65536/194182 (33%)]\tLoss: 0.302874\tGrad Norm: 1.652141\tLR: 0.030000\n",
      "Train Epoch: 1146 [86016/194182 (44%)]\tLoss: 0.297847\tGrad Norm: 1.927532\tLR: 0.030000\n",
      "Train Epoch: 1146 [106496/194182 (54%)]\tLoss: 0.290746\tGrad Norm: 1.478767\tLR: 0.030000\n",
      "Train Epoch: 1146 [126976/194182 (65%)]\tLoss: 0.300656\tGrad Norm: 1.772561\tLR: 0.030000\n",
      "Train Epoch: 1146 [147456/194182 (75%)]\tLoss: 0.289017\tGrad Norm: 1.288621\tLR: 0.030000\n",
      "Train Epoch: 1146 [167936/194182 (85%)]\tLoss: 0.291924\tGrad Norm: 1.182413\tLR: 0.030000\n",
      "Train Epoch: 1146 [188416/194182 (96%)]\tLoss: 0.287052\tGrad Norm: 1.274422\tLR: 0.030000\n",
      "Train set: Average loss: 0.2934\n",
      "Test set: Average loss: 0.2480, Average MAE: 0.3450\n",
      "Train Epoch: 1147 [4096/194182 (2%)]\tLoss: 0.297727\tGrad Norm: 1.644844\tLR: 0.030000\n",
      "Train Epoch: 1147 [24576/194182 (12%)]\tLoss: 0.288578\tGrad Norm: 1.499722\tLR: 0.030000\n",
      "Train Epoch: 1147 [45056/194182 (23%)]\tLoss: 0.287325\tGrad Norm: 1.288338\tLR: 0.030000\n",
      "Train Epoch: 1147 [65536/194182 (33%)]\tLoss: 0.295320\tGrad Norm: 1.426244\tLR: 0.030000\n",
      "Train Epoch: 1147 [86016/194182 (44%)]\tLoss: 0.283083\tGrad Norm: 1.076010\tLR: 0.030000\n",
      "Train Epoch: 1147 [106496/194182 (54%)]\tLoss: 0.295291\tGrad Norm: 1.549123\tLR: 0.030000\n",
      "Train Epoch: 1147 [126976/194182 (65%)]\tLoss: 0.301688\tGrad Norm: 1.905255\tLR: 0.030000\n",
      "Train Epoch: 1147 [147456/194182 (75%)]\tLoss: 0.288257\tGrad Norm: 1.056005\tLR: 0.030000\n",
      "Train Epoch: 1147 [167936/194182 (85%)]\tLoss: 0.299012\tGrad Norm: 1.524342\tLR: 0.030000\n",
      "Train Epoch: 1147 [188416/194182 (96%)]\tLoss: 0.285226\tGrad Norm: 1.086860\tLR: 0.030000\n",
      "Train set: Average loss: 0.2933\n",
      "Test set: Average loss: 0.2405, Average MAE: 0.3371\n",
      "Train Epoch: 1148 [4096/194182 (2%)]\tLoss: 0.285855\tGrad Norm: 1.108483\tLR: 0.030000\n",
      "Train Epoch: 1148 [24576/194182 (12%)]\tLoss: 0.300162\tGrad Norm: 1.379648\tLR: 0.030000\n",
      "Train Epoch: 1148 [45056/194182 (23%)]\tLoss: 0.302820\tGrad Norm: 1.696237\tLR: 0.030000\n",
      "Train Epoch: 1148 [65536/194182 (33%)]\tLoss: 0.292958\tGrad Norm: 1.259067\tLR: 0.030000\n",
      "Train Epoch: 1148 [86016/194182 (44%)]\tLoss: 0.298786\tGrad Norm: 1.163778\tLR: 0.030000\n",
      "Train Epoch: 1148 [106496/194182 (54%)]\tLoss: 0.291815\tGrad Norm: 1.341044\tLR: 0.030000\n",
      "Train Epoch: 1148 [126976/194182 (65%)]\tLoss: 0.294497\tGrad Norm: 1.748344\tLR: 0.030000\n",
      "Train Epoch: 1148 [147456/194182 (75%)]\tLoss: 0.292618\tGrad Norm: 1.544137\tLR: 0.030000\n",
      "Train Epoch: 1148 [167936/194182 (85%)]\tLoss: 0.290219\tGrad Norm: 1.370769\tLR: 0.030000\n",
      "Train Epoch: 1148 [188416/194182 (96%)]\tLoss: 0.288877\tGrad Norm: 1.590042\tLR: 0.030000\n",
      "Train set: Average loss: 0.2934\n",
      "Test set: Average loss: 0.2411, Average MAE: 0.3340\n",
      "Train Epoch: 1149 [4096/194182 (2%)]\tLoss: 0.291352\tGrad Norm: 1.273698\tLR: 0.030000\n",
      "Train Epoch: 1149 [24576/194182 (12%)]\tLoss: 0.293857\tGrad Norm: 1.283785\tLR: 0.030000\n",
      "Train Epoch: 1149 [45056/194182 (23%)]\tLoss: 0.304889\tGrad Norm: 1.621693\tLR: 0.030000\n",
      "Train Epoch: 1149 [65536/194182 (33%)]\tLoss: 0.290047\tGrad Norm: 1.579529\tLR: 0.030000\n",
      "Train Epoch: 1149 [86016/194182 (44%)]\tLoss: 0.289744\tGrad Norm: 1.466713\tLR: 0.030000\n",
      "Train Epoch: 1149 [106496/194182 (54%)]\tLoss: 0.296344\tGrad Norm: 1.567932\tLR: 0.030000\n",
      "Train Epoch: 1149 [126976/194182 (65%)]\tLoss: 0.295704\tGrad Norm: 1.466846\tLR: 0.030000\n",
      "Train Epoch: 1149 [147456/194182 (75%)]\tLoss: 0.301247\tGrad Norm: 1.455183\tLR: 0.030000\n",
      "Train Epoch: 1149 [167936/194182 (85%)]\tLoss: 0.294179\tGrad Norm: 1.440459\tLR: 0.030000\n",
      "Train Epoch: 1149 [188416/194182 (96%)]\tLoss: 0.295115\tGrad Norm: 1.066371\tLR: 0.030000\n",
      "Train set: Average loss: 0.2926\n",
      "Test set: Average loss: 0.2405, Average MAE: 0.3331\n",
      "Train Epoch: 1150 [4096/194182 (2%)]\tLoss: 0.285672\tGrad Norm: 1.188794\tLR: 0.030000\n",
      "Train Epoch: 1150 [24576/194182 (12%)]\tLoss: 0.288036\tGrad Norm: 1.114297\tLR: 0.030000\n",
      "Train Epoch: 1150 [45056/194182 (23%)]\tLoss: 0.293762\tGrad Norm: 1.684988\tLR: 0.030000\n",
      "Train Epoch: 1150 [65536/194182 (33%)]\tLoss: 0.298480\tGrad Norm: 1.641173\tLR: 0.030000\n",
      "Train Epoch: 1150 [86016/194182 (44%)]\tLoss: 0.285114\tGrad Norm: 1.487349\tLR: 0.030000\n",
      "Train Epoch: 1150 [106496/194182 (54%)]\tLoss: 0.295183\tGrad Norm: 1.635811\tLR: 0.030000\n",
      "Train Epoch: 1150 [126976/194182 (65%)]\tLoss: 0.293729\tGrad Norm: 1.550847\tLR: 0.030000\n",
      "Train Epoch: 1150 [147456/194182 (75%)]\tLoss: 0.298952\tGrad Norm: 1.531652\tLR: 0.030000\n",
      "Train Epoch: 1150 [167936/194182 (85%)]\tLoss: 0.295187\tGrad Norm: 1.450354\tLR: 0.030000\n",
      "Train Epoch: 1150 [188416/194182 (96%)]\tLoss: 0.291025\tGrad Norm: 1.288157\tLR: 0.030000\n",
      "Train set: Average loss: 0.2935\n",
      "Test set: Average loss: 0.2500, Average MAE: 0.3559\n",
      "Epoch 1150: Mean reward = 0.046 +/- 0.001\n",
      "Train Epoch: 1151 [4096/194182 (2%)]\tLoss: 0.295170\tGrad Norm: 1.697810\tLR: 0.030000\n",
      "Train Epoch: 1151 [24576/194182 (12%)]\tLoss: 0.301006\tGrad Norm: 1.786153\tLR: 0.030000\n",
      "Train Epoch: 1151 [45056/194182 (23%)]\tLoss: 0.288232\tGrad Norm: 1.273176\tLR: 0.030000\n",
      "Train Epoch: 1151 [65536/194182 (33%)]\tLoss: 0.287016\tGrad Norm: 1.042173\tLR: 0.030000\n",
      "Train Epoch: 1151 [86016/194182 (44%)]\tLoss: 0.290877\tGrad Norm: 1.434449\tLR: 0.030000\n",
      "Train Epoch: 1151 [106496/194182 (54%)]\tLoss: 0.293993\tGrad Norm: 1.283283\tLR: 0.030000\n",
      "Train Epoch: 1151 [126976/194182 (65%)]\tLoss: 0.287327\tGrad Norm: 1.466833\tLR: 0.030000\n",
      "Train Epoch: 1151 [147456/194182 (75%)]\tLoss: 0.303868\tGrad Norm: 1.855591\tLR: 0.030000\n",
      "Train Epoch: 1151 [167936/194182 (85%)]\tLoss: 0.289239\tGrad Norm: 1.312325\tLR: 0.030000\n",
      "Train Epoch: 1151 [188416/194182 (96%)]\tLoss: 0.287954\tGrad Norm: 1.364160\tLR: 0.030000\n",
      "Train set: Average loss: 0.2925\n",
      "Test set: Average loss: 0.2488, Average MAE: 0.3550\n",
      "Train Epoch: 1152 [4096/194182 (2%)]\tLoss: 0.297743\tGrad Norm: 1.653840\tLR: 0.030000\n",
      "Train Epoch: 1152 [24576/194182 (12%)]\tLoss: 0.299857\tGrad Norm: 1.948458\tLR: 0.030000\n",
      "Train Epoch: 1152 [45056/194182 (23%)]\tLoss: 0.297533\tGrad Norm: 1.746902\tLR: 0.030000\n",
      "Train Epoch: 1152 [65536/194182 (33%)]\tLoss: 0.294066\tGrad Norm: 1.485788\tLR: 0.030000\n",
      "Train Epoch: 1152 [86016/194182 (44%)]\tLoss: 0.298033\tGrad Norm: 1.546641\tLR: 0.030000\n",
      "Train Epoch: 1152 [106496/194182 (54%)]\tLoss: 0.288352\tGrad Norm: 1.259361\tLR: 0.030000\n",
      "Train Epoch: 1152 [126976/194182 (65%)]\tLoss: 0.293774\tGrad Norm: 1.414732\tLR: 0.030000\n",
      "Train Epoch: 1152 [147456/194182 (75%)]\tLoss: 0.293817\tGrad Norm: 1.519036\tLR: 0.030000\n",
      "Train Epoch: 1152 [167936/194182 (85%)]\tLoss: 0.296546\tGrad Norm: 1.405442\tLR: 0.030000\n",
      "Train Epoch: 1152 [188416/194182 (96%)]\tLoss: 0.289404\tGrad Norm: 1.053906\tLR: 0.030000\n",
      "Train set: Average loss: 0.2944\n",
      "Test set: Average loss: 0.2391, Average MAE: 0.3378\n",
      "Train Epoch: 1153 [4096/194182 (2%)]\tLoss: 0.286092\tGrad Norm: 0.924470\tLR: 0.030000\n",
      "Train Epoch: 1153 [24576/194182 (12%)]\tLoss: 0.295551\tGrad Norm: 1.552124\tLR: 0.030000\n",
      "Train Epoch: 1153 [45056/194182 (23%)]\tLoss: 0.291736\tGrad Norm: 1.485944\tLR: 0.030000\n",
      "Train Epoch: 1153 [65536/194182 (33%)]\tLoss: 0.292086\tGrad Norm: 1.586013\tLR: 0.030000\n",
      "Train Epoch: 1153 [86016/194182 (44%)]\tLoss: 0.281606\tGrad Norm: 1.045970\tLR: 0.030000\n",
      "Train Epoch: 1153 [106496/194182 (54%)]\tLoss: 0.280135\tGrad Norm: 0.947898\tLR: 0.030000\n",
      "Train Epoch: 1153 [126976/194182 (65%)]\tLoss: 0.288338\tGrad Norm: 1.263397\tLR: 0.030000\n",
      "Train Epoch: 1153 [147456/194182 (75%)]\tLoss: 0.293790\tGrad Norm: 1.415809\tLR: 0.030000\n",
      "Train Epoch: 1153 [167936/194182 (85%)]\tLoss: 0.288094\tGrad Norm: 1.620917\tLR: 0.030000\n",
      "Train Epoch: 1153 [188416/194182 (96%)]\tLoss: 0.302314\tGrad Norm: 2.004896\tLR: 0.030000\n",
      "Train set: Average loss: 0.2912\n",
      "Test set: Average loss: 0.2460, Average MAE: 0.3523\n",
      "Train Epoch: 1154 [4096/194182 (2%)]\tLoss: 0.290869\tGrad Norm: 1.448140\tLR: 0.030000\n",
      "Train Epoch: 1154 [24576/194182 (12%)]\tLoss: 0.293686\tGrad Norm: 1.458910\tLR: 0.030000\n",
      "Train Epoch: 1154 [45056/194182 (23%)]\tLoss: 0.297384\tGrad Norm: 1.487663\tLR: 0.030000\n",
      "Train Epoch: 1154 [65536/194182 (33%)]\tLoss: 0.293866\tGrad Norm: 1.485951\tLR: 0.030000\n",
      "Train Epoch: 1154 [86016/194182 (44%)]\tLoss: 0.296952\tGrad Norm: 1.627084\tLR: 0.030000\n",
      "Train Epoch: 1154 [106496/194182 (54%)]\tLoss: 0.295694\tGrad Norm: 1.335281\tLR: 0.030000\n",
      "Train Epoch: 1154 [126976/194182 (65%)]\tLoss: 0.289850\tGrad Norm: 1.175590\tLR: 0.030000\n",
      "Train Epoch: 1154 [147456/194182 (75%)]\tLoss: 0.280739\tGrad Norm: 1.090220\tLR: 0.030000\n",
      "Train Epoch: 1154 [167936/194182 (85%)]\tLoss: 0.290909\tGrad Norm: 1.374720\tLR: 0.030000\n",
      "Train Epoch: 1154 [188416/194182 (96%)]\tLoss: 0.298458\tGrad Norm: 1.375991\tLR: 0.030000\n",
      "Train set: Average loss: 0.2918\n",
      "Test set: Average loss: 0.2472, Average MAE: 0.3513\n",
      "Train Epoch: 1155 [4096/194182 (2%)]\tLoss: 0.287568\tGrad Norm: 1.510985\tLR: 0.030000\n",
      "Train Epoch: 1155 [24576/194182 (12%)]\tLoss: 0.296875\tGrad Norm: 1.552540\tLR: 0.030000\n",
      "Train Epoch: 1155 [45056/194182 (23%)]\tLoss: 0.288079\tGrad Norm: 1.187613\tLR: 0.030000\n",
      "Train Epoch: 1155 [65536/194182 (33%)]\tLoss: 0.286231\tGrad Norm: 1.048557\tLR: 0.030000\n",
      "Train Epoch: 1155 [86016/194182 (44%)]\tLoss: 0.287006\tGrad Norm: 1.000916\tLR: 0.030000\n",
      "Train Epoch: 1155 [106496/194182 (54%)]\tLoss: 0.298944\tGrad Norm: 1.679885\tLR: 0.030000\n",
      "Train Epoch: 1155 [126976/194182 (65%)]\tLoss: 0.285727\tGrad Norm: 1.196157\tLR: 0.030000\n",
      "Train Epoch: 1155 [147456/194182 (75%)]\tLoss: 0.301443\tGrad Norm: 1.533778\tLR: 0.030000\n",
      "Train Epoch: 1155 [167936/194182 (85%)]\tLoss: 0.293014\tGrad Norm: 1.619378\tLR: 0.030000\n",
      "Train Epoch: 1155 [188416/194182 (96%)]\tLoss: 0.285388\tGrad Norm: 1.180438\tLR: 0.030000\n",
      "Train set: Average loss: 0.2903\n",
      "Test set: Average loss: 0.2432, Average MAE: 0.3412\n",
      "Epoch 1155: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 1156 [4096/194182 (2%)]\tLoss: 0.292342\tGrad Norm: 1.366345\tLR: 0.030000\n",
      "Train Epoch: 1156 [24576/194182 (12%)]\tLoss: 0.289464\tGrad Norm: 1.638502\tLR: 0.030000\n",
      "Train Epoch: 1156 [45056/194182 (23%)]\tLoss: 0.296022\tGrad Norm: 1.516960\tLR: 0.030000\n",
      "Train Epoch: 1156 [65536/194182 (33%)]\tLoss: 0.294081\tGrad Norm: 1.580047\tLR: 0.030000\n",
      "Train Epoch: 1156 [86016/194182 (44%)]\tLoss: 0.291909\tGrad Norm: 1.474066\tLR: 0.030000\n",
      "Train Epoch: 1156 [106496/194182 (54%)]\tLoss: 0.295566\tGrad Norm: 1.227409\tLR: 0.030000\n",
      "Train Epoch: 1156 [126976/194182 (65%)]\tLoss: 0.287021\tGrad Norm: 1.453374\tLR: 0.030000\n",
      "Train Epoch: 1156 [147456/194182 (75%)]\tLoss: 0.291400\tGrad Norm: 1.361700\tLR: 0.030000\n",
      "Train Epoch: 1156 [167936/194182 (85%)]\tLoss: 0.290656\tGrad Norm: 1.467140\tLR: 0.030000\n",
      "Train Epoch: 1156 [188416/194182 (96%)]\tLoss: 0.288229\tGrad Norm: 1.616283\tLR: 0.030000\n",
      "Train set: Average loss: 0.2918\n",
      "Test set: Average loss: 0.2483, Average MAE: 0.3424\n",
      "Train Epoch: 1157 [4096/194182 (2%)]\tLoss: 0.292935\tGrad Norm: 1.532237\tLR: 0.030000\n",
      "Train Epoch: 1157 [24576/194182 (12%)]\tLoss: 0.304580\tGrad Norm: 1.961790\tLR: 0.030000\n",
      "Train Epoch: 1157 [45056/194182 (23%)]\tLoss: 0.296573\tGrad Norm: 1.558067\tLR: 0.030000\n",
      "Train Epoch: 1157 [65536/194182 (33%)]\tLoss: 0.298983\tGrad Norm: 1.711925\tLR: 0.030000\n",
      "Train Epoch: 1157 [86016/194182 (44%)]\tLoss: 0.295537\tGrad Norm: 1.496983\tLR: 0.030000\n",
      "Train Epoch: 1157 [106496/194182 (54%)]\tLoss: 0.292489\tGrad Norm: 1.506178\tLR: 0.030000\n",
      "Train Epoch: 1157 [126976/194182 (65%)]\tLoss: 0.295100\tGrad Norm: 1.728024\tLR: 0.030000\n",
      "Train Epoch: 1157 [147456/194182 (75%)]\tLoss: 0.293981\tGrad Norm: 1.758123\tLR: 0.030000\n",
      "Train Epoch: 1157 [167936/194182 (85%)]\tLoss: 0.297498\tGrad Norm: 1.701554\tLR: 0.030000\n",
      "Train Epoch: 1157 [188416/194182 (96%)]\tLoss: 0.290604\tGrad Norm: 1.711798\tLR: 0.030000\n",
      "Train set: Average loss: 0.2940\n",
      "Test set: Average loss: 0.2454, Average MAE: 0.3347\n",
      "Train Epoch: 1158 [4096/194182 (2%)]\tLoss: 0.288008\tGrad Norm: 1.471212\tLR: 0.030000\n",
      "Train Epoch: 1158 [24576/194182 (12%)]\tLoss: 0.289850\tGrad Norm: 1.305436\tLR: 0.030000\n",
      "Train Epoch: 1158 [45056/194182 (23%)]\tLoss: 0.292287\tGrad Norm: 1.444848\tLR: 0.030000\n",
      "Train Epoch: 1158 [65536/194182 (33%)]\tLoss: 0.289571\tGrad Norm: 1.589479\tLR: 0.030000\n",
      "Train Epoch: 1158 [86016/194182 (44%)]\tLoss: 0.283261\tGrad Norm: 1.075186\tLR: 0.030000\n",
      "Train Epoch: 1158 [106496/194182 (54%)]\tLoss: 0.286756\tGrad Norm: 1.312604\tLR: 0.030000\n",
      "Train Epoch: 1158 [126976/194182 (65%)]\tLoss: 0.292991\tGrad Norm: 1.164672\tLR: 0.030000\n",
      "Train Epoch: 1158 [147456/194182 (75%)]\tLoss: 0.288953\tGrad Norm: 1.413125\tLR: 0.030000\n",
      "Train Epoch: 1158 [167936/194182 (85%)]\tLoss: 0.295252\tGrad Norm: 1.867905\tLR: 0.030000\n",
      "Train Epoch: 1158 [188416/194182 (96%)]\tLoss: 0.297993\tGrad Norm: 1.535491\tLR: 0.030000\n",
      "Train set: Average loss: 0.2908\n",
      "Test set: Average loss: 0.2501, Average MAE: 0.3345\n",
      "Train Epoch: 1159 [4096/194182 (2%)]\tLoss: 0.299123\tGrad Norm: 2.086230\tLR: 0.030000\n",
      "Train Epoch: 1159 [24576/194182 (12%)]\tLoss: 0.289535\tGrad Norm: 1.366831\tLR: 0.030000\n",
      "Train Epoch: 1159 [45056/194182 (23%)]\tLoss: 0.282181\tGrad Norm: 1.309115\tLR: 0.030000\n",
      "Train Epoch: 1159 [65536/194182 (33%)]\tLoss: 0.286146\tGrad Norm: 1.043939\tLR: 0.030000\n",
      "Train Epoch: 1159 [86016/194182 (44%)]\tLoss: 0.306739\tGrad Norm: 1.857884\tLR: 0.030000\n",
      "Train Epoch: 1159 [106496/194182 (54%)]\tLoss: 0.308112\tGrad Norm: 1.839920\tLR: 0.030000\n",
      "Train Epoch: 1159 [126976/194182 (65%)]\tLoss: 0.295798\tGrad Norm: 1.496144\tLR: 0.030000\n",
      "Train Epoch: 1159 [147456/194182 (75%)]\tLoss: 0.302142\tGrad Norm: 1.792512\tLR: 0.030000\n",
      "Train Epoch: 1159 [167936/194182 (85%)]\tLoss: 0.290552\tGrad Norm: 1.366979\tLR: 0.030000\n",
      "Train Epoch: 1159 [188416/194182 (96%)]\tLoss: 0.287252\tGrad Norm: 1.084159\tLR: 0.030000\n",
      "Train set: Average loss: 0.2928\n",
      "Test set: Average loss: 0.2420, Average MAE: 0.3342\n",
      "Train Epoch: 1160 [4096/194182 (2%)]\tLoss: 0.294094\tGrad Norm: 1.596224\tLR: 0.030000\n",
      "Train Epoch: 1160 [24576/194182 (12%)]\tLoss: 0.280731\tGrad Norm: 1.116918\tLR: 0.030000\n",
      "Train Epoch: 1160 [45056/194182 (23%)]\tLoss: 0.284917\tGrad Norm: 1.464063\tLR: 0.030000\n",
      "Train Epoch: 1160 [65536/194182 (33%)]\tLoss: 0.289625\tGrad Norm: 1.390798\tLR: 0.030000\n",
      "Train Epoch: 1160 [86016/194182 (44%)]\tLoss: 0.296930\tGrad Norm: 1.657493\tLR: 0.030000\n",
      "Train Epoch: 1160 [106496/194182 (54%)]\tLoss: 0.283014\tGrad Norm: 1.541138\tLR: 0.030000\n",
      "Train Epoch: 1160 [126976/194182 (65%)]\tLoss: 0.290044\tGrad Norm: 1.356723\tLR: 0.030000\n",
      "Train Epoch: 1160 [147456/194182 (75%)]\tLoss: 0.287657\tGrad Norm: 1.374544\tLR: 0.030000\n",
      "Train Epoch: 1160 [167936/194182 (85%)]\tLoss: 0.299159\tGrad Norm: 1.987993\tLR: 0.030000\n",
      "Train Epoch: 1160 [188416/194182 (96%)]\tLoss: 0.297282\tGrad Norm: 1.891677\tLR: 0.030000\n",
      "Train set: Average loss: 0.2914\n",
      "Test set: Average loss: 0.2494, Average MAE: 0.3606\n",
      "Epoch 1160: Mean reward = 0.042 +/- 0.014\n",
      "Train Epoch: 1161 [4096/194182 (2%)]\tLoss: 0.296018\tGrad Norm: 1.802545\tLR: 0.030000\n",
      "Train Epoch: 1161 [24576/194182 (12%)]\tLoss: 0.294530\tGrad Norm: 1.405121\tLR: 0.030000\n",
      "Train Epoch: 1161 [45056/194182 (23%)]\tLoss: 0.285462\tGrad Norm: 1.207157\tLR: 0.030000\n",
      "Train Epoch: 1161 [65536/194182 (33%)]\tLoss: 0.279524\tGrad Norm: 1.022438\tLR: 0.030000\n",
      "Train Epoch: 1161 [86016/194182 (44%)]\tLoss: 0.287173\tGrad Norm: 1.208891\tLR: 0.030000\n",
      "Train Epoch: 1161 [106496/194182 (54%)]\tLoss: 0.291997\tGrad Norm: 1.289854\tLR: 0.030000\n",
      "Train Epoch: 1161 [126976/194182 (65%)]\tLoss: 0.285651\tGrad Norm: 1.457205\tLR: 0.030000\n",
      "Train Epoch: 1161 [147456/194182 (75%)]\tLoss: 0.292316\tGrad Norm: 1.465533\tLR: 0.030000\n",
      "Train Epoch: 1161 [167936/194182 (85%)]\tLoss: 0.288321\tGrad Norm: 1.119868\tLR: 0.030000\n",
      "Train Epoch: 1161 [188416/194182 (96%)]\tLoss: 0.292686\tGrad Norm: 1.256020\tLR: 0.030000\n",
      "Train set: Average loss: 0.2884\n",
      "Test set: Average loss: 0.2480, Average MAE: 0.3420\n",
      "Train Epoch: 1162 [4096/194182 (2%)]\tLoss: 0.291169\tGrad Norm: 1.663493\tLR: 0.030000\n",
      "Train Epoch: 1162 [24576/194182 (12%)]\tLoss: 0.283669\tGrad Norm: 1.398098\tLR: 0.030000\n",
      "Train Epoch: 1162 [45056/194182 (23%)]\tLoss: 0.285060\tGrad Norm: 1.427381\tLR: 0.030000\n",
      "Train Epoch: 1162 [65536/194182 (33%)]\tLoss: 0.286218\tGrad Norm: 1.189773\tLR: 0.030000\n",
      "Train Epoch: 1162 [86016/194182 (44%)]\tLoss: 0.291690\tGrad Norm: 1.572361\tLR: 0.030000\n",
      "Train Epoch: 1162 [106496/194182 (54%)]\tLoss: 0.298383\tGrad Norm: 1.690590\tLR: 0.030000\n",
      "Train Epoch: 1162 [126976/194182 (65%)]\tLoss: 0.293424\tGrad Norm: 1.347459\tLR: 0.030000\n",
      "Train Epoch: 1162 [147456/194182 (75%)]\tLoss: 0.283098\tGrad Norm: 1.425873\tLR: 0.030000\n",
      "Train Epoch: 1162 [167936/194182 (85%)]\tLoss: 0.297618\tGrad Norm: 1.671646\tLR: 0.030000\n",
      "Train Epoch: 1162 [188416/194182 (96%)]\tLoss: 0.297736\tGrad Norm: 1.869194\tLR: 0.030000\n",
      "Train set: Average loss: 0.2906\n",
      "Test set: Average loss: 0.2483, Average MAE: 0.3319\n",
      "Train Epoch: 1163 [4096/194182 (2%)]\tLoss: 0.301452\tGrad Norm: 1.849433\tLR: 0.030000\n",
      "Train Epoch: 1163 [24576/194182 (12%)]\tLoss: 0.296385\tGrad Norm: 1.780164\tLR: 0.030000\n",
      "Train Epoch: 1163 [45056/194182 (23%)]\tLoss: 0.285484\tGrad Norm: 1.427296\tLR: 0.030000\n",
      "Train Epoch: 1163 [65536/194182 (33%)]\tLoss: 0.288808\tGrad Norm: 1.307944\tLR: 0.030000\n",
      "Train Epoch: 1163 [86016/194182 (44%)]\tLoss: 0.287675\tGrad Norm: 1.393484\tLR: 0.030000\n",
      "Train Epoch: 1163 [106496/194182 (54%)]\tLoss: 0.288746\tGrad Norm: 1.695705\tLR: 0.030000\n",
      "Train Epoch: 1163 [126976/194182 (65%)]\tLoss: 0.295915\tGrad Norm: 1.624047\tLR: 0.030000\n",
      "Train Epoch: 1163 [147456/194182 (75%)]\tLoss: 0.294603\tGrad Norm: 1.548139\tLR: 0.030000\n",
      "Train Epoch: 1163 [167936/194182 (85%)]\tLoss: 0.301993\tGrad Norm: 1.774158\tLR: 0.030000\n",
      "Train Epoch: 1163 [188416/194182 (96%)]\tLoss: 0.289849\tGrad Norm: 1.124394\tLR: 0.030000\n",
      "Train set: Average loss: 0.2923\n",
      "Test set: Average loss: 0.2429, Average MAE: 0.3327\n",
      "Train Epoch: 1164 [4096/194182 (2%)]\tLoss: 0.291524\tGrad Norm: 1.413579\tLR: 0.030000\n",
      "Train Epoch: 1164 [24576/194182 (12%)]\tLoss: 0.288972\tGrad Norm: 1.462655\tLR: 0.030000\n",
      "Train Epoch: 1164 [45056/194182 (23%)]\tLoss: 0.294005\tGrad Norm: 1.596577\tLR: 0.030000\n",
      "Train Epoch: 1164 [65536/194182 (33%)]\tLoss: 0.289008\tGrad Norm: 1.280414\tLR: 0.030000\n",
      "Train Epoch: 1164 [86016/194182 (44%)]\tLoss: 0.291018\tGrad Norm: 1.645268\tLR: 0.030000\n",
      "Train Epoch: 1164 [106496/194182 (54%)]\tLoss: 0.293549\tGrad Norm: 1.455937\tLR: 0.030000\n",
      "Train Epoch: 1164 [126976/194182 (65%)]\tLoss: 0.288120\tGrad Norm: 1.402899\tLR: 0.030000\n",
      "Train Epoch: 1164 [147456/194182 (75%)]\tLoss: 0.298407\tGrad Norm: 1.676767\tLR: 0.030000\n",
      "Train Epoch: 1164 [167936/194182 (85%)]\tLoss: 0.292219\tGrad Norm: 1.491083\tLR: 0.030000\n",
      "Train Epoch: 1164 [188416/194182 (96%)]\tLoss: 0.290378\tGrad Norm: 1.430756\tLR: 0.030000\n",
      "Train set: Average loss: 0.2909\n",
      "Test set: Average loss: 0.2517, Average MAE: 0.3584\n",
      "Train Epoch: 1165 [4096/194182 (2%)]\tLoss: 0.296247\tGrad Norm: 1.835006\tLR: 0.030000\n",
      "Train Epoch: 1165 [24576/194182 (12%)]\tLoss: 0.290939\tGrad Norm: 1.506141\tLR: 0.030000\n",
      "Train Epoch: 1165 [45056/194182 (23%)]\tLoss: 0.292166\tGrad Norm: 1.581253\tLR: 0.030000\n",
      "Train Epoch: 1165 [65536/194182 (33%)]\tLoss: 0.293230\tGrad Norm: 1.589469\tLR: 0.030000\n",
      "Train Epoch: 1165 [86016/194182 (44%)]\tLoss: 0.291204\tGrad Norm: 1.467422\tLR: 0.030000\n",
      "Train Epoch: 1165 [106496/194182 (54%)]\tLoss: 0.287713\tGrad Norm: 1.370070\tLR: 0.030000\n",
      "Train Epoch: 1165 [126976/194182 (65%)]\tLoss: 0.289211\tGrad Norm: 1.289671\tLR: 0.030000\n",
      "Train Epoch: 1165 [147456/194182 (75%)]\tLoss: 0.279660\tGrad Norm: 1.337568\tLR: 0.030000\n",
      "Train Epoch: 1165 [167936/194182 (85%)]\tLoss: 0.283988\tGrad Norm: 1.335724\tLR: 0.030000\n",
      "Train Epoch: 1165 [188416/194182 (96%)]\tLoss: 0.280291\tGrad Norm: 1.308455\tLR: 0.030000\n",
      "Train set: Average loss: 0.2904\n",
      "Test set: Average loss: 0.2468, Average MAE: 0.3341\n",
      "Epoch 1165: Mean reward = 0.055 +/- 0.057\n",
      "Train Epoch: 1166 [4096/194182 (2%)]\tLoss: 0.295873\tGrad Norm: 1.602952\tLR: 0.030000\n",
      "Train Epoch: 1166 [24576/194182 (12%)]\tLoss: 0.293610\tGrad Norm: 1.761129\tLR: 0.030000\n",
      "Train Epoch: 1166 [45056/194182 (23%)]\tLoss: 0.282865\tGrad Norm: 1.471218\tLR: 0.030000\n",
      "Train Epoch: 1166 [65536/194182 (33%)]\tLoss: 0.286473\tGrad Norm: 1.389783\tLR: 0.030000\n",
      "Train Epoch: 1166 [86016/194182 (44%)]\tLoss: 0.292542\tGrad Norm: 1.417313\tLR: 0.030000\n",
      "Train Epoch: 1166 [106496/194182 (54%)]\tLoss: 0.285781\tGrad Norm: 1.191647\tLR: 0.030000\n",
      "Train Epoch: 1166 [126976/194182 (65%)]\tLoss: 0.283851\tGrad Norm: 1.248870\tLR: 0.030000\n",
      "Train Epoch: 1166 [147456/194182 (75%)]\tLoss: 0.289405\tGrad Norm: 1.499472\tLR: 0.030000\n",
      "Train Epoch: 1166 [167936/194182 (85%)]\tLoss: 0.290326\tGrad Norm: 1.385849\tLR: 0.030000\n",
      "Train Epoch: 1166 [188416/194182 (96%)]\tLoss: 0.288806\tGrad Norm: 1.250614\tLR: 0.030000\n",
      "Train set: Average loss: 0.2894\n",
      "Test set: Average loss: 0.2418, Average MAE: 0.3419\n",
      "Train Epoch: 1167 [4096/194182 (2%)]\tLoss: 0.284152\tGrad Norm: 1.198299\tLR: 0.030000\n",
      "Train Epoch: 1167 [24576/194182 (12%)]\tLoss: 0.290995\tGrad Norm: 1.640624\tLR: 0.030000\n",
      "Train Epoch: 1167 [45056/194182 (23%)]\tLoss: 0.290493\tGrad Norm: 1.558140\tLR: 0.030000\n",
      "Train Epoch: 1167 [65536/194182 (33%)]\tLoss: 0.283320\tGrad Norm: 1.050024\tLR: 0.030000\n",
      "Train Epoch: 1167 [86016/194182 (44%)]\tLoss: 0.283246\tGrad Norm: 1.147307\tLR: 0.030000\n",
      "Train Epoch: 1167 [106496/194182 (54%)]\tLoss: 0.302888\tGrad Norm: 1.919003\tLR: 0.030000\n",
      "Train Epoch: 1167 [126976/194182 (65%)]\tLoss: 0.286341\tGrad Norm: 1.440811\tLR: 0.030000\n",
      "Train Epoch: 1167 [147456/194182 (75%)]\tLoss: 0.286995\tGrad Norm: 1.492686\tLR: 0.030000\n",
      "Train Epoch: 1167 [167936/194182 (85%)]\tLoss: 0.295640\tGrad Norm: 1.884122\tLR: 0.030000\n",
      "Train Epoch: 1167 [188416/194182 (96%)]\tLoss: 0.297539\tGrad Norm: 1.803685\tLR: 0.030000\n",
      "Train set: Average loss: 0.2910\n",
      "Test set: Average loss: 0.2458, Average MAE: 0.3330\n",
      "Train Epoch: 1168 [4096/194182 (2%)]\tLoss: 0.295812\tGrad Norm: 1.713640\tLR: 0.030000\n",
      "Train Epoch: 1168 [24576/194182 (12%)]\tLoss: 0.288334\tGrad Norm: 1.585512\tLR: 0.030000\n",
      "Train Epoch: 1168 [45056/194182 (23%)]\tLoss: 0.287478\tGrad Norm: 1.083340\tLR: 0.030000\n",
      "Train Epoch: 1168 [65536/194182 (33%)]\tLoss: 0.290632\tGrad Norm: 1.208355\tLR: 0.030000\n",
      "Train Epoch: 1168 [86016/194182 (44%)]\tLoss: 0.289920\tGrad Norm: 1.568108\tLR: 0.030000\n",
      "Train Epoch: 1168 [106496/194182 (54%)]\tLoss: 0.293979\tGrad Norm: 1.627864\tLR: 0.030000\n",
      "Train Epoch: 1168 [126976/194182 (65%)]\tLoss: 0.289753\tGrad Norm: 1.534576\tLR: 0.030000\n",
      "Train Epoch: 1168 [147456/194182 (75%)]\tLoss: 0.288624\tGrad Norm: 1.145067\tLR: 0.030000\n",
      "Train Epoch: 1168 [167936/194182 (85%)]\tLoss: 0.285724\tGrad Norm: 1.518565\tLR: 0.030000\n",
      "Train Epoch: 1168 [188416/194182 (96%)]\tLoss: 0.287716\tGrad Norm: 1.205298\tLR: 0.030000\n",
      "Train set: Average loss: 0.2889\n",
      "Test set: Average loss: 0.2453, Average MAE: 0.3546\n",
      "Train Epoch: 1169 [4096/194182 (2%)]\tLoss: 0.292486\tGrad Norm: 1.546194\tLR: 0.030000\n",
      "Train Epoch: 1169 [24576/194182 (12%)]\tLoss: 0.283902\tGrad Norm: 1.363604\tLR: 0.030000\n",
      "Train Epoch: 1169 [45056/194182 (23%)]\tLoss: 0.285996\tGrad Norm: 1.380933\tLR: 0.030000\n",
      "Train Epoch: 1169 [65536/194182 (33%)]\tLoss: 0.301776\tGrad Norm: 2.116831\tLR: 0.030000\n",
      "Train Epoch: 1169 [86016/194182 (44%)]\tLoss: 0.295622\tGrad Norm: 1.864740\tLR: 0.030000\n",
      "Train Epoch: 1169 [106496/194182 (54%)]\tLoss: 0.283336\tGrad Norm: 1.489290\tLR: 0.030000\n",
      "Train Epoch: 1169 [126976/194182 (65%)]\tLoss: 0.290670\tGrad Norm: 1.398916\tLR: 0.030000\n",
      "Train Epoch: 1169 [147456/194182 (75%)]\tLoss: 0.289958\tGrad Norm: 1.324338\tLR: 0.030000\n",
      "Train Epoch: 1169 [167936/194182 (85%)]\tLoss: 0.298287\tGrad Norm: 1.676078\tLR: 0.030000\n",
      "Train Epoch: 1169 [188416/194182 (96%)]\tLoss: 0.285266\tGrad Norm: 1.556522\tLR: 0.030000\n",
      "Train set: Average loss: 0.2914\n",
      "Test set: Average loss: 0.2444, Average MAE: 0.3468\n",
      "Train Epoch: 1170 [4096/194182 (2%)]\tLoss: 0.289176\tGrad Norm: 1.385035\tLR: 0.030000\n",
      "Train Epoch: 1170 [24576/194182 (12%)]\tLoss: 0.287600\tGrad Norm: 1.218810\tLR: 0.030000\n",
      "Train Epoch: 1170 [45056/194182 (23%)]\tLoss: 0.281787\tGrad Norm: 1.459799\tLR: 0.030000\n",
      "Train Epoch: 1170 [65536/194182 (33%)]\tLoss: 0.286212\tGrad Norm: 1.161383\tLR: 0.030000\n",
      "Train Epoch: 1170 [86016/194182 (44%)]\tLoss: 0.285575\tGrad Norm: 1.365103\tLR: 0.030000\n",
      "Train Epoch: 1170 [106496/194182 (54%)]\tLoss: 0.286825\tGrad Norm: 1.346449\tLR: 0.030000\n",
      "Train Epoch: 1170 [126976/194182 (65%)]\tLoss: 0.283623\tGrad Norm: 1.322123\tLR: 0.030000\n",
      "Train Epoch: 1170 [147456/194182 (75%)]\tLoss: 0.298618\tGrad Norm: 1.843042\tLR: 0.030000\n",
      "Train Epoch: 1170 [167936/194182 (85%)]\tLoss: 0.284584\tGrad Norm: 1.188104\tLR: 0.030000\n",
      "Train Epoch: 1170 [188416/194182 (96%)]\tLoss: 0.285998\tGrad Norm: 1.617057\tLR: 0.030000\n",
      "Train set: Average loss: 0.2884\n",
      "Test set: Average loss: 0.2466, Average MAE: 0.3342\n",
      "Epoch 1170: Mean reward = 0.045 +/- 0.034\n",
      "Train Epoch: 1171 [4096/194182 (2%)]\tLoss: 0.283259\tGrad Norm: 1.593691\tLR: 0.030000\n",
      "Train Epoch: 1171 [24576/194182 (12%)]\tLoss: 0.289618\tGrad Norm: 1.534524\tLR: 0.030000\n",
      "Train Epoch: 1171 [45056/194182 (23%)]\tLoss: 0.291437\tGrad Norm: 1.702152\tLR: 0.030000\n",
      "Train Epoch: 1171 [65536/194182 (33%)]\tLoss: 0.296438\tGrad Norm: 1.480982\tLR: 0.030000\n",
      "Train Epoch: 1171 [86016/194182 (44%)]\tLoss: 0.286523\tGrad Norm: 1.391497\tLR: 0.030000\n",
      "Train Epoch: 1171 [106496/194182 (54%)]\tLoss: 0.296528\tGrad Norm: 1.682357\tLR: 0.030000\n",
      "Train Epoch: 1171 [126976/194182 (65%)]\tLoss: 0.285998\tGrad Norm: 1.394174\tLR: 0.030000\n",
      "Train Epoch: 1171 [147456/194182 (75%)]\tLoss: 0.287816\tGrad Norm: 1.208436\tLR: 0.030000\n",
      "Train Epoch: 1171 [167936/194182 (85%)]\tLoss: 0.290314\tGrad Norm: 1.705363\tLR: 0.030000\n",
      "Train Epoch: 1171 [188416/194182 (96%)]\tLoss: 0.288988\tGrad Norm: 1.630030\tLR: 0.030000\n",
      "Train set: Average loss: 0.2898\n",
      "Test set: Average loss: 0.2452, Average MAE: 0.3393\n",
      "Train Epoch: 1172 [4096/194182 (2%)]\tLoss: 0.288999\tGrad Norm: 1.388899\tLR: 0.030000\n",
      "Train Epoch: 1172 [24576/194182 (12%)]\tLoss: 0.294361\tGrad Norm: 1.418560\tLR: 0.030000\n",
      "Train Epoch: 1172 [45056/194182 (23%)]\tLoss: 0.292373\tGrad Norm: 1.669000\tLR: 0.030000\n",
      "Train Epoch: 1172 [65536/194182 (33%)]\tLoss: 0.289111\tGrad Norm: 1.604593\tLR: 0.030000\n",
      "Train Epoch: 1172 [86016/194182 (44%)]\tLoss: 0.292404\tGrad Norm: 1.545280\tLR: 0.030000\n",
      "Train Epoch: 1172 [106496/194182 (54%)]\tLoss: 0.286471\tGrad Norm: 1.434174\tLR: 0.030000\n",
      "Train Epoch: 1172 [126976/194182 (65%)]\tLoss: 0.297032\tGrad Norm: 1.733938\tLR: 0.030000\n",
      "Train Epoch: 1172 [147456/194182 (75%)]\tLoss: 0.292841\tGrad Norm: 1.610625\tLR: 0.030000\n",
      "Train Epoch: 1172 [167936/194182 (85%)]\tLoss: 0.288474\tGrad Norm: 1.649250\tLR: 0.030000\n",
      "Train Epoch: 1172 [188416/194182 (96%)]\tLoss: 0.285303\tGrad Norm: 1.318966\tLR: 0.030000\n",
      "Train set: Average loss: 0.2906\n",
      "Test set: Average loss: 0.2456, Average MAE: 0.3525\n",
      "Train Epoch: 1173 [4096/194182 (2%)]\tLoss: 0.290211\tGrad Norm: 1.246207\tLR: 0.030000\n",
      "Train Epoch: 1173 [24576/194182 (12%)]\tLoss: 0.285712\tGrad Norm: 1.349959\tLR: 0.030000\n",
      "Train Epoch: 1173 [45056/194182 (23%)]\tLoss: 0.294907\tGrad Norm: 1.529411\tLR: 0.030000\n",
      "Train Epoch: 1173 [65536/194182 (33%)]\tLoss: 0.303223\tGrad Norm: 1.871708\tLR: 0.030000\n",
      "Train Epoch: 1173 [86016/194182 (44%)]\tLoss: 0.289942\tGrad Norm: 1.459690\tLR: 0.030000\n",
      "Train Epoch: 1173 [106496/194182 (54%)]\tLoss: 0.297169\tGrad Norm: 1.536953\tLR: 0.030000\n",
      "Train Epoch: 1173 [126976/194182 (65%)]\tLoss: 0.288954\tGrad Norm: 1.568097\tLR: 0.030000\n",
      "Train Epoch: 1173 [147456/194182 (75%)]\tLoss: 0.277469\tGrad Norm: 0.980934\tLR: 0.030000\n",
      "Train Epoch: 1173 [167936/194182 (85%)]\tLoss: 0.287383\tGrad Norm: 1.153165\tLR: 0.030000\n",
      "Train Epoch: 1173 [188416/194182 (96%)]\tLoss: 0.280315\tGrad Norm: 1.298889\tLR: 0.030000\n",
      "Train set: Average loss: 0.2883\n",
      "Test set: Average loss: 0.2435, Average MAE: 0.3461\n",
      "Train Epoch: 1174 [4096/194182 (2%)]\tLoss: 0.282718\tGrad Norm: 1.179692\tLR: 0.030000\n",
      "Train Epoch: 1174 [24576/194182 (12%)]\tLoss: 0.292135\tGrad Norm: 1.538416\tLR: 0.030000\n",
      "Train Epoch: 1174 [45056/194182 (23%)]\tLoss: 0.298572\tGrad Norm: 1.756633\tLR: 0.030000\n",
      "Train Epoch: 1174 [65536/194182 (33%)]\tLoss: 0.284924\tGrad Norm: 1.132082\tLR: 0.030000\n",
      "Train Epoch: 1174 [86016/194182 (44%)]\tLoss: 0.285043\tGrad Norm: 1.268348\tLR: 0.030000\n",
      "Train Epoch: 1174 [106496/194182 (54%)]\tLoss: 0.291069\tGrad Norm: 1.689080\tLR: 0.030000\n",
      "Train Epoch: 1174 [126976/194182 (65%)]\tLoss: 0.294971\tGrad Norm: 1.677906\tLR: 0.030000\n",
      "Train Epoch: 1174 [147456/194182 (75%)]\tLoss: 0.285943\tGrad Norm: 1.255905\tLR: 0.030000\n",
      "Train Epoch: 1174 [167936/194182 (85%)]\tLoss: 0.301975\tGrad Norm: 2.035151\tLR: 0.030000\n",
      "Train Epoch: 1174 [188416/194182 (96%)]\tLoss: 0.290369\tGrad Norm: 1.338267\tLR: 0.030000\n",
      "Train set: Average loss: 0.2889\n",
      "Test set: Average loss: 0.2508, Average MAE: 0.3385\n",
      "Train Epoch: 1175 [4096/194182 (2%)]\tLoss: 0.294191\tGrad Norm: 1.882666\tLR: 0.030000\n",
      "Train Epoch: 1175 [24576/194182 (12%)]\tLoss: 0.294353\tGrad Norm: 1.776077\tLR: 0.030000\n",
      "Train Epoch: 1175 [45056/194182 (23%)]\tLoss: 0.286916\tGrad Norm: 1.274449\tLR: 0.030000\n",
      "Train Epoch: 1175 [65536/194182 (33%)]\tLoss: 0.286786\tGrad Norm: 1.175919\tLR: 0.030000\n",
      "Train Epoch: 1175 [86016/194182 (44%)]\tLoss: 0.285331\tGrad Norm: 1.357042\tLR: 0.030000\n",
      "Train Epoch: 1175 [106496/194182 (54%)]\tLoss: 0.288152\tGrad Norm: 1.260963\tLR: 0.030000\n",
      "Train Epoch: 1175 [126976/194182 (65%)]\tLoss: 0.288175\tGrad Norm: 1.339439\tLR: 0.030000\n",
      "Train Epoch: 1175 [147456/194182 (75%)]\tLoss: 0.281946\tGrad Norm: 1.179971\tLR: 0.030000\n",
      "Train Epoch: 1175 [167936/194182 (85%)]\tLoss: 0.284363\tGrad Norm: 1.334071\tLR: 0.030000\n",
      "Train Epoch: 1175 [188416/194182 (96%)]\tLoss: 0.283092\tGrad Norm: 1.357585\tLR: 0.030000\n",
      "Train set: Average loss: 0.2870\n",
      "Test set: Average loss: 0.2428, Average MAE: 0.3393\n",
      "Epoch 1175: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 1176 [4096/194182 (2%)]\tLoss: 0.292670\tGrad Norm: 1.319285\tLR: 0.030000\n",
      "Train Epoch: 1176 [24576/194182 (12%)]\tLoss: 0.294685\tGrad Norm: 1.655943\tLR: 0.030000\n",
      "Train Epoch: 1176 [45056/194182 (23%)]\tLoss: 0.293492\tGrad Norm: 1.777040\tLR: 0.030000\n",
      "Train Epoch: 1176 [65536/194182 (33%)]\tLoss: 0.288319\tGrad Norm: 1.669179\tLR: 0.030000\n",
      "Train Epoch: 1176 [86016/194182 (44%)]\tLoss: 0.284426\tGrad Norm: 1.557145\tLR: 0.030000\n",
      "Train Epoch: 1176 [106496/194182 (54%)]\tLoss: 0.280669\tGrad Norm: 1.399293\tLR: 0.030000\n",
      "Train Epoch: 1176 [126976/194182 (65%)]\tLoss: 0.281355\tGrad Norm: 1.631196\tLR: 0.030000\n",
      "Train Epoch: 1176 [147456/194182 (75%)]\tLoss: 0.285290\tGrad Norm: 1.350918\tLR: 0.030000\n",
      "Train Epoch: 1176 [167936/194182 (85%)]\tLoss: 0.283742\tGrad Norm: 1.545994\tLR: 0.030000\n",
      "Train Epoch: 1176 [188416/194182 (96%)]\tLoss: 0.297221\tGrad Norm: 1.540675\tLR: 0.030000\n",
      "Train set: Average loss: 0.2893\n",
      "Test set: Average loss: 0.2449, Average MAE: 0.3466\n",
      "Train Epoch: 1177 [4096/194182 (2%)]\tLoss: 0.281290\tGrad Norm: 1.319051\tLR: 0.030000\n",
      "Train Epoch: 1177 [24576/194182 (12%)]\tLoss: 0.298083\tGrad Norm: 1.759420\tLR: 0.030000\n",
      "Train Epoch: 1177 [45056/194182 (23%)]\tLoss: 0.280861\tGrad Norm: 1.254092\tLR: 0.030000\n",
      "Train Epoch: 1177 [65536/194182 (33%)]\tLoss: 0.294994\tGrad Norm: 1.572068\tLR: 0.030000\n",
      "Train Epoch: 1177 [86016/194182 (44%)]\tLoss: 0.294070\tGrad Norm: 1.810506\tLR: 0.030000\n",
      "Train Epoch: 1177 [106496/194182 (54%)]\tLoss: 0.292281\tGrad Norm: 1.641383\tLR: 0.030000\n",
      "Train Epoch: 1177 [126976/194182 (65%)]\tLoss: 0.287895\tGrad Norm: 1.384083\tLR: 0.030000\n",
      "Train Epoch: 1177 [147456/194182 (75%)]\tLoss: 0.274674\tGrad Norm: 1.415429\tLR: 0.030000\n",
      "Train Epoch: 1177 [167936/194182 (85%)]\tLoss: 0.285411\tGrad Norm: 1.219046\tLR: 0.030000\n",
      "Train Epoch: 1177 [188416/194182 (96%)]\tLoss: 0.281848\tGrad Norm: 1.243793\tLR: 0.030000\n",
      "Train set: Average loss: 0.2885\n",
      "Test set: Average loss: 0.2529, Average MAE: 0.3565\n",
      "Train Epoch: 1178 [4096/194182 (2%)]\tLoss: 0.300939\tGrad Norm: 1.686900\tLR: 0.030000\n",
      "Train Epoch: 1178 [24576/194182 (12%)]\tLoss: 0.291018\tGrad Norm: 1.618854\tLR: 0.030000\n",
      "Train Epoch: 1178 [45056/194182 (23%)]\tLoss: 0.294418\tGrad Norm: 1.640818\tLR: 0.030000\n",
      "Train Epoch: 1178 [65536/194182 (33%)]\tLoss: 0.288784\tGrad Norm: 1.877876\tLR: 0.030000\n",
      "Train Epoch: 1178 [86016/194182 (44%)]\tLoss: 0.275126\tGrad Norm: 1.066332\tLR: 0.030000\n",
      "Train Epoch: 1178 [106496/194182 (54%)]\tLoss: 0.292090\tGrad Norm: 1.589894\tLR: 0.030000\n",
      "Train Epoch: 1178 [126976/194182 (65%)]\tLoss: 0.292523\tGrad Norm: 1.666333\tLR: 0.030000\n",
      "Train Epoch: 1178 [147456/194182 (75%)]\tLoss: 0.286829\tGrad Norm: 1.390462\tLR: 0.030000\n",
      "Train Epoch: 1178 [167936/194182 (85%)]\tLoss: 0.285441\tGrad Norm: 1.373071\tLR: 0.030000\n",
      "Train Epoch: 1178 [188416/194182 (96%)]\tLoss: 0.285924\tGrad Norm: 1.305822\tLR: 0.030000\n",
      "Train set: Average loss: 0.2891\n",
      "Test set: Average loss: 0.2430, Average MAE: 0.3370\n",
      "Train Epoch: 1179 [4096/194182 (2%)]\tLoss: 0.284481\tGrad Norm: 1.339730\tLR: 0.030000\n",
      "Train Epoch: 1179 [24576/194182 (12%)]\tLoss: 0.285412\tGrad Norm: 1.418782\tLR: 0.030000\n",
      "Train Epoch: 1179 [45056/194182 (23%)]\tLoss: 0.288167\tGrad Norm: 1.603203\tLR: 0.030000\n",
      "Train Epoch: 1179 [65536/194182 (33%)]\tLoss: 0.280160\tGrad Norm: 1.460405\tLR: 0.030000\n",
      "Train Epoch: 1179 [86016/194182 (44%)]\tLoss: 0.281477\tGrad Norm: 1.541698\tLR: 0.030000\n",
      "Train Epoch: 1179 [106496/194182 (54%)]\tLoss: 0.294806\tGrad Norm: 1.830947\tLR: 0.030000\n",
      "Train Epoch: 1179 [126976/194182 (65%)]\tLoss: 0.296559\tGrad Norm: 1.645062\tLR: 0.030000\n",
      "Train Epoch: 1179 [147456/194182 (75%)]\tLoss: 0.290452\tGrad Norm: 1.345145\tLR: 0.030000\n",
      "Train Epoch: 1179 [167936/194182 (85%)]\tLoss: 0.297244\tGrad Norm: 1.911327\tLR: 0.030000\n",
      "Train Epoch: 1179 [188416/194182 (96%)]\tLoss: 0.284106\tGrad Norm: 1.513435\tLR: 0.030000\n",
      "Train set: Average loss: 0.2886\n",
      "Test set: Average loss: 0.2416, Average MAE: 0.3318\n",
      "Train Epoch: 1180 [4096/194182 (2%)]\tLoss: 0.282608\tGrad Norm: 1.361839\tLR: 0.030000\n",
      "Train Epoch: 1180 [24576/194182 (12%)]\tLoss: 0.292722\tGrad Norm: 1.553055\tLR: 0.030000\n",
      "Train Epoch: 1180 [45056/194182 (23%)]\tLoss: 0.290690\tGrad Norm: 1.742392\tLR: 0.030000\n",
      "Train Epoch: 1180 [65536/194182 (33%)]\tLoss: 0.287485\tGrad Norm: 1.574564\tLR: 0.030000\n",
      "Train Epoch: 1180 [86016/194182 (44%)]\tLoss: 0.286562\tGrad Norm: 1.567099\tLR: 0.030000\n",
      "Train Epoch: 1180 [106496/194182 (54%)]\tLoss: 0.286086\tGrad Norm: 1.429942\tLR: 0.030000\n",
      "Train Epoch: 1180 [126976/194182 (65%)]\tLoss: 0.291246\tGrad Norm: 1.487369\tLR: 0.030000\n",
      "Train Epoch: 1180 [147456/194182 (75%)]\tLoss: 0.279946\tGrad Norm: 1.117929\tLR: 0.030000\n",
      "Train Epoch: 1180 [167936/194182 (85%)]\tLoss: 0.298924\tGrad Norm: 1.579433\tLR: 0.030000\n",
      "Train Epoch: 1180 [188416/194182 (96%)]\tLoss: 0.290117\tGrad Norm: 1.114657\tLR: 0.030000\n",
      "Train set: Average loss: 0.2876\n",
      "Test set: Average loss: 0.2415, Average MAE: 0.3462\n",
      "Epoch 1180: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 1181 [4096/194182 (2%)]\tLoss: 0.288053\tGrad Norm: 1.273738\tLR: 0.030000\n",
      "Train Epoch: 1181 [24576/194182 (12%)]\tLoss: 0.275377\tGrad Norm: 1.301830\tLR: 0.030000\n",
      "Train Epoch: 1181 [45056/194182 (23%)]\tLoss: 0.289319\tGrad Norm: 1.528872\tLR: 0.030000\n",
      "Train Epoch: 1181 [65536/194182 (33%)]\tLoss: 0.282285\tGrad Norm: 1.192759\tLR: 0.030000\n",
      "Train Epoch: 1181 [86016/194182 (44%)]\tLoss: 0.292057\tGrad Norm: 1.620522\tLR: 0.030000\n",
      "Train Epoch: 1181 [106496/194182 (54%)]\tLoss: 0.294825\tGrad Norm: 1.515307\tLR: 0.030000\n",
      "Train Epoch: 1181 [126976/194182 (65%)]\tLoss: 0.282071\tGrad Norm: 1.286966\tLR: 0.030000\n",
      "Train Epoch: 1181 [147456/194182 (75%)]\tLoss: 0.291754\tGrad Norm: 1.719570\tLR: 0.030000\n",
      "Train Epoch: 1181 [167936/194182 (85%)]\tLoss: 0.291344\tGrad Norm: 1.488460\tLR: 0.030000\n",
      "Train Epoch: 1181 [188416/194182 (96%)]\tLoss: 0.290169\tGrad Norm: 1.518589\tLR: 0.030000\n",
      "Train set: Average loss: 0.2874\n",
      "Test set: Average loss: 0.2466, Average MAE: 0.3395\n",
      "Train Epoch: 1182 [4096/194182 (2%)]\tLoss: 0.285226\tGrad Norm: 1.394536\tLR: 0.030000\n",
      "Train Epoch: 1182 [24576/194182 (12%)]\tLoss: 0.281430\tGrad Norm: 1.300986\tLR: 0.030000\n",
      "Train Epoch: 1182 [45056/194182 (23%)]\tLoss: 0.296911\tGrad Norm: 1.617107\tLR: 0.030000\n",
      "Train Epoch: 1182 [65536/194182 (33%)]\tLoss: 0.290656\tGrad Norm: 1.403763\tLR: 0.030000\n",
      "Train Epoch: 1182 [86016/194182 (44%)]\tLoss: 0.293474\tGrad Norm: 1.578884\tLR: 0.030000\n",
      "Train Epoch: 1182 [106496/194182 (54%)]\tLoss: 0.291848\tGrad Norm: 1.813224\tLR: 0.030000\n",
      "Train Epoch: 1182 [126976/194182 (65%)]\tLoss: 0.279651\tGrad Norm: 1.556527\tLR: 0.030000\n",
      "Train Epoch: 1182 [147456/194182 (75%)]\tLoss: 0.287060\tGrad Norm: 1.471365\tLR: 0.030000\n",
      "Train Epoch: 1182 [167936/194182 (85%)]\tLoss: 0.297415\tGrad Norm: 1.906046\tLR: 0.030000\n",
      "Train Epoch: 1182 [188416/194182 (96%)]\tLoss: 0.281736\tGrad Norm: 1.231707\tLR: 0.030000\n",
      "Train set: Average loss: 0.2879\n",
      "Test set: Average loss: 0.2476, Average MAE: 0.3521\n",
      "Train Epoch: 1183 [4096/194182 (2%)]\tLoss: 0.280096\tGrad Norm: 1.474288\tLR: 0.030000\n",
      "Train Epoch: 1183 [24576/194182 (12%)]\tLoss: 0.285848\tGrad Norm: 1.353154\tLR: 0.030000\n",
      "Train Epoch: 1183 [45056/194182 (23%)]\tLoss: 0.282119\tGrad Norm: 1.470681\tLR: 0.030000\n",
      "Train Epoch: 1183 [65536/194182 (33%)]\tLoss: 0.284411\tGrad Norm: 1.615303\tLR: 0.030000\n",
      "Train Epoch: 1183 [86016/194182 (44%)]\tLoss: 0.283893\tGrad Norm: 1.533870\tLR: 0.030000\n",
      "Train Epoch: 1183 [106496/194182 (54%)]\tLoss: 0.289992\tGrad Norm: 1.545518\tLR: 0.030000\n",
      "Train Epoch: 1183 [126976/194182 (65%)]\tLoss: 0.286670\tGrad Norm: 1.428412\tLR: 0.030000\n",
      "Train Epoch: 1183 [147456/194182 (75%)]\tLoss: 0.287882\tGrad Norm: 1.360788\tLR: 0.030000\n",
      "Train Epoch: 1183 [167936/194182 (85%)]\tLoss: 0.286124\tGrad Norm: 1.245439\tLR: 0.030000\n",
      "Train Epoch: 1183 [188416/194182 (96%)]\tLoss: 0.292822\tGrad Norm: 1.375329\tLR: 0.030000\n",
      "Train set: Average loss: 0.2866\n",
      "Test set: Average loss: 0.2436, Average MAE: 0.3432\n",
      "Train Epoch: 1184 [4096/194182 (2%)]\tLoss: 0.280543\tGrad Norm: 1.085173\tLR: 0.030000\n",
      "Train Epoch: 1184 [24576/194182 (12%)]\tLoss: 0.287194\tGrad Norm: 1.424187\tLR: 0.030000\n",
      "Train Epoch: 1184 [45056/194182 (23%)]\tLoss: 0.288247\tGrad Norm: 1.632850\tLR: 0.030000\n",
      "Train Epoch: 1184 [65536/194182 (33%)]\tLoss: 0.292400\tGrad Norm: 1.737208\tLR: 0.030000\n",
      "Train Epoch: 1184 [86016/194182 (44%)]\tLoss: 0.282492\tGrad Norm: 1.600876\tLR: 0.030000\n",
      "Train Epoch: 1184 [106496/194182 (54%)]\tLoss: 0.299177\tGrad Norm: 1.722904\tLR: 0.030000\n",
      "Train Epoch: 1184 [126976/194182 (65%)]\tLoss: 0.285807\tGrad Norm: 1.377049\tLR: 0.030000\n",
      "Train Epoch: 1184 [147456/194182 (75%)]\tLoss: 0.291056\tGrad Norm: 1.508779\tLR: 0.030000\n",
      "Train Epoch: 1184 [167936/194182 (85%)]\tLoss: 0.294460\tGrad Norm: 1.593185\tLR: 0.030000\n",
      "Train Epoch: 1184 [188416/194182 (96%)]\tLoss: 0.297384\tGrad Norm: 1.849234\tLR: 0.030000\n",
      "Train set: Average loss: 0.2874\n",
      "Test set: Average loss: 0.2433, Average MAE: 0.3403\n",
      "Train Epoch: 1185 [4096/194182 (2%)]\tLoss: 0.292403\tGrad Norm: 1.365296\tLR: 0.030000\n",
      "Train Epoch: 1185 [24576/194182 (12%)]\tLoss: 0.288431\tGrad Norm: 1.466006\tLR: 0.030000\n",
      "Train Epoch: 1185 [45056/194182 (23%)]\tLoss: 0.288483\tGrad Norm: 1.532923\tLR: 0.030000\n",
      "Train Epoch: 1185 [65536/194182 (33%)]\tLoss: 0.289350\tGrad Norm: 1.431927\tLR: 0.030000\n",
      "Train Epoch: 1185 [86016/194182 (44%)]\tLoss: 0.293248\tGrad Norm: 1.729877\tLR: 0.030000\n",
      "Train Epoch: 1185 [106496/194182 (54%)]\tLoss: 0.292760\tGrad Norm: 1.629893\tLR: 0.030000\n",
      "Train Epoch: 1185 [126976/194182 (65%)]\tLoss: 0.290222\tGrad Norm: 1.535054\tLR: 0.030000\n",
      "Train Epoch: 1185 [147456/194182 (75%)]\tLoss: 0.299370\tGrad Norm: 2.108842\tLR: 0.030000\n",
      "Train Epoch: 1185 [167936/194182 (85%)]\tLoss: 0.293043\tGrad Norm: 1.680554\tLR: 0.030000\n",
      "Train Epoch: 1185 [188416/194182 (96%)]\tLoss: 0.284338\tGrad Norm: 1.242093\tLR: 0.030000\n",
      "Train set: Average loss: 0.2886\n",
      "Test set: Average loss: 0.2417, Average MAE: 0.3326\n",
      "Epoch 1185: Mean reward = 0.051 +/- 0.048\n",
      "Train Epoch: 1186 [4096/194182 (2%)]\tLoss: 0.295767\tGrad Norm: 1.435658\tLR: 0.030000\n",
      "Train Epoch: 1186 [24576/194182 (12%)]\tLoss: 0.289002\tGrad Norm: 1.820215\tLR: 0.030000\n",
      "Train Epoch: 1186 [45056/194182 (23%)]\tLoss: 0.289474\tGrad Norm: 1.561262\tLR: 0.030000\n",
      "Train Epoch: 1186 [65536/194182 (33%)]\tLoss: 0.281990\tGrad Norm: 1.391342\tLR: 0.030000\n",
      "Train Epoch: 1186 [86016/194182 (44%)]\tLoss: 0.282642\tGrad Norm: 1.501787\tLR: 0.030000\n",
      "Train Epoch: 1186 [106496/194182 (54%)]\tLoss: 0.279596\tGrad Norm: 1.326101\tLR: 0.030000\n",
      "Train Epoch: 1186 [126976/194182 (65%)]\tLoss: 0.293868\tGrad Norm: 1.628910\tLR: 0.030000\n",
      "Train Epoch: 1186 [147456/194182 (75%)]\tLoss: 0.287459\tGrad Norm: 1.212883\tLR: 0.030000\n",
      "Train Epoch: 1186 [167936/194182 (85%)]\tLoss: 0.286430\tGrad Norm: 1.444812\tLR: 0.030000\n",
      "Train Epoch: 1186 [188416/194182 (96%)]\tLoss: 0.288994\tGrad Norm: 1.313106\tLR: 0.030000\n",
      "Train set: Average loss: 0.2872\n",
      "Test set: Average loss: 0.2463, Average MAE: 0.3462\n",
      "Train Epoch: 1187 [4096/194182 (2%)]\tLoss: 0.287291\tGrad Norm: 1.557614\tLR: 0.030000\n",
      "Train Epoch: 1187 [24576/194182 (12%)]\tLoss: 0.292782\tGrad Norm: 1.456126\tLR: 0.030000\n",
      "Train Epoch: 1187 [45056/194182 (23%)]\tLoss: 0.285298\tGrad Norm: 1.491719\tLR: 0.030000\n",
      "Train Epoch: 1187 [65536/194182 (33%)]\tLoss: 0.280009\tGrad Norm: 1.404280\tLR: 0.030000\n",
      "Train Epoch: 1187 [86016/194182 (44%)]\tLoss: 0.287812\tGrad Norm: 1.471186\tLR: 0.030000\n",
      "Train Epoch: 1187 [106496/194182 (54%)]\tLoss: 0.287525\tGrad Norm: 1.506404\tLR: 0.030000\n",
      "Train Epoch: 1187 [126976/194182 (65%)]\tLoss: 0.281558\tGrad Norm: 1.185326\tLR: 0.030000\n",
      "Train Epoch: 1187 [147456/194182 (75%)]\tLoss: 0.290012\tGrad Norm: 1.299575\tLR: 0.030000\n",
      "Train Epoch: 1187 [167936/194182 (85%)]\tLoss: 0.290132\tGrad Norm: 1.520743\tLR: 0.030000\n",
      "Train Epoch: 1187 [188416/194182 (96%)]\tLoss: 0.294024\tGrad Norm: 1.974215\tLR: 0.030000\n",
      "Train set: Average loss: 0.2865\n",
      "Test set: Average loss: 0.2450, Average MAE: 0.3334\n",
      "Train Epoch: 1188 [4096/194182 (2%)]\tLoss: 0.286344\tGrad Norm: 1.604836\tLR: 0.030000\n",
      "Train Epoch: 1188 [24576/194182 (12%)]\tLoss: 0.299232\tGrad Norm: 2.128734\tLR: 0.030000\n",
      "Train Epoch: 1188 [45056/194182 (23%)]\tLoss: 0.288438\tGrad Norm: 1.389020\tLR: 0.030000\n",
      "Train Epoch: 1188 [65536/194182 (33%)]\tLoss: 0.283311\tGrad Norm: 1.438123\tLR: 0.030000\n",
      "Train Epoch: 1188 [86016/194182 (44%)]\tLoss: 0.285892\tGrad Norm: 1.282250\tLR: 0.030000\n",
      "Train Epoch: 1188 [106496/194182 (54%)]\tLoss: 0.284899\tGrad Norm: 1.519229\tLR: 0.030000\n",
      "Train Epoch: 1188 [126976/194182 (65%)]\tLoss: 0.285558\tGrad Norm: 1.466067\tLR: 0.030000\n",
      "Train Epoch: 1188 [147456/194182 (75%)]\tLoss: 0.278581\tGrad Norm: 0.873389\tLR: 0.030000\n",
      "Train Epoch: 1188 [167936/194182 (85%)]\tLoss: 0.279099\tGrad Norm: 1.110707\tLR: 0.030000\n",
      "Train Epoch: 1188 [188416/194182 (96%)]\tLoss: 0.287537\tGrad Norm: 1.423176\tLR: 0.030000\n",
      "Train set: Average loss: 0.2859\n",
      "Test set: Average loss: 0.2474, Average MAE: 0.3417\n",
      "Train Epoch: 1189 [4096/194182 (2%)]\tLoss: 0.283421\tGrad Norm: 1.539243\tLR: 0.030000\n",
      "Train Epoch: 1189 [24576/194182 (12%)]\tLoss: 0.286769\tGrad Norm: 1.507400\tLR: 0.030000\n",
      "Train Epoch: 1189 [45056/194182 (23%)]\tLoss: 0.281766\tGrad Norm: 1.308179\tLR: 0.030000\n",
      "Train Epoch: 1189 [65536/194182 (33%)]\tLoss: 0.284093\tGrad Norm: 1.476852\tLR: 0.030000\n",
      "Train Epoch: 1189 [86016/194182 (44%)]\tLoss: 0.283504\tGrad Norm: 1.457744\tLR: 0.030000\n",
      "Train Epoch: 1189 [106496/194182 (54%)]\tLoss: 0.279237\tGrad Norm: 1.112810\tLR: 0.030000\n",
      "Train Epoch: 1189 [126976/194182 (65%)]\tLoss: 0.285874\tGrad Norm: 1.262794\tLR: 0.030000\n",
      "Train Epoch: 1189 [147456/194182 (75%)]\tLoss: 0.284750\tGrad Norm: 1.645256\tLR: 0.030000\n",
      "Train Epoch: 1189 [167936/194182 (85%)]\tLoss: 0.288037\tGrad Norm: 1.669781\tLR: 0.030000\n",
      "Train Epoch: 1189 [188416/194182 (96%)]\tLoss: 0.291801\tGrad Norm: 1.315831\tLR: 0.030000\n",
      "Train set: Average loss: 0.2850\n",
      "Test set: Average loss: 0.2398, Average MAE: 0.3306\n",
      "Train Epoch: 1190 [4096/194182 (2%)]\tLoss: 0.282589\tGrad Norm: 1.240391\tLR: 0.030000\n",
      "Train Epoch: 1190 [24576/194182 (12%)]\tLoss: 0.287603\tGrad Norm: 1.602563\tLR: 0.030000\n",
      "Train Epoch: 1190 [45056/194182 (23%)]\tLoss: 0.293206\tGrad Norm: 1.924983\tLR: 0.030000\n",
      "Train Epoch: 1190 [65536/194182 (33%)]\tLoss: 0.285552\tGrad Norm: 1.441882\tLR: 0.030000\n",
      "Train Epoch: 1190 [86016/194182 (44%)]\tLoss: 0.288168\tGrad Norm: 1.472060\tLR: 0.030000\n",
      "Train Epoch: 1190 [106496/194182 (54%)]\tLoss: 0.283251\tGrad Norm: 1.417850\tLR: 0.030000\n",
      "Train Epoch: 1190 [126976/194182 (65%)]\tLoss: 0.287952\tGrad Norm: 1.573093\tLR: 0.030000\n",
      "Train Epoch: 1190 [147456/194182 (75%)]\tLoss: 0.288978\tGrad Norm: 1.411350\tLR: 0.030000\n",
      "Train Epoch: 1190 [167936/194182 (85%)]\tLoss: 0.292388\tGrad Norm: 1.755796\tLR: 0.030000\n",
      "Train Epoch: 1190 [188416/194182 (96%)]\tLoss: 0.291076\tGrad Norm: 1.781871\tLR: 0.030000\n",
      "Train set: Average loss: 0.2878\n",
      "Test set: Average loss: 0.2566, Average MAE: 0.3444\n",
      "Epoch 1190: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 1191 [4096/194182 (2%)]\tLoss: 0.303625\tGrad Norm: 2.233309\tLR: 0.030000\n",
      "Train Epoch: 1191 [24576/194182 (12%)]\tLoss: 0.284138\tGrad Norm: 1.329487\tLR: 0.030000\n",
      "Train Epoch: 1191 [45056/194182 (23%)]\tLoss: 0.291174\tGrad Norm: 1.676494\tLR: 0.030000\n",
      "Train Epoch: 1191 [65536/194182 (33%)]\tLoss: 0.285114\tGrad Norm: 1.571867\tLR: 0.030000\n",
      "Train Epoch: 1191 [86016/194182 (44%)]\tLoss: 0.286790\tGrad Norm: 1.809372\tLR: 0.030000\n",
      "Train Epoch: 1191 [106496/194182 (54%)]\tLoss: 0.272489\tGrad Norm: 1.278879\tLR: 0.030000\n",
      "Train Epoch: 1191 [126976/194182 (65%)]\tLoss: 0.287835\tGrad Norm: 1.494459\tLR: 0.030000\n",
      "Train Epoch: 1191 [147456/194182 (75%)]\tLoss: 0.282297\tGrad Norm: 1.358588\tLR: 0.030000\n",
      "Train Epoch: 1191 [167936/194182 (85%)]\tLoss: 0.288930\tGrad Norm: 1.458566\tLR: 0.030000\n",
      "Train Epoch: 1191 [188416/194182 (96%)]\tLoss: 0.288710\tGrad Norm: 1.617376\tLR: 0.030000\n",
      "Train set: Average loss: 0.2869\n",
      "Test set: Average loss: 0.2473, Average MAE: 0.3519\n",
      "Train Epoch: 1192 [4096/194182 (2%)]\tLoss: 0.288005\tGrad Norm: 1.397049\tLR: 0.030000\n",
      "Train Epoch: 1192 [24576/194182 (12%)]\tLoss: 0.292258\tGrad Norm: 1.773707\tLR: 0.030000\n",
      "Train Epoch: 1192 [45056/194182 (23%)]\tLoss: 0.291202\tGrad Norm: 1.812680\tLR: 0.030000\n",
      "Train Epoch: 1192 [65536/194182 (33%)]\tLoss: 0.287299\tGrad Norm: 1.664269\tLR: 0.030000\n",
      "Train Epoch: 1192 [86016/194182 (44%)]\tLoss: 0.282787\tGrad Norm: 1.504742\tLR: 0.030000\n",
      "Train Epoch: 1192 [106496/194182 (54%)]\tLoss: 0.282094\tGrad Norm: 1.932897\tLR: 0.030000\n",
      "Train Epoch: 1192 [126976/194182 (65%)]\tLoss: 0.287192\tGrad Norm: 1.394992\tLR: 0.030000\n",
      "Train Epoch: 1192 [147456/194182 (75%)]\tLoss: 0.296212\tGrad Norm: 1.841996\tLR: 0.030000\n",
      "Train Epoch: 1192 [167936/194182 (85%)]\tLoss: 0.287977\tGrad Norm: 1.711406\tLR: 0.030000\n",
      "Train Epoch: 1192 [188416/194182 (96%)]\tLoss: 0.299258\tGrad Norm: 2.040770\tLR: 0.030000\n",
      "Train set: Average loss: 0.2874\n",
      "Test set: Average loss: 0.2429, Average MAE: 0.3462\n",
      "Train Epoch: 1193 [4096/194182 (2%)]\tLoss: 0.274116\tGrad Norm: 1.211414\tLR: 0.030000\n",
      "Train Epoch: 1193 [24576/194182 (12%)]\tLoss: 0.273960\tGrad Norm: 1.026408\tLR: 0.030000\n",
      "Train Epoch: 1193 [45056/194182 (23%)]\tLoss: 0.277342\tGrad Norm: 0.929986\tLR: 0.030000\n",
      "Train Epoch: 1193 [65536/194182 (33%)]\tLoss: 0.285850\tGrad Norm: 1.357450\tLR: 0.030000\n",
      "Train Epoch: 1193 [86016/194182 (44%)]\tLoss: 0.291454\tGrad Norm: 1.951523\tLR: 0.030000\n",
      "Train Epoch: 1193 [106496/194182 (54%)]\tLoss: 0.282543\tGrad Norm: 1.350816\tLR: 0.030000\n",
      "Train Epoch: 1193 [126976/194182 (65%)]\tLoss: 0.285463\tGrad Norm: 1.234259\tLR: 0.030000\n",
      "Train Epoch: 1193 [147456/194182 (75%)]\tLoss: 0.288653\tGrad Norm: 1.450663\tLR: 0.030000\n",
      "Train Epoch: 1193 [167936/194182 (85%)]\tLoss: 0.287604\tGrad Norm: 1.469706\tLR: 0.030000\n",
      "Train Epoch: 1193 [188416/194182 (96%)]\tLoss: 0.281186\tGrad Norm: 1.451598\tLR: 0.030000\n",
      "Train set: Average loss: 0.2842\n",
      "Test set: Average loss: 0.2488, Average MAE: 0.3427\n",
      "Train Epoch: 1194 [4096/194182 (2%)]\tLoss: 0.283843\tGrad Norm: 1.605109\tLR: 0.030000\n",
      "Train Epoch: 1194 [24576/194182 (12%)]\tLoss: 0.283022\tGrad Norm: 1.278115\tLR: 0.030000\n",
      "Train Epoch: 1194 [45056/194182 (23%)]\tLoss: 0.283246\tGrad Norm: 1.319822\tLR: 0.030000\n",
      "Train Epoch: 1194 [65536/194182 (33%)]\tLoss: 0.286375\tGrad Norm: 1.607175\tLR: 0.030000\n",
      "Train Epoch: 1194 [86016/194182 (44%)]\tLoss: 0.284420\tGrad Norm: 1.354354\tLR: 0.030000\n",
      "Train Epoch: 1194 [106496/194182 (54%)]\tLoss: 0.285691\tGrad Norm: 1.399484\tLR: 0.030000\n",
      "Train Epoch: 1194 [126976/194182 (65%)]\tLoss: 0.284606\tGrad Norm: 1.098149\tLR: 0.030000\n",
      "Train Epoch: 1194 [147456/194182 (75%)]\tLoss: 0.286872\tGrad Norm: 1.282165\tLR: 0.030000\n",
      "Train Epoch: 1194 [167936/194182 (85%)]\tLoss: 0.289058\tGrad Norm: 1.564886\tLR: 0.030000\n",
      "Train Epoch: 1194 [188416/194182 (96%)]\tLoss: 0.289524\tGrad Norm: 1.855031\tLR: 0.030000\n",
      "Train set: Average loss: 0.2844\n",
      "Test set: Average loss: 0.2507, Average MAE: 0.3334\n",
      "Train Epoch: 1195 [4096/194182 (2%)]\tLoss: 0.286291\tGrad Norm: 1.840218\tLR: 0.030000\n",
      "Train Epoch: 1195 [24576/194182 (12%)]\tLoss: 0.292012\tGrad Norm: 1.532941\tLR: 0.030000\n",
      "Train Epoch: 1195 [45056/194182 (23%)]\tLoss: 0.296694\tGrad Norm: 1.829402\tLR: 0.030000\n",
      "Train Epoch: 1195 [65536/194182 (33%)]\tLoss: 0.294088\tGrad Norm: 1.562469\tLR: 0.030000\n",
      "Train Epoch: 1195 [86016/194182 (44%)]\tLoss: 0.296638\tGrad Norm: 1.903949\tLR: 0.030000\n",
      "Train Epoch: 1195 [106496/194182 (54%)]\tLoss: 0.280510\tGrad Norm: 1.371999\tLR: 0.030000\n",
      "Train Epoch: 1195 [126976/194182 (65%)]\tLoss: 0.285955\tGrad Norm: 1.191195\tLR: 0.030000\n",
      "Train Epoch: 1195 [147456/194182 (75%)]\tLoss: 0.282772\tGrad Norm: 0.955874\tLR: 0.030000\n",
      "Train Epoch: 1195 [167936/194182 (85%)]\tLoss: 0.279281\tGrad Norm: 1.187370\tLR: 0.030000\n",
      "Train Epoch: 1195 [188416/194182 (96%)]\tLoss: 0.278356\tGrad Norm: 1.271197\tLR: 0.030000\n",
      "Train set: Average loss: 0.2854\n",
      "Test set: Average loss: 0.2441, Average MAE: 0.3391\n",
      "Epoch 1195: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 1196 [4096/194182 (2%)]\tLoss: 0.277526\tGrad Norm: 1.308806\tLR: 0.030000\n",
      "Train Epoch: 1196 [24576/194182 (12%)]\tLoss: 0.282288\tGrad Norm: 1.300485\tLR: 0.030000\n",
      "Train Epoch: 1196 [45056/194182 (23%)]\tLoss: 0.282660\tGrad Norm: 1.443577\tLR: 0.030000\n",
      "Train Epoch: 1196 [65536/194182 (33%)]\tLoss: 0.292663\tGrad Norm: 1.559992\tLR: 0.030000\n",
      "Train Epoch: 1196 [86016/194182 (44%)]\tLoss: 0.290610\tGrad Norm: 1.636847\tLR: 0.030000\n",
      "Train Epoch: 1196 [106496/194182 (54%)]\tLoss: 0.294149\tGrad Norm: 2.032580\tLR: 0.030000\n",
      "Train Epoch: 1196 [126976/194182 (65%)]\tLoss: 0.285709\tGrad Norm: 1.580969\tLR: 0.030000\n",
      "Train Epoch: 1196 [147456/194182 (75%)]\tLoss: 0.285539\tGrad Norm: 1.438168\tLR: 0.030000\n",
      "Train Epoch: 1196 [167936/194182 (85%)]\tLoss: 0.289049\tGrad Norm: 1.344250\tLR: 0.030000\n",
      "Train Epoch: 1196 [188416/194182 (96%)]\tLoss: 0.287452\tGrad Norm: 1.586301\tLR: 0.030000\n",
      "Train set: Average loss: 0.2859\n",
      "Test set: Average loss: 0.2502, Average MAE: 0.3535\n",
      "Train Epoch: 1197 [4096/194182 (2%)]\tLoss: 0.283593\tGrad Norm: 1.678348\tLR: 0.030000\n",
      "Train Epoch: 1197 [24576/194182 (12%)]\tLoss: 0.289183\tGrad Norm: 1.550007\tLR: 0.030000\n",
      "Train Epoch: 1197 [45056/194182 (23%)]\tLoss: 0.285503\tGrad Norm: 1.496581\tLR: 0.030000\n",
      "Train Epoch: 1197 [65536/194182 (33%)]\tLoss: 0.280232\tGrad Norm: 1.424457\tLR: 0.030000\n",
      "Train Epoch: 1197 [86016/194182 (44%)]\tLoss: 0.286485\tGrad Norm: 1.483518\tLR: 0.030000\n",
      "Train Epoch: 1197 [106496/194182 (54%)]\tLoss: 0.287116\tGrad Norm: 1.618186\tLR: 0.030000\n",
      "Train Epoch: 1197 [126976/194182 (65%)]\tLoss: 0.280654\tGrad Norm: 1.531768\tLR: 0.030000\n",
      "Train Epoch: 1197 [147456/194182 (75%)]\tLoss: 0.280189\tGrad Norm: 1.270107\tLR: 0.030000\n",
      "Train Epoch: 1197 [167936/194182 (85%)]\tLoss: 0.282837\tGrad Norm: 1.081206\tLR: 0.030000\n",
      "Train Epoch: 1197 [188416/194182 (96%)]\tLoss: 0.285464\tGrad Norm: 1.216660\tLR: 0.030000\n",
      "Train set: Average loss: 0.2836\n",
      "Test set: Average loss: 0.2505, Average MAE: 0.3401\n",
      "Train Epoch: 1198 [4096/194182 (2%)]\tLoss: 0.288535\tGrad Norm: 1.813120\tLR: 0.030000\n",
      "Train Epoch: 1198 [24576/194182 (12%)]\tLoss: 0.286117\tGrad Norm: 1.505298\tLR: 0.030000\n",
      "Train Epoch: 1198 [45056/194182 (23%)]\tLoss: 0.282528\tGrad Norm: 1.343549\tLR: 0.030000\n",
      "Train Epoch: 1198 [65536/194182 (33%)]\tLoss: 0.287203\tGrad Norm: 1.906474\tLR: 0.030000\n",
      "Train Epoch: 1198 [86016/194182 (44%)]\tLoss: 0.276535\tGrad Norm: 1.305430\tLR: 0.030000\n",
      "Train Epoch: 1198 [106496/194182 (54%)]\tLoss: 0.280106\tGrad Norm: 1.206381\tLR: 0.030000\n",
      "Train Epoch: 1198 [126976/194182 (65%)]\tLoss: 0.286953\tGrad Norm: 1.331327\tLR: 0.030000\n",
      "Train Epoch: 1198 [147456/194182 (75%)]\tLoss: 0.285617\tGrad Norm: 1.562958\tLR: 0.030000\n",
      "Train Epoch: 1198 [167936/194182 (85%)]\tLoss: 0.277602\tGrad Norm: 0.979850\tLR: 0.030000\n",
      "Train Epoch: 1198 [188416/194182 (96%)]\tLoss: 0.275322\tGrad Norm: 1.368380\tLR: 0.030000\n",
      "Train set: Average loss: 0.2838\n",
      "Test set: Average loss: 0.2515, Average MAE: 0.3507\n",
      "Train Epoch: 1199 [4096/194182 (2%)]\tLoss: 0.297441\tGrad Norm: 1.810649\tLR: 0.030000\n",
      "Train Epoch: 1199 [24576/194182 (12%)]\tLoss: 0.282913\tGrad Norm: 1.568978\tLR: 0.030000\n",
      "Train Epoch: 1199 [45056/194182 (23%)]\tLoss: 0.292178\tGrad Norm: 1.540927\tLR: 0.030000\n",
      "Train Epoch: 1199 [65536/194182 (33%)]\tLoss: 0.281787\tGrad Norm: 1.747048\tLR: 0.030000\n",
      "Train Epoch: 1199 [86016/194182 (44%)]\tLoss: 0.282808\tGrad Norm: 1.281225\tLR: 0.030000\n",
      "Train Epoch: 1199 [106496/194182 (54%)]\tLoss: 0.278640\tGrad Norm: 1.440027\tLR: 0.030000\n",
      "Train Epoch: 1199 [126976/194182 (65%)]\tLoss: 0.283211\tGrad Norm: 1.475095\tLR: 0.030000\n",
      "Train Epoch: 1199 [147456/194182 (75%)]\tLoss: 0.284469\tGrad Norm: 1.485635\tLR: 0.030000\n",
      "Train Epoch: 1199 [167936/194182 (85%)]\tLoss: 0.285181\tGrad Norm: 1.175208\tLR: 0.030000\n",
      "Train Epoch: 1199 [188416/194182 (96%)]\tLoss: 0.287847\tGrad Norm: 1.643031\tLR: 0.030000\n",
      "Train set: Average loss: 0.2853\n",
      "Test set: Average loss: 0.2530, Average MAE: 0.3490\n",
      "Train Epoch: 1200 [4096/194182 (2%)]\tLoss: 0.291156\tGrad Norm: 1.765173\tLR: 0.030000\n",
      "Train Epoch: 1200 [24576/194182 (12%)]\tLoss: 0.285121\tGrad Norm: 1.442077\tLR: 0.030000\n",
      "Train Epoch: 1200 [45056/194182 (23%)]\tLoss: 0.279454\tGrad Norm: 1.306197\tLR: 0.030000\n",
      "Train Epoch: 1200 [65536/194182 (33%)]\tLoss: 0.275749\tGrad Norm: 1.228913\tLR: 0.030000\n",
      "Train Epoch: 1200 [86016/194182 (44%)]\tLoss: 0.282224\tGrad Norm: 1.644711\tLR: 0.030000\n",
      "Train Epoch: 1200 [106496/194182 (54%)]\tLoss: 0.283771\tGrad Norm: 1.751617\tLR: 0.030000\n",
      "Train Epoch: 1200 [126976/194182 (65%)]\tLoss: 0.302333\tGrad Norm: 1.804705\tLR: 0.030000\n",
      "Train Epoch: 1200 [147456/194182 (75%)]\tLoss: 0.280025\tGrad Norm: 1.382613\tLR: 0.030000\n",
      "Train Epoch: 1200 [167936/194182 (85%)]\tLoss: 0.283347\tGrad Norm: 1.280299\tLR: 0.030000\n",
      "Train Epoch: 1200 [188416/194182 (96%)]\tLoss: 0.270351\tGrad Norm: 1.153833\tLR: 0.030000\n",
      "Train set: Average loss: 0.2841\n",
      "Test set: Average loss: 0.2398, Average MAE: 0.3332\n",
      "Epoch 1200: Mean reward = 0.054 +/- 0.023\n",
      "Train Epoch: 1201 [4096/194182 (2%)]\tLoss: 0.280059\tGrad Norm: 1.152490\tLR: 0.030000\n",
      "Train Epoch: 1201 [24576/194182 (12%)]\tLoss: 0.276227\tGrad Norm: 1.465532\tLR: 0.030000\n",
      "Train Epoch: 1201 [45056/194182 (23%)]\tLoss: 0.281776\tGrad Norm: 1.275094\tLR: 0.030000\n",
      "Train Epoch: 1201 [65536/194182 (33%)]\tLoss: 0.285646\tGrad Norm: 1.655936\tLR: 0.030000\n",
      "Train Epoch: 1201 [86016/194182 (44%)]\tLoss: 0.286411\tGrad Norm: 1.249623\tLR: 0.030000\n",
      "Train Epoch: 1201 [106496/194182 (54%)]\tLoss: 0.284315\tGrad Norm: 1.490742\tLR: 0.030000\n",
      "Train Epoch: 1201 [126976/194182 (65%)]\tLoss: 0.281923\tGrad Norm: 1.614300\tLR: 0.030000\n",
      "Train Epoch: 1201 [147456/194182 (75%)]\tLoss: 0.290766\tGrad Norm: 1.680844\tLR: 0.030000\n",
      "Train Epoch: 1201 [167936/194182 (85%)]\tLoss: 0.286326\tGrad Norm: 1.885631\tLR: 0.030000\n",
      "Train Epoch: 1201 [188416/194182 (96%)]\tLoss: 0.287899\tGrad Norm: 1.845144\tLR: 0.030000\n",
      "Train set: Average loss: 0.2857\n",
      "Test set: Average loss: 0.2499, Average MAE: 0.3414\n",
      "Train Epoch: 1202 [4096/194182 (2%)]\tLoss: 0.289394\tGrad Norm: 1.700632\tLR: 0.030000\n",
      "Train Epoch: 1202 [24576/194182 (12%)]\tLoss: 0.278590\tGrad Norm: 1.252693\tLR: 0.030000\n",
      "Train Epoch: 1202 [45056/194182 (23%)]\tLoss: 0.280178\tGrad Norm: 1.458949\tLR: 0.030000\n",
      "Train Epoch: 1202 [65536/194182 (33%)]\tLoss: 0.275020\tGrad Norm: 1.292327\tLR: 0.030000\n",
      "Train Epoch: 1202 [86016/194182 (44%)]\tLoss: 0.280024\tGrad Norm: 1.391909\tLR: 0.030000\n",
      "Train Epoch: 1202 [106496/194182 (54%)]\tLoss: 0.286290\tGrad Norm: 1.374831\tLR: 0.030000\n",
      "Train Epoch: 1202 [126976/194182 (65%)]\tLoss: 0.273640\tGrad Norm: 1.241473\tLR: 0.030000\n",
      "Train Epoch: 1202 [147456/194182 (75%)]\tLoss: 0.286438\tGrad Norm: 1.521206\tLR: 0.030000\n",
      "Train Epoch: 1202 [167936/194182 (85%)]\tLoss: 0.291212\tGrad Norm: 1.432782\tLR: 0.030000\n",
      "Train Epoch: 1202 [188416/194182 (96%)]\tLoss: 0.283310\tGrad Norm: 1.501913\tLR: 0.030000\n",
      "Train set: Average loss: 0.2835\n",
      "Test set: Average loss: 0.2562, Average MAE: 0.3656\n",
      "Train Epoch: 1203 [4096/194182 (2%)]\tLoss: 0.295244\tGrad Norm: 1.942633\tLR: 0.030000\n",
      "Train Epoch: 1203 [24576/194182 (12%)]\tLoss: 0.282379\tGrad Norm: 1.575700\tLR: 0.030000\n",
      "Train Epoch: 1203 [45056/194182 (23%)]\tLoss: 0.284768\tGrad Norm: 1.398158\tLR: 0.030000\n",
      "Train Epoch: 1203 [65536/194182 (33%)]\tLoss: 0.279351\tGrad Norm: 1.357277\tLR: 0.030000\n",
      "Train Epoch: 1203 [86016/194182 (44%)]\tLoss: 0.280158\tGrad Norm: 1.546903\tLR: 0.030000\n",
      "Train Epoch: 1203 [106496/194182 (54%)]\tLoss: 0.275584\tGrad Norm: 1.174317\tLR: 0.030000\n",
      "Train Epoch: 1203 [126976/194182 (65%)]\tLoss: 0.283266\tGrad Norm: 1.118827\tLR: 0.030000\n",
      "Train Epoch: 1203 [147456/194182 (75%)]\tLoss: 0.284691\tGrad Norm: 1.189694\tLR: 0.030000\n",
      "Train Epoch: 1203 [167936/194182 (85%)]\tLoss: 0.282032\tGrad Norm: 1.243964\tLR: 0.030000\n",
      "Train Epoch: 1203 [188416/194182 (96%)]\tLoss: 0.289869\tGrad Norm: 1.551762\tLR: 0.030000\n",
      "Train set: Average loss: 0.2824\n",
      "Test set: Average loss: 0.2493, Average MAE: 0.3502\n",
      "Train Epoch: 1204 [4096/194182 (2%)]\tLoss: 0.290392\tGrad Norm: 1.696871\tLR: 0.030000\n",
      "Train Epoch: 1204 [24576/194182 (12%)]\tLoss: 0.284417\tGrad Norm: 1.491197\tLR: 0.030000\n",
      "Train Epoch: 1204 [45056/194182 (23%)]\tLoss: 0.279039\tGrad Norm: 1.400991\tLR: 0.030000\n",
      "Train Epoch: 1204 [65536/194182 (33%)]\tLoss: 0.284755\tGrad Norm: 1.502906\tLR: 0.030000\n",
      "Train Epoch: 1204 [86016/194182 (44%)]\tLoss: 0.282228\tGrad Norm: 1.406280\tLR: 0.030000\n",
      "Train Epoch: 1204 [106496/194182 (54%)]\tLoss: 0.294752\tGrad Norm: 1.923696\tLR: 0.030000\n",
      "Train Epoch: 1204 [126976/194182 (65%)]\tLoss: 0.288150\tGrad Norm: 1.803673\tLR: 0.030000\n",
      "Train Epoch: 1204 [147456/194182 (75%)]\tLoss: 0.286812\tGrad Norm: 1.427625\tLR: 0.030000\n",
      "Train Epoch: 1204 [167936/194182 (85%)]\tLoss: 0.284565\tGrad Norm: 1.505141\tLR: 0.030000\n",
      "Train Epoch: 1204 [188416/194182 (96%)]\tLoss: 0.279555\tGrad Norm: 1.522245\tLR: 0.030000\n",
      "Train set: Average loss: 0.2846\n",
      "Test set: Average loss: 0.2470, Average MAE: 0.3329\n",
      "Train Epoch: 1205 [4096/194182 (2%)]\tLoss: 0.288449\tGrad Norm: 1.548005\tLR: 0.030000\n",
      "Train Epoch: 1205 [24576/194182 (12%)]\tLoss: 0.287522\tGrad Norm: 1.477907\tLR: 0.030000\n",
      "Train Epoch: 1205 [45056/194182 (23%)]\tLoss: 0.275267\tGrad Norm: 1.316263\tLR: 0.030000\n",
      "Train Epoch: 1205 [65536/194182 (33%)]\tLoss: 0.292243\tGrad Norm: 1.572447\tLR: 0.030000\n",
      "Train Epoch: 1205 [86016/194182 (44%)]\tLoss: 0.282348\tGrad Norm: 1.447929\tLR: 0.030000\n",
      "Train Epoch: 1205 [106496/194182 (54%)]\tLoss: 0.284236\tGrad Norm: 1.353610\tLR: 0.030000\n",
      "Train Epoch: 1205 [126976/194182 (65%)]\tLoss: 0.287530\tGrad Norm: 1.565250\tLR: 0.030000\n",
      "Train Epoch: 1205 [147456/194182 (75%)]\tLoss: 0.289383\tGrad Norm: 1.587570\tLR: 0.030000\n",
      "Train Epoch: 1205 [167936/194182 (85%)]\tLoss: 0.285668\tGrad Norm: 1.321387\tLR: 0.030000\n",
      "Train Epoch: 1205 [188416/194182 (96%)]\tLoss: 0.275315\tGrad Norm: 1.155590\tLR: 0.030000\n",
      "Train set: Average loss: 0.2822\n",
      "Test set: Average loss: 0.2412, Average MAE: 0.3367\n",
      "Epoch 1205: Mean reward = 0.059 +/- 0.038\n",
      "Train Epoch: 1206 [4096/194182 (2%)]\tLoss: 0.280325\tGrad Norm: 1.189819\tLR: 0.030000\n",
      "Train Epoch: 1206 [24576/194182 (12%)]\tLoss: 0.281397\tGrad Norm: 1.549930\tLR: 0.030000\n",
      "Train Epoch: 1206 [45056/194182 (23%)]\tLoss: 0.289762\tGrad Norm: 1.661823\tLR: 0.030000\n",
      "Train Epoch: 1206 [65536/194182 (33%)]\tLoss: 0.288304\tGrad Norm: 1.775287\tLR: 0.030000\n",
      "Train Epoch: 1206 [86016/194182 (44%)]\tLoss: 0.279660\tGrad Norm: 1.366969\tLR: 0.030000\n",
      "Train Epoch: 1206 [106496/194182 (54%)]\tLoss: 0.282662\tGrad Norm: 1.519001\tLR: 0.030000\n",
      "Train Epoch: 1206 [126976/194182 (65%)]\tLoss: 0.287373\tGrad Norm: 1.378836\tLR: 0.030000\n",
      "Train Epoch: 1206 [147456/194182 (75%)]\tLoss: 0.284078\tGrad Norm: 1.853622\tLR: 0.030000\n",
      "Train Epoch: 1206 [167936/194182 (85%)]\tLoss: 0.296919\tGrad Norm: 1.967912\tLR: 0.030000\n",
      "Train Epoch: 1206 [188416/194182 (96%)]\tLoss: 0.282590\tGrad Norm: 1.082063\tLR: 0.030000\n",
      "Train set: Average loss: 0.2844\n",
      "Test set: Average loss: 0.2389, Average MAE: 0.3350\n",
      "Train Epoch: 1207 [4096/194182 (2%)]\tLoss: 0.278690\tGrad Norm: 1.170525\tLR: 0.030000\n",
      "Train Epoch: 1207 [24576/194182 (12%)]\tLoss: 0.276912\tGrad Norm: 1.128965\tLR: 0.030000\n",
      "Train Epoch: 1207 [45056/194182 (23%)]\tLoss: 0.277160\tGrad Norm: 1.615032\tLR: 0.030000\n",
      "Train Epoch: 1207 [65536/194182 (33%)]\tLoss: 0.280483\tGrad Norm: 1.438773\tLR: 0.030000\n",
      "Train Epoch: 1207 [86016/194182 (44%)]\tLoss: 0.274762\tGrad Norm: 1.554630\tLR: 0.030000\n",
      "Train Epoch: 1207 [106496/194182 (54%)]\tLoss: 0.290427\tGrad Norm: 1.767365\tLR: 0.030000\n",
      "Train Epoch: 1207 [126976/194182 (65%)]\tLoss: 0.279785\tGrad Norm: 1.477803\tLR: 0.030000\n",
      "Train Epoch: 1207 [147456/194182 (75%)]\tLoss: 0.278218\tGrad Norm: 1.319004\tLR: 0.030000\n",
      "Train Epoch: 1207 [167936/194182 (85%)]\tLoss: 0.277158\tGrad Norm: 1.460758\tLR: 0.030000\n",
      "Train Epoch: 1207 [188416/194182 (96%)]\tLoss: 0.282768\tGrad Norm: 1.694351\tLR: 0.030000\n",
      "Train set: Average loss: 0.2827\n",
      "Test set: Average loss: 0.2461, Average MAE: 0.3411\n",
      "Train Epoch: 1208 [4096/194182 (2%)]\tLoss: 0.286985\tGrad Norm: 1.450795\tLR: 0.030000\n",
      "Train Epoch: 1208 [24576/194182 (12%)]\tLoss: 0.277583\tGrad Norm: 1.269473\tLR: 0.030000\n",
      "Train Epoch: 1208 [45056/194182 (23%)]\tLoss: 0.280095\tGrad Norm: 1.507123\tLR: 0.030000\n",
      "Train Epoch: 1208 [65536/194182 (33%)]\tLoss: 0.296161\tGrad Norm: 1.635189\tLR: 0.030000\n",
      "Train Epoch: 1208 [86016/194182 (44%)]\tLoss: 0.281481\tGrad Norm: 1.615740\tLR: 0.030000\n",
      "Train Epoch: 1208 [106496/194182 (54%)]\tLoss: 0.287560\tGrad Norm: 1.655663\tLR: 0.030000\n",
      "Train Epoch: 1208 [126976/194182 (65%)]\tLoss: 0.283574\tGrad Norm: 1.484340\tLR: 0.030000\n",
      "Train Epoch: 1208 [147456/194182 (75%)]\tLoss: 0.285002\tGrad Norm: 1.367342\tLR: 0.030000\n",
      "Train Epoch: 1208 [167936/194182 (85%)]\tLoss: 0.285919\tGrad Norm: 1.435340\tLR: 0.030000\n",
      "Train Epoch: 1208 [188416/194182 (96%)]\tLoss: 0.282083\tGrad Norm: 1.429505\tLR: 0.030000\n",
      "Train set: Average loss: 0.2826\n",
      "Test set: Average loss: 0.2434, Average MAE: 0.3373\n",
      "Train Epoch: 1209 [4096/194182 (2%)]\tLoss: 0.281885\tGrad Norm: 1.373237\tLR: 0.030000\n",
      "Train Epoch: 1209 [24576/194182 (12%)]\tLoss: 0.278620\tGrad Norm: 1.424950\tLR: 0.030000\n",
      "Train Epoch: 1209 [45056/194182 (23%)]\tLoss: 0.289839\tGrad Norm: 1.594076\tLR: 0.030000\n",
      "Train Epoch: 1209 [65536/194182 (33%)]\tLoss: 0.284572\tGrad Norm: 1.748702\tLR: 0.030000\n",
      "Train Epoch: 1209 [86016/194182 (44%)]\tLoss: 0.294633\tGrad Norm: 1.820132\tLR: 0.030000\n",
      "Train Epoch: 1209 [106496/194182 (54%)]\tLoss: 0.288109\tGrad Norm: 1.647012\tLR: 0.030000\n",
      "Train Epoch: 1209 [126976/194182 (65%)]\tLoss: 0.282177\tGrad Norm: 1.407723\tLR: 0.030000\n",
      "Train Epoch: 1209 [147456/194182 (75%)]\tLoss: 0.284010\tGrad Norm: 1.374869\tLR: 0.030000\n",
      "Train Epoch: 1209 [167936/194182 (85%)]\tLoss: 0.279284\tGrad Norm: 1.398735\tLR: 0.030000\n",
      "Train Epoch: 1209 [188416/194182 (96%)]\tLoss: 0.281685\tGrad Norm: 1.274166\tLR: 0.030000\n",
      "Train set: Average loss: 0.2835\n",
      "Test set: Average loss: 0.2464, Average MAE: 0.3532\n",
      "Train Epoch: 1210 [4096/194182 (2%)]\tLoss: 0.281531\tGrad Norm: 1.425442\tLR: 0.030000\n",
      "Train Epoch: 1210 [24576/194182 (12%)]\tLoss: 0.276202\tGrad Norm: 1.025955\tLR: 0.030000\n",
      "Train Epoch: 1210 [45056/194182 (23%)]\tLoss: 0.268797\tGrad Norm: 1.468466\tLR: 0.030000\n",
      "Train Epoch: 1210 [65536/194182 (33%)]\tLoss: 0.276380\tGrad Norm: 1.530747\tLR: 0.030000\n",
      "Train Epoch: 1210 [86016/194182 (44%)]\tLoss: 0.279226\tGrad Norm: 1.640707\tLR: 0.030000\n",
      "Train Epoch: 1210 [106496/194182 (54%)]\tLoss: 0.286657\tGrad Norm: 1.608627\tLR: 0.030000\n",
      "Train Epoch: 1210 [126976/194182 (65%)]\tLoss: 0.281591\tGrad Norm: 1.478847\tLR: 0.030000\n",
      "Train Epoch: 1210 [147456/194182 (75%)]\tLoss: 0.278787\tGrad Norm: 1.603208\tLR: 0.030000\n",
      "Train Epoch: 1210 [167936/194182 (85%)]\tLoss: 0.289628\tGrad Norm: 1.500760\tLR: 0.030000\n",
      "Train Epoch: 1210 [188416/194182 (96%)]\tLoss: 0.286447\tGrad Norm: 1.596057\tLR: 0.030000\n",
      "Train set: Average loss: 0.2824\n",
      "Test set: Average loss: 0.2485, Average MAE: 0.3437\n",
      "Epoch 1210: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 1211 [4096/194182 (2%)]\tLoss: 0.282884\tGrad Norm: 1.800983\tLR: 0.030000\n",
      "Train Epoch: 1211 [24576/194182 (12%)]\tLoss: 0.277350\tGrad Norm: 1.149138\tLR: 0.030000\n",
      "Train Epoch: 1211 [45056/194182 (23%)]\tLoss: 0.284973\tGrad Norm: 1.530195\tLR: 0.030000\n",
      "Train Epoch: 1211 [65536/194182 (33%)]\tLoss: 0.278761\tGrad Norm: 1.590721\tLR: 0.030000\n",
      "Train Epoch: 1211 [86016/194182 (44%)]\tLoss: 0.281441\tGrad Norm: 1.605811\tLR: 0.030000\n",
      "Train Epoch: 1211 [106496/194182 (54%)]\tLoss: 0.279894\tGrad Norm: 1.386821\tLR: 0.030000\n",
      "Train Epoch: 1211 [126976/194182 (65%)]\tLoss: 0.280707\tGrad Norm: 1.333864\tLR: 0.030000\n",
      "Train Epoch: 1211 [147456/194182 (75%)]\tLoss: 0.275003\tGrad Norm: 1.199428\tLR: 0.030000\n",
      "Train Epoch: 1211 [167936/194182 (85%)]\tLoss: 0.278843\tGrad Norm: 1.489534\tLR: 0.030000\n",
      "Train Epoch: 1211 [188416/194182 (96%)]\tLoss: 0.289663\tGrad Norm: 1.754346\tLR: 0.030000\n",
      "Train set: Average loss: 0.2816\n",
      "Test set: Average loss: 0.2496, Average MAE: 0.3517\n",
      "Train Epoch: 1212 [4096/194182 (2%)]\tLoss: 0.280930\tGrad Norm: 1.540177\tLR: 0.030000\n",
      "Train Epoch: 1212 [24576/194182 (12%)]\tLoss: 0.278309\tGrad Norm: 1.237392\tLR: 0.030000\n",
      "Train Epoch: 1212 [45056/194182 (23%)]\tLoss: 0.275662\tGrad Norm: 1.045912\tLR: 0.030000\n",
      "Train Epoch: 1212 [65536/194182 (33%)]\tLoss: 0.286738\tGrad Norm: 1.383062\tLR: 0.030000\n",
      "Train Epoch: 1212 [86016/194182 (44%)]\tLoss: 0.285890\tGrad Norm: 1.485855\tLR: 0.030000\n",
      "Train Epoch: 1212 [106496/194182 (54%)]\tLoss: 0.280829\tGrad Norm: 1.523019\tLR: 0.030000\n",
      "Train Epoch: 1212 [126976/194182 (65%)]\tLoss: 0.282203\tGrad Norm: 1.648806\tLR: 0.030000\n",
      "Train Epoch: 1212 [147456/194182 (75%)]\tLoss: 0.278828\tGrad Norm: 1.603463\tLR: 0.030000\n",
      "Train Epoch: 1212 [167936/194182 (85%)]\tLoss: 0.284053\tGrad Norm: 1.823134\tLR: 0.030000\n",
      "Train Epoch: 1212 [188416/194182 (96%)]\tLoss: 0.287391\tGrad Norm: 1.981210\tLR: 0.030000\n",
      "Train set: Average loss: 0.2824\n",
      "Test set: Average loss: 0.2517, Average MAE: 0.3617\n",
      "Train Epoch: 1213 [4096/194182 (2%)]\tLoss: 0.292946\tGrad Norm: 1.786923\tLR: 0.030000\n",
      "Train Epoch: 1213 [24576/194182 (12%)]\tLoss: 0.278214\tGrad Norm: 1.536280\tLR: 0.030000\n",
      "Train Epoch: 1213 [45056/194182 (23%)]\tLoss: 0.293634\tGrad Norm: 1.932392\tLR: 0.030000\n",
      "Train Epoch: 1213 [65536/194182 (33%)]\tLoss: 0.284290\tGrad Norm: 1.699441\tLR: 0.030000\n",
      "Train Epoch: 1213 [86016/194182 (44%)]\tLoss: 0.287823\tGrad Norm: 1.688280\tLR: 0.030000\n",
      "Train Epoch: 1213 [106496/194182 (54%)]\tLoss: 0.283574\tGrad Norm: 1.458520\tLR: 0.030000\n",
      "Train Epoch: 1213 [126976/194182 (65%)]\tLoss: 0.286116\tGrad Norm: 1.454823\tLR: 0.030000\n",
      "Train Epoch: 1213 [147456/194182 (75%)]\tLoss: 0.284514\tGrad Norm: 1.633859\tLR: 0.030000\n",
      "Train Epoch: 1213 [167936/194182 (85%)]\tLoss: 0.287961\tGrad Norm: 1.470627\tLR: 0.030000\n",
      "Train Epoch: 1213 [188416/194182 (96%)]\tLoss: 0.288059\tGrad Norm: 1.518034\tLR: 0.030000\n",
      "Train set: Average loss: 0.2844\n",
      "Test set: Average loss: 0.2442, Average MAE: 0.3385\n",
      "Train Epoch: 1214 [4096/194182 (2%)]\tLoss: 0.287668\tGrad Norm: 1.420023\tLR: 0.030000\n",
      "Train Epoch: 1214 [24576/194182 (12%)]\tLoss: 0.287684\tGrad Norm: 1.624860\tLR: 0.030000\n",
      "Train Epoch: 1214 [45056/194182 (23%)]\tLoss: 0.286826\tGrad Norm: 1.571390\tLR: 0.030000\n",
      "Train Epoch: 1214 [65536/194182 (33%)]\tLoss: 0.278579\tGrad Norm: 1.556189\tLR: 0.030000\n",
      "Train Epoch: 1214 [86016/194182 (44%)]\tLoss: 0.276989\tGrad Norm: 1.379039\tLR: 0.030000\n",
      "Train Epoch: 1214 [106496/194182 (54%)]\tLoss: 0.280980\tGrad Norm: 1.631883\tLR: 0.030000\n",
      "Train Epoch: 1214 [126976/194182 (65%)]\tLoss: 0.283700\tGrad Norm: 1.831372\tLR: 0.030000\n",
      "Train Epoch: 1214 [147456/194182 (75%)]\tLoss: 0.284589\tGrad Norm: 1.728367\tLR: 0.030000\n",
      "Train Epoch: 1214 [167936/194182 (85%)]\tLoss: 0.277286\tGrad Norm: 1.299061\tLR: 0.030000\n",
      "Train Epoch: 1214 [188416/194182 (96%)]\tLoss: 0.287468\tGrad Norm: 1.449939\tLR: 0.030000\n",
      "Train set: Average loss: 0.2826\n",
      "Test set: Average loss: 0.2503, Average MAE: 0.3559\n",
      "Train Epoch: 1215 [4096/194182 (2%)]\tLoss: 0.282358\tGrad Norm: 1.635832\tLR: 0.030000\n",
      "Train Epoch: 1215 [24576/194182 (12%)]\tLoss: 0.285413\tGrad Norm: 1.375204\tLR: 0.030000\n",
      "Train Epoch: 1215 [45056/194182 (23%)]\tLoss: 0.286464\tGrad Norm: 1.495104\tLR: 0.030000\n",
      "Train Epoch: 1215 [65536/194182 (33%)]\tLoss: 0.288218\tGrad Norm: 1.487857\tLR: 0.030000\n",
      "Train Epoch: 1215 [86016/194182 (44%)]\tLoss: 0.281701\tGrad Norm: 1.400817\tLR: 0.030000\n",
      "Train Epoch: 1215 [106496/194182 (54%)]\tLoss: 0.282499\tGrad Norm: 1.618315\tLR: 0.030000\n",
      "Train Epoch: 1215 [126976/194182 (65%)]\tLoss: 0.284473\tGrad Norm: 1.501666\tLR: 0.030000\n",
      "Train Epoch: 1215 [147456/194182 (75%)]\tLoss: 0.278666\tGrad Norm: 1.451965\tLR: 0.030000\n",
      "Train Epoch: 1215 [167936/194182 (85%)]\tLoss: 0.280272\tGrad Norm: 1.611184\tLR: 0.030000\n",
      "Train Epoch: 1215 [188416/194182 (96%)]\tLoss: 0.272907\tGrad Norm: 1.340216\tLR: 0.030000\n",
      "Train set: Average loss: 0.2810\n",
      "Test set: Average loss: 0.2470, Average MAE: 0.3552\n",
      "Epoch 1215: Mean reward = 0.057 +/- 0.051\n",
      "Train Epoch: 1216 [4096/194182 (2%)]\tLoss: 0.282804\tGrad Norm: 1.566974\tLR: 0.030000\n",
      "Train Epoch: 1216 [24576/194182 (12%)]\tLoss: 0.283451\tGrad Norm: 1.409999\tLR: 0.030000\n",
      "Train Epoch: 1216 [45056/194182 (23%)]\tLoss: 0.289840\tGrad Norm: 1.913676\tLR: 0.030000\n",
      "Train Epoch: 1216 [65536/194182 (33%)]\tLoss: 0.284920\tGrad Norm: 1.593093\tLR: 0.030000\n",
      "Train Epoch: 1216 [86016/194182 (44%)]\tLoss: 0.274767\tGrad Norm: 1.141967\tLR: 0.030000\n",
      "Train Epoch: 1216 [106496/194182 (54%)]\tLoss: 0.282167\tGrad Norm: 1.508633\tLR: 0.030000\n",
      "Train Epoch: 1216 [126976/194182 (65%)]\tLoss: 0.282388\tGrad Norm: 1.196220\tLR: 0.030000\n",
      "Train Epoch: 1216 [147456/194182 (75%)]\tLoss: 0.282280\tGrad Norm: 1.592510\tLR: 0.030000\n",
      "Train Epoch: 1216 [167936/194182 (85%)]\tLoss: 0.289928\tGrad Norm: 1.727374\tLR: 0.030000\n",
      "Train Epoch: 1216 [188416/194182 (96%)]\tLoss: 0.281749\tGrad Norm: 1.588711\tLR: 0.030000\n",
      "Train set: Average loss: 0.2825\n",
      "Test set: Average loss: 0.2461, Average MAE: 0.3418\n",
      "Train Epoch: 1217 [4096/194182 (2%)]\tLoss: 0.278485\tGrad Norm: 1.370172\tLR: 0.030000\n",
      "Train Epoch: 1217 [24576/194182 (12%)]\tLoss: 0.286042\tGrad Norm: 1.755972\tLR: 0.030000\n",
      "Train Epoch: 1217 [45056/194182 (23%)]\tLoss: 0.278871\tGrad Norm: 1.165931\tLR: 0.030000\n",
      "Train Epoch: 1217 [65536/194182 (33%)]\tLoss: 0.265154\tGrad Norm: 1.198383\tLR: 0.030000\n",
      "Train Epoch: 1217 [86016/194182 (44%)]\tLoss: 0.275736\tGrad Norm: 0.933028\tLR: 0.030000\n",
      "Train Epoch: 1217 [106496/194182 (54%)]\tLoss: 0.283254\tGrad Norm: 1.469222\tLR: 0.030000\n",
      "Train Epoch: 1217 [126976/194182 (65%)]\tLoss: 0.290205\tGrad Norm: 1.742628\tLR: 0.030000\n",
      "Train Epoch: 1217 [147456/194182 (75%)]\tLoss: 0.274911\tGrad Norm: 1.302617\tLR: 0.030000\n",
      "Train Epoch: 1217 [167936/194182 (85%)]\tLoss: 0.289374\tGrad Norm: 1.713764\tLR: 0.030000\n",
      "Train Epoch: 1217 [188416/194182 (96%)]\tLoss: 0.276108\tGrad Norm: 1.501152\tLR: 0.030000\n",
      "Train set: Average loss: 0.2804\n",
      "Test set: Average loss: 0.2440, Average MAE: 0.3341\n",
      "Train Epoch: 1218 [4096/194182 (2%)]\tLoss: 0.279225\tGrad Norm: 1.291909\tLR: 0.030000\n",
      "Train Epoch: 1218 [24576/194182 (12%)]\tLoss: 0.278782\tGrad Norm: 1.412356\tLR: 0.030000\n",
      "Train Epoch: 1218 [45056/194182 (23%)]\tLoss: 0.284995\tGrad Norm: 1.829844\tLR: 0.030000\n",
      "Train Epoch: 1218 [65536/194182 (33%)]\tLoss: 0.288213\tGrad Norm: 1.972171\tLR: 0.030000\n",
      "Train Epoch: 1218 [86016/194182 (44%)]\tLoss: 0.274424\tGrad Norm: 1.431407\tLR: 0.030000\n",
      "Train Epoch: 1218 [106496/194182 (54%)]\tLoss: 0.277559\tGrad Norm: 1.434260\tLR: 0.030000\n",
      "Train Epoch: 1218 [126976/194182 (65%)]\tLoss: 0.279488\tGrad Norm: 1.182523\tLR: 0.030000\n",
      "Train Epoch: 1218 [147456/194182 (75%)]\tLoss: 0.275713\tGrad Norm: 1.261800\tLR: 0.030000\n",
      "Train Epoch: 1218 [167936/194182 (85%)]\tLoss: 0.288308\tGrad Norm: 1.852436\tLR: 0.030000\n",
      "Train Epoch: 1218 [188416/194182 (96%)]\tLoss: 0.278863\tGrad Norm: 1.482627\tLR: 0.030000\n",
      "Train set: Average loss: 0.2810\n",
      "Test set: Average loss: 0.2436, Average MAE: 0.3316\n",
      "Train Epoch: 1219 [4096/194182 (2%)]\tLoss: 0.284811\tGrad Norm: 1.565553\tLR: 0.030000\n",
      "Train Epoch: 1219 [24576/194182 (12%)]\tLoss: 0.282698\tGrad Norm: 1.748797\tLR: 0.030000\n",
      "Train Epoch: 1219 [45056/194182 (23%)]\tLoss: 0.281872\tGrad Norm: 1.355182\tLR: 0.030000\n",
      "Train Epoch: 1219 [65536/194182 (33%)]\tLoss: 0.278677\tGrad Norm: 1.567462\tLR: 0.030000\n",
      "Train Epoch: 1219 [86016/194182 (44%)]\tLoss: 0.279729\tGrad Norm: 1.132556\tLR: 0.030000\n",
      "Train Epoch: 1219 [106496/194182 (54%)]\tLoss: 0.277271\tGrad Norm: 1.230626\tLR: 0.030000\n",
      "Train Epoch: 1219 [126976/194182 (65%)]\tLoss: 0.284477\tGrad Norm: 1.628680\tLR: 0.030000\n",
      "Train Epoch: 1219 [147456/194182 (75%)]\tLoss: 0.278737\tGrad Norm: 1.478066\tLR: 0.030000\n",
      "Train Epoch: 1219 [167936/194182 (85%)]\tLoss: 0.283858\tGrad Norm: 1.427458\tLR: 0.030000\n",
      "Train Epoch: 1219 [188416/194182 (96%)]\tLoss: 0.287927\tGrad Norm: 1.519327\tLR: 0.030000\n",
      "Train set: Average loss: 0.2813\n",
      "Test set: Average loss: 0.2488, Average MAE: 0.3545\n",
      "Train Epoch: 1220 [4096/194182 (2%)]\tLoss: 0.284438\tGrad Norm: 1.567278\tLR: 0.030000\n",
      "Train Epoch: 1220 [24576/194182 (12%)]\tLoss: 0.282267\tGrad Norm: 1.749697\tLR: 0.030000\n",
      "Train Epoch: 1220 [45056/194182 (23%)]\tLoss: 0.281495\tGrad Norm: 1.543017\tLR: 0.030000\n",
      "Train Epoch: 1220 [65536/194182 (33%)]\tLoss: 0.280821\tGrad Norm: 1.330061\tLR: 0.030000\n",
      "Train Epoch: 1220 [86016/194182 (44%)]\tLoss: 0.278067\tGrad Norm: 1.167113\tLR: 0.030000\n",
      "Train Epoch: 1220 [106496/194182 (54%)]\tLoss: 0.283013\tGrad Norm: 1.614558\tLR: 0.030000\n",
      "Train Epoch: 1220 [126976/194182 (65%)]\tLoss: 0.270529\tGrad Norm: 1.751013\tLR: 0.030000\n",
      "Train Epoch: 1220 [147456/194182 (75%)]\tLoss: 0.276527\tGrad Norm: 1.263121\tLR: 0.030000\n",
      "Train Epoch: 1220 [167936/194182 (85%)]\tLoss: 0.270718\tGrad Norm: 0.886557\tLR: 0.030000\n",
      "Train Epoch: 1220 [188416/194182 (96%)]\tLoss: 0.279493\tGrad Norm: 1.672059\tLR: 0.030000\n",
      "Train set: Average loss: 0.2800\n",
      "Test set: Average loss: 0.2519, Average MAE: 0.3369\n",
      "Epoch 1220: Mean reward = 0.063 +/- 0.068\n",
      "Train Epoch: 1221 [4096/194182 (2%)]\tLoss: 0.283721\tGrad Norm: 1.901971\tLR: 0.030000\n",
      "Train Epoch: 1221 [24576/194182 (12%)]\tLoss: 0.280833\tGrad Norm: 1.649012\tLR: 0.030000\n",
      "Train Epoch: 1221 [45056/194182 (23%)]\tLoss: 0.277165\tGrad Norm: 1.182231\tLR: 0.030000\n",
      "Train Epoch: 1221 [65536/194182 (33%)]\tLoss: 0.283582\tGrad Norm: 1.177925\tLR: 0.030000\n",
      "Train Epoch: 1221 [86016/194182 (44%)]\tLoss: 0.277411\tGrad Norm: 1.381112\tLR: 0.030000\n",
      "Train Epoch: 1221 [106496/194182 (54%)]\tLoss: 0.285684\tGrad Norm: 1.587286\tLR: 0.030000\n",
      "Train Epoch: 1221 [126976/194182 (65%)]\tLoss: 0.275629\tGrad Norm: 1.304928\tLR: 0.030000\n",
      "Train Epoch: 1221 [147456/194182 (75%)]\tLoss: 0.274029\tGrad Norm: 1.297799\tLR: 0.030000\n",
      "Train Epoch: 1221 [167936/194182 (85%)]\tLoss: 0.281297\tGrad Norm: 1.574060\tLR: 0.030000\n",
      "Train Epoch: 1221 [188416/194182 (96%)]\tLoss: 0.283649\tGrad Norm: 1.562759\tLR: 0.030000\n",
      "Train set: Average loss: 0.2809\n",
      "Test set: Average loss: 0.2465, Average MAE: 0.3504\n",
      "Train Epoch: 1222 [4096/194182 (2%)]\tLoss: 0.281798\tGrad Norm: 1.449823\tLR: 0.030000\n",
      "Train Epoch: 1222 [24576/194182 (12%)]\tLoss: 0.278924\tGrad Norm: 1.249515\tLR: 0.030000\n",
      "Train Epoch: 1222 [45056/194182 (23%)]\tLoss: 0.281144\tGrad Norm: 1.598208\tLR: 0.030000\n",
      "Train Epoch: 1222 [65536/194182 (33%)]\tLoss: 0.279874\tGrad Norm: 1.633082\tLR: 0.030000\n",
      "Train Epoch: 1222 [86016/194182 (44%)]\tLoss: 0.287922\tGrad Norm: 1.486387\tLR: 0.030000\n",
      "Train Epoch: 1222 [106496/194182 (54%)]\tLoss: 0.277168\tGrad Norm: 1.438438\tLR: 0.030000\n",
      "Train Epoch: 1222 [126976/194182 (65%)]\tLoss: 0.280627\tGrad Norm: 1.840659\tLR: 0.030000\n",
      "Train Epoch: 1222 [147456/194182 (75%)]\tLoss: 0.291446\tGrad Norm: 2.317021\tLR: 0.030000\n",
      "Train Epoch: 1222 [167936/194182 (85%)]\tLoss: 0.279914\tGrad Norm: 1.708626\tLR: 0.030000\n",
      "Train Epoch: 1222 [188416/194182 (96%)]\tLoss: 0.284645\tGrad Norm: 1.576033\tLR: 0.030000\n",
      "Train set: Average loss: 0.2816\n",
      "Test set: Average loss: 0.2459, Average MAE: 0.3491\n",
      "Train Epoch: 1223 [4096/194182 (2%)]\tLoss: 0.277053\tGrad Norm: 1.388228\tLR: 0.030000\n",
      "Train Epoch: 1223 [24576/194182 (12%)]\tLoss: 0.289699\tGrad Norm: 1.824047\tLR: 0.030000\n",
      "Train Epoch: 1223 [45056/194182 (23%)]\tLoss: 0.277174\tGrad Norm: 1.359421\tLR: 0.030000\n",
      "Train Epoch: 1223 [65536/194182 (33%)]\tLoss: 0.287603\tGrad Norm: 1.750641\tLR: 0.030000\n",
      "Train Epoch: 1223 [86016/194182 (44%)]\tLoss: 0.281169\tGrad Norm: 1.697521\tLR: 0.030000\n",
      "Train Epoch: 1223 [106496/194182 (54%)]\tLoss: 0.270748\tGrad Norm: 1.048357\tLR: 0.030000\n",
      "Train Epoch: 1223 [126976/194182 (65%)]\tLoss: 0.275097\tGrad Norm: 1.086300\tLR: 0.030000\n",
      "Train Epoch: 1223 [147456/194182 (75%)]\tLoss: 0.282895\tGrad Norm: 1.606098\tLR: 0.030000\n",
      "Train Epoch: 1223 [167936/194182 (85%)]\tLoss: 0.290576\tGrad Norm: 1.681032\tLR: 0.030000\n",
      "Train Epoch: 1223 [188416/194182 (96%)]\tLoss: 0.275997\tGrad Norm: 1.276488\tLR: 0.030000\n",
      "Train set: Average loss: 0.2806\n",
      "Test set: Average loss: 0.2433, Average MAE: 0.3433\n",
      "Train Epoch: 1224 [4096/194182 (2%)]\tLoss: 0.273679\tGrad Norm: 1.241160\tLR: 0.030000\n",
      "Train Epoch: 1224 [24576/194182 (12%)]\tLoss: 0.278123\tGrad Norm: 1.580032\tLR: 0.030000\n",
      "Train Epoch: 1224 [45056/194182 (23%)]\tLoss: 0.274966\tGrad Norm: 1.389588\tLR: 0.030000\n",
      "Train Epoch: 1224 [65536/194182 (33%)]\tLoss: 0.285798\tGrad Norm: 1.793274\tLR: 0.030000\n",
      "Train Epoch: 1224 [86016/194182 (44%)]\tLoss: 0.274079\tGrad Norm: 1.326431\tLR: 0.030000\n",
      "Train Epoch: 1224 [106496/194182 (54%)]\tLoss: 0.271553\tGrad Norm: 1.508768\tLR: 0.030000\n",
      "Train Epoch: 1224 [126976/194182 (65%)]\tLoss: 0.288021\tGrad Norm: 1.932818\tLR: 0.030000\n",
      "Train Epoch: 1224 [147456/194182 (75%)]\tLoss: 0.292521\tGrad Norm: 1.946141\tLR: 0.030000\n",
      "Train Epoch: 1224 [167936/194182 (85%)]\tLoss: 0.276731\tGrad Norm: 1.574007\tLR: 0.030000\n",
      "Train Epoch: 1224 [188416/194182 (96%)]\tLoss: 0.274296\tGrad Norm: 1.216455\tLR: 0.030000\n",
      "Train set: Average loss: 0.2809\n",
      "Test set: Average loss: 0.2500, Average MAE: 0.3342\n",
      "Train Epoch: 1225 [4096/194182 (2%)]\tLoss: 0.286804\tGrad Norm: 1.780490\tLR: 0.030000\n",
      "Train Epoch: 1225 [24576/194182 (12%)]\tLoss: 0.278788\tGrad Norm: 1.482362\tLR: 0.030000\n",
      "Train Epoch: 1225 [45056/194182 (23%)]\tLoss: 0.275242\tGrad Norm: 1.749514\tLR: 0.030000\n",
      "Train Epoch: 1225 [65536/194182 (33%)]\tLoss: 0.280091\tGrad Norm: 1.583338\tLR: 0.030000\n",
      "Train Epoch: 1225 [86016/194182 (44%)]\tLoss: 0.272487\tGrad Norm: 1.503064\tLR: 0.030000\n",
      "Train Epoch: 1225 [106496/194182 (54%)]\tLoss: 0.279165\tGrad Norm: 1.329680\tLR: 0.030000\n",
      "Train Epoch: 1225 [126976/194182 (65%)]\tLoss: 0.278347\tGrad Norm: 1.324501\tLR: 0.030000\n",
      "Train Epoch: 1225 [147456/194182 (75%)]\tLoss: 0.283397\tGrad Norm: 1.412237\tLR: 0.030000\n",
      "Train Epoch: 1225 [167936/194182 (85%)]\tLoss: 0.271294\tGrad Norm: 1.386875\tLR: 0.030000\n",
      "Train Epoch: 1225 [188416/194182 (96%)]\tLoss: 0.279462\tGrad Norm: 1.280691\tLR: 0.030000\n",
      "Train set: Average loss: 0.2798\n",
      "Test set: Average loss: 0.2429, Average MAE: 0.3454\n",
      "Epoch 1225: Mean reward = 0.046 +/- 0.001\n",
      "Train Epoch: 1226 [4096/194182 (2%)]\tLoss: 0.272508\tGrad Norm: 1.184309\tLR: 0.030000\n",
      "Train Epoch: 1226 [24576/194182 (12%)]\tLoss: 0.281726\tGrad Norm: 1.533168\tLR: 0.030000\n",
      "Train Epoch: 1226 [45056/194182 (23%)]\tLoss: 0.276962\tGrad Norm: 1.117162\tLR: 0.030000\n",
      "Train Epoch: 1226 [65536/194182 (33%)]\tLoss: 0.269323\tGrad Norm: 1.003196\tLR: 0.030000\n",
      "Train Epoch: 1226 [86016/194182 (44%)]\tLoss: 0.277462\tGrad Norm: 1.232275\tLR: 0.030000\n",
      "Train Epoch: 1226 [106496/194182 (54%)]\tLoss: 0.277629\tGrad Norm: 1.678009\tLR: 0.030000\n",
      "Train Epoch: 1226 [126976/194182 (65%)]\tLoss: 0.276366\tGrad Norm: 1.265259\tLR: 0.030000\n",
      "Train Epoch: 1226 [147456/194182 (75%)]\tLoss: 0.280174\tGrad Norm: 1.328490\tLR: 0.030000\n",
      "Train Epoch: 1226 [167936/194182 (85%)]\tLoss: 0.285298\tGrad Norm: 1.837523\tLR: 0.030000\n",
      "Train Epoch: 1226 [188416/194182 (96%)]\tLoss: 0.294071\tGrad Norm: 1.845002\tLR: 0.030000\n",
      "Train set: Average loss: 0.2796\n",
      "Test set: Average loss: 0.2438, Average MAE: 0.3328\n",
      "Train Epoch: 1227 [4096/194182 (2%)]\tLoss: 0.284498\tGrad Norm: 1.595248\tLR: 0.030000\n",
      "Train Epoch: 1227 [24576/194182 (12%)]\tLoss: 0.281161\tGrad Norm: 1.734609\tLR: 0.030000\n",
      "Train Epoch: 1227 [45056/194182 (23%)]\tLoss: 0.286680\tGrad Norm: 1.758140\tLR: 0.030000\n",
      "Train Epoch: 1227 [65536/194182 (33%)]\tLoss: 0.279383\tGrad Norm: 1.554232\tLR: 0.030000\n",
      "Train Epoch: 1227 [86016/194182 (44%)]\tLoss: 0.285823\tGrad Norm: 1.586164\tLR: 0.030000\n",
      "Train Epoch: 1227 [106496/194182 (54%)]\tLoss: 0.288524\tGrad Norm: 1.812098\tLR: 0.030000\n",
      "Train Epoch: 1227 [126976/194182 (65%)]\tLoss: 0.273398\tGrad Norm: 1.092600\tLR: 0.030000\n",
      "Train Epoch: 1227 [147456/194182 (75%)]\tLoss: 0.274930\tGrad Norm: 1.420038\tLR: 0.030000\n",
      "Train Epoch: 1227 [167936/194182 (85%)]\tLoss: 0.275612\tGrad Norm: 1.300093\tLR: 0.030000\n",
      "Train Epoch: 1227 [188416/194182 (96%)]\tLoss: 0.279192\tGrad Norm: 1.234991\tLR: 0.030000\n",
      "Train set: Average loss: 0.2802\n",
      "Test set: Average loss: 0.2497, Average MAE: 0.3443\n",
      "Train Epoch: 1228 [4096/194182 (2%)]\tLoss: 0.278801\tGrad Norm: 1.508679\tLR: 0.030000\n",
      "Train Epoch: 1228 [24576/194182 (12%)]\tLoss: 0.277143\tGrad Norm: 1.588436\tLR: 0.030000\n",
      "Train Epoch: 1228 [45056/194182 (23%)]\tLoss: 0.284685\tGrad Norm: 1.426990\tLR: 0.030000\n",
      "Train Epoch: 1228 [65536/194182 (33%)]\tLoss: 0.272768\tGrad Norm: 1.134641\tLR: 0.030000\n",
      "Train Epoch: 1228 [86016/194182 (44%)]\tLoss: 0.274581\tGrad Norm: 1.455558\tLR: 0.030000\n",
      "Train Epoch: 1228 [106496/194182 (54%)]\tLoss: 0.268179\tGrad Norm: 1.159343\tLR: 0.030000\n",
      "Train Epoch: 1228 [126976/194182 (65%)]\tLoss: 0.284225\tGrad Norm: 1.611432\tLR: 0.030000\n",
      "Train Epoch: 1228 [147456/194182 (75%)]\tLoss: 0.285486\tGrad Norm: 1.672014\tLR: 0.030000\n",
      "Train Epoch: 1228 [167936/194182 (85%)]\tLoss: 0.288933\tGrad Norm: 1.348995\tLR: 0.030000\n",
      "Train Epoch: 1228 [188416/194182 (96%)]\tLoss: 0.274177\tGrad Norm: 1.532199\tLR: 0.030000\n",
      "Train set: Average loss: 0.2793\n",
      "Test set: Average loss: 0.2465, Average MAE: 0.3486\n",
      "Train Epoch: 1229 [4096/194182 (2%)]\tLoss: 0.282512\tGrad Norm: 1.385273\tLR: 0.030000\n",
      "Train Epoch: 1229 [24576/194182 (12%)]\tLoss: 0.284236\tGrad Norm: 1.701366\tLR: 0.030000\n",
      "Train Epoch: 1229 [45056/194182 (23%)]\tLoss: 0.275128\tGrad Norm: 1.309790\tLR: 0.030000\n",
      "Train Epoch: 1229 [65536/194182 (33%)]\tLoss: 0.276221\tGrad Norm: 1.331852\tLR: 0.030000\n",
      "Train Epoch: 1229 [86016/194182 (44%)]\tLoss: 0.272738\tGrad Norm: 1.317876\tLR: 0.030000\n",
      "Train Epoch: 1229 [106496/194182 (54%)]\tLoss: 0.283116\tGrad Norm: 1.538980\tLR: 0.030000\n",
      "Train Epoch: 1229 [126976/194182 (65%)]\tLoss: 0.277807\tGrad Norm: 1.516089\tLR: 0.030000\n",
      "Train Epoch: 1229 [147456/194182 (75%)]\tLoss: 0.289744\tGrad Norm: 1.717902\tLR: 0.030000\n",
      "Train Epoch: 1229 [167936/194182 (85%)]\tLoss: 0.285842\tGrad Norm: 1.973908\tLR: 0.030000\n",
      "Train Epoch: 1229 [188416/194182 (96%)]\tLoss: 0.279552\tGrad Norm: 1.568920\tLR: 0.030000\n",
      "Train set: Average loss: 0.2796\n",
      "Test set: Average loss: 0.2447, Average MAE: 0.3315\n",
      "Train Epoch: 1230 [4096/194182 (2%)]\tLoss: 0.285210\tGrad Norm: 1.687564\tLR: 0.030000\n",
      "Train Epoch: 1230 [24576/194182 (12%)]\tLoss: 0.280431\tGrad Norm: 1.528959\tLR: 0.030000\n",
      "Train Epoch: 1230 [45056/194182 (23%)]\tLoss: 0.276803\tGrad Norm: 1.387792\tLR: 0.030000\n",
      "Train Epoch: 1230 [65536/194182 (33%)]\tLoss: 0.275783\tGrad Norm: 1.326831\tLR: 0.030000\n",
      "Train Epoch: 1230 [86016/194182 (44%)]\tLoss: 0.274623\tGrad Norm: 1.552076\tLR: 0.030000\n",
      "Train Epoch: 1230 [106496/194182 (54%)]\tLoss: 0.280971\tGrad Norm: 1.415297\tLR: 0.030000\n",
      "Train Epoch: 1230 [126976/194182 (65%)]\tLoss: 0.275447\tGrad Norm: 1.202081\tLR: 0.030000\n",
      "Train Epoch: 1230 [147456/194182 (75%)]\tLoss: 0.281308\tGrad Norm: 1.600292\tLR: 0.030000\n",
      "Train Epoch: 1230 [167936/194182 (85%)]\tLoss: 0.273304\tGrad Norm: 1.461096\tLR: 0.030000\n",
      "Train Epoch: 1230 [188416/194182 (96%)]\tLoss: 0.275072\tGrad Norm: 1.243499\tLR: 0.030000\n",
      "Train set: Average loss: 0.2778\n",
      "Test set: Average loss: 0.2447, Average MAE: 0.3418\n",
      "Epoch 1230: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 1231 [4096/194182 (2%)]\tLoss: 0.283945\tGrad Norm: 1.464130\tLR: 0.030000\n",
      "Train Epoch: 1231 [24576/194182 (12%)]\tLoss: 0.274280\tGrad Norm: 1.266115\tLR: 0.030000\n",
      "Train Epoch: 1231 [45056/194182 (23%)]\tLoss: 0.281358\tGrad Norm: 1.435731\tLR: 0.030000\n",
      "Train Epoch: 1231 [65536/194182 (33%)]\tLoss: 0.276818\tGrad Norm: 1.670339\tLR: 0.030000\n",
      "Train Epoch: 1231 [86016/194182 (44%)]\tLoss: 0.268944\tGrad Norm: 1.529754\tLR: 0.030000\n",
      "Train Epoch: 1231 [106496/194182 (54%)]\tLoss: 0.285468\tGrad Norm: 1.883241\tLR: 0.030000\n",
      "Train Epoch: 1231 [126976/194182 (65%)]\tLoss: 0.283444\tGrad Norm: 1.641181\tLR: 0.030000\n",
      "Train Epoch: 1231 [147456/194182 (75%)]\tLoss: 0.278119\tGrad Norm: 1.503871\tLR: 0.030000\n",
      "Train Epoch: 1231 [167936/194182 (85%)]\tLoss: 0.277973\tGrad Norm: 1.277052\tLR: 0.030000\n",
      "Train Epoch: 1231 [188416/194182 (96%)]\tLoss: 0.282581\tGrad Norm: 1.678640\tLR: 0.030000\n",
      "Train set: Average loss: 0.2800\n",
      "Test set: Average loss: 0.2485, Average MAE: 0.3403\n",
      "Train Epoch: 1232 [4096/194182 (2%)]\tLoss: 0.279194\tGrad Norm: 1.646804\tLR: 0.030000\n",
      "Train Epoch: 1232 [24576/194182 (12%)]\tLoss: 0.275594\tGrad Norm: 1.230074\tLR: 0.030000\n",
      "Train Epoch: 1232 [45056/194182 (23%)]\tLoss: 0.278886\tGrad Norm: 1.204979\tLR: 0.030000\n",
      "Train Epoch: 1232 [65536/194182 (33%)]\tLoss: 0.277375\tGrad Norm: 1.659107\tLR: 0.030000\n",
      "Train Epoch: 1232 [86016/194182 (44%)]\tLoss: 0.278840\tGrad Norm: 1.363123\tLR: 0.030000\n",
      "Train Epoch: 1232 [106496/194182 (54%)]\tLoss: 0.281047\tGrad Norm: 1.541712\tLR: 0.030000\n",
      "Train Epoch: 1232 [126976/194182 (65%)]\tLoss: 0.293066\tGrad Norm: 1.922398\tLR: 0.030000\n",
      "Train Epoch: 1232 [147456/194182 (75%)]\tLoss: 0.278647\tGrad Norm: 1.240326\tLR: 0.030000\n",
      "Train Epoch: 1232 [167936/194182 (85%)]\tLoss: 0.279576\tGrad Norm: 1.756301\tLR: 0.030000\n",
      "Train Epoch: 1232 [188416/194182 (96%)]\tLoss: 0.280906\tGrad Norm: 1.458040\tLR: 0.030000\n",
      "Train set: Average loss: 0.2785\n",
      "Test set: Average loss: 0.2439, Average MAE: 0.3361\n",
      "Train Epoch: 1233 [4096/194182 (2%)]\tLoss: 0.274907\tGrad Norm: 1.492406\tLR: 0.030000\n",
      "Train Epoch: 1233 [24576/194182 (12%)]\tLoss: 0.276744\tGrad Norm: 1.468013\tLR: 0.030000\n",
      "Train Epoch: 1233 [45056/194182 (23%)]\tLoss: 0.269621\tGrad Norm: 1.032019\tLR: 0.030000\n",
      "Train Epoch: 1233 [65536/194182 (33%)]\tLoss: 0.282278\tGrad Norm: 1.448916\tLR: 0.030000\n",
      "Train Epoch: 1233 [86016/194182 (44%)]\tLoss: 0.270749\tGrad Norm: 1.496641\tLR: 0.030000\n",
      "Train Epoch: 1233 [106496/194182 (54%)]\tLoss: 0.282267\tGrad Norm: 1.687775\tLR: 0.030000\n",
      "Train Epoch: 1233 [126976/194182 (65%)]\tLoss: 0.283023\tGrad Norm: 1.550253\tLR: 0.030000\n",
      "Train Epoch: 1233 [147456/194182 (75%)]\tLoss: 0.278585\tGrad Norm: 1.160158\tLR: 0.030000\n",
      "Train Epoch: 1233 [167936/194182 (85%)]\tLoss: 0.277426\tGrad Norm: 1.388020\tLR: 0.030000\n",
      "Train Epoch: 1233 [188416/194182 (96%)]\tLoss: 0.278724\tGrad Norm: 1.551292\tLR: 0.030000\n",
      "Train set: Average loss: 0.2780\n",
      "Test set: Average loss: 0.2560, Average MAE: 0.3495\n",
      "Train Epoch: 1234 [4096/194182 (2%)]\tLoss: 0.298853\tGrad Norm: 2.041073\tLR: 0.030000\n",
      "Train Epoch: 1234 [24576/194182 (12%)]\tLoss: 0.275697\tGrad Norm: 1.295793\tLR: 0.030000\n",
      "Train Epoch: 1234 [45056/194182 (23%)]\tLoss: 0.263561\tGrad Norm: 0.901306\tLR: 0.030000\n",
      "Train Epoch: 1234 [65536/194182 (33%)]\tLoss: 0.272650\tGrad Norm: 1.180533\tLR: 0.030000\n",
      "Train Epoch: 1234 [86016/194182 (44%)]\tLoss: 0.270734\tGrad Norm: 1.392586\tLR: 0.030000\n",
      "Train Epoch: 1234 [106496/194182 (54%)]\tLoss: 0.273497\tGrad Norm: 1.238355\tLR: 0.030000\n",
      "Train Epoch: 1234 [126976/194182 (65%)]\tLoss: 0.276780\tGrad Norm: 1.424514\tLR: 0.030000\n",
      "Train Epoch: 1234 [147456/194182 (75%)]\tLoss: 0.274930\tGrad Norm: 1.436492\tLR: 0.030000\n",
      "Train Epoch: 1234 [167936/194182 (85%)]\tLoss: 0.272967\tGrad Norm: 1.494949\tLR: 0.030000\n",
      "Train Epoch: 1234 [188416/194182 (96%)]\tLoss: 0.285176\tGrad Norm: 1.813886\tLR: 0.030000\n",
      "Train set: Average loss: 0.2764\n",
      "Test set: Average loss: 0.2583, Average MAE: 0.3642\n",
      "Train Epoch: 1235 [4096/194182 (2%)]\tLoss: 0.287733\tGrad Norm: 2.008949\tLR: 0.030000\n",
      "Train Epoch: 1235 [24576/194182 (12%)]\tLoss: 0.288974\tGrad Norm: 1.802454\tLR: 0.030000\n",
      "Train Epoch: 1235 [45056/194182 (23%)]\tLoss: 0.280999\tGrad Norm: 1.530343\tLR: 0.030000\n",
      "Train Epoch: 1235 [65536/194182 (33%)]\tLoss: 0.280249\tGrad Norm: 1.788699\tLR: 0.030000\n",
      "Train Epoch: 1235 [86016/194182 (44%)]\tLoss: 0.274508\tGrad Norm: 1.626148\tLR: 0.030000\n",
      "Train Epoch: 1235 [106496/194182 (54%)]\tLoss: 0.270303\tGrad Norm: 1.087071\tLR: 0.030000\n",
      "Train Epoch: 1235 [126976/194182 (65%)]\tLoss: 0.275514\tGrad Norm: 1.355826\tLR: 0.030000\n",
      "Train Epoch: 1235 [147456/194182 (75%)]\tLoss: 0.274779\tGrad Norm: 1.261602\tLR: 0.030000\n",
      "Train Epoch: 1235 [167936/194182 (85%)]\tLoss: 0.278557\tGrad Norm: 1.475614\tLR: 0.030000\n",
      "Train Epoch: 1235 [188416/194182 (96%)]\tLoss: 0.274347\tGrad Norm: 1.474095\tLR: 0.030000\n",
      "Train set: Average loss: 0.2782\n",
      "Test set: Average loss: 0.2507, Average MAE: 0.3410\n",
      "Epoch 1235: Mean reward = 0.037 +/- 0.019\n",
      "Train Epoch: 1236 [4096/194182 (2%)]\tLoss: 0.280542\tGrad Norm: 1.622134\tLR: 0.030000\n",
      "Train Epoch: 1236 [24576/194182 (12%)]\tLoss: 0.269239\tGrad Norm: 1.098046\tLR: 0.030000\n",
      "Train Epoch: 1236 [45056/194182 (23%)]\tLoss: 0.280997\tGrad Norm: 1.520242\tLR: 0.030000\n",
      "Train Epoch: 1236 [65536/194182 (33%)]\tLoss: 0.279349\tGrad Norm: 1.649177\tLR: 0.030000\n",
      "Train Epoch: 1236 [86016/194182 (44%)]\tLoss: 0.272465\tGrad Norm: 1.533843\tLR: 0.030000\n",
      "Train Epoch: 1236 [106496/194182 (54%)]\tLoss: 0.282191\tGrad Norm: 2.002049\tLR: 0.030000\n",
      "Train Epoch: 1236 [126976/194182 (65%)]\tLoss: 0.283691\tGrad Norm: 1.929592\tLR: 0.030000\n",
      "Train Epoch: 1236 [147456/194182 (75%)]\tLoss: 0.279972\tGrad Norm: 1.720958\tLR: 0.030000\n",
      "Train Epoch: 1236 [167936/194182 (85%)]\tLoss: 0.284138\tGrad Norm: 1.595049\tLR: 0.030000\n",
      "Train Epoch: 1236 [188416/194182 (96%)]\tLoss: 0.276352\tGrad Norm: 1.327412\tLR: 0.030000\n",
      "Train set: Average loss: 0.2787\n",
      "Test set: Average loss: 0.2454, Average MAE: 0.3363\n",
      "Train Epoch: 1237 [4096/194182 (2%)]\tLoss: 0.273605\tGrad Norm: 1.499084\tLR: 0.030000\n",
      "Train Epoch: 1237 [24576/194182 (12%)]\tLoss: 0.288518\tGrad Norm: 1.689969\tLR: 0.030000\n",
      "Train Epoch: 1237 [45056/194182 (23%)]\tLoss: 0.284229\tGrad Norm: 1.606312\tLR: 0.030000\n",
      "Train Epoch: 1237 [65536/194182 (33%)]\tLoss: 0.276537\tGrad Norm: 1.664588\tLR: 0.030000\n",
      "Train Epoch: 1237 [86016/194182 (44%)]\tLoss: 0.276972\tGrad Norm: 1.102215\tLR: 0.030000\n",
      "Train Epoch: 1237 [106496/194182 (54%)]\tLoss: 0.278158\tGrad Norm: 1.493130\tLR: 0.030000\n",
      "Train Epoch: 1237 [126976/194182 (65%)]\tLoss: 0.269620\tGrad Norm: 1.378395\tLR: 0.030000\n",
      "Train Epoch: 1237 [147456/194182 (75%)]\tLoss: 0.283263\tGrad Norm: 1.557272\tLR: 0.030000\n",
      "Train Epoch: 1237 [167936/194182 (85%)]\tLoss: 0.276724\tGrad Norm: 1.685712\tLR: 0.030000\n",
      "Train Epoch: 1237 [188416/194182 (96%)]\tLoss: 0.286240\tGrad Norm: 1.732832\tLR: 0.030000\n",
      "Train set: Average loss: 0.2791\n",
      "Test set: Average loss: 0.2511, Average MAE: 0.3567\n",
      "Train Epoch: 1238 [4096/194182 (2%)]\tLoss: 0.280407\tGrad Norm: 1.665873\tLR: 0.030000\n",
      "Train Epoch: 1238 [24576/194182 (12%)]\tLoss: 0.289860\tGrad Norm: 2.157839\tLR: 0.030000\n",
      "Train Epoch: 1238 [45056/194182 (23%)]\tLoss: 0.277482\tGrad Norm: 1.491364\tLR: 0.030000\n",
      "Train Epoch: 1238 [65536/194182 (33%)]\tLoss: 0.274441\tGrad Norm: 1.179275\tLR: 0.030000\n",
      "Train Epoch: 1238 [86016/194182 (44%)]\tLoss: 0.273090\tGrad Norm: 1.223839\tLR: 0.030000\n",
      "Train Epoch: 1238 [106496/194182 (54%)]\tLoss: 0.274106\tGrad Norm: 1.526956\tLR: 0.030000\n",
      "Train Epoch: 1238 [126976/194182 (65%)]\tLoss: 0.280045\tGrad Norm: 1.638244\tLR: 0.030000\n",
      "Train Epoch: 1238 [147456/194182 (75%)]\tLoss: 0.288683\tGrad Norm: 1.583681\tLR: 0.030000\n",
      "Train Epoch: 1238 [167936/194182 (85%)]\tLoss: 0.277657\tGrad Norm: 1.639702\tLR: 0.030000\n",
      "Train Epoch: 1238 [188416/194182 (96%)]\tLoss: 0.276388\tGrad Norm: 1.470762\tLR: 0.030000\n",
      "Train set: Average loss: 0.2790\n",
      "Test set: Average loss: 0.2425, Average MAE: 0.3416\n",
      "Train Epoch: 1239 [4096/194182 (2%)]\tLoss: 0.273142\tGrad Norm: 1.146093\tLR: 0.030000\n",
      "Train Epoch: 1239 [24576/194182 (12%)]\tLoss: 0.281525\tGrad Norm: 1.325715\tLR: 0.030000\n",
      "Train Epoch: 1239 [45056/194182 (23%)]\tLoss: 0.271507\tGrad Norm: 1.392202\tLR: 0.030000\n",
      "Train Epoch: 1239 [65536/194182 (33%)]\tLoss: 0.283820\tGrad Norm: 1.750334\tLR: 0.030000\n",
      "Train Epoch: 1239 [86016/194182 (44%)]\tLoss: 0.277960\tGrad Norm: 1.538027\tLR: 0.030000\n",
      "Train Epoch: 1239 [106496/194182 (54%)]\tLoss: 0.276953\tGrad Norm: 1.277508\tLR: 0.030000\n",
      "Train Epoch: 1239 [126976/194182 (65%)]\tLoss: 0.276134\tGrad Norm: 1.373498\tLR: 0.030000\n",
      "Train Epoch: 1239 [147456/194182 (75%)]\tLoss: 0.276189\tGrad Norm: 1.368123\tLR: 0.030000\n",
      "Train Epoch: 1239 [167936/194182 (85%)]\tLoss: 0.281066\tGrad Norm: 1.588625\tLR: 0.030000\n",
      "Train Epoch: 1239 [188416/194182 (96%)]\tLoss: 0.278980\tGrad Norm: 1.376262\tLR: 0.030000\n",
      "Train set: Average loss: 0.2762\n",
      "Test set: Average loss: 0.2456, Average MAE: 0.3467\n",
      "Train Epoch: 1240 [4096/194182 (2%)]\tLoss: 0.278592\tGrad Norm: 1.343655\tLR: 0.030000\n",
      "Train Epoch: 1240 [24576/194182 (12%)]\tLoss: 0.270716\tGrad Norm: 1.247430\tLR: 0.030000\n",
      "Train Epoch: 1240 [45056/194182 (23%)]\tLoss: 0.279776\tGrad Norm: 1.634887\tLR: 0.030000\n",
      "Train Epoch: 1240 [65536/194182 (33%)]\tLoss: 0.279368\tGrad Norm: 1.687887\tLR: 0.030000\n",
      "Train Epoch: 1240 [86016/194182 (44%)]\tLoss: 0.276070\tGrad Norm: 1.496526\tLR: 0.030000\n",
      "Train Epoch: 1240 [106496/194182 (54%)]\tLoss: 0.273742\tGrad Norm: 1.302221\tLR: 0.030000\n",
      "Train Epoch: 1240 [126976/194182 (65%)]\tLoss: 0.275533\tGrad Norm: 1.499439\tLR: 0.030000\n",
      "Train Epoch: 1240 [147456/194182 (75%)]\tLoss: 0.285094\tGrad Norm: 1.697494\tLR: 0.030000\n",
      "Train Epoch: 1240 [167936/194182 (85%)]\tLoss: 0.277883\tGrad Norm: 1.749366\tLR: 0.030000\n",
      "Train Epoch: 1240 [188416/194182 (96%)]\tLoss: 0.282345\tGrad Norm: 1.901524\tLR: 0.030000\n",
      "Train set: Average loss: 0.2785\n",
      "Test set: Average loss: 0.2480, Average MAE: 0.3347\n",
      "Epoch 1240: Mean reward = 0.050 +/- 0.047\n",
      "Train Epoch: 1241 [4096/194182 (2%)]\tLoss: 0.273967\tGrad Norm: 1.713975\tLR: 0.030000\n",
      "Train Epoch: 1241 [24576/194182 (12%)]\tLoss: 0.276822\tGrad Norm: 1.464241\tLR: 0.030000\n",
      "Train Epoch: 1241 [45056/194182 (23%)]\tLoss: 0.274266\tGrad Norm: 1.378117\tLR: 0.030000\n",
      "Train Epoch: 1241 [65536/194182 (33%)]\tLoss: 0.268754\tGrad Norm: 1.131289\tLR: 0.030000\n",
      "Train Epoch: 1241 [86016/194182 (44%)]\tLoss: 0.277830\tGrad Norm: 1.183341\tLR: 0.030000\n",
      "Train Epoch: 1241 [106496/194182 (54%)]\tLoss: 0.279132\tGrad Norm: 1.713987\tLR: 0.030000\n",
      "Train Epoch: 1241 [126976/194182 (65%)]\tLoss: 0.281111\tGrad Norm: 1.571255\tLR: 0.030000\n",
      "Train Epoch: 1241 [147456/194182 (75%)]\tLoss: 0.270996\tGrad Norm: 1.132259\tLR: 0.030000\n",
      "Train Epoch: 1241 [167936/194182 (85%)]\tLoss: 0.272138\tGrad Norm: 1.609369\tLR: 0.030000\n",
      "Train Epoch: 1241 [188416/194182 (96%)]\tLoss: 0.276038\tGrad Norm: 1.274577\tLR: 0.030000\n",
      "Train set: Average loss: 0.2765\n",
      "Test set: Average loss: 0.2432, Average MAE: 0.3368\n",
      "Train Epoch: 1242 [4096/194182 (2%)]\tLoss: 0.271824\tGrad Norm: 1.363192\tLR: 0.030000\n",
      "Train Epoch: 1242 [24576/194182 (12%)]\tLoss: 0.268909\tGrad Norm: 1.557645\tLR: 0.030000\n",
      "Train Epoch: 1242 [45056/194182 (23%)]\tLoss: 0.277017\tGrad Norm: 1.649615\tLR: 0.030000\n",
      "Train Epoch: 1242 [65536/194182 (33%)]\tLoss: 0.288587\tGrad Norm: 2.106040\tLR: 0.030000\n",
      "Train Epoch: 1242 [86016/194182 (44%)]\tLoss: 0.288095\tGrad Norm: 2.177947\tLR: 0.030000\n",
      "Train Epoch: 1242 [106496/194182 (54%)]\tLoss: 0.279328\tGrad Norm: 1.603007\tLR: 0.030000\n",
      "Train Epoch: 1242 [126976/194182 (65%)]\tLoss: 0.271599\tGrad Norm: 1.529935\tLR: 0.030000\n",
      "Train Epoch: 1242 [147456/194182 (75%)]\tLoss: 0.271341\tGrad Norm: 1.264770\tLR: 0.030000\n",
      "Train Epoch: 1242 [167936/194182 (85%)]\tLoss: 0.260589\tGrad Norm: 0.980889\tLR: 0.030000\n",
      "Train Epoch: 1242 [188416/194182 (96%)]\tLoss: 0.270367\tGrad Norm: 0.878863\tLR: 0.030000\n",
      "Train set: Average loss: 0.2767\n",
      "Test set: Average loss: 0.2431, Average MAE: 0.3449\n",
      "Train Epoch: 1243 [4096/194182 (2%)]\tLoss: 0.273323\tGrad Norm: 1.247196\tLR: 0.030000\n",
      "Train Epoch: 1243 [24576/194182 (12%)]\tLoss: 0.277460\tGrad Norm: 1.181428\tLR: 0.030000\n",
      "Train Epoch: 1243 [45056/194182 (23%)]\tLoss: 0.271951\tGrad Norm: 1.276235\tLR: 0.030000\n",
      "Train Epoch: 1243 [65536/194182 (33%)]\tLoss: 0.280050\tGrad Norm: 1.529478\tLR: 0.030000\n",
      "Train Epoch: 1243 [86016/194182 (44%)]\tLoss: 0.281869\tGrad Norm: 1.513774\tLR: 0.030000\n",
      "Train Epoch: 1243 [106496/194182 (54%)]\tLoss: 0.278271\tGrad Norm: 1.663091\tLR: 0.030000\n",
      "Train Epoch: 1243 [126976/194182 (65%)]\tLoss: 0.282467\tGrad Norm: 1.822563\tLR: 0.030000\n",
      "Train Epoch: 1243 [147456/194182 (75%)]\tLoss: 0.277022\tGrad Norm: 1.663808\tLR: 0.030000\n",
      "Train Epoch: 1243 [167936/194182 (85%)]\tLoss: 0.279043\tGrad Norm: 1.932565\tLR: 0.030000\n",
      "Train Epoch: 1243 [188416/194182 (96%)]\tLoss: 0.281721\tGrad Norm: 1.994369\tLR: 0.030000\n",
      "Train set: Average loss: 0.2784\n",
      "Test set: Average loss: 0.2551, Average MAE: 0.3361\n",
      "Train Epoch: 1244 [4096/194182 (2%)]\tLoss: 0.288652\tGrad Norm: 2.144250\tLR: 0.030000\n",
      "Train Epoch: 1244 [24576/194182 (12%)]\tLoss: 0.280580\tGrad Norm: 1.882289\tLR: 0.030000\n",
      "Train Epoch: 1244 [45056/194182 (23%)]\tLoss: 0.273308\tGrad Norm: 1.520146\tLR: 0.030000\n",
      "Train Epoch: 1244 [65536/194182 (33%)]\tLoss: 0.267181\tGrad Norm: 0.931026\tLR: 0.030000\n",
      "Train Epoch: 1244 [86016/194182 (44%)]\tLoss: 0.267500\tGrad Norm: 1.134558\tLR: 0.030000\n",
      "Train Epoch: 1244 [106496/194182 (54%)]\tLoss: 0.276503\tGrad Norm: 1.479612\tLR: 0.030000\n",
      "Train Epoch: 1244 [126976/194182 (65%)]\tLoss: 0.273811\tGrad Norm: 1.576954\tLR: 0.030000\n",
      "Train Epoch: 1244 [147456/194182 (75%)]\tLoss: 0.275242\tGrad Norm: 1.548192\tLR: 0.030000\n",
      "Train Epoch: 1244 [167936/194182 (85%)]\tLoss: 0.272581\tGrad Norm: 1.449439\tLR: 0.030000\n",
      "Train Epoch: 1244 [188416/194182 (96%)]\tLoss: 0.274748\tGrad Norm: 1.427713\tLR: 0.030000\n",
      "Train set: Average loss: 0.2763\n",
      "Test set: Average loss: 0.2454, Average MAE: 0.3377\n",
      "Train Epoch: 1245 [4096/194182 (2%)]\tLoss: 0.276505\tGrad Norm: 1.549757\tLR: 0.030000\n",
      "Train Epoch: 1245 [24576/194182 (12%)]\tLoss: 0.276174\tGrad Norm: 1.466002\tLR: 0.030000\n",
      "Train Epoch: 1245 [45056/194182 (23%)]\tLoss: 0.272135\tGrad Norm: 1.176890\tLR: 0.030000\n",
      "Train Epoch: 1245 [65536/194182 (33%)]\tLoss: 0.280023\tGrad Norm: 1.811609\tLR: 0.030000\n",
      "Train Epoch: 1245 [86016/194182 (44%)]\tLoss: 0.278832\tGrad Norm: 1.746693\tLR: 0.030000\n",
      "Train Epoch: 1245 [106496/194182 (54%)]\tLoss: 0.284413\tGrad Norm: 1.730005\tLR: 0.030000\n",
      "Train Epoch: 1245 [126976/194182 (65%)]\tLoss: 0.267635\tGrad Norm: 1.324440\tLR: 0.030000\n",
      "Train Epoch: 1245 [147456/194182 (75%)]\tLoss: 0.285676\tGrad Norm: 1.688702\tLR: 0.030000\n",
      "Train Epoch: 1245 [167936/194182 (85%)]\tLoss: 0.286029\tGrad Norm: 1.836656\tLR: 0.030000\n",
      "Train Epoch: 1245 [188416/194182 (96%)]\tLoss: 0.271266\tGrad Norm: 1.324520\tLR: 0.030000\n",
      "Train set: Average loss: 0.2781\n",
      "Test set: Average loss: 0.2444, Average MAE: 0.3404\n",
      "Epoch 1245: Mean reward = 0.063 +/- 0.067\n",
      "Train Epoch: 1246 [4096/194182 (2%)]\tLoss: 0.275644\tGrad Norm: 1.350845\tLR: 0.030000\n",
      "Train Epoch: 1246 [24576/194182 (12%)]\tLoss: 0.277136\tGrad Norm: 1.550485\tLR: 0.030000\n",
      "Train Epoch: 1246 [45056/194182 (23%)]\tLoss: 0.277361\tGrad Norm: 1.664828\tLR: 0.030000\n",
      "Train Epoch: 1246 [65536/194182 (33%)]\tLoss: 0.278302\tGrad Norm: 1.612162\tLR: 0.030000\n",
      "Train Epoch: 1246 [86016/194182 (44%)]\tLoss: 0.263399\tGrad Norm: 1.074527\tLR: 0.030000\n",
      "Train Epoch: 1246 [106496/194182 (54%)]\tLoss: 0.280651\tGrad Norm: 1.458417\tLR: 0.030000\n",
      "Train Epoch: 1246 [126976/194182 (65%)]\tLoss: 0.277352\tGrad Norm: 1.367699\tLR: 0.030000\n",
      "Train Epoch: 1246 [147456/194182 (75%)]\tLoss: 0.282801\tGrad Norm: 1.810648\tLR: 0.030000\n",
      "Train Epoch: 1246 [167936/194182 (85%)]\tLoss: 0.280817\tGrad Norm: 1.498041\tLR: 0.030000\n",
      "Train Epoch: 1246 [188416/194182 (96%)]\tLoss: 0.270316\tGrad Norm: 1.353300\tLR: 0.030000\n",
      "Train set: Average loss: 0.2764\n",
      "Test set: Average loss: 0.2464, Average MAE: 0.3486\n",
      "Train Epoch: 1247 [4096/194182 (2%)]\tLoss: 0.273980\tGrad Norm: 1.275333\tLR: 0.030000\n",
      "Train Epoch: 1247 [24576/194182 (12%)]\tLoss: 0.279836\tGrad Norm: 1.498777\tLR: 0.030000\n",
      "Train Epoch: 1247 [45056/194182 (23%)]\tLoss: 0.281398\tGrad Norm: 1.812308\tLR: 0.030000\n",
      "Train Epoch: 1247 [65536/194182 (33%)]\tLoss: 0.271134\tGrad Norm: 1.369495\tLR: 0.030000\n",
      "Train Epoch: 1247 [86016/194182 (44%)]\tLoss: 0.280830\tGrad Norm: 1.697095\tLR: 0.030000\n",
      "Train Epoch: 1247 [106496/194182 (54%)]\tLoss: 0.279216\tGrad Norm: 1.614492\tLR: 0.030000\n",
      "Train Epoch: 1247 [126976/194182 (65%)]\tLoss: 0.274614\tGrad Norm: 1.540063\tLR: 0.030000\n",
      "Train Epoch: 1247 [147456/194182 (75%)]\tLoss: 0.269019\tGrad Norm: 1.336923\tLR: 0.030000\n",
      "Train Epoch: 1247 [167936/194182 (85%)]\tLoss: 0.272838\tGrad Norm: 1.231605\tLR: 0.030000\n",
      "Train Epoch: 1247 [188416/194182 (96%)]\tLoss: 0.270304\tGrad Norm: 1.077512\tLR: 0.030000\n",
      "Train set: Average loss: 0.2750\n",
      "Test set: Average loss: 0.2495, Average MAE: 0.3513\n",
      "Train Epoch: 1248 [4096/194182 (2%)]\tLoss: 0.273405\tGrad Norm: 1.480703\tLR: 0.030000\n",
      "Train Epoch: 1248 [24576/194182 (12%)]\tLoss: 0.272099\tGrad Norm: 1.631045\tLR: 0.030000\n",
      "Train Epoch: 1248 [45056/194182 (23%)]\tLoss: 0.289530\tGrad Norm: 1.684785\tLR: 0.030000\n",
      "Train Epoch: 1248 [65536/194182 (33%)]\tLoss: 0.268666\tGrad Norm: 1.082438\tLR: 0.030000\n",
      "Train Epoch: 1248 [86016/194182 (44%)]\tLoss: 0.272210\tGrad Norm: 1.580124\tLR: 0.030000\n",
      "Train Epoch: 1248 [106496/194182 (54%)]\tLoss: 0.270424\tGrad Norm: 1.443408\tLR: 0.030000\n",
      "Train Epoch: 1248 [126976/194182 (65%)]\tLoss: 0.279659\tGrad Norm: 1.595749\tLR: 0.030000\n",
      "Train Epoch: 1248 [147456/194182 (75%)]\tLoss: 0.279870\tGrad Norm: 1.513516\tLR: 0.030000\n",
      "Train Epoch: 1248 [167936/194182 (85%)]\tLoss: 0.278811\tGrad Norm: 1.433762\tLR: 0.030000\n",
      "Train Epoch: 1248 [188416/194182 (96%)]\tLoss: 0.284728\tGrad Norm: 1.741562\tLR: 0.030000\n",
      "Train set: Average loss: 0.2768\n",
      "Test set: Average loss: 0.2505, Average MAE: 0.3359\n",
      "Train Epoch: 1249 [4096/194182 (2%)]\tLoss: 0.283027\tGrad Norm: 1.914221\tLR: 0.030000\n",
      "Train Epoch: 1249 [24576/194182 (12%)]\tLoss: 0.278731\tGrad Norm: 1.686162\tLR: 0.030000\n",
      "Train Epoch: 1249 [45056/194182 (23%)]\tLoss: 0.280317\tGrad Norm: 1.344051\tLR: 0.030000\n",
      "Train Epoch: 1249 [65536/194182 (33%)]\tLoss: 0.272027\tGrad Norm: 1.463798\tLR: 0.030000\n",
      "Train Epoch: 1249 [86016/194182 (44%)]\tLoss: 0.281780\tGrad Norm: 1.805616\tLR: 0.030000\n",
      "Train Epoch: 1249 [106496/194182 (54%)]\tLoss: 0.282104\tGrad Norm: 1.699479\tLR: 0.030000\n",
      "Train Epoch: 1249 [126976/194182 (65%)]\tLoss: 0.273322\tGrad Norm: 1.271910\tLR: 0.030000\n",
      "Train Epoch: 1249 [147456/194182 (75%)]\tLoss: 0.265810\tGrad Norm: 1.430290\tLR: 0.030000\n",
      "Train Epoch: 1249 [167936/194182 (85%)]\tLoss: 0.283019\tGrad Norm: 1.779512\tLR: 0.030000\n",
      "Train Epoch: 1249 [188416/194182 (96%)]\tLoss: 0.274079\tGrad Norm: 1.608591\tLR: 0.030000\n",
      "Train set: Average loss: 0.2775\n",
      "Test set: Average loss: 0.2454, Average MAE: 0.3347\n",
      "Train Epoch: 1250 [4096/194182 (2%)]\tLoss: 0.279106\tGrad Norm: 1.454154\tLR: 0.030000\n",
      "Train Epoch: 1250 [24576/194182 (12%)]\tLoss: 0.274288\tGrad Norm: 1.111949\tLR: 0.030000\n",
      "Train Epoch: 1250 [45056/194182 (23%)]\tLoss: 0.275093\tGrad Norm: 1.464683\tLR: 0.030000\n",
      "Train Epoch: 1250 [65536/194182 (33%)]\tLoss: 0.276490\tGrad Norm: 1.356568\tLR: 0.030000\n",
      "Train Epoch: 1250 [86016/194182 (44%)]\tLoss: 0.272920\tGrad Norm: 1.516092\tLR: 0.030000\n",
      "Train Epoch: 1250 [106496/194182 (54%)]\tLoss: 0.278439\tGrad Norm: 1.943728\tLR: 0.030000\n",
      "Train Epoch: 1250 [126976/194182 (65%)]\tLoss: 0.282524\tGrad Norm: 1.466211\tLR: 0.030000\n",
      "Train Epoch: 1250 [147456/194182 (75%)]\tLoss: 0.279148\tGrad Norm: 1.692615\tLR: 0.030000\n",
      "Train Epoch: 1250 [167936/194182 (85%)]\tLoss: 0.276240\tGrad Norm: 1.423150\tLR: 0.030000\n",
      "Train Epoch: 1250 [188416/194182 (96%)]\tLoss: 0.275265\tGrad Norm: 1.563166\tLR: 0.030000\n",
      "Train set: Average loss: 0.2754\n",
      "Test set: Average loss: 0.2458, Average MAE: 0.3403\n",
      "Epoch 1250: Mean reward = 0.075 +/- 0.059\n",
      "Train Epoch: 1251 [4096/194182 (2%)]\tLoss: 0.272780\tGrad Norm: 1.602051\tLR: 0.030000\n",
      "Train Epoch: 1251 [24576/194182 (12%)]\tLoss: 0.279721\tGrad Norm: 1.572764\tLR: 0.030000\n",
      "Train Epoch: 1251 [45056/194182 (23%)]\tLoss: 0.279423\tGrad Norm: 1.399808\tLR: 0.030000\n",
      "Train Epoch: 1251 [65536/194182 (33%)]\tLoss: 0.279016\tGrad Norm: 1.453662\tLR: 0.030000\n",
      "Train Epoch: 1251 [86016/194182 (44%)]\tLoss: 0.281715\tGrad Norm: 2.003728\tLR: 0.030000\n",
      "Train Epoch: 1251 [106496/194182 (54%)]\tLoss: 0.277555\tGrad Norm: 1.719178\tLR: 0.030000\n",
      "Train Epoch: 1251 [126976/194182 (65%)]\tLoss: 0.281453\tGrad Norm: 1.527982\tLR: 0.030000\n",
      "Train Epoch: 1251 [147456/194182 (75%)]\tLoss: 0.272636\tGrad Norm: 1.488170\tLR: 0.030000\n",
      "Train Epoch: 1251 [167936/194182 (85%)]\tLoss: 0.276852\tGrad Norm: 1.733036\tLR: 0.030000\n",
      "Train Epoch: 1251 [188416/194182 (96%)]\tLoss: 0.273163\tGrad Norm: 1.730127\tLR: 0.030000\n",
      "Train set: Average loss: 0.2779\n",
      "Test set: Average loss: 0.2524, Average MAE: 0.3591\n",
      "Train Epoch: 1252 [4096/194182 (2%)]\tLoss: 0.281793\tGrad Norm: 1.877669\tLR: 0.030000\n",
      "Train Epoch: 1252 [24576/194182 (12%)]\tLoss: 0.271874\tGrad Norm: 1.532625\tLR: 0.030000\n",
      "Train Epoch: 1252 [45056/194182 (23%)]\tLoss: 0.274814\tGrad Norm: 1.180730\tLR: 0.030000\n",
      "Train Epoch: 1252 [65536/194182 (33%)]\tLoss: 0.267520\tGrad Norm: 1.257247\tLR: 0.030000\n",
      "Train Epoch: 1252 [86016/194182 (44%)]\tLoss: 0.277571\tGrad Norm: 1.533152\tLR: 0.030000\n",
      "Train Epoch: 1252 [106496/194182 (54%)]\tLoss: 0.278718\tGrad Norm: 1.746078\tLR: 0.030000\n",
      "Train Epoch: 1252 [126976/194182 (65%)]\tLoss: 0.266380\tGrad Norm: 1.214954\tLR: 0.030000\n",
      "Train Epoch: 1252 [147456/194182 (75%)]\tLoss: 0.267637\tGrad Norm: 1.312678\tLR: 0.030000\n",
      "Train Epoch: 1252 [167936/194182 (85%)]\tLoss: 0.272124\tGrad Norm: 1.225724\tLR: 0.030000\n",
      "Train Epoch: 1252 [188416/194182 (96%)]\tLoss: 0.272439\tGrad Norm: 1.679300\tLR: 0.030000\n",
      "Train set: Average loss: 0.2747\n",
      "Test set: Average loss: 0.2480, Average MAE: 0.3430\n",
      "Train Epoch: 1253 [4096/194182 (2%)]\tLoss: 0.281318\tGrad Norm: 1.521991\tLR: 0.030000\n",
      "Train Epoch: 1253 [24576/194182 (12%)]\tLoss: 0.275646\tGrad Norm: 1.394919\tLR: 0.030000\n",
      "Train Epoch: 1253 [45056/194182 (23%)]\tLoss: 0.278160\tGrad Norm: 1.424929\tLR: 0.030000\n",
      "Train Epoch: 1253 [65536/194182 (33%)]\tLoss: 0.277242\tGrad Norm: 1.718944\tLR: 0.030000\n",
      "Train Epoch: 1253 [86016/194182 (44%)]\tLoss: 0.274865\tGrad Norm: 1.402371\tLR: 0.030000\n",
      "Train Epoch: 1253 [106496/194182 (54%)]\tLoss: 0.277452\tGrad Norm: 1.475763\tLR: 0.030000\n",
      "Train Epoch: 1253 [126976/194182 (65%)]\tLoss: 0.271051\tGrad Norm: 1.429277\tLR: 0.030000\n",
      "Train Epoch: 1253 [147456/194182 (75%)]\tLoss: 0.275101\tGrad Norm: 1.498856\tLR: 0.030000\n",
      "Train Epoch: 1253 [167936/194182 (85%)]\tLoss: 0.275383\tGrad Norm: 1.661905\tLR: 0.030000\n",
      "Train Epoch: 1253 [188416/194182 (96%)]\tLoss: 0.272208\tGrad Norm: 1.397669\tLR: 0.030000\n",
      "Train set: Average loss: 0.2750\n",
      "Test set: Average loss: 0.2452, Average MAE: 0.3434\n",
      "Train Epoch: 1254 [4096/194182 (2%)]\tLoss: 0.271177\tGrad Norm: 1.350044\tLR: 0.030000\n",
      "Train Epoch: 1254 [24576/194182 (12%)]\tLoss: 0.274211\tGrad Norm: 1.486020\tLR: 0.030000\n",
      "Train Epoch: 1254 [45056/194182 (23%)]\tLoss: 0.274053\tGrad Norm: 1.891487\tLR: 0.030000\n",
      "Train Epoch: 1254 [65536/194182 (33%)]\tLoss: 0.274651\tGrad Norm: 1.677624\tLR: 0.030000\n",
      "Train Epoch: 1254 [86016/194182 (44%)]\tLoss: 0.275164\tGrad Norm: 1.316603\tLR: 0.030000\n",
      "Train Epoch: 1254 [106496/194182 (54%)]\tLoss: 0.279769\tGrad Norm: 1.643541\tLR: 0.030000\n",
      "Train Epoch: 1254 [126976/194182 (65%)]\tLoss: 0.259664\tGrad Norm: 0.994468\tLR: 0.030000\n",
      "Train Epoch: 1254 [147456/194182 (75%)]\tLoss: 0.272466\tGrad Norm: 1.576888\tLR: 0.030000\n",
      "Train Epoch: 1254 [167936/194182 (85%)]\tLoss: 0.285967\tGrad Norm: 1.617205\tLR: 0.030000\n",
      "Train Epoch: 1254 [188416/194182 (96%)]\tLoss: 0.284791\tGrad Norm: 1.736936\tLR: 0.030000\n",
      "Train set: Average loss: 0.2753\n",
      "Test set: Average loss: 0.2483, Average MAE: 0.3415\n",
      "Train Epoch: 1255 [4096/194182 (2%)]\tLoss: 0.278479\tGrad Norm: 1.607056\tLR: 0.030000\n",
      "Train Epoch: 1255 [24576/194182 (12%)]\tLoss: 0.270179\tGrad Norm: 1.070487\tLR: 0.030000\n",
      "Train Epoch: 1255 [45056/194182 (23%)]\tLoss: 0.263594\tGrad Norm: 1.094183\tLR: 0.030000\n",
      "Train Epoch: 1255 [65536/194182 (33%)]\tLoss: 0.268804\tGrad Norm: 1.675666\tLR: 0.030000\n",
      "Train Epoch: 1255 [86016/194182 (44%)]\tLoss: 0.279639\tGrad Norm: 1.664866\tLR: 0.030000\n",
      "Train Epoch: 1255 [106496/194182 (54%)]\tLoss: 0.272680\tGrad Norm: 1.495082\tLR: 0.030000\n",
      "Train Epoch: 1255 [126976/194182 (65%)]\tLoss: 0.295774\tGrad Norm: 2.217665\tLR: 0.030000\n",
      "Train Epoch: 1255 [147456/194182 (75%)]\tLoss: 0.273986\tGrad Norm: 1.492864\tLR: 0.030000\n",
      "Train Epoch: 1255 [167936/194182 (85%)]\tLoss: 0.276832\tGrad Norm: 1.681183\tLR: 0.030000\n",
      "Train Epoch: 1255 [188416/194182 (96%)]\tLoss: 0.272580\tGrad Norm: 1.497212\tLR: 0.030000\n",
      "Train set: Average loss: 0.2751\n",
      "Test set: Average loss: 0.2459, Average MAE: 0.3400\n",
      "Epoch 1255: Mean reward = 0.049 +/- 0.030\n",
      "Train Epoch: 1256 [4096/194182 (2%)]\tLoss: 0.270155\tGrad Norm: 1.512481\tLR: 0.030000\n",
      "Train Epoch: 1256 [24576/194182 (12%)]\tLoss: 0.269661\tGrad Norm: 1.423135\tLR: 0.030000\n",
      "Train Epoch: 1256 [45056/194182 (23%)]\tLoss: 0.270161\tGrad Norm: 1.337352\tLR: 0.030000\n",
      "Train Epoch: 1256 [65536/194182 (33%)]\tLoss: 0.276508\tGrad Norm: 1.575004\tLR: 0.030000\n",
      "Train Epoch: 1256 [86016/194182 (44%)]\tLoss: 0.274796\tGrad Norm: 1.474926\tLR: 0.030000\n",
      "Train Epoch: 1256 [106496/194182 (54%)]\tLoss: 0.273721\tGrad Norm: 1.483712\tLR: 0.030000\n",
      "Train Epoch: 1256 [126976/194182 (65%)]\tLoss: 0.264955\tGrad Norm: 1.107484\tLR: 0.030000\n",
      "Train Epoch: 1256 [147456/194182 (75%)]\tLoss: 0.268627\tGrad Norm: 1.076440\tLR: 0.030000\n",
      "Train Epoch: 1256 [167936/194182 (85%)]\tLoss: 0.267575\tGrad Norm: 1.220933\tLR: 0.030000\n",
      "Train Epoch: 1256 [188416/194182 (96%)]\tLoss: 0.278243\tGrad Norm: 1.693422\tLR: 0.030000\n",
      "Train set: Average loss: 0.2731\n",
      "Test set: Average loss: 0.2518, Average MAE: 0.3567\n",
      "Train Epoch: 1257 [4096/194182 (2%)]\tLoss: 0.282998\tGrad Norm: 1.727290\tLR: 0.030000\n",
      "Train Epoch: 1257 [24576/194182 (12%)]\tLoss: 0.280523\tGrad Norm: 1.702044\tLR: 0.030000\n",
      "Train Epoch: 1257 [45056/194182 (23%)]\tLoss: 0.278483\tGrad Norm: 1.555040\tLR: 0.030000\n",
      "Train Epoch: 1257 [65536/194182 (33%)]\tLoss: 0.269958\tGrad Norm: 1.473271\tLR: 0.030000\n",
      "Train Epoch: 1257 [86016/194182 (44%)]\tLoss: 0.272648\tGrad Norm: 1.457401\tLR: 0.030000\n",
      "Train Epoch: 1257 [106496/194182 (54%)]\tLoss: 0.295014\tGrad Norm: 5.729830\tLR: 0.030000\n",
      "Train Epoch: 1257 [126976/194182 (65%)]\tLoss: 0.272782\tGrad Norm: 1.417004\tLR: 0.030000\n",
      "Train Epoch: 1257 [147456/194182 (75%)]\tLoss: 0.274913\tGrad Norm: 1.898567\tLR: 0.030000\n",
      "Train Epoch: 1257 [167936/194182 (85%)]\tLoss: 0.280414\tGrad Norm: 1.960510\tLR: 0.030000\n",
      "Train Epoch: 1257 [188416/194182 (96%)]\tLoss: 0.273910\tGrad Norm: 1.519759\tLR: 0.030000\n",
      "Train set: Average loss: 0.2764\n",
      "Test set: Average loss: 0.2417, Average MAE: 0.3443\n",
      "Train Epoch: 1258 [4096/194182 (2%)]\tLoss: 0.270942\tGrad Norm: 1.131883\tLR: 0.030000\n",
      "Train Epoch: 1258 [24576/194182 (12%)]\tLoss: 0.274562\tGrad Norm: 1.445914\tLR: 0.030000\n",
      "Train Epoch: 1258 [45056/194182 (23%)]\tLoss: 0.272982\tGrad Norm: 1.348282\tLR: 0.030000\n",
      "Train Epoch: 1258 [65536/194182 (33%)]\tLoss: 0.276815\tGrad Norm: 1.483387\tLR: 0.030000\n",
      "Train Epoch: 1258 [86016/194182 (44%)]\tLoss: 0.273765\tGrad Norm: 1.361325\tLR: 0.030000\n",
      "Train Epoch: 1258 [106496/194182 (54%)]\tLoss: 0.272876\tGrad Norm: 1.375421\tLR: 0.030000\n",
      "Train Epoch: 1258 [126976/194182 (65%)]\tLoss: 0.272570\tGrad Norm: 1.203407\tLR: 0.030000\n",
      "Train Epoch: 1258 [147456/194182 (75%)]\tLoss: 0.270277\tGrad Norm: 1.532995\tLR: 0.030000\n",
      "Train Epoch: 1258 [167936/194182 (85%)]\tLoss: 0.271899\tGrad Norm: 1.389218\tLR: 0.030000\n",
      "Train Epoch: 1258 [188416/194182 (96%)]\tLoss: 0.274431\tGrad Norm: 1.548941\tLR: 0.030000\n",
      "Train set: Average loss: 0.2733\n",
      "Test set: Average loss: 0.2491, Average MAE: 0.3486\n",
      "Train Epoch: 1259 [4096/194182 (2%)]\tLoss: 0.279507\tGrad Norm: 1.637428\tLR: 0.030000\n",
      "Train Epoch: 1259 [24576/194182 (12%)]\tLoss: 0.282575\tGrad Norm: 1.251611\tLR: 0.030000\n",
      "Train Epoch: 1259 [45056/194182 (23%)]\tLoss: 0.264354\tGrad Norm: 1.433527\tLR: 0.030000\n",
      "Train Epoch: 1259 [65536/194182 (33%)]\tLoss: 0.268210\tGrad Norm: 1.448150\tLR: 0.030000\n",
      "Train Epoch: 1259 [86016/194182 (44%)]\tLoss: 0.267420\tGrad Norm: 1.012164\tLR: 0.030000\n",
      "Train Epoch: 1259 [106496/194182 (54%)]\tLoss: 0.273678\tGrad Norm: 1.286462\tLR: 0.030000\n",
      "Train Epoch: 1259 [126976/194182 (65%)]\tLoss: 0.264098\tGrad Norm: 1.020787\tLR: 0.030000\n",
      "Train Epoch: 1259 [147456/194182 (75%)]\tLoss: 0.270107\tGrad Norm: 1.528387\tLR: 0.030000\n",
      "Train Epoch: 1259 [167936/194182 (85%)]\tLoss: 0.266666\tGrad Norm: 1.502631\tLR: 0.030000\n",
      "Train Epoch: 1259 [188416/194182 (96%)]\tLoss: 0.278399\tGrad Norm: 1.609767\tLR: 0.030000\n",
      "Train set: Average loss: 0.2731\n",
      "Test set: Average loss: 0.2539, Average MAE: 0.3595\n",
      "Train Epoch: 1260 [4096/194182 (2%)]\tLoss: 0.275955\tGrad Norm: 1.695966\tLR: 0.030000\n",
      "Train Epoch: 1260 [24576/194182 (12%)]\tLoss: 0.276390\tGrad Norm: 1.822833\tLR: 0.030000\n",
      "Train Epoch: 1260 [45056/194182 (23%)]\tLoss: 0.279853\tGrad Norm: 2.059387\tLR: 0.030000\n",
      "Train Epoch: 1260 [65536/194182 (33%)]\tLoss: 0.282083\tGrad Norm: 1.976521\tLR: 0.030000\n",
      "Train Epoch: 1260 [86016/194182 (44%)]\tLoss: 0.282798\tGrad Norm: 1.712203\tLR: 0.030000\n",
      "Train Epoch: 1260 [106496/194182 (54%)]\tLoss: 0.274799\tGrad Norm: 1.842816\tLR: 0.030000\n",
      "Train Epoch: 1260 [126976/194182 (65%)]\tLoss: 0.275444\tGrad Norm: 1.733079\tLR: 0.030000\n",
      "Train Epoch: 1260 [147456/194182 (75%)]\tLoss: 0.275341\tGrad Norm: 1.409312\tLR: 0.030000\n",
      "Train Epoch: 1260 [167936/194182 (85%)]\tLoss: 0.267435\tGrad Norm: 1.243631\tLR: 0.030000\n",
      "Train Epoch: 1260 [188416/194182 (96%)]\tLoss: 0.272856\tGrad Norm: 1.707018\tLR: 0.030000\n",
      "Train set: Average loss: 0.2767\n",
      "Test set: Average loss: 0.2453, Average MAE: 0.3415\n",
      "Epoch 1260: Mean reward = 0.042 +/- 0.014\n",
      "Train Epoch: 1261 [4096/194182 (2%)]\tLoss: 0.270810\tGrad Norm: 1.402568\tLR: 0.030000\n",
      "Train Epoch: 1261 [24576/194182 (12%)]\tLoss: 0.277494\tGrad Norm: 1.622975\tLR: 0.030000\n",
      "Train Epoch: 1261 [45056/194182 (23%)]\tLoss: 0.283850\tGrad Norm: 1.970006\tLR: 0.030000\n",
      "Train Epoch: 1261 [65536/194182 (33%)]\tLoss: 0.271074\tGrad Norm: 1.854349\tLR: 0.030000\n",
      "Train Epoch: 1261 [86016/194182 (44%)]\tLoss: 0.277674\tGrad Norm: 1.722429\tLR: 0.030000\n",
      "Train Epoch: 1261 [106496/194182 (54%)]\tLoss: 0.268265\tGrad Norm: 1.241301\tLR: 0.030000\n",
      "Train Epoch: 1261 [126976/194182 (65%)]\tLoss: 0.266142\tGrad Norm: 1.111797\tLR: 0.030000\n",
      "Train Epoch: 1261 [147456/194182 (75%)]\tLoss: 0.269816\tGrad Norm: 1.139027\tLR: 0.030000\n",
      "Train Epoch: 1261 [167936/194182 (85%)]\tLoss: 0.267589\tGrad Norm: 1.207198\tLR: 0.030000\n",
      "Train Epoch: 1261 [188416/194182 (96%)]\tLoss: 0.268875\tGrad Norm: 1.441053\tLR: 0.030000\n",
      "Train set: Average loss: 0.2728\n",
      "Test set: Average loss: 0.2499, Average MAE: 0.3428\n",
      "Train Epoch: 1262 [4096/194182 (2%)]\tLoss: 0.277085\tGrad Norm: 1.750256\tLR: 0.030000\n",
      "Train Epoch: 1262 [24576/194182 (12%)]\tLoss: 0.284495\tGrad Norm: 2.026945\tLR: 0.030000\n",
      "Train Epoch: 1262 [45056/194182 (23%)]\tLoss: 0.265864\tGrad Norm: 1.253671\tLR: 0.030000\n",
      "Train Epoch: 1262 [65536/194182 (33%)]\tLoss: 0.275655\tGrad Norm: 1.203315\tLR: 0.030000\n",
      "Train Epoch: 1262 [86016/194182 (44%)]\tLoss: 0.265675\tGrad Norm: 1.195538\tLR: 0.030000\n",
      "Train Epoch: 1262 [106496/194182 (54%)]\tLoss: 0.267415\tGrad Norm: 1.019927\tLR: 0.030000\n",
      "Train Epoch: 1262 [126976/194182 (65%)]\tLoss: 0.269951\tGrad Norm: 1.448962\tLR: 0.030000\n",
      "Train Epoch: 1262 [147456/194182 (75%)]\tLoss: 0.276417\tGrad Norm: 1.695243\tLR: 0.030000\n",
      "Train Epoch: 1262 [167936/194182 (85%)]\tLoss: 0.267369\tGrad Norm: 1.408779\tLR: 0.030000\n",
      "Train Epoch: 1262 [188416/194182 (96%)]\tLoss: 0.279914\tGrad Norm: 1.511475\tLR: 0.030000\n",
      "Train set: Average loss: 0.2729\n",
      "Test set: Average loss: 0.2521, Average MAE: 0.3473\n",
      "Train Epoch: 1263 [4096/194182 (2%)]\tLoss: 0.272055\tGrad Norm: 1.780709\tLR: 0.030000\n",
      "Train Epoch: 1263 [24576/194182 (12%)]\tLoss: 0.269189\tGrad Norm: 1.221544\tLR: 0.030000\n",
      "Train Epoch: 1263 [45056/194182 (23%)]\tLoss: 0.272230\tGrad Norm: 1.480565\tLR: 0.030000\n",
      "Train Epoch: 1263 [65536/194182 (33%)]\tLoss: 0.270678\tGrad Norm: 1.414452\tLR: 0.030000\n",
      "Train Epoch: 1263 [86016/194182 (44%)]\tLoss: 0.271376\tGrad Norm: 1.525714\tLR: 0.030000\n",
      "Train Epoch: 1263 [106496/194182 (54%)]\tLoss: 0.279707\tGrad Norm: 1.773022\tLR: 0.030000\n",
      "Train Epoch: 1263 [126976/194182 (65%)]\tLoss: 0.275142\tGrad Norm: 1.807621\tLR: 0.030000\n",
      "Train Epoch: 1263 [147456/194182 (75%)]\tLoss: 0.278027\tGrad Norm: 1.976485\tLR: 0.030000\n",
      "Train Epoch: 1263 [167936/194182 (85%)]\tLoss: 0.282021\tGrad Norm: 1.796032\tLR: 0.030000\n",
      "Train Epoch: 1263 [188416/194182 (96%)]\tLoss: 0.263644\tGrad Norm: 1.415340\tLR: 0.030000\n",
      "Train set: Average loss: 0.2747\n",
      "Test set: Average loss: 0.2443, Average MAE: 0.3505\n",
      "Train Epoch: 1264 [4096/194182 (2%)]\tLoss: 0.263923\tGrad Norm: 1.327622\tLR: 0.030000\n",
      "Train Epoch: 1264 [24576/194182 (12%)]\tLoss: 0.269089\tGrad Norm: 1.288415\tLR: 0.030000\n",
      "Train Epoch: 1264 [45056/194182 (23%)]\tLoss: 0.272386\tGrad Norm: 1.370961\tLR: 0.030000\n",
      "Train Epoch: 1264 [65536/194182 (33%)]\tLoss: 0.277105\tGrad Norm: 1.455807\tLR: 0.030000\n",
      "Train Epoch: 1264 [86016/194182 (44%)]\tLoss: 0.273169\tGrad Norm: 1.506299\tLR: 0.030000\n",
      "Train Epoch: 1264 [106496/194182 (54%)]\tLoss: 0.280815\tGrad Norm: 1.620591\tLR: 0.030000\n",
      "Train Epoch: 1264 [126976/194182 (65%)]\tLoss: 0.266904\tGrad Norm: 1.314464\tLR: 0.030000\n",
      "Train Epoch: 1264 [147456/194182 (75%)]\tLoss: 0.276413\tGrad Norm: 1.491776\tLR: 0.030000\n",
      "Train Epoch: 1264 [167936/194182 (85%)]\tLoss: 0.279446\tGrad Norm: 1.868011\tLR: 0.030000\n",
      "Train Epoch: 1264 [188416/194182 (96%)]\tLoss: 0.277848\tGrad Norm: 1.440260\tLR: 0.030000\n",
      "Train set: Average loss: 0.2733\n",
      "Test set: Average loss: 0.2424, Average MAE: 0.3318\n",
      "Train Epoch: 1265 [4096/194182 (2%)]\tLoss: 0.265939\tGrad Norm: 1.327342\tLR: 0.030000\n",
      "Train Epoch: 1265 [24576/194182 (12%)]\tLoss: 0.267803\tGrad Norm: 1.412389\tLR: 0.030000\n",
      "Train Epoch: 1265 [45056/194182 (23%)]\tLoss: 0.269554\tGrad Norm: 1.200293\tLR: 0.030000\n",
      "Train Epoch: 1265 [65536/194182 (33%)]\tLoss: 0.270340\tGrad Norm: 1.328663\tLR: 0.030000\n",
      "Train Epoch: 1265 [86016/194182 (44%)]\tLoss: 0.268003\tGrad Norm: 1.472295\tLR: 0.030000\n",
      "Train Epoch: 1265 [106496/194182 (54%)]\tLoss: 0.277704\tGrad Norm: 1.604025\tLR: 0.030000\n",
      "Train Epoch: 1265 [126976/194182 (65%)]\tLoss: 0.270884\tGrad Norm: 1.431282\tLR: 0.030000\n",
      "Train Epoch: 1265 [147456/194182 (75%)]\tLoss: 0.269633\tGrad Norm: 1.216848\tLR: 0.030000\n",
      "Train Epoch: 1265 [167936/194182 (85%)]\tLoss: 0.271237\tGrad Norm: 1.240703\tLR: 0.030000\n",
      "Train Epoch: 1265 [188416/194182 (96%)]\tLoss: 0.274223\tGrad Norm: 1.644179\tLR: 0.030000\n",
      "Train set: Average loss: 0.2715\n",
      "Test set: Average loss: 0.2461, Average MAE: 0.3479\n",
      "Epoch 1265: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 1266 [4096/194182 (2%)]\tLoss: 0.276491\tGrad Norm: 1.453167\tLR: 0.030000\n",
      "Train Epoch: 1266 [24576/194182 (12%)]\tLoss: 0.270081\tGrad Norm: 1.264796\tLR: 0.030000\n",
      "Train Epoch: 1266 [45056/194182 (23%)]\tLoss: 0.275016\tGrad Norm: 1.377145\tLR: 0.030000\n",
      "Train Epoch: 1266 [65536/194182 (33%)]\tLoss: 0.264768\tGrad Norm: 1.434705\tLR: 0.030000\n",
      "Train Epoch: 1266 [86016/194182 (44%)]\tLoss: 0.273204\tGrad Norm: 1.776307\tLR: 0.030000\n",
      "Train Epoch: 1266 [106496/194182 (54%)]\tLoss: 0.281346\tGrad Norm: 2.104790\tLR: 0.030000\n",
      "Train Epoch: 1266 [126976/194182 (65%)]\tLoss: 0.274434\tGrad Norm: 1.913160\tLR: 0.030000\n",
      "Train Epoch: 1266 [147456/194182 (75%)]\tLoss: 0.273918\tGrad Norm: 1.825489\tLR: 0.030000\n",
      "Train Epoch: 1266 [167936/194182 (85%)]\tLoss: 0.274715\tGrad Norm: 1.620610\tLR: 0.030000\n",
      "Train Epoch: 1266 [188416/194182 (96%)]\tLoss: 0.281277\tGrad Norm: 1.681830\tLR: 0.030000\n",
      "Train set: Average loss: 0.2740\n",
      "Test set: Average loss: 0.2385, Average MAE: 0.3296\n",
      "Train Epoch: 1267 [4096/194182 (2%)]\tLoss: 0.265135\tGrad Norm: 0.995042\tLR: 0.030000\n",
      "Train Epoch: 1267 [24576/194182 (12%)]\tLoss: 0.276697\tGrad Norm: 1.248643\tLR: 0.030000\n",
      "Train Epoch: 1267 [45056/194182 (23%)]\tLoss: 0.273181\tGrad Norm: 1.337082\tLR: 0.030000\n",
      "Train Epoch: 1267 [65536/194182 (33%)]\tLoss: 0.268338\tGrad Norm: 1.477251\tLR: 0.030000\n",
      "Train Epoch: 1267 [86016/194182 (44%)]\tLoss: 0.275391\tGrad Norm: 1.974374\tLR: 0.030000\n",
      "Train Epoch: 1267 [106496/194182 (54%)]\tLoss: 0.278904\tGrad Norm: 1.708420\tLR: 0.030000\n",
      "Train Epoch: 1267 [126976/194182 (65%)]\tLoss: 0.269089\tGrad Norm: 1.556779\tLR: 0.030000\n",
      "Train Epoch: 1267 [147456/194182 (75%)]\tLoss: 0.276713\tGrad Norm: 1.826568\tLR: 0.030000\n",
      "Train Epoch: 1267 [167936/194182 (85%)]\tLoss: 0.274185\tGrad Norm: 1.733999\tLR: 0.030000\n",
      "Train Epoch: 1267 [188416/194182 (96%)]\tLoss: 0.273493\tGrad Norm: 1.540810\tLR: 0.030000\n",
      "Train set: Average loss: 0.2736\n",
      "Test set: Average loss: 0.2475, Average MAE: 0.3471\n",
      "Train Epoch: 1268 [4096/194182 (2%)]\tLoss: 0.268530\tGrad Norm: 1.386408\tLR: 0.030000\n",
      "Train Epoch: 1268 [24576/194182 (12%)]\tLoss: 0.280226\tGrad Norm: 1.587944\tLR: 0.030000\n",
      "Train Epoch: 1268 [45056/194182 (23%)]\tLoss: 0.272477\tGrad Norm: 1.466794\tLR: 0.030000\n",
      "Train Epoch: 1268 [65536/194182 (33%)]\tLoss: 0.274876\tGrad Norm: 1.582014\tLR: 0.030000\n",
      "Train Epoch: 1268 [86016/194182 (44%)]\tLoss: 0.276230\tGrad Norm: 1.524163\tLR: 0.030000\n",
      "Train Epoch: 1268 [106496/194182 (54%)]\tLoss: 0.277587\tGrad Norm: 1.649517\tLR: 0.030000\n",
      "Train Epoch: 1268 [126976/194182 (65%)]\tLoss: 0.276065\tGrad Norm: 1.755510\tLR: 0.030000\n",
      "Train Epoch: 1268 [147456/194182 (75%)]\tLoss: 0.269731\tGrad Norm: 1.177394\tLR: 0.030000\n",
      "Train Epoch: 1268 [167936/194182 (85%)]\tLoss: 0.286640\tGrad Norm: 1.786833\tLR: 0.030000\n",
      "Train Epoch: 1268 [188416/194182 (96%)]\tLoss: 0.273907\tGrad Norm: 1.562552\tLR: 0.030000\n",
      "Train set: Average loss: 0.2739\n",
      "Test set: Average loss: 0.2517, Average MAE: 0.3574\n",
      "Train Epoch: 1269 [4096/194182 (2%)]\tLoss: 0.276111\tGrad Norm: 1.744427\tLR: 0.030000\n",
      "Train Epoch: 1269 [24576/194182 (12%)]\tLoss: 0.279778\tGrad Norm: 1.894302\tLR: 0.030000\n",
      "Train Epoch: 1269 [45056/194182 (23%)]\tLoss: 0.268512\tGrad Norm: 1.741385\tLR: 0.030000\n",
      "Train Epoch: 1269 [65536/194182 (33%)]\tLoss: 0.270894\tGrad Norm: 1.290389\tLR: 0.030000\n",
      "Train Epoch: 1269 [86016/194182 (44%)]\tLoss: 0.272401\tGrad Norm: 1.691051\tLR: 0.030000\n",
      "Train Epoch: 1269 [106496/194182 (54%)]\tLoss: 0.285482\tGrad Norm: 1.617669\tLR: 0.030000\n",
      "Train Epoch: 1269 [126976/194182 (65%)]\tLoss: 0.273652\tGrad Norm: 1.346078\tLR: 0.030000\n",
      "Train Epoch: 1269 [147456/194182 (75%)]\tLoss: 0.271217\tGrad Norm: 1.311254\tLR: 0.030000\n",
      "Train Epoch: 1269 [167936/194182 (85%)]\tLoss: 0.278450\tGrad Norm: 1.747875\tLR: 0.030000\n",
      "Train Epoch: 1269 [188416/194182 (96%)]\tLoss: 0.270253\tGrad Norm: 1.371406\tLR: 0.030000\n",
      "Train set: Average loss: 0.2741\n",
      "Test set: Average loss: 0.2470, Average MAE: 0.3446\n",
      "Train Epoch: 1270 [4096/194182 (2%)]\tLoss: 0.279223\tGrad Norm: 1.603346\tLR: 0.030000\n",
      "Train Epoch: 1270 [24576/194182 (12%)]\tLoss: 0.266306\tGrad Norm: 1.382978\tLR: 0.030000\n",
      "Train Epoch: 1270 [45056/194182 (23%)]\tLoss: 0.265387\tGrad Norm: 1.371228\tLR: 0.030000\n",
      "Train Epoch: 1270 [65536/194182 (33%)]\tLoss: 0.274891\tGrad Norm: 1.568729\tLR: 0.030000\n",
      "Train Epoch: 1270 [86016/194182 (44%)]\tLoss: 0.270966\tGrad Norm: 1.534709\tLR: 0.030000\n",
      "Train Epoch: 1270 [106496/194182 (54%)]\tLoss: 0.271853\tGrad Norm: 1.673325\tLR: 0.030000\n",
      "Train Epoch: 1270 [126976/194182 (65%)]\tLoss: 0.267361\tGrad Norm: 1.380167\tLR: 0.030000\n",
      "Train Epoch: 1270 [147456/194182 (75%)]\tLoss: 0.271945\tGrad Norm: 1.541482\tLR: 0.030000\n",
      "Train Epoch: 1270 [167936/194182 (85%)]\tLoss: 0.267927\tGrad Norm: 1.424879\tLR: 0.030000\n",
      "Train Epoch: 1270 [188416/194182 (96%)]\tLoss: 0.266479\tGrad Norm: 1.234211\tLR: 0.030000\n",
      "Train set: Average loss: 0.2714\n",
      "Test set: Average loss: 0.2445, Average MAE: 0.3369\n",
      "Epoch 1270: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 1271 [4096/194182 (2%)]\tLoss: 0.268542\tGrad Norm: 1.271992\tLR: 0.030000\n",
      "Train Epoch: 1271 [24576/194182 (12%)]\tLoss: 0.278539\tGrad Norm: 1.420709\tLR: 0.030000\n",
      "Train Epoch: 1271 [45056/194182 (23%)]\tLoss: 0.265654\tGrad Norm: 1.582332\tLR: 0.030000\n",
      "Train Epoch: 1271 [65536/194182 (33%)]\tLoss: 0.273007\tGrad Norm: 1.772559\tLR: 0.030000\n",
      "Train Epoch: 1271 [86016/194182 (44%)]\tLoss: 0.273079\tGrad Norm: 1.602921\tLR: 0.030000\n",
      "Train Epoch: 1271 [106496/194182 (54%)]\tLoss: 0.276060\tGrad Norm: 1.832122\tLR: 0.030000\n",
      "Train Epoch: 1271 [126976/194182 (65%)]\tLoss: 0.277469\tGrad Norm: 1.744996\tLR: 0.030000\n",
      "Train Epoch: 1271 [147456/194182 (75%)]\tLoss: 0.283278\tGrad Norm: 1.700213\tLR: 0.030000\n",
      "Train Epoch: 1271 [167936/194182 (85%)]\tLoss: 0.268624\tGrad Norm: 1.626207\tLR: 0.030000\n",
      "Train Epoch: 1271 [188416/194182 (96%)]\tLoss: 0.265304\tGrad Norm: 0.887267\tLR: 0.030000\n",
      "Train set: Average loss: 0.2727\n",
      "Test set: Average loss: 0.2436, Average MAE: 0.3487\n",
      "Train Epoch: 1272 [4096/194182 (2%)]\tLoss: 0.264181\tGrad Norm: 1.247966\tLR: 0.030000\n",
      "Train Epoch: 1272 [24576/194182 (12%)]\tLoss: 0.269900\tGrad Norm: 1.523005\tLR: 0.030000\n",
      "Train Epoch: 1272 [45056/194182 (23%)]\tLoss: 0.276872\tGrad Norm: 1.729529\tLR: 0.030000\n",
      "Train Epoch: 1272 [65536/194182 (33%)]\tLoss: 0.283400\tGrad Norm: 1.988275\tLR: 0.030000\n",
      "Train Epoch: 1272 [86016/194182 (44%)]\tLoss: 0.268274\tGrad Norm: 1.347150\tLR: 0.030000\n",
      "Train Epoch: 1272 [106496/194182 (54%)]\tLoss: 0.267040\tGrad Norm: 1.250323\tLR: 0.030000\n",
      "Train Epoch: 1272 [126976/194182 (65%)]\tLoss: 0.272297\tGrad Norm: 1.161368\tLR: 0.030000\n",
      "Train Epoch: 1272 [147456/194182 (75%)]\tLoss: 0.274135\tGrad Norm: 1.918615\tLR: 0.030000\n",
      "Train Epoch: 1272 [167936/194182 (85%)]\tLoss: 0.270459\tGrad Norm: 1.701662\tLR: 0.030000\n",
      "Train Epoch: 1272 [188416/194182 (96%)]\tLoss: 0.278434\tGrad Norm: 1.393030\tLR: 0.030000\n",
      "Train set: Average loss: 0.2717\n",
      "Test set: Average loss: 0.2430, Average MAE: 0.3365\n",
      "Train Epoch: 1273 [4096/194182 (2%)]\tLoss: 0.261855\tGrad Norm: 1.255684\tLR: 0.030000\n",
      "Train Epoch: 1273 [24576/194182 (12%)]\tLoss: 0.267620\tGrad Norm: 1.177038\tLR: 0.030000\n",
      "Train Epoch: 1273 [45056/194182 (23%)]\tLoss: 0.272244\tGrad Norm: 1.577840\tLR: 0.030000\n",
      "Train Epoch: 1273 [65536/194182 (33%)]\tLoss: 0.271689\tGrad Norm: 1.625131\tLR: 0.030000\n",
      "Train Epoch: 1273 [86016/194182 (44%)]\tLoss: 0.275421\tGrad Norm: 1.653757\tLR: 0.030000\n",
      "Train Epoch: 1273 [106496/194182 (54%)]\tLoss: 0.271970\tGrad Norm: 1.597613\tLR: 0.030000\n",
      "Train Epoch: 1273 [126976/194182 (65%)]\tLoss: 0.272789\tGrad Norm: 1.706154\tLR: 0.030000\n",
      "Train Epoch: 1273 [147456/194182 (75%)]\tLoss: 0.270661\tGrad Norm: 1.477460\tLR: 0.030000\n",
      "Train Epoch: 1273 [167936/194182 (85%)]\tLoss: 0.274187\tGrad Norm: 1.398792\tLR: 0.030000\n",
      "Train Epoch: 1273 [188416/194182 (96%)]\tLoss: 0.278751\tGrad Norm: 1.804926\tLR: 0.030000\n",
      "Train set: Average loss: 0.2718\n",
      "Test set: Average loss: 0.2519, Average MAE: 0.3539\n",
      "Train Epoch: 1274 [4096/194182 (2%)]\tLoss: 0.273423\tGrad Norm: 1.892500\tLR: 0.030000\n",
      "Train Epoch: 1274 [24576/194182 (12%)]\tLoss: 0.276461\tGrad Norm: 1.599921\tLR: 0.030000\n",
      "Train Epoch: 1274 [45056/194182 (23%)]\tLoss: 0.269421\tGrad Norm: 1.617553\tLR: 0.030000\n",
      "Train Epoch: 1274 [65536/194182 (33%)]\tLoss: 0.273069\tGrad Norm: 1.405382\tLR: 0.030000\n",
      "Train Epoch: 1274 [86016/194182 (44%)]\tLoss: 0.263600\tGrad Norm: 1.346829\tLR: 0.030000\n",
      "Train Epoch: 1274 [106496/194182 (54%)]\tLoss: 0.271016\tGrad Norm: 1.508993\tLR: 0.030000\n",
      "Train Epoch: 1274 [126976/194182 (65%)]\tLoss: 0.279377\tGrad Norm: 1.351198\tLR: 0.030000\n",
      "Train Epoch: 1274 [147456/194182 (75%)]\tLoss: 0.267435\tGrad Norm: 1.527972\tLR: 0.030000\n",
      "Train Epoch: 1274 [167936/194182 (85%)]\tLoss: 0.274854\tGrad Norm: 1.445386\tLR: 0.030000\n",
      "Train Epoch: 1274 [188416/194182 (96%)]\tLoss: 0.272123\tGrad Norm: 1.538643\tLR: 0.030000\n",
      "Train set: Average loss: 0.2718\n",
      "Test set: Average loss: 0.2461, Average MAE: 0.3413\n",
      "Train Epoch: 1275 [4096/194182 (2%)]\tLoss: 0.263878\tGrad Norm: 1.360406\tLR: 0.030000\n",
      "Train Epoch: 1275 [24576/194182 (12%)]\tLoss: 0.270868\tGrad Norm: 1.793263\tLR: 0.030000\n",
      "Train Epoch: 1275 [45056/194182 (23%)]\tLoss: 0.275404\tGrad Norm: 1.828246\tLR: 0.030000\n",
      "Train Epoch: 1275 [65536/194182 (33%)]\tLoss: 0.273015\tGrad Norm: 1.617657\tLR: 0.030000\n",
      "Train Epoch: 1275 [86016/194182 (44%)]\tLoss: 0.265671\tGrad Norm: 1.502847\tLR: 0.030000\n",
      "Train Epoch: 1275 [106496/194182 (54%)]\tLoss: 0.274348\tGrad Norm: 1.667221\tLR: 0.030000\n",
      "Train Epoch: 1275 [126976/194182 (65%)]\tLoss: 0.274721\tGrad Norm: 1.598817\tLR: 0.030000\n",
      "Train Epoch: 1275 [147456/194182 (75%)]\tLoss: 0.271161\tGrad Norm: 1.532107\tLR: 0.030000\n",
      "Train Epoch: 1275 [167936/194182 (85%)]\tLoss: 0.268225\tGrad Norm: 1.434423\tLR: 0.030000\n",
      "Train Epoch: 1275 [188416/194182 (96%)]\tLoss: 0.276123\tGrad Norm: 1.686361\tLR: 0.030000\n",
      "Train set: Average loss: 0.2723\n",
      "Test set: Average loss: 0.2537, Average MAE: 0.3465\n",
      "Epoch 1275: Mean reward = 0.046 +/- 0.001\n",
      "Train Epoch: 1276 [4096/194182 (2%)]\tLoss: 0.282656\tGrad Norm: 1.859376\tLR: 0.030000\n",
      "Train Epoch: 1276 [24576/194182 (12%)]\tLoss: 0.267879\tGrad Norm: 1.568852\tLR: 0.030000\n",
      "Train Epoch: 1276 [45056/194182 (23%)]\tLoss: 0.278579\tGrad Norm: 1.622075\tLR: 0.030000\n",
      "Train Epoch: 1276 [65536/194182 (33%)]\tLoss: 0.277551\tGrad Norm: 1.479582\tLR: 0.030000\n",
      "Train Epoch: 1276 [86016/194182 (44%)]\tLoss: 0.278438\tGrad Norm: 1.241917\tLR: 0.030000\n",
      "Train Epoch: 1276 [106496/194182 (54%)]\tLoss: 0.270355\tGrad Norm: 1.823981\tLR: 0.030000\n",
      "Train Epoch: 1276 [126976/194182 (65%)]\tLoss: 0.270735\tGrad Norm: 1.689872\tLR: 0.030000\n",
      "Train Epoch: 1276 [147456/194182 (75%)]\tLoss: 0.272575\tGrad Norm: 1.579188\tLR: 0.030000\n",
      "Train Epoch: 1276 [167936/194182 (85%)]\tLoss: 0.268482\tGrad Norm: 1.374009\tLR: 0.030000\n",
      "Train Epoch: 1276 [188416/194182 (96%)]\tLoss: 0.268438\tGrad Norm: 1.610411\tLR: 0.030000\n",
      "Train set: Average loss: 0.2721\n",
      "Test set: Average loss: 0.2494, Average MAE: 0.3426\n",
      "Train Epoch: 1277 [4096/194182 (2%)]\tLoss: 0.271204\tGrad Norm: 1.589249\tLR: 0.030000\n",
      "Train Epoch: 1277 [24576/194182 (12%)]\tLoss: 0.273994\tGrad Norm: 1.585214\tLR: 0.030000\n",
      "Train Epoch: 1277 [45056/194182 (23%)]\tLoss: 0.270633\tGrad Norm: 1.657418\tLR: 0.030000\n",
      "Train Epoch: 1277 [65536/194182 (33%)]\tLoss: 0.271558\tGrad Norm: 1.441429\tLR: 0.030000\n",
      "Train Epoch: 1277 [86016/194182 (44%)]\tLoss: 0.271463\tGrad Norm: 1.544262\tLR: 0.030000\n",
      "Train Epoch: 1277 [106496/194182 (54%)]\tLoss: 0.272733\tGrad Norm: 1.496377\tLR: 0.030000\n",
      "Train Epoch: 1277 [126976/194182 (65%)]\tLoss: 0.270844\tGrad Norm: 1.683212\tLR: 0.030000\n",
      "Train Epoch: 1277 [147456/194182 (75%)]\tLoss: 0.273010\tGrad Norm: 1.465156\tLR: 0.030000\n",
      "Train Epoch: 1277 [167936/194182 (85%)]\tLoss: 0.278906\tGrad Norm: 2.114809\tLR: 0.030000\n",
      "Train Epoch: 1277 [188416/194182 (96%)]\tLoss: 0.276296\tGrad Norm: 1.879353\tLR: 0.030000\n",
      "Train set: Average loss: 0.2730\n",
      "Test set: Average loss: 0.2480, Average MAE: 0.3534\n",
      "Train Epoch: 1278 [4096/194182 (2%)]\tLoss: 0.269735\tGrad Norm: 1.496558\tLR: 0.030000\n",
      "Train Epoch: 1278 [24576/194182 (12%)]\tLoss: 0.271843\tGrad Norm: 1.595755\tLR: 0.030000\n",
      "Train Epoch: 1278 [45056/194182 (23%)]\tLoss: 0.273969\tGrad Norm: 1.610567\tLR: 0.030000\n",
      "Train Epoch: 1278 [65536/194182 (33%)]\tLoss: 0.274948\tGrad Norm: 1.453225\tLR: 0.030000\n",
      "Train Epoch: 1278 [86016/194182 (44%)]\tLoss: 0.271826\tGrad Norm: 1.467932\tLR: 0.030000\n",
      "Train Epoch: 1278 [106496/194182 (54%)]\tLoss: 0.275454\tGrad Norm: 1.960967\tLR: 0.030000\n",
      "Train Epoch: 1278 [126976/194182 (65%)]\tLoss: 0.278694\tGrad Norm: 1.855319\tLR: 0.030000\n",
      "Train Epoch: 1278 [147456/194182 (75%)]\tLoss: 0.269522\tGrad Norm: 1.686811\tLR: 0.030000\n",
      "Train Epoch: 1278 [167936/194182 (85%)]\tLoss: 0.270168\tGrad Norm: 1.662230\tLR: 0.030000\n",
      "Train Epoch: 1278 [188416/194182 (96%)]\tLoss: 0.261139\tGrad Norm: 1.280057\tLR: 0.030000\n",
      "Train set: Average loss: 0.2728\n",
      "Test set: Average loss: 0.2444, Average MAE: 0.3472\n",
      "Train Epoch: 1279 [4096/194182 (2%)]\tLoss: 0.269329\tGrad Norm: 1.181979\tLR: 0.030000\n",
      "Train Epoch: 1279 [24576/194182 (12%)]\tLoss: 0.267401\tGrad Norm: 1.451086\tLR: 0.030000\n",
      "Train Epoch: 1279 [45056/194182 (23%)]\tLoss: 0.264979\tGrad Norm: 1.382815\tLR: 0.030000\n",
      "Train Epoch: 1279 [65536/194182 (33%)]\tLoss: 0.275787\tGrad Norm: 1.399447\tLR: 0.030000\n",
      "Train Epoch: 1279 [86016/194182 (44%)]\tLoss: 0.273701\tGrad Norm: 1.596895\tLR: 0.030000\n",
      "Train Epoch: 1279 [106496/194182 (54%)]\tLoss: 0.276299\tGrad Norm: 1.948951\tLR: 0.030000\n",
      "Train Epoch: 1279 [126976/194182 (65%)]\tLoss: 0.273230\tGrad Norm: 1.742087\tLR: 0.030000\n",
      "Train Epoch: 1279 [147456/194182 (75%)]\tLoss: 0.274410\tGrad Norm: 1.702231\tLR: 0.030000\n",
      "Train Epoch: 1279 [167936/194182 (85%)]\tLoss: 0.271197\tGrad Norm: 1.472835\tLR: 0.030000\n",
      "Train Epoch: 1279 [188416/194182 (96%)]\tLoss: 0.266356\tGrad Norm: 1.308612\tLR: 0.030000\n",
      "Train set: Average loss: 0.2715\n",
      "Test set: Average loss: 0.2459, Average MAE: 0.3388\n",
      "Train Epoch: 1280 [4096/194182 (2%)]\tLoss: 0.266327\tGrad Norm: 1.480364\tLR: 0.030000\n",
      "Train Epoch: 1280 [24576/194182 (12%)]\tLoss: 0.272109\tGrad Norm: 1.406565\tLR: 0.030000\n",
      "Train Epoch: 1280 [45056/194182 (23%)]\tLoss: 0.265282\tGrad Norm: 1.457941\tLR: 0.030000\n",
      "Train Epoch: 1280 [65536/194182 (33%)]\tLoss: 0.261648\tGrad Norm: 1.208750\tLR: 0.030000\n",
      "Train Epoch: 1280 [86016/194182 (44%)]\tLoss: 0.272289\tGrad Norm: 1.106239\tLR: 0.030000\n",
      "Train Epoch: 1280 [106496/194182 (54%)]\tLoss: 0.268742\tGrad Norm: 1.573082\tLR: 0.030000\n",
      "Train Epoch: 1280 [126976/194182 (65%)]\tLoss: 0.272492\tGrad Norm: 1.720320\tLR: 0.030000\n",
      "Train Epoch: 1280 [147456/194182 (75%)]\tLoss: 0.269535\tGrad Norm: 1.644502\tLR: 0.030000\n",
      "Train Epoch: 1280 [167936/194182 (85%)]\tLoss: 0.266571\tGrad Norm: 1.523631\tLR: 0.030000\n",
      "Train Epoch: 1280 [188416/194182 (96%)]\tLoss: 0.270932\tGrad Norm: 1.478435\tLR: 0.030000\n",
      "Train set: Average loss: 0.2694\n",
      "Test set: Average loss: 0.2556, Average MAE: 0.3458\n",
      "Epoch 1280: Mean reward = 0.088 +/- 0.079\n",
      "Train Epoch: 1281 [4096/194182 (2%)]\tLoss: 0.279598\tGrad Norm: 1.922585\tLR: 0.030000\n",
      "Train Epoch: 1281 [24576/194182 (12%)]\tLoss: 0.272109\tGrad Norm: 1.428851\tLR: 0.030000\n",
      "Train Epoch: 1281 [45056/194182 (23%)]\tLoss: 0.266407\tGrad Norm: 1.594102\tLR: 0.030000\n",
      "Train Epoch: 1281 [65536/194182 (33%)]\tLoss: 0.275466\tGrad Norm: 1.441107\tLR: 0.030000\n",
      "Train Epoch: 1281 [86016/194182 (44%)]\tLoss: 0.280027\tGrad Norm: 2.104624\tLR: 0.030000\n",
      "Train Epoch: 1281 [106496/194182 (54%)]\tLoss: 0.273642\tGrad Norm: 1.488390\tLR: 0.030000\n",
      "Train Epoch: 1281 [126976/194182 (65%)]\tLoss: 0.267087\tGrad Norm: 1.532695\tLR: 0.030000\n",
      "Train Epoch: 1281 [147456/194182 (75%)]\tLoss: 0.273263\tGrad Norm: 1.690958\tLR: 0.030000\n",
      "Train Epoch: 1281 [167936/194182 (85%)]\tLoss: 0.272628\tGrad Norm: 1.660184\tLR: 0.030000\n",
      "Train Epoch: 1281 [188416/194182 (96%)]\tLoss: 0.279771\tGrad Norm: 1.709805\tLR: 0.030000\n",
      "Train set: Average loss: 0.2721\n",
      "Test set: Average loss: 0.2460, Average MAE: 0.3439\n",
      "Train Epoch: 1282 [4096/194182 (2%)]\tLoss: 0.266997\tGrad Norm: 1.400536\tLR: 0.030000\n",
      "Train Epoch: 1282 [24576/194182 (12%)]\tLoss: 0.268073\tGrad Norm: 1.671095\tLR: 0.030000\n",
      "Train Epoch: 1282 [45056/194182 (23%)]\tLoss: 0.277898\tGrad Norm: 1.672192\tLR: 0.030000\n",
      "Train Epoch: 1282 [65536/194182 (33%)]\tLoss: 0.271628\tGrad Norm: 1.531379\tLR: 0.030000\n",
      "Train Epoch: 1282 [86016/194182 (44%)]\tLoss: 0.272731\tGrad Norm: 1.609061\tLR: 0.030000\n",
      "Train Epoch: 1282 [106496/194182 (54%)]\tLoss: 0.271555\tGrad Norm: 1.755159\tLR: 0.030000\n",
      "Train Epoch: 1282 [126976/194182 (65%)]\tLoss: 0.276334\tGrad Norm: 1.877605\tLR: 0.030000\n",
      "Train Epoch: 1282 [147456/194182 (75%)]\tLoss: 0.266966\tGrad Norm: 1.502745\tLR: 0.030000\n",
      "Train Epoch: 1282 [167936/194182 (85%)]\tLoss: 0.274819\tGrad Norm: 1.874649\tLR: 0.030000\n",
      "Train Epoch: 1282 [188416/194182 (96%)]\tLoss: 0.268253\tGrad Norm: 1.704066\tLR: 0.030000\n",
      "Train set: Average loss: 0.2722\n",
      "Test set: Average loss: 0.2494, Average MAE: 0.3539\n",
      "Train Epoch: 1283 [4096/194182 (2%)]\tLoss: 0.270447\tGrad Norm: 1.725697\tLR: 0.030000\n",
      "Train Epoch: 1283 [24576/194182 (12%)]\tLoss: 0.263603\tGrad Norm: 1.283090\tLR: 0.030000\n",
      "Train Epoch: 1283 [45056/194182 (23%)]\tLoss: 0.263847\tGrad Norm: 1.165058\tLR: 0.030000\n",
      "Train Epoch: 1283 [65536/194182 (33%)]\tLoss: 0.266674\tGrad Norm: 1.212746\tLR: 0.030000\n",
      "Train Epoch: 1283 [86016/194182 (44%)]\tLoss: 0.268887\tGrad Norm: 1.488712\tLR: 0.030000\n",
      "Train Epoch: 1283 [106496/194182 (54%)]\tLoss: 0.281960\tGrad Norm: 1.707723\tLR: 0.030000\n",
      "Train Epoch: 1283 [126976/194182 (65%)]\tLoss: 0.270627\tGrad Norm: 1.406133\tLR: 0.030000\n",
      "Train Epoch: 1283 [147456/194182 (75%)]\tLoss: 0.261774\tGrad Norm: 1.066352\tLR: 0.030000\n",
      "Train Epoch: 1283 [167936/194182 (85%)]\tLoss: 0.256807\tGrad Norm: 1.183751\tLR: 0.030000\n",
      "Train Epoch: 1283 [188416/194182 (96%)]\tLoss: 0.270588\tGrad Norm: 1.449661\tLR: 0.030000\n",
      "Train set: Average loss: 0.2690\n",
      "Test set: Average loss: 0.2508, Average MAE: 0.3451\n",
      "Train Epoch: 1284 [4096/194182 (2%)]\tLoss: 0.274566\tGrad Norm: 1.535432\tLR: 0.030000\n",
      "Train Epoch: 1284 [24576/194182 (12%)]\tLoss: 0.268162\tGrad Norm: 1.492022\tLR: 0.030000\n",
      "Train Epoch: 1284 [45056/194182 (23%)]\tLoss: 0.272499\tGrad Norm: 1.594189\tLR: 0.030000\n",
      "Train Epoch: 1284 [65536/194182 (33%)]\tLoss: 0.269337\tGrad Norm: 1.544047\tLR: 0.030000\n",
      "Train Epoch: 1284 [86016/194182 (44%)]\tLoss: 0.276154\tGrad Norm: 1.952650\tLR: 0.030000\n",
      "Train Epoch: 1284 [106496/194182 (54%)]\tLoss: 0.269901\tGrad Norm: 1.640374\tLR: 0.030000\n",
      "Train Epoch: 1284 [126976/194182 (65%)]\tLoss: 0.279038\tGrad Norm: 1.901384\tLR: 0.030000\n",
      "Train Epoch: 1284 [147456/194182 (75%)]\tLoss: 0.268403\tGrad Norm: 1.462432\tLR: 0.030000\n",
      "Train Epoch: 1284 [167936/194182 (85%)]\tLoss: 0.281246\tGrad Norm: 1.887637\tLR: 0.030000\n",
      "Train Epoch: 1284 [188416/194182 (96%)]\tLoss: 0.272698\tGrad Norm: 1.504352\tLR: 0.030000\n",
      "Train set: Average loss: 0.2714\n",
      "Test set: Average loss: 0.2435, Average MAE: 0.3355\n",
      "Train Epoch: 1285 [4096/194182 (2%)]\tLoss: 0.269037\tGrad Norm: 1.539960\tLR: 0.030000\n",
      "Train Epoch: 1285 [24576/194182 (12%)]\tLoss: 0.260580\tGrad Norm: 1.337021\tLR: 0.030000\n",
      "Train Epoch: 1285 [45056/194182 (23%)]\tLoss: 0.268590\tGrad Norm: 1.347185\tLR: 0.030000\n",
      "Train Epoch: 1285 [65536/194182 (33%)]\tLoss: 0.270740\tGrad Norm: 1.696189\tLR: 0.030000\n",
      "Train Epoch: 1285 [86016/194182 (44%)]\tLoss: 0.269969\tGrad Norm: 1.628780\tLR: 0.030000\n",
      "Train Epoch: 1285 [106496/194182 (54%)]\tLoss: 0.266677\tGrad Norm: 1.553338\tLR: 0.030000\n",
      "Train Epoch: 1285 [126976/194182 (65%)]\tLoss: 0.272426\tGrad Norm: 1.417048\tLR: 0.030000\n",
      "Train Epoch: 1285 [147456/194182 (75%)]\tLoss: 0.266093\tGrad Norm: 1.304656\tLR: 0.030000\n",
      "Train Epoch: 1285 [167936/194182 (85%)]\tLoss: 0.260421\tGrad Norm: 1.288235\tLR: 0.030000\n",
      "Train Epoch: 1285 [188416/194182 (96%)]\tLoss: 0.276380\tGrad Norm: 1.648903\tLR: 0.030000\n",
      "Train set: Average loss: 0.2692\n",
      "Test set: Average loss: 0.2514, Average MAE: 0.3360\n",
      "Epoch 1285: Mean reward = 0.056 +/- 0.046\n",
      "Train Epoch: 1286 [4096/194182 (2%)]\tLoss: 0.275127\tGrad Norm: 1.980848\tLR: 0.030000\n",
      "Train Epoch: 1286 [24576/194182 (12%)]\tLoss: 0.262870\tGrad Norm: 1.626345\tLR: 0.030000\n",
      "Train Epoch: 1286 [45056/194182 (23%)]\tLoss: 0.263987\tGrad Norm: 1.592705\tLR: 0.030000\n",
      "Train Epoch: 1286 [65536/194182 (33%)]\tLoss: 0.266509\tGrad Norm: 1.199764\tLR: 0.030000\n",
      "Train Epoch: 1286 [86016/194182 (44%)]\tLoss: 0.264153\tGrad Norm: 1.432340\tLR: 0.030000\n",
      "Train Epoch: 1286 [106496/194182 (54%)]\tLoss: 0.283121\tGrad Norm: 1.748046\tLR: 0.030000\n",
      "Train Epoch: 1286 [126976/194182 (65%)]\tLoss: 0.266183\tGrad Norm: 1.271309\tLR: 0.030000\n",
      "Train Epoch: 1286 [147456/194182 (75%)]\tLoss: 0.269357\tGrad Norm: 1.523979\tLR: 0.030000\n",
      "Train Epoch: 1286 [167936/194182 (85%)]\tLoss: 0.265974\tGrad Norm: 1.447976\tLR: 0.030000\n",
      "Train Epoch: 1286 [188416/194182 (96%)]\tLoss: 0.278167\tGrad Norm: 1.701330\tLR: 0.030000\n",
      "Train set: Average loss: 0.2700\n",
      "Test set: Average loss: 0.2427, Average MAE: 0.3435\n",
      "Train Epoch: 1287 [4096/194182 (2%)]\tLoss: 0.259773\tGrad Norm: 1.118662\tLR: 0.030000\n",
      "Train Epoch: 1287 [24576/194182 (12%)]\tLoss: 0.267228\tGrad Norm: 1.375752\tLR: 0.030000\n",
      "Train Epoch: 1287 [45056/194182 (23%)]\tLoss: 0.278957\tGrad Norm: 1.493555\tLR: 0.030000\n",
      "Train Epoch: 1287 [65536/194182 (33%)]\tLoss: 0.272993\tGrad Norm: 1.702161\tLR: 0.030000\n",
      "Train Epoch: 1287 [86016/194182 (44%)]\tLoss: 0.267650\tGrad Norm: 1.361212\tLR: 0.030000\n",
      "Train Epoch: 1287 [106496/194182 (54%)]\tLoss: 0.268274\tGrad Norm: 1.381537\tLR: 0.030000\n",
      "Train Epoch: 1287 [126976/194182 (65%)]\tLoss: 0.268371\tGrad Norm: 1.489319\tLR: 0.030000\n",
      "Train Epoch: 1287 [147456/194182 (75%)]\tLoss: 0.267833\tGrad Norm: 1.480678\tLR: 0.030000\n",
      "Train Epoch: 1287 [167936/194182 (85%)]\tLoss: 0.268231\tGrad Norm: 1.514534\tLR: 0.030000\n",
      "Train Epoch: 1287 [188416/194182 (96%)]\tLoss: 0.278156\tGrad Norm: 1.756447\tLR: 0.030000\n",
      "Train set: Average loss: 0.2695\n",
      "Test set: Average loss: 0.2523, Average MAE: 0.3387\n",
      "Train Epoch: 1288 [4096/194182 (2%)]\tLoss: 0.276006\tGrad Norm: 1.829839\tLR: 0.030000\n",
      "Train Epoch: 1288 [24576/194182 (12%)]\tLoss: 0.271992\tGrad Norm: 1.914676\tLR: 0.030000\n",
      "Train Epoch: 1288 [45056/194182 (23%)]\tLoss: 0.273043\tGrad Norm: 1.745605\tLR: 0.030000\n",
      "Train Epoch: 1288 [65536/194182 (33%)]\tLoss: 0.275934\tGrad Norm: 1.946438\tLR: 0.030000\n",
      "Train Epoch: 1288 [86016/194182 (44%)]\tLoss: 0.273265\tGrad Norm: 1.750178\tLR: 0.030000\n",
      "Train Epoch: 1288 [106496/194182 (54%)]\tLoss: 0.274807\tGrad Norm: 1.416368\tLR: 0.030000\n",
      "Train Epoch: 1288 [126976/194182 (65%)]\tLoss: 0.264066\tGrad Norm: 1.284083\tLR: 0.030000\n",
      "Train Epoch: 1288 [147456/194182 (75%)]\tLoss: 0.263264\tGrad Norm: 1.308364\tLR: 0.030000\n",
      "Train Epoch: 1288 [167936/194182 (85%)]\tLoss: 0.269635\tGrad Norm: 1.514425\tLR: 0.030000\n",
      "Train Epoch: 1288 [188416/194182 (96%)]\tLoss: 0.276722\tGrad Norm: 1.592086\tLR: 0.030000\n",
      "Train set: Average loss: 0.2714\n",
      "Test set: Average loss: 0.2455, Average MAE: 0.3412\n",
      "Train Epoch: 1289 [4096/194182 (2%)]\tLoss: 0.266297\tGrad Norm: 1.479423\tLR: 0.030000\n",
      "Train Epoch: 1289 [24576/194182 (12%)]\tLoss: 0.263320\tGrad Norm: 1.511998\tLR: 0.030000\n",
      "Train Epoch: 1289 [45056/194182 (23%)]\tLoss: 0.279515\tGrad Norm: 2.476356\tLR: 0.030000\n",
      "Train Epoch: 1289 [65536/194182 (33%)]\tLoss: 0.280463\tGrad Norm: 2.039551\tLR: 0.030000\n",
      "Train Epoch: 1289 [86016/194182 (44%)]\tLoss: 0.273021\tGrad Norm: 1.939425\tLR: 0.030000\n",
      "Train Epoch: 1289 [106496/194182 (54%)]\tLoss: 0.277052\tGrad Norm: 1.596995\tLR: 0.030000\n",
      "Train Epoch: 1289 [126976/194182 (65%)]\tLoss: 0.270852\tGrad Norm: 1.641219\tLR: 0.030000\n",
      "Train Epoch: 1289 [147456/194182 (75%)]\tLoss: 0.263263\tGrad Norm: 1.296981\tLR: 0.030000\n",
      "Train Epoch: 1289 [167936/194182 (85%)]\tLoss: 0.278978\tGrad Norm: 1.698325\tLR: 0.030000\n",
      "Train Epoch: 1289 [188416/194182 (96%)]\tLoss: 0.270957\tGrad Norm: 1.135555\tLR: 0.030000\n",
      "Train set: Average loss: 0.2710\n",
      "Test set: Average loss: 0.2443, Average MAE: 0.3370\n",
      "Train Epoch: 1290 [4096/194182 (2%)]\tLoss: 0.270907\tGrad Norm: 1.328164\tLR: 0.030000\n",
      "Train Epoch: 1290 [24576/194182 (12%)]\tLoss: 0.264900\tGrad Norm: 1.493628\tLR: 0.030000\n",
      "Train Epoch: 1290 [45056/194182 (23%)]\tLoss: 0.265992\tGrad Norm: 1.175262\tLR: 0.030000\n",
      "Train Epoch: 1290 [65536/194182 (33%)]\tLoss: 0.267619\tGrad Norm: 1.207299\tLR: 0.030000\n",
      "Train Epoch: 1290 [86016/194182 (44%)]\tLoss: 0.265114\tGrad Norm: 1.324974\tLR: 0.030000\n",
      "Train Epoch: 1290 [106496/194182 (54%)]\tLoss: 0.262289\tGrad Norm: 1.329815\tLR: 0.030000\n",
      "Train Epoch: 1290 [126976/194182 (65%)]\tLoss: 0.272466\tGrad Norm: 1.647313\tLR: 0.030000\n",
      "Train Epoch: 1290 [147456/194182 (75%)]\tLoss: 0.281865\tGrad Norm: 2.208189\tLR: 0.030000\n",
      "Train Epoch: 1290 [167936/194182 (85%)]\tLoss: 0.271639\tGrad Norm: 1.864935\tLR: 0.030000\n",
      "Train Epoch: 1290 [188416/194182 (96%)]\tLoss: 0.272843\tGrad Norm: 1.761145\tLR: 0.030000\n",
      "Train set: Average loss: 0.2695\n",
      "Test set: Average loss: 0.2473, Average MAE: 0.3410\n",
      "Epoch 1290: Mean reward = 0.055 +/- 0.033\n",
      "Train Epoch: 1291 [4096/194182 (2%)]\tLoss: 0.272477\tGrad Norm: 1.478114\tLR: 0.030000\n",
      "Train Epoch: 1291 [24576/194182 (12%)]\tLoss: 0.263947\tGrad Norm: 1.333790\tLR: 0.030000\n",
      "Train Epoch: 1291 [45056/194182 (23%)]\tLoss: 0.265193\tGrad Norm: 1.588004\tLR: 0.030000\n",
      "Train Epoch: 1291 [65536/194182 (33%)]\tLoss: 0.270735\tGrad Norm: 1.736686\tLR: 0.030000\n",
      "Train Epoch: 1291 [86016/194182 (44%)]\tLoss: 0.278460\tGrad Norm: 2.513914\tLR: 0.030000\n",
      "Train Epoch: 1291 [106496/194182 (54%)]\tLoss: 0.263327\tGrad Norm: 1.319768\tLR: 0.030000\n",
      "Train Epoch: 1291 [126976/194182 (65%)]\tLoss: 0.274330\tGrad Norm: 1.702717\tLR: 0.030000\n",
      "Train Epoch: 1291 [147456/194182 (75%)]\tLoss: 0.269854\tGrad Norm: 1.553528\tLR: 0.030000\n",
      "Train Epoch: 1291 [167936/194182 (85%)]\tLoss: 0.262648\tGrad Norm: 1.307094\tLR: 0.030000\n",
      "Train Epoch: 1291 [188416/194182 (96%)]\tLoss: 0.276679\tGrad Norm: 1.718367\tLR: 0.030000\n",
      "Train set: Average loss: 0.2703\n",
      "Test set: Average loss: 0.2454, Average MAE: 0.3488\n",
      "Train Epoch: 1292 [4096/194182 (2%)]\tLoss: 0.268865\tGrad Norm: 1.310880\tLR: 0.030000\n",
      "Train Epoch: 1292 [24576/194182 (12%)]\tLoss: 0.260686\tGrad Norm: 1.314350\tLR: 0.030000\n",
      "Train Epoch: 1292 [45056/194182 (23%)]\tLoss: 0.268759\tGrad Norm: 1.659295\tLR: 0.030000\n",
      "Train Epoch: 1292 [65536/194182 (33%)]\tLoss: 0.275231\tGrad Norm: 1.778851\tLR: 0.030000\n",
      "Train Epoch: 1292 [86016/194182 (44%)]\tLoss: 0.265833\tGrad Norm: 1.592322\tLR: 0.030000\n",
      "Train Epoch: 1292 [106496/194182 (54%)]\tLoss: 0.265900\tGrad Norm: 1.145418\tLR: 0.030000\n",
      "Train Epoch: 1292 [126976/194182 (65%)]\tLoss: 0.269270\tGrad Norm: 1.278987\tLR: 0.030000\n",
      "Train Epoch: 1292 [147456/194182 (75%)]\tLoss: 0.270149\tGrad Norm: 1.596877\tLR: 0.030000\n",
      "Train Epoch: 1292 [167936/194182 (85%)]\tLoss: 0.270880\tGrad Norm: 1.501156\tLR: 0.030000\n",
      "Train Epoch: 1292 [188416/194182 (96%)]\tLoss: 0.260775\tGrad Norm: 1.151822\tLR: 0.030000\n",
      "Train set: Average loss: 0.2689\n",
      "Test set: Average loss: 0.2483, Average MAE: 0.3410\n",
      "Train Epoch: 1293 [4096/194182 (2%)]\tLoss: 0.266611\tGrad Norm: 1.592125\tLR: 0.030000\n",
      "Train Epoch: 1293 [24576/194182 (12%)]\tLoss: 0.266941\tGrad Norm: 1.442533\tLR: 0.030000\n",
      "Train Epoch: 1293 [45056/194182 (23%)]\tLoss: 0.266865\tGrad Norm: 1.373076\tLR: 0.030000\n",
      "Train Epoch: 1293 [65536/194182 (33%)]\tLoss: 0.273683\tGrad Norm: 1.649892\tLR: 0.030000\n",
      "Train Epoch: 1293 [86016/194182 (44%)]\tLoss: 0.277665\tGrad Norm: 1.525058\tLR: 0.030000\n",
      "Train Epoch: 1293 [106496/194182 (54%)]\tLoss: 0.262548\tGrad Norm: 1.388205\tLR: 0.030000\n",
      "Train Epoch: 1293 [126976/194182 (65%)]\tLoss: 0.265283\tGrad Norm: 1.707785\tLR: 0.030000\n",
      "Train Epoch: 1293 [147456/194182 (75%)]\tLoss: 0.269857\tGrad Norm: 1.953485\tLR: 0.030000\n",
      "Train Epoch: 1293 [167936/194182 (85%)]\tLoss: 0.268520\tGrad Norm: 1.425741\tLR: 0.030000\n",
      "Train Epoch: 1293 [188416/194182 (96%)]\tLoss: 0.271468\tGrad Norm: 1.432194\tLR: 0.030000\n",
      "Train set: Average loss: 0.2690\n",
      "Test set: Average loss: 0.2505, Average MAE: 0.3553\n",
      "Train Epoch: 1294 [4096/194182 (2%)]\tLoss: 0.268806\tGrad Norm: 1.524813\tLR: 0.030000\n",
      "Train Epoch: 1294 [24576/194182 (12%)]\tLoss: 0.284221\tGrad Norm: 2.250814\tLR: 0.030000\n",
      "Train Epoch: 1294 [45056/194182 (23%)]\tLoss: 0.263269\tGrad Norm: 1.420526\tLR: 0.030000\n",
      "Train Epoch: 1294 [65536/194182 (33%)]\tLoss: 0.267434\tGrad Norm: 1.418415\tLR: 0.030000\n",
      "Train Epoch: 1294 [86016/194182 (44%)]\tLoss: 0.271118\tGrad Norm: 1.468537\tLR: 0.030000\n",
      "Train Epoch: 1294 [106496/194182 (54%)]\tLoss: 0.258512\tGrad Norm: 1.266325\tLR: 0.030000\n",
      "Train Epoch: 1294 [126976/194182 (65%)]\tLoss: 0.258878\tGrad Norm: 1.282442\tLR: 0.030000\n",
      "Train Epoch: 1294 [147456/194182 (75%)]\tLoss: 0.269694\tGrad Norm: 1.707728\tLR: 0.030000\n",
      "Train Epoch: 1294 [167936/194182 (85%)]\tLoss: 0.273994\tGrad Norm: 1.873455\tLR: 0.030000\n",
      "Train Epoch: 1294 [188416/194182 (96%)]\tLoss: 0.267580\tGrad Norm: 1.364709\tLR: 0.030000\n",
      "Train set: Average loss: 0.2695\n",
      "Test set: Average loss: 0.2460, Average MAE: 0.3452\n",
      "Train Epoch: 1295 [4096/194182 (2%)]\tLoss: 0.260487\tGrad Norm: 1.272918\tLR: 0.030000\n",
      "Train Epoch: 1295 [24576/194182 (12%)]\tLoss: 0.268797\tGrad Norm: 1.419609\tLR: 0.030000\n",
      "Train Epoch: 1295 [45056/194182 (23%)]\tLoss: 0.262756\tGrad Norm: 1.630868\tLR: 0.030000\n",
      "Train Epoch: 1295 [65536/194182 (33%)]\tLoss: 0.260297\tGrad Norm: 1.456018\tLR: 0.030000\n",
      "Train Epoch: 1295 [86016/194182 (44%)]\tLoss: 0.263897\tGrad Norm: 1.465385\tLR: 0.030000\n",
      "Train Epoch: 1295 [106496/194182 (54%)]\tLoss: 0.267070\tGrad Norm: 1.428689\tLR: 0.030000\n",
      "Train Epoch: 1295 [126976/194182 (65%)]\tLoss: 0.261566\tGrad Norm: 1.132237\tLR: 0.030000\n",
      "Train Epoch: 1295 [147456/194182 (75%)]\tLoss: 0.266089\tGrad Norm: 1.434596\tLR: 0.030000\n",
      "Train Epoch: 1295 [167936/194182 (85%)]\tLoss: 0.271932\tGrad Norm: 1.585729\tLR: 0.030000\n",
      "Train Epoch: 1295 [188416/194182 (96%)]\tLoss: 0.277276\tGrad Norm: 1.569654\tLR: 0.030000\n",
      "Train set: Average loss: 0.2677\n",
      "Test set: Average loss: 0.2508, Average MAE: 0.3558\n",
      "Epoch 1295: Mean reward = 0.062 +/- 0.048\n",
      "Train Epoch: 1296 [4096/194182 (2%)]\tLoss: 0.268653\tGrad Norm: 1.533220\tLR: 0.030000\n",
      "Train Epoch: 1296 [24576/194182 (12%)]\tLoss: 0.273211\tGrad Norm: 1.748507\tLR: 0.030000\n",
      "Train Epoch: 1296 [45056/194182 (23%)]\tLoss: 0.273602\tGrad Norm: 1.519619\tLR: 0.030000\n",
      "Train Epoch: 1296 [65536/194182 (33%)]\tLoss: 0.258946\tGrad Norm: 1.162612\tLR: 0.030000\n",
      "Train Epoch: 1296 [86016/194182 (44%)]\tLoss: 0.272326\tGrad Norm: 1.675823\tLR: 0.030000\n",
      "Train Epoch: 1296 [106496/194182 (54%)]\tLoss: 0.267005\tGrad Norm: 1.531256\tLR: 0.030000\n",
      "Train Epoch: 1296 [126976/194182 (65%)]\tLoss: 0.276232\tGrad Norm: 2.010620\tLR: 0.030000\n",
      "Train Epoch: 1296 [147456/194182 (75%)]\tLoss: 0.275788\tGrad Norm: 1.798119\tLR: 0.030000\n",
      "Train Epoch: 1296 [167936/194182 (85%)]\tLoss: 0.268787\tGrad Norm: 1.829331\tLR: 0.030000\n",
      "Train Epoch: 1296 [188416/194182 (96%)]\tLoss: 0.266450\tGrad Norm: 1.535762\tLR: 0.030000\n",
      "Train set: Average loss: 0.2695\n",
      "Test set: Average loss: 0.2488, Average MAE: 0.3560\n",
      "Train Epoch: 1297 [4096/194182 (2%)]\tLoss: 0.272718\tGrad Norm: 1.525145\tLR: 0.030000\n",
      "Train Epoch: 1297 [24576/194182 (12%)]\tLoss: 0.272239\tGrad Norm: 1.534220\tLR: 0.030000\n",
      "Train Epoch: 1297 [45056/194182 (23%)]\tLoss: 0.263805\tGrad Norm: 1.220785\tLR: 0.030000\n",
      "Train Epoch: 1297 [65536/194182 (33%)]\tLoss: 0.272504\tGrad Norm: 1.632010\tLR: 0.030000\n",
      "Train Epoch: 1297 [86016/194182 (44%)]\tLoss: 0.272607\tGrad Norm: 1.511858\tLR: 0.030000\n",
      "Train Epoch: 1297 [106496/194182 (54%)]\tLoss: 0.273177\tGrad Norm: 1.922651\tLR: 0.030000\n",
      "Train Epoch: 1297 [126976/194182 (65%)]\tLoss: 0.266066\tGrad Norm: 1.430330\tLR: 0.030000\n",
      "Train Epoch: 1297 [147456/194182 (75%)]\tLoss: 0.276511\tGrad Norm: 1.322852\tLR: 0.030000\n",
      "Train Epoch: 1297 [167936/194182 (85%)]\tLoss: 0.265231\tGrad Norm: 1.380669\tLR: 0.030000\n",
      "Train Epoch: 1297 [188416/194182 (96%)]\tLoss: 0.264759\tGrad Norm: 1.471326\tLR: 0.030000\n",
      "Train set: Average loss: 0.2676\n",
      "Test set: Average loss: 0.2475, Average MAE: 0.3378\n",
      "Train Epoch: 1298 [4096/194182 (2%)]\tLoss: 0.263267\tGrad Norm: 1.573253\tLR: 0.030000\n",
      "Train Epoch: 1298 [24576/194182 (12%)]\tLoss: 0.262537\tGrad Norm: 1.456331\tLR: 0.030000\n",
      "Train Epoch: 1298 [45056/194182 (23%)]\tLoss: 0.258846\tGrad Norm: 1.388306\tLR: 0.030000\n",
      "Train Epoch: 1298 [65536/194182 (33%)]\tLoss: 0.269956\tGrad Norm: 1.585653\tLR: 0.030000\n",
      "Train Epoch: 1298 [86016/194182 (44%)]\tLoss: 0.269146\tGrad Norm: 1.624207\tLR: 0.030000\n",
      "Train Epoch: 1298 [106496/194182 (54%)]\tLoss: 0.270622\tGrad Norm: 1.444981\tLR: 0.030000\n",
      "Train Epoch: 1298 [126976/194182 (65%)]\tLoss: 0.272997\tGrad Norm: 1.645951\tLR: 0.030000\n",
      "Train Epoch: 1298 [147456/194182 (75%)]\tLoss: 0.278882\tGrad Norm: 2.107254\tLR: 0.030000\n",
      "Train Epoch: 1298 [167936/194182 (85%)]\tLoss: 0.271203\tGrad Norm: 1.697053\tLR: 0.030000\n",
      "Train Epoch: 1298 [188416/194182 (96%)]\tLoss: 0.260648\tGrad Norm: 0.853192\tLR: 0.030000\n",
      "Train set: Average loss: 0.2679\n",
      "Test set: Average loss: 0.2474, Average MAE: 0.3464\n",
      "Train Epoch: 1299 [4096/194182 (2%)]\tLoss: 0.267655\tGrad Norm: 1.345038\tLR: 0.030000\n",
      "Train Epoch: 1299 [24576/194182 (12%)]\tLoss: 0.267475\tGrad Norm: 1.457324\tLR: 0.030000\n",
      "Train Epoch: 1299 [45056/194182 (23%)]\tLoss: 0.267387\tGrad Norm: 1.582873\tLR: 0.030000\n",
      "Train Epoch: 1299 [65536/194182 (33%)]\tLoss: 0.264832\tGrad Norm: 1.359188\tLR: 0.030000\n",
      "Train Epoch: 1299 [86016/194182 (44%)]\tLoss: 0.270477\tGrad Norm: 1.130990\tLR: 0.030000\n",
      "Train Epoch: 1299 [106496/194182 (54%)]\tLoss: 0.265646\tGrad Norm: 1.679297\tLR: 0.030000\n",
      "Train Epoch: 1299 [126976/194182 (65%)]\tLoss: 0.265027\tGrad Norm: 1.469661\tLR: 0.030000\n",
      "Train Epoch: 1299 [147456/194182 (75%)]\tLoss: 0.265587\tGrad Norm: 1.421800\tLR: 0.030000\n",
      "Train Epoch: 1299 [167936/194182 (85%)]\tLoss: 0.268222\tGrad Norm: 1.406225\tLR: 0.030000\n",
      "Train Epoch: 1299 [188416/194182 (96%)]\tLoss: 0.259958\tGrad Norm: 1.347110\tLR: 0.030000\n",
      "Train set: Average loss: 0.2668\n",
      "Test set: Average loss: 0.2507, Average MAE: 0.3530\n",
      "Train Epoch: 1300 [4096/194182 (2%)]\tLoss: 0.263199\tGrad Norm: 1.601613\tLR: 0.030000\n",
      "Train Epoch: 1300 [24576/194182 (12%)]\tLoss: 0.272468\tGrad Norm: 1.769015\tLR: 0.030000\n",
      "Train Epoch: 1300 [45056/194182 (23%)]\tLoss: 0.267812\tGrad Norm: 1.642742\tLR: 0.030000\n",
      "Train Epoch: 1300 [65536/194182 (33%)]\tLoss: 0.271136\tGrad Norm: 1.837406\tLR: 0.030000\n",
      "Train Epoch: 1300 [86016/194182 (44%)]\tLoss: 0.259596\tGrad Norm: 1.378272\tLR: 0.030000\n",
      "Train Epoch: 1300 [106496/194182 (54%)]\tLoss: 0.275148\tGrad Norm: 1.446210\tLR: 0.030000\n",
      "Train Epoch: 1300 [126976/194182 (65%)]\tLoss: 0.275348\tGrad Norm: 1.547583\tLR: 0.030000\n",
      "Train Epoch: 1300 [147456/194182 (75%)]\tLoss: 0.273325\tGrad Norm: 1.549774\tLR: 0.030000\n",
      "Train Epoch: 1300 [167936/194182 (85%)]\tLoss: 0.263393\tGrad Norm: 1.216617\tLR: 0.030000\n",
      "Train Epoch: 1300 [188416/194182 (96%)]\tLoss: 0.263042\tGrad Norm: 1.219620\tLR: 0.030000\n",
      "Train set: Average loss: 0.2668\n",
      "Test set: Average loss: 0.2504, Average MAE: 0.3486\n",
      "Epoch 1300: Mean reward = 0.050 +/- 0.032\n",
      "Train Epoch: 1301 [4096/194182 (2%)]\tLoss: 0.266215\tGrad Norm: 1.559944\tLR: 0.030000\n",
      "Train Epoch: 1301 [24576/194182 (12%)]\tLoss: 0.270735\tGrad Norm: 1.586675\tLR: 0.030000\n",
      "Train Epoch: 1301 [45056/194182 (23%)]\tLoss: 0.271549\tGrad Norm: 1.993021\tLR: 0.030000\n",
      "Train Epoch: 1301 [65536/194182 (33%)]\tLoss: 0.272889\tGrad Norm: 1.712697\tLR: 0.030000\n",
      "Train Epoch: 1301 [86016/194182 (44%)]\tLoss: 0.272752\tGrad Norm: 1.889870\tLR: 0.030000\n",
      "Train Epoch: 1301 [106496/194182 (54%)]\tLoss: 0.271956\tGrad Norm: 1.895867\tLR: 0.030000\n",
      "Train Epoch: 1301 [126976/194182 (65%)]\tLoss: 0.264514\tGrad Norm: 1.584915\tLR: 0.030000\n",
      "Train Epoch: 1301 [147456/194182 (75%)]\tLoss: 0.263024\tGrad Norm: 1.393525\tLR: 0.030000\n",
      "Train Epoch: 1301 [167936/194182 (85%)]\tLoss: 0.270910\tGrad Norm: 1.460231\tLR: 0.030000\n",
      "Train Epoch: 1301 [188416/194182 (96%)]\tLoss: 0.274864\tGrad Norm: 1.799548\tLR: 0.030000\n",
      "Train set: Average loss: 0.2689\n",
      "Test set: Average loss: 0.2515, Average MAE: 0.3449\n",
      "Train Epoch: 1302 [4096/194182 (2%)]\tLoss: 0.273891\tGrad Norm: 1.720044\tLR: 0.030000\n",
      "Train Epoch: 1302 [24576/194182 (12%)]\tLoss: 0.267705\tGrad Norm: 1.573934\tLR: 0.030000\n",
      "Train Epoch: 1302 [45056/194182 (23%)]\tLoss: 0.267238\tGrad Norm: 1.513723\tLR: 0.030000\n",
      "Train Epoch: 1302 [65536/194182 (33%)]\tLoss: 0.267497\tGrad Norm: 1.453841\tLR: 0.030000\n",
      "Train Epoch: 1302 [86016/194182 (44%)]\tLoss: 0.267257\tGrad Norm: 1.588308\tLR: 0.030000\n",
      "Train Epoch: 1302 [106496/194182 (54%)]\tLoss: 0.262763\tGrad Norm: 1.315682\tLR: 0.030000\n",
      "Train Epoch: 1302 [126976/194182 (65%)]\tLoss: 0.266843\tGrad Norm: 1.569731\tLR: 0.030000\n",
      "Train Epoch: 1302 [147456/194182 (75%)]\tLoss: 0.269523\tGrad Norm: 1.720532\tLR: 0.030000\n",
      "Train Epoch: 1302 [167936/194182 (85%)]\tLoss: 0.267087\tGrad Norm: 1.360345\tLR: 0.030000\n",
      "Train Epoch: 1302 [188416/194182 (96%)]\tLoss: 0.278220\tGrad Norm: 1.786027\tLR: 0.030000\n",
      "Train set: Average loss: 0.2670\n",
      "Test set: Average loss: 0.2596, Average MAE: 0.3616\n",
      "Train Epoch: 1303 [4096/194182 (2%)]\tLoss: 0.280798\tGrad Norm: 2.422663\tLR: 0.030000\n",
      "Train Epoch: 1303 [24576/194182 (12%)]\tLoss: 0.266312\tGrad Norm: 1.595474\tLR: 0.030000\n",
      "Train Epoch: 1303 [45056/194182 (23%)]\tLoss: 0.269639\tGrad Norm: 1.691242\tLR: 0.030000\n",
      "Train Epoch: 1303 [65536/194182 (33%)]\tLoss: 0.270081\tGrad Norm: 1.559540\tLR: 0.030000\n",
      "Train Epoch: 1303 [86016/194182 (44%)]\tLoss: 0.265018\tGrad Norm: 1.612900\tLR: 0.030000\n",
      "Train Epoch: 1303 [106496/194182 (54%)]\tLoss: 0.275677\tGrad Norm: 1.819814\tLR: 0.030000\n",
      "Train Epoch: 1303 [126976/194182 (65%)]\tLoss: 0.272319\tGrad Norm: 1.530444\tLR: 0.030000\n",
      "Train Epoch: 1303 [147456/194182 (75%)]\tLoss: 0.266091\tGrad Norm: 1.405220\tLR: 0.030000\n",
      "Train Epoch: 1303 [167936/194182 (85%)]\tLoss: 0.258623\tGrad Norm: 1.141794\tLR: 0.030000\n",
      "Train Epoch: 1303 [188416/194182 (96%)]\tLoss: 0.272499\tGrad Norm: 1.692070\tLR: 0.030000\n",
      "Train set: Average loss: 0.2681\n",
      "Test set: Average loss: 0.2468, Average MAE: 0.3473\n",
      "Train Epoch: 1304 [4096/194182 (2%)]\tLoss: 0.265305\tGrad Norm: 1.370752\tLR: 0.030000\n",
      "Train Epoch: 1304 [24576/194182 (12%)]\tLoss: 0.265446\tGrad Norm: 1.493565\tLR: 0.030000\n",
      "Train Epoch: 1304 [45056/194182 (23%)]\tLoss: 0.257303\tGrad Norm: 1.314072\tLR: 0.030000\n",
      "Train Epoch: 1304 [65536/194182 (33%)]\tLoss: 0.266794\tGrad Norm: 1.064719\tLR: 0.030000\n",
      "Train Epoch: 1304 [86016/194182 (44%)]\tLoss: 0.259159\tGrad Norm: 1.592764\tLR: 0.030000\n",
      "Train Epoch: 1304 [106496/194182 (54%)]\tLoss: 0.263059\tGrad Norm: 1.592056\tLR: 0.030000\n",
      "Train Epoch: 1304 [126976/194182 (65%)]\tLoss: 0.266347\tGrad Norm: 1.617832\tLR: 0.030000\n",
      "Train Epoch: 1304 [147456/194182 (75%)]\tLoss: 0.272406\tGrad Norm: 1.779402\tLR: 0.030000\n",
      "Train Epoch: 1304 [167936/194182 (85%)]\tLoss: 0.267188\tGrad Norm: 1.686328\tLR: 0.030000\n",
      "Train Epoch: 1304 [188416/194182 (96%)]\tLoss: 0.267964\tGrad Norm: 1.583203\tLR: 0.030000\n",
      "Train set: Average loss: 0.2664\n",
      "Test set: Average loss: 0.2488, Average MAE: 0.3429\n",
      "Train Epoch: 1305 [4096/194182 (2%)]\tLoss: 0.269119\tGrad Norm: 1.463276\tLR: 0.030000\n",
      "Train Epoch: 1305 [24576/194182 (12%)]\tLoss: 0.266798\tGrad Norm: 1.385248\tLR: 0.030000\n",
      "Train Epoch: 1305 [45056/194182 (23%)]\tLoss: 0.264110\tGrad Norm: 1.486557\tLR: 0.030000\n",
      "Train Epoch: 1305 [65536/194182 (33%)]\tLoss: 0.265767\tGrad Norm: 1.383201\tLR: 0.030000\n",
      "Train Epoch: 1305 [86016/194182 (44%)]\tLoss: 0.258754\tGrad Norm: 1.310384\tLR: 0.030000\n",
      "Train Epoch: 1305 [106496/194182 (54%)]\tLoss: 0.269451\tGrad Norm: 1.573195\tLR: 0.030000\n",
      "Train Epoch: 1305 [126976/194182 (65%)]\tLoss: 0.271128\tGrad Norm: 1.549344\tLR: 0.030000\n",
      "Train Epoch: 1305 [147456/194182 (75%)]\tLoss: 0.271545\tGrad Norm: 1.955769\tLR: 0.030000\n",
      "Train Epoch: 1305 [167936/194182 (85%)]\tLoss: 0.262374\tGrad Norm: 1.881003\tLR: 0.030000\n",
      "Train Epoch: 1305 [188416/194182 (96%)]\tLoss: 0.270524\tGrad Norm: 1.748521\tLR: 0.030000\n",
      "Train set: Average loss: 0.2674\n",
      "Test set: Average loss: 0.2483, Average MAE: 0.3552\n",
      "Epoch 1305: Mean reward = 0.046 +/- 0.001\n",
      "Train Epoch: 1306 [4096/194182 (2%)]\tLoss: 0.270416\tGrad Norm: 1.520392\tLR: 0.030000\n",
      "Train Epoch: 1306 [24576/194182 (12%)]\tLoss: 0.269591\tGrad Norm: 1.668630\tLR: 0.030000\n",
      "Train Epoch: 1306 [45056/194182 (23%)]\tLoss: 0.269219\tGrad Norm: 1.699408\tLR: 0.030000\n",
      "Train Epoch: 1306 [65536/194182 (33%)]\tLoss: 0.271513\tGrad Norm: 1.974476\tLR: 0.030000\n",
      "Train Epoch: 1306 [86016/194182 (44%)]\tLoss: 0.275158\tGrad Norm: 1.901307\tLR: 0.030000\n",
      "Train Epoch: 1306 [106496/194182 (54%)]\tLoss: 0.255127\tGrad Norm: 1.146853\tLR: 0.030000\n",
      "Train Epoch: 1306 [126976/194182 (65%)]\tLoss: 0.262447\tGrad Norm: 1.578508\tLR: 0.030000\n",
      "Train Epoch: 1306 [147456/194182 (75%)]\tLoss: 0.265526\tGrad Norm: 1.309217\tLR: 0.030000\n",
      "Train Epoch: 1306 [167936/194182 (85%)]\tLoss: 0.274873\tGrad Norm: 1.446413\tLR: 0.030000\n",
      "Train Epoch: 1306 [188416/194182 (96%)]\tLoss: 0.262251\tGrad Norm: 1.597179\tLR: 0.030000\n",
      "Train set: Average loss: 0.2687\n",
      "Test set: Average loss: 0.2457, Average MAE: 0.3372\n",
      "Train Epoch: 1307 [4096/194182 (2%)]\tLoss: 0.260952\tGrad Norm: 1.405103\tLR: 0.030000\n",
      "Train Epoch: 1307 [24576/194182 (12%)]\tLoss: 0.271768\tGrad Norm: 1.753892\tLR: 0.030000\n",
      "Train Epoch: 1307 [45056/194182 (23%)]\tLoss: 0.264931\tGrad Norm: 1.236196\tLR: 0.030000\n",
      "Train Epoch: 1307 [65536/194182 (33%)]\tLoss: 0.263288\tGrad Norm: 1.209918\tLR: 0.030000\n",
      "Train Epoch: 1307 [86016/194182 (44%)]\tLoss: 0.263084\tGrad Norm: 1.276168\tLR: 0.030000\n",
      "Train Epoch: 1307 [106496/194182 (54%)]\tLoss: 0.265075\tGrad Norm: 1.109928\tLR: 0.030000\n",
      "Train Epoch: 1307 [126976/194182 (65%)]\tLoss: 0.270685\tGrad Norm: 1.788596\tLR: 0.030000\n",
      "Train Epoch: 1307 [147456/194182 (75%)]\tLoss: 0.268809\tGrad Norm: 1.595880\tLR: 0.030000\n",
      "Train Epoch: 1307 [167936/194182 (85%)]\tLoss: 0.257105\tGrad Norm: 1.236530\tLR: 0.030000\n",
      "Train Epoch: 1307 [188416/194182 (96%)]\tLoss: 0.268504\tGrad Norm: 1.208604\tLR: 0.030000\n",
      "Train set: Average loss: 0.2644\n",
      "Test set: Average loss: 0.2462, Average MAE: 0.3475\n",
      "Train Epoch: 1308 [4096/194182 (2%)]\tLoss: 0.255265\tGrad Norm: 1.339186\tLR: 0.030000\n",
      "Train Epoch: 1308 [24576/194182 (12%)]\tLoss: 0.261860\tGrad Norm: 1.660956\tLR: 0.030000\n",
      "Train Epoch: 1308 [45056/194182 (23%)]\tLoss: 0.269663\tGrad Norm: 1.564061\tLR: 0.030000\n",
      "Train Epoch: 1308 [65536/194182 (33%)]\tLoss: 0.276401\tGrad Norm: 1.908927\tLR: 0.030000\n",
      "Train Epoch: 1308 [86016/194182 (44%)]\tLoss: 0.265902\tGrad Norm: 1.629211\tLR: 0.030000\n",
      "Train Epoch: 1308 [106496/194182 (54%)]\tLoss: 0.274164\tGrad Norm: 1.948652\tLR: 0.030000\n",
      "Train Epoch: 1308 [126976/194182 (65%)]\tLoss: 0.271109\tGrad Norm: 1.656716\tLR: 0.030000\n",
      "Train Epoch: 1308 [147456/194182 (75%)]\tLoss: 0.265021\tGrad Norm: 1.278770\tLR: 0.030000\n",
      "Train Epoch: 1308 [167936/194182 (85%)]\tLoss: 0.267939\tGrad Norm: 1.647984\tLR: 0.030000\n",
      "Train Epoch: 1308 [188416/194182 (96%)]\tLoss: 0.271389\tGrad Norm: 1.539405\tLR: 0.030000\n",
      "Train set: Average loss: 0.2682\n",
      "Test set: Average loss: 0.2454, Average MAE: 0.3497\n",
      "Train Epoch: 1309 [4096/194182 (2%)]\tLoss: 0.265761\tGrad Norm: 1.312888\tLR: 0.030000\n",
      "Train Epoch: 1309 [24576/194182 (12%)]\tLoss: 0.267899\tGrad Norm: 1.634867\tLR: 0.030000\n",
      "Train Epoch: 1309 [45056/194182 (23%)]\tLoss: 0.261630\tGrad Norm: 1.438811\tLR: 0.030000\n",
      "Train Epoch: 1309 [65536/194182 (33%)]\tLoss: 0.262602\tGrad Norm: 1.448172\tLR: 0.030000\n",
      "Train Epoch: 1309 [86016/194182 (44%)]\tLoss: 0.253269\tGrad Norm: 1.229857\tLR: 0.030000\n",
      "Train Epoch: 1309 [106496/194182 (54%)]\tLoss: 0.270602\tGrad Norm: 1.731026\tLR: 0.030000\n",
      "Train Epoch: 1309 [126976/194182 (65%)]\tLoss: 0.266608\tGrad Norm: 1.198266\tLR: 0.030000\n",
      "Train Epoch: 1309 [147456/194182 (75%)]\tLoss: 0.265528\tGrad Norm: 1.529952\tLR: 0.030000\n",
      "Train Epoch: 1309 [167936/194182 (85%)]\tLoss: 0.261041\tGrad Norm: 1.499053\tLR: 0.030000\n",
      "Train Epoch: 1309 [188416/194182 (96%)]\tLoss: 0.260409\tGrad Norm: 1.436848\tLR: 0.030000\n",
      "Train set: Average loss: 0.2650\n",
      "Test set: Average loss: 0.2507, Average MAE: 0.3573\n",
      "Train Epoch: 1310 [4096/194182 (2%)]\tLoss: 0.268762\tGrad Norm: 1.699726\tLR: 0.030000\n",
      "Train Epoch: 1310 [24576/194182 (12%)]\tLoss: 0.274046\tGrad Norm: 2.158936\tLR: 0.030000\n",
      "Train Epoch: 1310 [45056/194182 (23%)]\tLoss: 0.273453\tGrad Norm: 1.857553\tLR: 0.030000\n",
      "Train Epoch: 1310 [65536/194182 (33%)]\tLoss: 0.264098\tGrad Norm: 1.835908\tLR: 0.030000\n",
      "Train Epoch: 1310 [86016/194182 (44%)]\tLoss: 0.271590\tGrad Norm: 1.802377\tLR: 0.030000\n",
      "Train Epoch: 1310 [106496/194182 (54%)]\tLoss: 0.272507\tGrad Norm: 1.766081\tLR: 0.030000\n",
      "Train Epoch: 1310 [126976/194182 (65%)]\tLoss: 0.265596\tGrad Norm: 1.779180\tLR: 0.030000\n",
      "Train Epoch: 1310 [147456/194182 (75%)]\tLoss: 0.268535\tGrad Norm: 1.506037\tLR: 0.030000\n",
      "Train Epoch: 1310 [167936/194182 (85%)]\tLoss: 0.264887\tGrad Norm: 1.440976\tLR: 0.030000\n",
      "Train Epoch: 1310 [188416/194182 (96%)]\tLoss: 0.258708\tGrad Norm: 1.507276\tLR: 0.030000\n",
      "Train set: Average loss: 0.2676\n",
      "Test set: Average loss: 0.2450, Average MAE: 0.3456\n",
      "Epoch 1310: Mean reward = 0.059 +/- 0.071\n",
      "Train Epoch: 1311 [4096/194182 (2%)]\tLoss: 0.263247\tGrad Norm: 1.267616\tLR: 0.030000\n",
      "Train Epoch: 1311 [24576/194182 (12%)]\tLoss: 0.265718\tGrad Norm: 1.388471\tLR: 0.030000\n",
      "Train Epoch: 1311 [45056/194182 (23%)]\tLoss: 0.273753\tGrad Norm: 1.659710\tLR: 0.030000\n",
      "Train Epoch: 1311 [65536/194182 (33%)]\tLoss: 0.257141\tGrad Norm: 1.322235\tLR: 0.030000\n",
      "Train Epoch: 1311 [86016/194182 (44%)]\tLoss: 0.265454\tGrad Norm: 1.496646\tLR: 0.030000\n",
      "Train Epoch: 1311 [106496/194182 (54%)]\tLoss: 0.275773\tGrad Norm: 1.906775\tLR: 0.030000\n",
      "Train Epoch: 1311 [126976/194182 (65%)]\tLoss: 0.269480\tGrad Norm: 1.702326\tLR: 0.030000\n",
      "Train Epoch: 1311 [147456/194182 (75%)]\tLoss: 0.261846\tGrad Norm: 1.361227\tLR: 0.030000\n",
      "Train Epoch: 1311 [167936/194182 (85%)]\tLoss: 0.263762\tGrad Norm: 1.618593\tLR: 0.030000\n",
      "Train Epoch: 1311 [188416/194182 (96%)]\tLoss: 0.263520\tGrad Norm: 1.482348\tLR: 0.030000\n",
      "Train set: Average loss: 0.2664\n",
      "Test set: Average loss: 0.2513, Average MAE: 0.3377\n",
      "Train Epoch: 1312 [4096/194182 (2%)]\tLoss: 0.261724\tGrad Norm: 1.800587\tLR: 0.030000\n",
      "Train Epoch: 1312 [24576/194182 (12%)]\tLoss: 0.267562\tGrad Norm: 1.708406\tLR: 0.030000\n",
      "Train Epoch: 1312 [45056/194182 (23%)]\tLoss: 0.268264\tGrad Norm: 1.789853\tLR: 0.030000\n",
      "Train Epoch: 1312 [65536/194182 (33%)]\tLoss: 0.258197\tGrad Norm: 1.244947\tLR: 0.030000\n",
      "Train Epoch: 1312 [86016/194182 (44%)]\tLoss: 0.263419\tGrad Norm: 1.166280\tLR: 0.030000\n",
      "Train Epoch: 1312 [106496/194182 (54%)]\tLoss: 0.270529\tGrad Norm: 1.873969\tLR: 0.030000\n",
      "Train Epoch: 1312 [126976/194182 (65%)]\tLoss: 0.267291\tGrad Norm: 1.131974\tLR: 0.030000\n",
      "Train Epoch: 1312 [147456/194182 (75%)]\tLoss: 0.265633\tGrad Norm: 1.530117\tLR: 0.030000\n",
      "Train Epoch: 1312 [167936/194182 (85%)]\tLoss: 0.261519\tGrad Norm: 1.425944\tLR: 0.030000\n",
      "Train Epoch: 1312 [188416/194182 (96%)]\tLoss: 0.261922\tGrad Norm: 1.468064\tLR: 0.030000\n",
      "Train set: Average loss: 0.2648\n",
      "Test set: Average loss: 0.2460, Average MAE: 0.3397\n",
      "Train Epoch: 1313 [4096/194182 (2%)]\tLoss: 0.267118\tGrad Norm: 1.498507\tLR: 0.030000\n",
      "Train Epoch: 1313 [24576/194182 (12%)]\tLoss: 0.262805\tGrad Norm: 1.364191\tLR: 0.030000\n",
      "Train Epoch: 1313 [45056/194182 (23%)]\tLoss: 0.273205\tGrad Norm: 1.853993\tLR: 0.030000\n",
      "Train Epoch: 1313 [65536/194182 (33%)]\tLoss: 0.262041\tGrad Norm: 1.522552\tLR: 0.030000\n",
      "Train Epoch: 1313 [86016/194182 (44%)]\tLoss: 0.266524\tGrad Norm: 1.492742\tLR: 0.030000\n",
      "Train Epoch: 1313 [106496/194182 (54%)]\tLoss: 0.267397\tGrad Norm: 1.478801\tLR: 0.030000\n",
      "Train Epoch: 1313 [126976/194182 (65%)]\tLoss: 0.266440\tGrad Norm: 1.566096\tLR: 0.030000\n",
      "Train Epoch: 1313 [147456/194182 (75%)]\tLoss: 0.275259\tGrad Norm: 1.878647\tLR: 0.030000\n",
      "Train Epoch: 1313 [167936/194182 (85%)]\tLoss: 0.273715\tGrad Norm: 2.006601\tLR: 0.030000\n",
      "Train Epoch: 1313 [188416/194182 (96%)]\tLoss: 0.264513\tGrad Norm: 1.692172\tLR: 0.030000\n",
      "Train set: Average loss: 0.2668\n",
      "Test set: Average loss: 0.2531, Average MAE: 0.3426\n",
      "Train Epoch: 1314 [4096/194182 (2%)]\tLoss: 0.270244\tGrad Norm: 1.833793\tLR: 0.030000\n",
      "Train Epoch: 1314 [24576/194182 (12%)]\tLoss: 0.264239\tGrad Norm: 1.587786\tLR: 0.030000\n",
      "Train Epoch: 1314 [45056/194182 (23%)]\tLoss: 0.264174\tGrad Norm: 1.397978\tLR: 0.030000\n",
      "Train Epoch: 1314 [65536/194182 (33%)]\tLoss: 0.272100\tGrad Norm: 1.696799\tLR: 0.030000\n",
      "Train Epoch: 1314 [86016/194182 (44%)]\tLoss: 0.263629\tGrad Norm: 1.566424\tLR: 0.030000\n",
      "Train Epoch: 1314 [106496/194182 (54%)]\tLoss: 0.267393\tGrad Norm: 1.472919\tLR: 0.030000\n",
      "Train Epoch: 1314 [126976/194182 (65%)]\tLoss: 0.262287\tGrad Norm: 1.165569\tLR: 0.030000\n",
      "Train Epoch: 1314 [147456/194182 (75%)]\tLoss: 0.262051\tGrad Norm: 1.519596\tLR: 0.030000\n",
      "Train Epoch: 1314 [167936/194182 (85%)]\tLoss: 0.262470\tGrad Norm: 1.539535\tLR: 0.030000\n",
      "Train Epoch: 1314 [188416/194182 (96%)]\tLoss: 0.273821\tGrad Norm: 1.489261\tLR: 0.030000\n",
      "Train set: Average loss: 0.2657\n",
      "Test set: Average loss: 0.2446, Average MAE: 0.3421\n",
      "Train Epoch: 1315 [4096/194182 (2%)]\tLoss: 0.255742\tGrad Norm: 1.304664\tLR: 0.030000\n",
      "Train Epoch: 1315 [24576/194182 (12%)]\tLoss: 0.271036\tGrad Norm: 1.857525\tLR: 0.030000\n",
      "Train Epoch: 1315 [45056/194182 (23%)]\tLoss: 0.261994\tGrad Norm: 1.525551\tLR: 0.030000\n",
      "Train Epoch: 1315 [65536/194182 (33%)]\tLoss: 0.264520\tGrad Norm: 1.472526\tLR: 0.030000\n",
      "Train Epoch: 1315 [86016/194182 (44%)]\tLoss: 0.260928\tGrad Norm: 1.260253\tLR: 0.030000\n",
      "Train Epoch: 1315 [106496/194182 (54%)]\tLoss: 0.257889\tGrad Norm: 1.261808\tLR: 0.030000\n",
      "Train Epoch: 1315 [126976/194182 (65%)]\tLoss: 0.260446\tGrad Norm: 1.301487\tLR: 0.030000\n",
      "Train Epoch: 1315 [147456/194182 (75%)]\tLoss: 0.267795\tGrad Norm: 1.454307\tLR: 0.030000\n",
      "Train Epoch: 1315 [167936/194182 (85%)]\tLoss: 0.265350\tGrad Norm: 1.597307\tLR: 0.030000\n",
      "Train Epoch: 1315 [188416/194182 (96%)]\tLoss: 0.270080\tGrad Norm: 1.577740\tLR: 0.030000\n",
      "Train set: Average loss: 0.2647\n",
      "Test set: Average loss: 0.2548, Average MAE: 0.3622\n",
      "Epoch 1315: Mean reward = 0.046 +/- 0.001\n",
      "Train Epoch: 1316 [4096/194182 (2%)]\tLoss: 0.268650\tGrad Norm: 1.877304\tLR: 0.030000\n",
      "Train Epoch: 1316 [24576/194182 (12%)]\tLoss: 0.274233\tGrad Norm: 2.193880\tLR: 0.030000\n",
      "Train Epoch: 1316 [45056/194182 (23%)]\tLoss: 0.268185\tGrad Norm: 1.750095\tLR: 0.030000\n",
      "Train Epoch: 1316 [65536/194182 (33%)]\tLoss: 0.268752\tGrad Norm: 1.327142\tLR: 0.030000\n",
      "Train Epoch: 1316 [86016/194182 (44%)]\tLoss: 0.266177\tGrad Norm: 1.460713\tLR: 0.030000\n",
      "Train Epoch: 1316 [106496/194182 (54%)]\tLoss: 0.264043\tGrad Norm: 1.411759\tLR: 0.030000\n",
      "Train Epoch: 1316 [126976/194182 (65%)]\tLoss: 0.268691\tGrad Norm: 1.537345\tLR: 0.030000\n",
      "Train Epoch: 1316 [147456/194182 (75%)]\tLoss: 0.258721\tGrad Norm: 1.724294\tLR: 0.030000\n",
      "Train Epoch: 1316 [167936/194182 (85%)]\tLoss: 0.263479\tGrad Norm: 1.523683\tLR: 0.030000\n",
      "Train Epoch: 1316 [188416/194182 (96%)]\tLoss: 0.261528\tGrad Norm: 1.589039\tLR: 0.030000\n",
      "Train set: Average loss: 0.2659\n",
      "Test set: Average loss: 0.2522, Average MAE: 0.3467\n",
      "Train Epoch: 1317 [4096/194182 (2%)]\tLoss: 0.268850\tGrad Norm: 1.627905\tLR: 0.030000\n",
      "Train Epoch: 1317 [24576/194182 (12%)]\tLoss: 0.263420\tGrad Norm: 1.454882\tLR: 0.030000\n",
      "Train Epoch: 1317 [45056/194182 (23%)]\tLoss: 0.263417\tGrad Norm: 1.393255\tLR: 0.030000\n",
      "Train Epoch: 1317 [65536/194182 (33%)]\tLoss: 0.267105\tGrad Norm: 1.226324\tLR: 0.030000\n",
      "Train Epoch: 1317 [86016/194182 (44%)]\tLoss: 0.262853\tGrad Norm: 1.542426\tLR: 0.030000\n",
      "Train Epoch: 1317 [106496/194182 (54%)]\tLoss: 0.257195\tGrad Norm: 1.323569\tLR: 0.030000\n",
      "Train Epoch: 1317 [126976/194182 (65%)]\tLoss: 0.262261\tGrad Norm: 1.724897\tLR: 0.030000\n",
      "Train Epoch: 1317 [147456/194182 (75%)]\tLoss: 0.258614\tGrad Norm: 1.608621\tLR: 0.030000\n",
      "Train Epoch: 1317 [167936/194182 (85%)]\tLoss: 0.269120\tGrad Norm: 1.811565\tLR: 0.030000\n",
      "Train Epoch: 1317 [188416/194182 (96%)]\tLoss: 0.261440\tGrad Norm: 1.529871\tLR: 0.030000\n",
      "Train set: Average loss: 0.2653\n",
      "Test set: Average loss: 0.2592, Average MAE: 0.3614\n",
      "Train Epoch: 1318 [4096/194182 (2%)]\tLoss: 0.269974\tGrad Norm: 2.035958\tLR: 0.030000\n",
      "Train Epoch: 1318 [24576/194182 (12%)]\tLoss: 0.268958\tGrad Norm: 1.592887\tLR: 0.030000\n",
      "Train Epoch: 1318 [45056/194182 (23%)]\tLoss: 0.267956\tGrad Norm: 1.945843\tLR: 0.030000\n",
      "Train Epoch: 1318 [65536/194182 (33%)]\tLoss: 0.266354\tGrad Norm: 1.826475\tLR: 0.030000\n",
      "Train Epoch: 1318 [86016/194182 (44%)]\tLoss: 0.269223\tGrad Norm: 1.577817\tLR: 0.030000\n",
      "Train Epoch: 1318 [106496/194182 (54%)]\tLoss: 0.263289\tGrad Norm: 1.294158\tLR: 0.030000\n",
      "Train Epoch: 1318 [126976/194182 (65%)]\tLoss: 0.267848\tGrad Norm: 1.525828\tLR: 0.030000\n",
      "Train Epoch: 1318 [147456/194182 (75%)]\tLoss: 0.266089\tGrad Norm: 1.285971\tLR: 0.030000\n",
      "Train Epoch: 1318 [167936/194182 (85%)]\tLoss: 0.255707\tGrad Norm: 1.387093\tLR: 0.030000\n",
      "Train Epoch: 1318 [188416/194182 (96%)]\tLoss: 0.264647\tGrad Norm: 1.489954\tLR: 0.030000\n",
      "Train set: Average loss: 0.2659\n",
      "Test set: Average loss: 0.2476, Average MAE: 0.3497\n",
      "Train Epoch: 1319 [4096/194182 (2%)]\tLoss: 0.258472\tGrad Norm: 1.505025\tLR: 0.030000\n",
      "Train Epoch: 1319 [24576/194182 (12%)]\tLoss: 0.261458\tGrad Norm: 1.424549\tLR: 0.030000\n",
      "Train Epoch: 1319 [45056/194182 (23%)]\tLoss: 0.266130\tGrad Norm: 1.400236\tLR: 0.030000\n",
      "Train Epoch: 1319 [65536/194182 (33%)]\tLoss: 0.257781\tGrad Norm: 1.555303\tLR: 0.030000\n",
      "Train Epoch: 1319 [86016/194182 (44%)]\tLoss: 0.266174\tGrad Norm: 1.766624\tLR: 0.030000\n",
      "Train Epoch: 1319 [106496/194182 (54%)]\tLoss: 0.265010\tGrad Norm: 1.409896\tLR: 0.030000\n",
      "Train Epoch: 1319 [126976/194182 (65%)]\tLoss: 0.271078\tGrad Norm: 1.705748\tLR: 0.030000\n",
      "Train Epoch: 1319 [147456/194182 (75%)]\tLoss: 0.256160\tGrad Norm: 1.342035\tLR: 0.030000\n",
      "Train Epoch: 1319 [167936/194182 (85%)]\tLoss: 0.272589\tGrad Norm: 1.692958\tLR: 0.030000\n",
      "Train Epoch: 1319 [188416/194182 (96%)]\tLoss: 0.268366\tGrad Norm: 1.631792\tLR: 0.030000\n",
      "Train set: Average loss: 0.2650\n",
      "Test set: Average loss: 0.2616, Average MAE: 0.3699\n",
      "Train Epoch: 1320 [4096/194182 (2%)]\tLoss: 0.275234\tGrad Norm: 2.074190\tLR: 0.030000\n",
      "Train Epoch: 1320 [24576/194182 (12%)]\tLoss: 0.267218\tGrad Norm: 1.806708\tLR: 0.030000\n",
      "Train Epoch: 1320 [45056/194182 (23%)]\tLoss: 0.260141\tGrad Norm: 1.281685\tLR: 0.030000\n",
      "Train Epoch: 1320 [65536/194182 (33%)]\tLoss: 0.273641\tGrad Norm: 1.789654\tLR: 0.030000\n",
      "Train Epoch: 1320 [86016/194182 (44%)]\tLoss: 0.267175\tGrad Norm: 1.727783\tLR: 0.030000\n",
      "Train Epoch: 1320 [106496/194182 (54%)]\tLoss: 0.260037\tGrad Norm: 1.353021\tLR: 0.030000\n",
      "Train Epoch: 1320 [126976/194182 (65%)]\tLoss: 0.259633\tGrad Norm: 1.181277\tLR: 0.030000\n",
      "Train Epoch: 1320 [147456/194182 (75%)]\tLoss: 0.271528\tGrad Norm: 1.818187\tLR: 0.030000\n",
      "Train Epoch: 1320 [167936/194182 (85%)]\tLoss: 0.262723\tGrad Norm: 1.496377\tLR: 0.030000\n",
      "Train Epoch: 1320 [188416/194182 (96%)]\tLoss: 0.268137\tGrad Norm: 1.559925\tLR: 0.030000\n",
      "Train set: Average loss: 0.2654\n",
      "Test set: Average loss: 0.2470, Average MAE: 0.3506\n",
      "Epoch 1320: Mean reward = 0.042 +/- 0.014\n",
      "Train Epoch: 1321 [4096/194182 (2%)]\tLoss: 0.255515\tGrad Norm: 1.232849\tLR: 0.030000\n",
      "Train Epoch: 1321 [24576/194182 (12%)]\tLoss: 0.256387\tGrad Norm: 0.958829\tLR: 0.030000\n",
      "Train Epoch: 1321 [45056/194182 (23%)]\tLoss: 0.262573\tGrad Norm: 1.299662\tLR: 0.030000\n",
      "Train Epoch: 1321 [65536/194182 (33%)]\tLoss: 0.257100\tGrad Norm: 1.202561\tLR: 0.030000\n",
      "Train Epoch: 1321 [86016/194182 (44%)]\tLoss: 0.258336\tGrad Norm: 1.426901\tLR: 0.030000\n",
      "Train Epoch: 1321 [106496/194182 (54%)]\tLoss: 0.265138\tGrad Norm: 1.517111\tLR: 0.030000\n",
      "Train Epoch: 1321 [126976/194182 (65%)]\tLoss: 0.267512\tGrad Norm: 1.425200\tLR: 0.030000\n",
      "Train Epoch: 1321 [147456/194182 (75%)]\tLoss: 0.274426\tGrad Norm: 1.891999\tLR: 0.030000\n",
      "Train Epoch: 1321 [167936/194182 (85%)]\tLoss: 0.260857\tGrad Norm: 1.339232\tLR: 0.030000\n",
      "Train Epoch: 1321 [188416/194182 (96%)]\tLoss: 0.271642\tGrad Norm: 1.716580\tLR: 0.030000\n",
      "Train set: Average loss: 0.2624\n",
      "Test set: Average loss: 0.2530, Average MAE: 0.3346\n",
      "Train Epoch: 1322 [4096/194182 (2%)]\tLoss: 0.272405\tGrad Norm: 1.985403\tLR: 0.030000\n",
      "Train Epoch: 1322 [24576/194182 (12%)]\tLoss: 0.264071\tGrad Norm: 1.551292\tLR: 0.030000\n",
      "Train Epoch: 1322 [45056/194182 (23%)]\tLoss: 0.259742\tGrad Norm: 1.301909\tLR: 0.030000\n",
      "Train Epoch: 1322 [65536/194182 (33%)]\tLoss: 0.265507\tGrad Norm: 1.613826\tLR: 0.030000\n",
      "Train Epoch: 1322 [86016/194182 (44%)]\tLoss: 0.265892\tGrad Norm: 1.510624\tLR: 0.030000\n",
      "Train Epoch: 1322 [106496/194182 (54%)]\tLoss: 0.263056\tGrad Norm: 1.610313\tLR: 0.030000\n",
      "Train Epoch: 1322 [126976/194182 (65%)]\tLoss: 0.255662\tGrad Norm: 1.245313\tLR: 0.030000\n",
      "Train Epoch: 1322 [147456/194182 (75%)]\tLoss: 0.267464\tGrad Norm: 1.693079\tLR: 0.030000\n",
      "Train Epoch: 1322 [167936/194182 (85%)]\tLoss: 0.268695\tGrad Norm: 1.751161\tLR: 0.030000\n",
      "Train Epoch: 1322 [188416/194182 (96%)]\tLoss: 0.274465\tGrad Norm: 1.624755\tLR: 0.030000\n",
      "Train set: Average loss: 0.2644\n",
      "Test set: Average loss: 0.2449, Average MAE: 0.3390\n",
      "Train Epoch: 1323 [4096/194182 (2%)]\tLoss: 0.256242\tGrad Norm: 1.422936\tLR: 0.030000\n",
      "Train Epoch: 1323 [24576/194182 (12%)]\tLoss: 0.262058\tGrad Norm: 1.615640\tLR: 0.030000\n",
      "Train Epoch: 1323 [45056/194182 (23%)]\tLoss: 0.266684\tGrad Norm: 1.548183\tLR: 0.030000\n",
      "Train Epoch: 1323 [65536/194182 (33%)]\tLoss: 0.259748\tGrad Norm: 1.494898\tLR: 0.030000\n",
      "Train Epoch: 1323 [86016/194182 (44%)]\tLoss: 0.266159\tGrad Norm: 1.533985\tLR: 0.030000\n",
      "Train Epoch: 1323 [106496/194182 (54%)]\tLoss: 0.270412\tGrad Norm: 1.537044\tLR: 0.030000\n",
      "Train Epoch: 1323 [126976/194182 (65%)]\tLoss: 0.270592\tGrad Norm: 1.806455\tLR: 0.030000\n",
      "Train Epoch: 1323 [147456/194182 (75%)]\tLoss: 0.265768\tGrad Norm: 1.684263\tLR: 0.030000\n",
      "Train Epoch: 1323 [167936/194182 (85%)]\tLoss: 0.264804\tGrad Norm: 1.582077\tLR: 0.030000\n",
      "Train Epoch: 1323 [188416/194182 (96%)]\tLoss: 0.265685\tGrad Norm: 1.458333\tLR: 0.030000\n",
      "Train set: Average loss: 0.2652\n",
      "Test set: Average loss: 0.2561, Average MAE: 0.3602\n",
      "Train Epoch: 1324 [4096/194182 (2%)]\tLoss: 0.268421\tGrad Norm: 1.882944\tLR: 0.030000\n",
      "Train Epoch: 1324 [24576/194182 (12%)]\tLoss: 0.261741\tGrad Norm: 1.592310\tLR: 0.030000\n",
      "Train Epoch: 1324 [45056/194182 (23%)]\tLoss: 0.266073\tGrad Norm: 1.496483\tLR: 0.030000\n",
      "Train Epoch: 1324 [65536/194182 (33%)]\tLoss: 0.258136\tGrad Norm: 1.297685\tLR: 0.030000\n",
      "Train Epoch: 1324 [86016/194182 (44%)]\tLoss: 0.261732\tGrad Norm: 1.408476\tLR: 0.030000\n",
      "Train Epoch: 1324 [106496/194182 (54%)]\tLoss: 0.270118\tGrad Norm: 1.583307\tLR: 0.030000\n",
      "Train Epoch: 1324 [126976/194182 (65%)]\tLoss: 0.259277\tGrad Norm: 1.325145\tLR: 0.030000\n",
      "Train Epoch: 1324 [147456/194182 (75%)]\tLoss: 0.271865\tGrad Norm: 1.738056\tLR: 0.030000\n",
      "Train Epoch: 1324 [167936/194182 (85%)]\tLoss: 0.266955\tGrad Norm: 1.454003\tLR: 0.030000\n",
      "Train Epoch: 1324 [188416/194182 (96%)]\tLoss: 0.265180\tGrad Norm: 1.619548\tLR: 0.030000\n",
      "Train set: Average loss: 0.2639\n",
      "Test set: Average loss: 0.2482, Average MAE: 0.3403\n",
      "Train Epoch: 1325 [4096/194182 (2%)]\tLoss: 0.268955\tGrad Norm: 1.698375\tLR: 0.030000\n",
      "Train Epoch: 1325 [24576/194182 (12%)]\tLoss: 0.271513\tGrad Norm: 1.679229\tLR: 0.030000\n",
      "Train Epoch: 1325 [45056/194182 (23%)]\tLoss: 0.260480\tGrad Norm: 1.385703\tLR: 0.030000\n",
      "Train Epoch: 1325 [65536/194182 (33%)]\tLoss: 0.259946\tGrad Norm: 1.720138\tLR: 0.030000\n",
      "Train Epoch: 1325 [86016/194182 (44%)]\tLoss: 0.264248\tGrad Norm: 1.696100\tLR: 0.030000\n",
      "Train Epoch: 1325 [106496/194182 (54%)]\tLoss: 0.266463\tGrad Norm: 1.775304\tLR: 0.030000\n",
      "Train Epoch: 1325 [126976/194182 (65%)]\tLoss: 0.269806\tGrad Norm: 1.949967\tLR: 0.030000\n",
      "Train Epoch: 1325 [147456/194182 (75%)]\tLoss: 0.256068\tGrad Norm: 1.347453\tLR: 0.030000\n",
      "Train Epoch: 1325 [167936/194182 (85%)]\tLoss: 0.256402\tGrad Norm: 1.569400\tLR: 0.030000\n",
      "Train Epoch: 1325 [188416/194182 (96%)]\tLoss: 0.266532\tGrad Norm: 1.655882\tLR: 0.030000\n",
      "Train set: Average loss: 0.2646\n",
      "Test set: Average loss: 0.2558, Average MAE: 0.3620\n",
      "Epoch 1325: Mean reward = 0.046 +/- 0.001\n",
      "Train Epoch: 1326 [4096/194182 (2%)]\tLoss: 0.267879\tGrad Norm: 1.816460\tLR: 0.030000\n",
      "Train Epoch: 1326 [24576/194182 (12%)]\tLoss: 0.255650\tGrad Norm: 1.396952\tLR: 0.030000\n",
      "Train Epoch: 1326 [45056/194182 (23%)]\tLoss: 0.272761\tGrad Norm: 2.153443\tLR: 0.030000\n",
      "Train Epoch: 1326 [65536/194182 (33%)]\tLoss: 0.268385\tGrad Norm: 1.679056\tLR: 0.030000\n",
      "Train Epoch: 1326 [86016/194182 (44%)]\tLoss: 0.266027\tGrad Norm: 1.741817\tLR: 0.030000\n",
      "Train Epoch: 1326 [106496/194182 (54%)]\tLoss: 0.271775\tGrad Norm: 1.617290\tLR: 0.030000\n",
      "Train Epoch: 1326 [126976/194182 (65%)]\tLoss: 0.266083\tGrad Norm: 1.590599\tLR: 0.030000\n",
      "Train Epoch: 1326 [147456/194182 (75%)]\tLoss: 0.259772\tGrad Norm: 1.116668\tLR: 0.030000\n",
      "Train Epoch: 1326 [167936/194182 (85%)]\tLoss: 0.253497\tGrad Norm: 1.230597\tLR: 0.030000\n",
      "Train Epoch: 1326 [188416/194182 (96%)]\tLoss: 0.267552\tGrad Norm: 1.678310\tLR: 0.030000\n",
      "Train set: Average loss: 0.2643\n",
      "Test set: Average loss: 0.2504, Average MAE: 0.3452\n",
      "Train Epoch: 1327 [4096/194182 (2%)]\tLoss: 0.263807\tGrad Norm: 1.486859\tLR: 0.030000\n",
      "Train Epoch: 1327 [24576/194182 (12%)]\tLoss: 0.273965\tGrad Norm: 1.689270\tLR: 0.030000\n",
      "Train Epoch: 1327 [45056/194182 (23%)]\tLoss: 0.257907\tGrad Norm: 1.307514\tLR: 0.030000\n",
      "Train Epoch: 1327 [65536/194182 (33%)]\tLoss: 0.265036\tGrad Norm: 1.459058\tLR: 0.030000\n",
      "Train Epoch: 1327 [86016/194182 (44%)]\tLoss: 0.268055\tGrad Norm: 1.762617\tLR: 0.030000\n",
      "Train Epoch: 1327 [106496/194182 (54%)]\tLoss: 0.268469\tGrad Norm: 1.726882\tLR: 0.030000\n",
      "Train Epoch: 1327 [126976/194182 (65%)]\tLoss: 0.267462\tGrad Norm: 1.433461\tLR: 0.030000\n",
      "Train Epoch: 1327 [147456/194182 (75%)]\tLoss: 0.262873\tGrad Norm: 1.445695\tLR: 0.030000\n",
      "Train Epoch: 1327 [167936/194182 (85%)]\tLoss: 0.267478\tGrad Norm: 1.675433\tLR: 0.030000\n",
      "Train Epoch: 1327 [188416/194182 (96%)]\tLoss: 0.269505\tGrad Norm: 1.976480\tLR: 0.030000\n",
      "Train set: Average loss: 0.2641\n",
      "Test set: Average loss: 0.2576, Average MAE: 0.3580\n",
      "Train Epoch: 1328 [4096/194182 (2%)]\tLoss: 0.270886\tGrad Norm: 1.953848\tLR: 0.030000\n",
      "Train Epoch: 1328 [24576/194182 (12%)]\tLoss: 0.258918\tGrad Norm: 1.414306\tLR: 0.030000\n",
      "Train Epoch: 1328 [45056/194182 (23%)]\tLoss: 0.265613\tGrad Norm: 1.618531\tLR: 0.030000\n",
      "Train Epoch: 1328 [65536/194182 (33%)]\tLoss: 0.269170\tGrad Norm: 1.727309\tLR: 0.030000\n",
      "Train Epoch: 1328 [86016/194182 (44%)]\tLoss: 0.274328\tGrad Norm: 1.941992\tLR: 0.030000\n",
      "Train Epoch: 1328 [106496/194182 (54%)]\tLoss: 0.263648\tGrad Norm: 1.613957\tLR: 0.030000\n",
      "Train Epoch: 1328 [126976/194182 (65%)]\tLoss: 0.269269\tGrad Norm: 1.909947\tLR: 0.030000\n",
      "Train Epoch: 1328 [147456/194182 (75%)]\tLoss: 0.267945\tGrad Norm: 1.730565\tLR: 0.030000\n",
      "Train Epoch: 1328 [167936/194182 (85%)]\tLoss: 0.261800\tGrad Norm: 1.476368\tLR: 0.030000\n",
      "Train Epoch: 1328 [188416/194182 (96%)]\tLoss: 0.262404\tGrad Norm: 1.578966\tLR: 0.030000\n",
      "Train set: Average loss: 0.2655\n",
      "Test set: Average loss: 0.2493, Average MAE: 0.3482\n",
      "Train Epoch: 1329 [4096/194182 (2%)]\tLoss: 0.264999\tGrad Norm: 1.577933\tLR: 0.030000\n",
      "Train Epoch: 1329 [24576/194182 (12%)]\tLoss: 0.269016\tGrad Norm: 1.787499\tLR: 0.030000\n",
      "Train Epoch: 1329 [45056/194182 (23%)]\tLoss: 0.256227\tGrad Norm: 1.307337\tLR: 0.030000\n",
      "Train Epoch: 1329 [65536/194182 (33%)]\tLoss: 0.260570\tGrad Norm: 1.334382\tLR: 0.030000\n",
      "Train Epoch: 1329 [86016/194182 (44%)]\tLoss: 0.262412\tGrad Norm: 1.571990\tLR: 0.030000\n",
      "Train Epoch: 1329 [106496/194182 (54%)]\tLoss: 0.267805\tGrad Norm: 1.734039\tLR: 0.030000\n",
      "Train Epoch: 1329 [126976/194182 (65%)]\tLoss: 0.264700\tGrad Norm: 1.260790\tLR: 0.030000\n",
      "Train Epoch: 1329 [147456/194182 (75%)]\tLoss: 0.259160\tGrad Norm: 1.360981\tLR: 0.030000\n",
      "Train Epoch: 1329 [167936/194182 (85%)]\tLoss: 0.259153\tGrad Norm: 2.162330\tLR: 0.030000\n",
      "Train Epoch: 1329 [188416/194182 (96%)]\tLoss: 0.255767\tGrad Norm: 1.521144\tLR: 0.030000\n",
      "Train set: Average loss: 0.2621\n",
      "Test set: Average loss: 0.2489, Average MAE: 0.3381\n",
      "Train Epoch: 1330 [4096/194182 (2%)]\tLoss: 0.267078\tGrad Norm: 1.616123\tLR: 0.030000\n",
      "Train Epoch: 1330 [24576/194182 (12%)]\tLoss: 0.265208\tGrad Norm: 1.381580\tLR: 0.030000\n",
      "Train Epoch: 1330 [45056/194182 (23%)]\tLoss: 0.259608\tGrad Norm: 1.373480\tLR: 0.030000\n",
      "Train Epoch: 1330 [65536/194182 (33%)]\tLoss: 0.269378\tGrad Norm: 1.921657\tLR: 0.030000\n",
      "Train Epoch: 1330 [86016/194182 (44%)]\tLoss: 0.262686\tGrad Norm: 1.704828\tLR: 0.030000\n",
      "Train Epoch: 1330 [106496/194182 (54%)]\tLoss: 0.261805\tGrad Norm: 1.803612\tLR: 0.030000\n",
      "Train Epoch: 1330 [126976/194182 (65%)]\tLoss: 0.255608\tGrad Norm: 1.531148\tLR: 0.030000\n",
      "Train Epoch: 1330 [147456/194182 (75%)]\tLoss: 0.264215\tGrad Norm: 1.586128\tLR: 0.030000\n",
      "Train Epoch: 1330 [167936/194182 (85%)]\tLoss: 0.260492\tGrad Norm: 1.591375\tLR: 0.030000\n",
      "Train Epoch: 1330 [188416/194182 (96%)]\tLoss: 0.268358\tGrad Norm: 1.848912\tLR: 0.030000\n",
      "Train set: Average loss: 0.2643\n",
      "Test set: Average loss: 0.2483, Average MAE: 0.3372\n",
      "Epoch 1330: Mean reward = 0.049 +/- 0.042\n",
      "Train Epoch: 1331 [4096/194182 (2%)]\tLoss: 0.262928\tGrad Norm: 1.559036\tLR: 0.030000\n",
      "Train Epoch: 1331 [24576/194182 (12%)]\tLoss: 0.257304\tGrad Norm: 1.272785\tLR: 0.030000\n",
      "Train Epoch: 1331 [45056/194182 (23%)]\tLoss: 0.262662\tGrad Norm: 1.560836\tLR: 0.030000\n",
      "Train Epoch: 1331 [65536/194182 (33%)]\tLoss: 0.266160\tGrad Norm: 1.601578\tLR: 0.030000\n",
      "Train Epoch: 1331 [86016/194182 (44%)]\tLoss: 0.270585\tGrad Norm: 1.963676\tLR: 0.030000\n",
      "Train Epoch: 1331 [106496/194182 (54%)]\tLoss: 0.266599\tGrad Norm: 1.797682\tLR: 0.030000\n",
      "Train Epoch: 1331 [126976/194182 (65%)]\tLoss: 0.255800\tGrad Norm: 1.326864\tLR: 0.030000\n",
      "Train Epoch: 1331 [147456/194182 (75%)]\tLoss: 0.268646\tGrad Norm: 1.932280\tLR: 0.030000\n",
      "Train Epoch: 1331 [167936/194182 (85%)]\tLoss: 0.258305\tGrad Norm: 1.497237\tLR: 0.030000\n",
      "Train Epoch: 1331 [188416/194182 (96%)]\tLoss: 0.267119\tGrad Norm: 1.510928\tLR: 0.030000\n",
      "Train set: Average loss: 0.2635\n",
      "Test set: Average loss: 0.2536, Average MAE: 0.3562\n",
      "Train Epoch: 1332 [4096/194182 (2%)]\tLoss: 0.264978\tGrad Norm: 1.718590\tLR: 0.030000\n",
      "Train Epoch: 1332 [24576/194182 (12%)]\tLoss: 0.272754\tGrad Norm: 1.768606\tLR: 0.030000\n",
      "Train Epoch: 1332 [45056/194182 (23%)]\tLoss: 0.254405\tGrad Norm: 1.350976\tLR: 0.030000\n",
      "Train Epoch: 1332 [65536/194182 (33%)]\tLoss: 0.261157\tGrad Norm: 1.584568\tLR: 0.030000\n",
      "Train Epoch: 1332 [86016/194182 (44%)]\tLoss: 0.268663\tGrad Norm: 1.662813\tLR: 0.030000\n",
      "Train Epoch: 1332 [106496/194182 (54%)]\tLoss: 0.260609\tGrad Norm: 1.425089\tLR: 0.030000\n",
      "Train Epoch: 1332 [126976/194182 (65%)]\tLoss: 0.266790\tGrad Norm: 1.219107\tLR: 0.030000\n",
      "Train Epoch: 1332 [147456/194182 (75%)]\tLoss: 0.264631\tGrad Norm: 1.560227\tLR: 0.030000\n",
      "Train Epoch: 1332 [167936/194182 (85%)]\tLoss: 0.256793\tGrad Norm: 1.275532\tLR: 0.030000\n",
      "Train Epoch: 1332 [188416/194182 (96%)]\tLoss: 0.262979\tGrad Norm: 1.571313\tLR: 0.030000\n",
      "Train set: Average loss: 0.2623\n",
      "Test set: Average loss: 0.2506, Average MAE: 0.3370\n",
      "Train Epoch: 1333 [4096/194182 (2%)]\tLoss: 0.260166\tGrad Norm: 2.555975\tLR: 0.030000\n",
      "Train Epoch: 1333 [24576/194182 (12%)]\tLoss: 0.264732\tGrad Norm: 1.577679\tLR: 0.030000\n",
      "Train Epoch: 1333 [45056/194182 (23%)]\tLoss: 0.266480\tGrad Norm: 1.687191\tLR: 0.030000\n",
      "Train Epoch: 1333 [65536/194182 (33%)]\tLoss: 0.258194\tGrad Norm: 1.392671\tLR: 0.030000\n",
      "Train Epoch: 1333 [86016/194182 (44%)]\tLoss: 0.261956\tGrad Norm: 1.313573\tLR: 0.030000\n",
      "Train Epoch: 1333 [106496/194182 (54%)]\tLoss: 0.265654\tGrad Norm: 1.828634\tLR: 0.030000\n",
      "Train Epoch: 1333 [126976/194182 (65%)]\tLoss: 0.269103\tGrad Norm: 1.766989\tLR: 0.030000\n",
      "Train Epoch: 1333 [147456/194182 (75%)]\tLoss: 0.260511\tGrad Norm: 1.357476\tLR: 0.030000\n",
      "Train Epoch: 1333 [167936/194182 (85%)]\tLoss: 0.258103\tGrad Norm: 1.621830\tLR: 0.030000\n",
      "Train Epoch: 1333 [188416/194182 (96%)]\tLoss: 0.262790\tGrad Norm: 1.669216\tLR: 0.030000\n",
      "Train set: Average loss: 0.2628\n",
      "Test set: Average loss: 0.2484, Average MAE: 0.3533\n",
      "Train Epoch: 1334 [4096/194182 (2%)]\tLoss: 0.262795\tGrad Norm: 1.579907\tLR: 0.030000\n",
      "Train Epoch: 1334 [24576/194182 (12%)]\tLoss: 0.273301\tGrad Norm: 1.681554\tLR: 0.030000\n",
      "Train Epoch: 1334 [45056/194182 (23%)]\tLoss: 0.260173\tGrad Norm: 1.610018\tLR: 0.030000\n",
      "Train Epoch: 1334 [65536/194182 (33%)]\tLoss: 0.265727\tGrad Norm: 1.640517\tLR: 0.030000\n",
      "Train Epoch: 1334 [86016/194182 (44%)]\tLoss: 0.254887\tGrad Norm: 1.554531\tLR: 0.030000\n",
      "Train Epoch: 1334 [106496/194182 (54%)]\tLoss: 0.263019\tGrad Norm: 1.730110\tLR: 0.030000\n",
      "Train Epoch: 1334 [126976/194182 (65%)]\tLoss: 0.265016\tGrad Norm: 1.361829\tLR: 0.030000\n",
      "Train Epoch: 1334 [147456/194182 (75%)]\tLoss: 0.271641\tGrad Norm: 1.656050\tLR: 0.030000\n",
      "Train Epoch: 1334 [167936/194182 (85%)]\tLoss: 0.256815\tGrad Norm: 1.192449\tLR: 0.030000\n",
      "Train Epoch: 1334 [188416/194182 (96%)]\tLoss: 0.256615\tGrad Norm: 1.057865\tLR: 0.030000\n",
      "Train set: Average loss: 0.2620\n",
      "Test set: Average loss: 0.2502, Average MAE: 0.3489\n",
      "Train Epoch: 1335 [4096/194182 (2%)]\tLoss: 0.260566\tGrad Norm: 1.490282\tLR: 0.030000\n",
      "Train Epoch: 1335 [24576/194182 (12%)]\tLoss: 0.263893\tGrad Norm: 1.646286\tLR: 0.030000\n",
      "Train Epoch: 1335 [45056/194182 (23%)]\tLoss: 0.268631\tGrad Norm: 1.994373\tLR: 0.030000\n",
      "Train Epoch: 1335 [65536/194182 (33%)]\tLoss: 0.260783\tGrad Norm: 1.628051\tLR: 0.030000\n",
      "Train Epoch: 1335 [86016/194182 (44%)]\tLoss: 0.257448\tGrad Norm: 1.453113\tLR: 0.030000\n",
      "Train Epoch: 1335 [106496/194182 (54%)]\tLoss: 0.267056\tGrad Norm: 1.750110\tLR: 0.030000\n",
      "Train Epoch: 1335 [126976/194182 (65%)]\tLoss: 0.264494\tGrad Norm: 1.867582\tLR: 0.030000\n",
      "Train Epoch: 1335 [147456/194182 (75%)]\tLoss: 0.270886\tGrad Norm: 1.793837\tLR: 0.030000\n",
      "Train Epoch: 1335 [167936/194182 (85%)]\tLoss: 0.252588\tGrad Norm: 1.151518\tLR: 0.030000\n",
      "Train Epoch: 1335 [188416/194182 (96%)]\tLoss: 0.271579\tGrad Norm: 1.674183\tLR: 0.030000\n",
      "Train set: Average loss: 0.2634\n",
      "Test set: Average loss: 0.2496, Average MAE: 0.3487\n",
      "Epoch 1335: Mean reward = 0.051 +/- 0.016\n",
      "Train Epoch: 1336 [4096/194182 (2%)]\tLoss: 0.265052\tGrad Norm: 1.700237\tLR: 0.030000\n",
      "Train Epoch: 1336 [24576/194182 (12%)]\tLoss: 0.258823\tGrad Norm: 1.488663\tLR: 0.030000\n",
      "Train Epoch: 1336 [45056/194182 (23%)]\tLoss: 0.262016\tGrad Norm: 1.481480\tLR: 0.030000\n",
      "Train Epoch: 1336 [65536/194182 (33%)]\tLoss: 0.257484\tGrad Norm: 1.552169\tLR: 0.030000\n",
      "Train Epoch: 1336 [86016/194182 (44%)]\tLoss: 0.263090\tGrad Norm: 1.497995\tLR: 0.030000\n",
      "Train Epoch: 1336 [106496/194182 (54%)]\tLoss: 0.253176\tGrad Norm: 1.343427\tLR: 0.030000\n",
      "Train Epoch: 1336 [126976/194182 (65%)]\tLoss: 0.262685\tGrad Norm: 1.737426\tLR: 0.030000\n",
      "Train Epoch: 1336 [147456/194182 (75%)]\tLoss: 0.259654\tGrad Norm: 1.575199\tLR: 0.030000\n",
      "Train Epoch: 1336 [167936/194182 (85%)]\tLoss: 0.255060\tGrad Norm: 1.451485\tLR: 0.030000\n",
      "Train Epoch: 1336 [188416/194182 (96%)]\tLoss: 0.266276\tGrad Norm: 1.659895\tLR: 0.030000\n",
      "Train set: Average loss: 0.2618\n",
      "Test set: Average loss: 0.2472, Average MAE: 0.3378\n",
      "Train Epoch: 1337 [4096/194182 (2%)]\tLoss: 0.258792\tGrad Norm: 1.562597\tLR: 0.030000\n",
      "Train Epoch: 1337 [24576/194182 (12%)]\tLoss: 0.264947\tGrad Norm: 1.562452\tLR: 0.030000\n",
      "Train Epoch: 1337 [45056/194182 (23%)]\tLoss: 0.256301\tGrad Norm: 1.460197\tLR: 0.030000\n",
      "Train Epoch: 1337 [65536/194182 (33%)]\tLoss: 0.273947\tGrad Norm: 2.013397\tLR: 0.030000\n",
      "Train Epoch: 1337 [86016/194182 (44%)]\tLoss: 0.267629\tGrad Norm: 1.588893\tLR: 0.030000\n",
      "Train Epoch: 1337 [106496/194182 (54%)]\tLoss: 0.260453\tGrad Norm: 1.683541\tLR: 0.030000\n",
      "Train Epoch: 1337 [126976/194182 (65%)]\tLoss: 0.265394\tGrad Norm: 1.922765\tLR: 0.030000\n",
      "Train Epoch: 1337 [147456/194182 (75%)]\tLoss: 0.267970\tGrad Norm: 1.679380\tLR: 0.030000\n",
      "Train Epoch: 1337 [167936/194182 (85%)]\tLoss: 0.265671\tGrad Norm: 1.578308\tLR: 0.030000\n",
      "Train Epoch: 1337 [188416/194182 (96%)]\tLoss: 0.257812\tGrad Norm: 1.292355\tLR: 0.030000\n",
      "Train set: Average loss: 0.2627\n",
      "Test set: Average loss: 0.2541, Average MAE: 0.3544\n",
      "Train Epoch: 1338 [4096/194182 (2%)]\tLoss: 0.262081\tGrad Norm: 1.806985\tLR: 0.030000\n",
      "Train Epoch: 1338 [24576/194182 (12%)]\tLoss: 0.257261\tGrad Norm: 1.336544\tLR: 0.030000\n",
      "Train Epoch: 1338 [45056/194182 (23%)]\tLoss: 0.261027\tGrad Norm: 1.312004\tLR: 0.030000\n",
      "Train Epoch: 1338 [65536/194182 (33%)]\tLoss: 0.256612\tGrad Norm: 1.260426\tLR: 0.030000\n",
      "Train Epoch: 1338 [86016/194182 (44%)]\tLoss: 0.263804\tGrad Norm: 1.301607\tLR: 0.030000\n",
      "Train Epoch: 1338 [106496/194182 (54%)]\tLoss: 0.253393\tGrad Norm: 1.244552\tLR: 0.030000\n",
      "Train Epoch: 1338 [126976/194182 (65%)]\tLoss: 0.255476\tGrad Norm: 1.658511\tLR: 0.030000\n",
      "Train Epoch: 1338 [147456/194182 (75%)]\tLoss: 0.257604\tGrad Norm: 1.412976\tLR: 0.030000\n",
      "Train Epoch: 1338 [167936/194182 (85%)]\tLoss: 0.265220\tGrad Norm: 1.348761\tLR: 0.030000\n",
      "Train Epoch: 1338 [188416/194182 (96%)]\tLoss: 0.262762\tGrad Norm: 1.624663\tLR: 0.030000\n",
      "Train set: Average loss: 0.2603\n",
      "Test set: Average loss: 0.2480, Average MAE: 0.3511\n",
      "Train Epoch: 1339 [4096/194182 (2%)]\tLoss: 0.259717\tGrad Norm: 1.363835\tLR: 0.030000\n",
      "Train Epoch: 1339 [24576/194182 (12%)]\tLoss: 0.257723\tGrad Norm: 1.466959\tLR: 0.030000\n",
      "Train Epoch: 1339 [45056/194182 (23%)]\tLoss: 0.265956\tGrad Norm: 1.969876\tLR: 0.030000\n",
      "Train Epoch: 1339 [65536/194182 (33%)]\tLoss: 0.268384\tGrad Norm: 1.983810\tLR: 0.030000\n",
      "Train Epoch: 1339 [86016/194182 (44%)]\tLoss: 0.262294\tGrad Norm: 1.618685\tLR: 0.030000\n",
      "Train Epoch: 1339 [106496/194182 (54%)]\tLoss: 0.253166\tGrad Norm: 1.302976\tLR: 0.030000\n",
      "Train Epoch: 1339 [126976/194182 (65%)]\tLoss: 0.261066\tGrad Norm: 1.750955\tLR: 0.030000\n",
      "Train Epoch: 1339 [147456/194182 (75%)]\tLoss: 0.260312\tGrad Norm: 1.518860\tLR: 0.030000\n",
      "Train Epoch: 1339 [167936/194182 (85%)]\tLoss: 0.261839\tGrad Norm: 1.651611\tLR: 0.030000\n",
      "Train Epoch: 1339 [188416/194182 (96%)]\tLoss: 0.261023\tGrad Norm: 1.606585\tLR: 0.030000\n",
      "Train set: Average loss: 0.2617\n",
      "Test set: Average loss: 0.2478, Average MAE: 0.3407\n",
      "Train Epoch: 1340 [4096/194182 (2%)]\tLoss: 0.258598\tGrad Norm: 1.515606\tLR: 0.030000\n",
      "Train Epoch: 1340 [24576/194182 (12%)]\tLoss: 0.259875\tGrad Norm: 1.563525\tLR: 0.030000\n",
      "Train Epoch: 1340 [45056/194182 (23%)]\tLoss: 0.261904\tGrad Norm: 1.456187\tLR: 0.030000\n",
      "Train Epoch: 1340 [65536/194182 (33%)]\tLoss: 0.266258\tGrad Norm: 1.527649\tLR: 0.030000\n",
      "Train Epoch: 1340 [86016/194182 (44%)]\tLoss: 0.269838\tGrad Norm: 1.770160\tLR: 0.030000\n",
      "Train Epoch: 1340 [106496/194182 (54%)]\tLoss: 0.258255\tGrad Norm: 1.207172\tLR: 0.030000\n",
      "Train Epoch: 1340 [126976/194182 (65%)]\tLoss: 0.260096\tGrad Norm: 1.252911\tLR: 0.030000\n",
      "Train Epoch: 1340 [147456/194182 (75%)]\tLoss: 0.258091\tGrad Norm: 1.422188\tLR: 0.030000\n",
      "Train Epoch: 1340 [167936/194182 (85%)]\tLoss: 0.252943\tGrad Norm: 1.419499\tLR: 0.030000\n",
      "Train Epoch: 1340 [188416/194182 (96%)]\tLoss: 0.265260\tGrad Norm: 1.799590\tLR: 0.030000\n",
      "Train set: Average loss: 0.2613\n",
      "Test set: Average loss: 0.2541, Average MAE: 0.3594\n",
      "Epoch 1340: Mean reward = 0.054 +/- 0.025\n",
      "Train Epoch: 1341 [4096/194182 (2%)]\tLoss: 0.257231\tGrad Norm: 1.744199\tLR: 0.030000\n",
      "Train Epoch: 1341 [24576/194182 (12%)]\tLoss: 0.271483\tGrad Norm: 2.169839\tLR: 0.030000\n",
      "Train Epoch: 1341 [45056/194182 (23%)]\tLoss: 0.262735\tGrad Norm: 1.659486\tLR: 0.030000\n",
      "Train Epoch: 1341 [65536/194182 (33%)]\tLoss: 0.256151\tGrad Norm: 1.258391\tLR: 0.030000\n",
      "Train Epoch: 1341 [86016/194182 (44%)]\tLoss: 0.260004\tGrad Norm: 1.478213\tLR: 0.030000\n",
      "Train Epoch: 1341 [106496/194182 (54%)]\tLoss: 0.256894\tGrad Norm: 1.448557\tLR: 0.030000\n",
      "Train Epoch: 1341 [126976/194182 (65%)]\tLoss: 0.254484\tGrad Norm: 1.341761\tLR: 0.030000\n",
      "Train Epoch: 1341 [147456/194182 (75%)]\tLoss: 0.262217\tGrad Norm: 1.427298\tLR: 0.030000\n",
      "Train Epoch: 1341 [167936/194182 (85%)]\tLoss: 0.261005\tGrad Norm: 1.312076\tLR: 0.030000\n",
      "Train Epoch: 1341 [188416/194182 (96%)]\tLoss: 0.266629\tGrad Norm: 1.629140\tLR: 0.030000\n",
      "Train set: Average loss: 0.2611\n",
      "Test set: Average loss: 0.2515, Average MAE: 0.3515\n",
      "Train Epoch: 1342 [4096/194182 (2%)]\tLoss: 0.266319\tGrad Norm: 1.672513\tLR: 0.030000\n",
      "Train Epoch: 1342 [24576/194182 (12%)]\tLoss: 0.265958\tGrad Norm: 1.497266\tLR: 0.030000\n",
      "Train Epoch: 1342 [45056/194182 (23%)]\tLoss: 0.264216\tGrad Norm: 1.484659\tLR: 0.030000\n",
      "Train Epoch: 1342 [65536/194182 (33%)]\tLoss: 0.258548\tGrad Norm: 1.646579\tLR: 0.030000\n",
      "Train Epoch: 1342 [86016/194182 (44%)]\tLoss: 0.256669\tGrad Norm: 1.464785\tLR: 0.030000\n",
      "Train Epoch: 1342 [106496/194182 (54%)]\tLoss: 0.266630\tGrad Norm: 1.870419\tLR: 0.030000\n",
      "Train Epoch: 1342 [126976/194182 (65%)]\tLoss: 0.282248\tGrad Norm: 2.366416\tLR: 0.030000\n",
      "Train Epoch: 1342 [147456/194182 (75%)]\tLoss: 0.275543\tGrad Norm: 1.913693\tLR: 0.030000\n",
      "Train Epoch: 1342 [167936/194182 (85%)]\tLoss: 0.262640\tGrad Norm: 1.682047\tLR: 0.030000\n",
      "Train Epoch: 1342 [188416/194182 (96%)]\tLoss: 0.261850\tGrad Norm: 1.401363\tLR: 0.030000\n",
      "Train set: Average loss: 0.2630\n",
      "Test set: Average loss: 0.2472, Average MAE: 0.3405\n",
      "Train Epoch: 1343 [4096/194182 (2%)]\tLoss: 0.258893\tGrad Norm: 1.273848\tLR: 0.030000\n",
      "Train Epoch: 1343 [24576/194182 (12%)]\tLoss: 0.265268\tGrad Norm: 1.701328\tLR: 0.030000\n",
      "Train Epoch: 1343 [45056/194182 (23%)]\tLoss: 0.263087\tGrad Norm: 1.494067\tLR: 0.030000\n",
      "Train Epoch: 1343 [65536/194182 (33%)]\tLoss: 0.265258\tGrad Norm: 1.684277\tLR: 0.030000\n",
      "Train Epoch: 1343 [86016/194182 (44%)]\tLoss: 0.258343\tGrad Norm: 1.738943\tLR: 0.030000\n",
      "Train Epoch: 1343 [106496/194182 (54%)]\tLoss: 0.264474\tGrad Norm: 1.579995\tLR: 0.030000\n",
      "Train Epoch: 1343 [126976/194182 (65%)]\tLoss: 0.256727\tGrad Norm: 1.296962\tLR: 0.030000\n",
      "Train Epoch: 1343 [147456/194182 (75%)]\tLoss: 0.249654\tGrad Norm: 1.141518\tLR: 0.030000\n",
      "Train Epoch: 1343 [167936/194182 (85%)]\tLoss: 0.253485\tGrad Norm: 1.202623\tLR: 0.030000\n",
      "Train Epoch: 1343 [188416/194182 (96%)]\tLoss: 0.264093\tGrad Norm: 1.216434\tLR: 0.030000\n",
      "Train set: Average loss: 0.2593\n",
      "Test set: Average loss: 0.2427, Average MAE: 0.3430\n",
      "Train Epoch: 1344 [4096/194182 (2%)]\tLoss: 0.249390\tGrad Norm: 0.978699\tLR: 0.030000\n",
      "Train Epoch: 1344 [24576/194182 (12%)]\tLoss: 0.254314\tGrad Norm: 1.258781\tLR: 0.030000\n",
      "Train Epoch: 1344 [45056/194182 (23%)]\tLoss: 0.256626\tGrad Norm: 1.468043\tLR: 0.030000\n",
      "Train Epoch: 1344 [65536/194182 (33%)]\tLoss: 0.261206\tGrad Norm: 1.748184\tLR: 0.030000\n",
      "Train Epoch: 1344 [86016/194182 (44%)]\tLoss: 0.256937\tGrad Norm: 1.438939\tLR: 0.030000\n",
      "Train Epoch: 1344 [106496/194182 (54%)]\tLoss: 0.266741\tGrad Norm: 1.570122\tLR: 0.030000\n",
      "Train Epoch: 1344 [126976/194182 (65%)]\tLoss: 0.266041\tGrad Norm: 1.781064\tLR: 0.030000\n",
      "Train Epoch: 1344 [147456/194182 (75%)]\tLoss: 0.259117\tGrad Norm: 1.621716\tLR: 0.030000\n",
      "Train Epoch: 1344 [167936/194182 (85%)]\tLoss: 0.264748\tGrad Norm: 2.246160\tLR: 0.030000\n",
      "Train Epoch: 1344 [188416/194182 (96%)]\tLoss: 0.269855\tGrad Norm: 1.839111\tLR: 0.030000\n",
      "Train set: Average loss: 0.2622\n",
      "Test set: Average loss: 0.2487, Average MAE: 0.3542\n",
      "Train Epoch: 1345 [4096/194182 (2%)]\tLoss: 0.264396\tGrad Norm: 1.482515\tLR: 0.030000\n",
      "Train Epoch: 1345 [24576/194182 (12%)]\tLoss: 0.268384\tGrad Norm: 1.842747\tLR: 0.030000\n",
      "Train Epoch: 1345 [45056/194182 (23%)]\tLoss: 0.257863\tGrad Norm: 1.576736\tLR: 0.030000\n",
      "Train Epoch: 1345 [65536/194182 (33%)]\tLoss: 0.262096\tGrad Norm: 1.466738\tLR: 0.030000\n",
      "Train Epoch: 1345 [86016/194182 (44%)]\tLoss: 0.261511\tGrad Norm: 1.702472\tLR: 0.030000\n",
      "Train Epoch: 1345 [106496/194182 (54%)]\tLoss: 0.256155\tGrad Norm: 1.293682\tLR: 0.030000\n",
      "Train Epoch: 1345 [126976/194182 (65%)]\tLoss: 0.254983\tGrad Norm: 1.312294\tLR: 0.030000\n",
      "Train Epoch: 1345 [147456/194182 (75%)]\tLoss: 0.263843\tGrad Norm: 1.686486\tLR: 0.030000\n",
      "Train Epoch: 1345 [167936/194182 (85%)]\tLoss: 0.266893\tGrad Norm: 1.634326\tLR: 0.030000\n",
      "Train Epoch: 1345 [188416/194182 (96%)]\tLoss: 0.268037\tGrad Norm: 1.851814\tLR: 0.030000\n",
      "Train set: Average loss: 0.2605\n",
      "Test set: Average loss: 0.2513, Average MAE: 0.3554\n",
      "Epoch 1345: Mean reward = 0.094 +/- 0.081\n",
      "Train Epoch: 1346 [4096/194182 (2%)]\tLoss: 0.255512\tGrad Norm: 1.509366\tLR: 0.030000\n",
      "Train Epoch: 1346 [24576/194182 (12%)]\tLoss: 0.255907\tGrad Norm: 1.677193\tLR: 0.030000\n",
      "Train Epoch: 1346 [45056/194182 (23%)]\tLoss: 0.263785\tGrad Norm: 1.883707\tLR: 0.030000\n",
      "Train Epoch: 1346 [65536/194182 (33%)]\tLoss: 0.266830\tGrad Norm: 1.739842\tLR: 0.030000\n",
      "Train Epoch: 1346 [86016/194182 (44%)]\tLoss: 0.257774\tGrad Norm: 1.455175\tLR: 0.030000\n",
      "Train Epoch: 1346 [106496/194182 (54%)]\tLoss: 0.258916\tGrad Norm: 1.398605\tLR: 0.030000\n",
      "Train Epoch: 1346 [126976/194182 (65%)]\tLoss: 0.254479\tGrad Norm: 1.250347\tLR: 0.030000\n",
      "Train Epoch: 1346 [147456/194182 (75%)]\tLoss: 0.264589\tGrad Norm: 1.584445\tLR: 0.030000\n",
      "Train Epoch: 1346 [167936/194182 (85%)]\tLoss: 0.252214\tGrad Norm: 1.121765\tLR: 0.030000\n",
      "Train Epoch: 1346 [188416/194182 (96%)]\tLoss: 0.257521\tGrad Norm: 1.522809\tLR: 0.030000\n",
      "Train set: Average loss: 0.2598\n",
      "Test set: Average loss: 0.2573, Average MAE: 0.3618\n",
      "Train Epoch: 1347 [4096/194182 (2%)]\tLoss: 0.266833\tGrad Norm: 1.960458\tLR: 0.030000\n",
      "Train Epoch: 1347 [24576/194182 (12%)]\tLoss: 0.259028\tGrad Norm: 1.434647\tLR: 0.030000\n",
      "Train Epoch: 1347 [45056/194182 (23%)]\tLoss: 0.258213\tGrad Norm: 1.572103\tLR: 0.030000\n",
      "Train Epoch: 1347 [65536/194182 (33%)]\tLoss: 0.256530\tGrad Norm: 1.422461\tLR: 0.030000\n",
      "Train Epoch: 1347 [86016/194182 (44%)]\tLoss: 0.262864\tGrad Norm: 1.479802\tLR: 0.030000\n",
      "Train Epoch: 1347 [106496/194182 (54%)]\tLoss: 0.256526\tGrad Norm: 1.515494\tLR: 0.030000\n",
      "Train Epoch: 1347 [126976/194182 (65%)]\tLoss: 0.258255\tGrad Norm: 1.585596\tLR: 0.030000\n",
      "Train Epoch: 1347 [147456/194182 (75%)]\tLoss: 0.256130\tGrad Norm: 1.382256\tLR: 0.030000\n",
      "Train Epoch: 1347 [167936/194182 (85%)]\tLoss: 0.258043\tGrad Norm: 1.467743\tLR: 0.030000\n",
      "Train Epoch: 1347 [188416/194182 (96%)]\tLoss: 0.268682\tGrad Norm: 1.820868\tLR: 0.030000\n",
      "Train set: Average loss: 0.2615\n",
      "Test set: Average loss: 0.2561, Average MAE: 0.3465\n",
      "Train Epoch: 1348 [4096/194182 (2%)]\tLoss: 0.269253\tGrad Norm: 1.951567\tLR: 0.030000\n",
      "Train Epoch: 1348 [24576/194182 (12%)]\tLoss: 0.257481\tGrad Norm: 1.688252\tLR: 0.030000\n",
      "Train Epoch: 1348 [45056/194182 (23%)]\tLoss: 0.260654\tGrad Norm: 1.689374\tLR: 0.030000\n",
      "Train Epoch: 1348 [65536/194182 (33%)]\tLoss: 0.260949\tGrad Norm: 1.631554\tLR: 0.030000\n",
      "Train Epoch: 1348 [86016/194182 (44%)]\tLoss: 0.265519\tGrad Norm: 2.085719\tLR: 0.030000\n",
      "Train Epoch: 1348 [106496/194182 (54%)]\tLoss: 0.275224\tGrad Norm: 2.012126\tLR: 0.030000\n",
      "Train Epoch: 1348 [126976/194182 (65%)]\tLoss: 0.244259\tGrad Norm: 1.150303\tLR: 0.030000\n",
      "Train Epoch: 1348 [147456/194182 (75%)]\tLoss: 0.253859\tGrad Norm: 1.377504\tLR: 0.030000\n",
      "Train Epoch: 1348 [167936/194182 (85%)]\tLoss: 0.256831\tGrad Norm: 1.467805\tLR: 0.030000\n",
      "Train Epoch: 1348 [188416/194182 (96%)]\tLoss: 0.264827\tGrad Norm: 1.592405\tLR: 0.030000\n",
      "Train set: Average loss: 0.2620\n",
      "Test set: Average loss: 0.2456, Average MAE: 0.3385\n",
      "Train Epoch: 1349 [4096/194182 (2%)]\tLoss: 0.257131\tGrad Norm: 1.334925\tLR: 0.030000\n",
      "Train Epoch: 1349 [24576/194182 (12%)]\tLoss: 0.265716\tGrad Norm: 1.617999\tLR: 0.030000\n",
      "Train Epoch: 1349 [45056/194182 (23%)]\tLoss: 0.257220\tGrad Norm: 1.629012\tLR: 0.030000\n",
      "Train Epoch: 1349 [65536/194182 (33%)]\tLoss: 0.261848\tGrad Norm: 1.394542\tLR: 0.030000\n",
      "Train Epoch: 1349 [86016/194182 (44%)]\tLoss: 0.258648\tGrad Norm: 1.372172\tLR: 0.030000\n",
      "Train Epoch: 1349 [106496/194182 (54%)]\tLoss: 0.259907\tGrad Norm: 1.546185\tLR: 0.030000\n",
      "Train Epoch: 1349 [126976/194182 (65%)]\tLoss: 0.266057\tGrad Norm: 1.846822\tLR: 0.030000\n",
      "Train Epoch: 1349 [147456/194182 (75%)]\tLoss: 0.268969\tGrad Norm: 1.822928\tLR: 0.030000\n",
      "Train Epoch: 1349 [167936/194182 (85%)]\tLoss: 0.267869\tGrad Norm: 1.962089\tLR: 0.030000\n",
      "Train Epoch: 1349 [188416/194182 (96%)]\tLoss: 0.259015\tGrad Norm: 1.629007\tLR: 0.030000\n",
      "Train set: Average loss: 0.2607\n",
      "Test set: Average loss: 0.2480, Average MAE: 0.3502\n",
      "Train Epoch: 1350 [4096/194182 (2%)]\tLoss: 0.253642\tGrad Norm: 1.591984\tLR: 0.030000\n",
      "Train Epoch: 1350 [24576/194182 (12%)]\tLoss: 0.265437\tGrad Norm: 1.637396\tLR: 0.030000\n",
      "Train Epoch: 1350 [45056/194182 (23%)]\tLoss: 0.256164\tGrad Norm: 1.499523\tLR: 0.030000\n",
      "Train Epoch: 1350 [65536/194182 (33%)]\tLoss: 0.255387\tGrad Norm: 1.415289\tLR: 0.030000\n",
      "Train Epoch: 1350 [86016/194182 (44%)]\tLoss: 0.266501\tGrad Norm: 1.519688\tLR: 0.030000\n",
      "Train Epoch: 1350 [106496/194182 (54%)]\tLoss: 0.268226\tGrad Norm: 1.856782\tLR: 0.030000\n",
      "Train Epoch: 1350 [126976/194182 (65%)]\tLoss: 0.255027\tGrad Norm: 1.352589\tLR: 0.030000\n",
      "Train Epoch: 1350 [147456/194182 (75%)]\tLoss: 0.249331\tGrad Norm: 1.146261\tLR: 0.030000\n",
      "Train Epoch: 1350 [167936/194182 (85%)]\tLoss: 0.256015\tGrad Norm: 1.451537\tLR: 0.030000\n",
      "Train Epoch: 1350 [188416/194182 (96%)]\tLoss: 0.261446\tGrad Norm: 1.592297\tLR: 0.030000\n",
      "Train set: Average loss: 0.2596\n",
      "Test set: Average loss: 0.2562, Average MAE: 0.3584\n",
      "Epoch 1350: Mean reward = 0.064 +/- 0.038\n",
      "Train Epoch: 1351 [4096/194182 (2%)]\tLoss: 0.269684\tGrad Norm: 1.867384\tLR: 0.030000\n",
      "Train Epoch: 1351 [24576/194182 (12%)]\tLoss: 0.260993\tGrad Norm: 1.487523\tLR: 0.030000\n",
      "Train Epoch: 1351 [45056/194182 (23%)]\tLoss: 0.253431\tGrad Norm: 1.473578\tLR: 0.030000\n",
      "Train Epoch: 1351 [65536/194182 (33%)]\tLoss: 0.259331\tGrad Norm: 1.795162\tLR: 0.030000\n",
      "Train Epoch: 1351 [86016/194182 (44%)]\tLoss: 0.262403\tGrad Norm: 1.718352\tLR: 0.030000\n",
      "Train Epoch: 1351 [106496/194182 (54%)]\tLoss: 0.264088\tGrad Norm: 1.726436\tLR: 0.030000\n",
      "Train Epoch: 1351 [126976/194182 (65%)]\tLoss: 0.257646\tGrad Norm: 1.361645\tLR: 0.030000\n",
      "Train Epoch: 1351 [147456/194182 (75%)]\tLoss: 0.254734\tGrad Norm: 1.277735\tLR: 0.030000\n",
      "Train Epoch: 1351 [167936/194182 (85%)]\tLoss: 0.262056\tGrad Norm: 1.701686\tLR: 0.030000\n",
      "Train Epoch: 1351 [188416/194182 (96%)]\tLoss: 0.278021\tGrad Norm: 1.813368\tLR: 0.030000\n",
      "Train set: Average loss: 0.2613\n",
      "Test set: Average loss: 0.2478, Average MAE: 0.3375\n",
      "Train Epoch: 1352 [4096/194182 (2%)]\tLoss: 0.256516\tGrad Norm: 1.460733\tLR: 0.030000\n",
      "Train Epoch: 1352 [24576/194182 (12%)]\tLoss: 0.257481\tGrad Norm: 1.507374\tLR: 0.030000\n",
      "Train Epoch: 1352 [45056/194182 (23%)]\tLoss: 0.253302\tGrad Norm: 1.366165\tLR: 0.030000\n",
      "Train Epoch: 1352 [65536/194182 (33%)]\tLoss: 0.256417\tGrad Norm: 1.571854\tLR: 0.030000\n",
      "Train Epoch: 1352 [86016/194182 (44%)]\tLoss: 0.255341\tGrad Norm: 1.325106\tLR: 0.030000\n",
      "Train Epoch: 1352 [106496/194182 (54%)]\tLoss: 0.258496\tGrad Norm: 1.369594\tLR: 0.030000\n",
      "Train Epoch: 1352 [126976/194182 (65%)]\tLoss: 0.257682\tGrad Norm: 1.535326\tLR: 0.030000\n",
      "Train Epoch: 1352 [147456/194182 (75%)]\tLoss: 0.265322\tGrad Norm: 1.715621\tLR: 0.030000\n",
      "Train Epoch: 1352 [167936/194182 (85%)]\tLoss: 0.259415\tGrad Norm: 1.450679\tLR: 0.030000\n",
      "Train Epoch: 1352 [188416/194182 (96%)]\tLoss: 0.266259\tGrad Norm: 1.899548\tLR: 0.030000\n",
      "Train set: Average loss: 0.2598\n",
      "Test set: Average loss: 0.2617, Average MAE: 0.3641\n",
      "Train Epoch: 1353 [4096/194182 (2%)]\tLoss: 0.274031\tGrad Norm: 2.119654\tLR: 0.030000\n",
      "Train Epoch: 1353 [24576/194182 (12%)]\tLoss: 0.256364\tGrad Norm: 1.351139\tLR: 0.030000\n",
      "Train Epoch: 1353 [45056/194182 (23%)]\tLoss: 0.252662\tGrad Norm: 1.343955\tLR: 0.030000\n",
      "Train Epoch: 1353 [65536/194182 (33%)]\tLoss: 0.245732\tGrad Norm: 1.414107\tLR: 0.030000\n",
      "Train Epoch: 1353 [86016/194182 (44%)]\tLoss: 0.263610\tGrad Norm: 1.206941\tLR: 0.030000\n",
      "Train Epoch: 1353 [106496/194182 (54%)]\tLoss: 0.261242\tGrad Norm: 1.430563\tLR: 0.030000\n",
      "Train Epoch: 1353 [126976/194182 (65%)]\tLoss: 0.257237\tGrad Norm: 1.644980\tLR: 0.030000\n",
      "Train Epoch: 1353 [147456/194182 (75%)]\tLoss: 0.263320\tGrad Norm: 1.728186\tLR: 0.030000\n",
      "Train Epoch: 1353 [167936/194182 (85%)]\tLoss: 0.254141\tGrad Norm: 1.424103\tLR: 0.030000\n",
      "Train Epoch: 1353 [188416/194182 (96%)]\tLoss: 0.256245\tGrad Norm: 1.178986\tLR: 0.030000\n",
      "Train set: Average loss: 0.2578\n",
      "Test set: Average loss: 0.2473, Average MAE: 0.3469\n",
      "Train Epoch: 1354 [4096/194182 (2%)]\tLoss: 0.255925\tGrad Norm: 1.454687\tLR: 0.030000\n",
      "Train Epoch: 1354 [24576/194182 (12%)]\tLoss: 0.253625\tGrad Norm: 1.423454\tLR: 0.030000\n",
      "Train Epoch: 1354 [45056/194182 (23%)]\tLoss: 0.254769\tGrad Norm: 1.412323\tLR: 0.030000\n",
      "Train Epoch: 1354 [65536/194182 (33%)]\tLoss: 0.259784\tGrad Norm: 1.477749\tLR: 0.030000\n",
      "Train Epoch: 1354 [86016/194182 (44%)]\tLoss: 0.259240\tGrad Norm: 1.468517\tLR: 0.030000\n",
      "Train Epoch: 1354 [106496/194182 (54%)]\tLoss: 0.259214\tGrad Norm: 1.513395\tLR: 0.030000\n",
      "Train Epoch: 1354 [126976/194182 (65%)]\tLoss: 0.253947\tGrad Norm: 1.459588\tLR: 0.030000\n",
      "Train Epoch: 1354 [147456/194182 (75%)]\tLoss: 0.256847\tGrad Norm: 1.452999\tLR: 0.030000\n",
      "Train Epoch: 1354 [167936/194182 (85%)]\tLoss: 0.266290\tGrad Norm: 1.772727\tLR: 0.030000\n",
      "Train Epoch: 1354 [188416/194182 (96%)]\tLoss: 0.261075\tGrad Norm: 1.445480\tLR: 0.030000\n",
      "Train set: Average loss: 0.2589\n",
      "Test set: Average loss: 0.2506, Average MAE: 0.3441\n",
      "Train Epoch: 1355 [4096/194182 (2%)]\tLoss: 0.260876\tGrad Norm: 1.579716\tLR: 0.030000\n",
      "Train Epoch: 1355 [24576/194182 (12%)]\tLoss: 0.258401\tGrad Norm: 1.769338\tLR: 0.030000\n",
      "Train Epoch: 1355 [45056/194182 (23%)]\tLoss: 0.266109\tGrad Norm: 2.205305\tLR: 0.030000\n",
      "Train Epoch: 1355 [65536/194182 (33%)]\tLoss: 0.265326\tGrad Norm: 1.923494\tLR: 0.030000\n",
      "Train Epoch: 1355 [86016/194182 (44%)]\tLoss: 0.264807\tGrad Norm: 1.729330\tLR: 0.030000\n",
      "Train Epoch: 1355 [106496/194182 (54%)]\tLoss: 0.256418\tGrad Norm: 1.515769\tLR: 0.030000\n",
      "Train Epoch: 1355 [126976/194182 (65%)]\tLoss: 0.258966\tGrad Norm: 1.577094\tLR: 0.030000\n",
      "Train Epoch: 1355 [147456/194182 (75%)]\tLoss: 0.260294\tGrad Norm: 1.179096\tLR: 0.030000\n",
      "Train Epoch: 1355 [167936/194182 (85%)]\tLoss: 0.254445\tGrad Norm: 1.639026\tLR: 0.030000\n",
      "Train Epoch: 1355 [188416/194182 (96%)]\tLoss: 0.254601\tGrad Norm: 1.426696\tLR: 0.030000\n",
      "Train set: Average loss: 0.2607\n",
      "Test set: Average loss: 0.2505, Average MAE: 0.3412\n",
      "Epoch 1355: Mean reward = 0.051 +/- 0.050\n",
      "Train Epoch: 1356 [4096/194182 (2%)]\tLoss: 0.261322\tGrad Norm: 1.641690\tLR: 0.030000\n",
      "Train Epoch: 1356 [24576/194182 (12%)]\tLoss: 0.261887\tGrad Norm: 1.491860\tLR: 0.030000\n",
      "Train Epoch: 1356 [45056/194182 (23%)]\tLoss: 0.249204\tGrad Norm: 1.087336\tLR: 0.030000\n",
      "Train Epoch: 1356 [65536/194182 (33%)]\tLoss: 0.261041\tGrad Norm: 1.496400\tLR: 0.030000\n",
      "Train Epoch: 1356 [86016/194182 (44%)]\tLoss: 0.258575\tGrad Norm: 1.359386\tLR: 0.030000\n",
      "Train Epoch: 1356 [106496/194182 (54%)]\tLoss: 0.262691\tGrad Norm: 1.524878\tLR: 0.030000\n",
      "Train Epoch: 1356 [126976/194182 (65%)]\tLoss: 0.252572\tGrad Norm: 1.641297\tLR: 0.030000\n",
      "Train Epoch: 1356 [147456/194182 (75%)]\tLoss: 0.265668\tGrad Norm: 1.732990\tLR: 0.030000\n",
      "Train Epoch: 1356 [167936/194182 (85%)]\tLoss: 0.266902\tGrad Norm: 2.038958\tLR: 0.030000\n",
      "Train Epoch: 1356 [188416/194182 (96%)]\tLoss: 0.263353\tGrad Norm: 1.728703\tLR: 0.030000\n",
      "Train set: Average loss: 0.2602\n",
      "Test set: Average loss: 0.2561, Average MAE: 0.3572\n",
      "Train Epoch: 1357 [4096/194182 (2%)]\tLoss: 0.261131\tGrad Norm: 1.805343\tLR: 0.030000\n",
      "Train Epoch: 1357 [24576/194182 (12%)]\tLoss: 0.253738\tGrad Norm: 1.379073\tLR: 0.030000\n",
      "Train Epoch: 1357 [45056/194182 (23%)]\tLoss: 0.265375\tGrad Norm: 1.891753\tLR: 0.030000\n",
      "Train Epoch: 1357 [65536/194182 (33%)]\tLoss: 0.261553\tGrad Norm: 1.700319\tLR: 0.030000\n",
      "Train Epoch: 1357 [86016/194182 (44%)]\tLoss: 0.254655\tGrad Norm: 1.456332\tLR: 0.030000\n",
      "Train Epoch: 1357 [106496/194182 (54%)]\tLoss: 0.264019\tGrad Norm: 1.476291\tLR: 0.030000\n",
      "Train Epoch: 1357 [126976/194182 (65%)]\tLoss: 0.262308\tGrad Norm: 1.492858\tLR: 0.030000\n",
      "Train Epoch: 1357 [147456/194182 (75%)]\tLoss: 0.266506\tGrad Norm: 1.462963\tLR: 0.030000\n",
      "Train Epoch: 1357 [167936/194182 (85%)]\tLoss: 0.242768\tGrad Norm: 0.918942\tLR: 0.030000\n",
      "Train Epoch: 1357 [188416/194182 (96%)]\tLoss: 0.254644\tGrad Norm: 0.884263\tLR: 0.030000\n",
      "Train set: Average loss: 0.2572\n",
      "Test set: Average loss: 0.2435, Average MAE: 0.3425\n",
      "Train Epoch: 1358 [4096/194182 (2%)]\tLoss: 0.248959\tGrad Norm: 1.161993\tLR: 0.030000\n",
      "Train Epoch: 1358 [24576/194182 (12%)]\tLoss: 0.263281\tGrad Norm: 1.674708\tLR: 0.030000\n",
      "Train Epoch: 1358 [45056/194182 (23%)]\tLoss: 0.260868\tGrad Norm: 1.714928\tLR: 0.030000\n",
      "Train Epoch: 1358 [65536/194182 (33%)]\tLoss: 0.256343\tGrad Norm: 1.600206\tLR: 0.030000\n",
      "Train Epoch: 1358 [86016/194182 (44%)]\tLoss: 0.263796\tGrad Norm: 1.859012\tLR: 0.030000\n",
      "Train Epoch: 1358 [106496/194182 (54%)]\tLoss: 0.259797\tGrad Norm: 1.681146\tLR: 0.030000\n",
      "Train Epoch: 1358 [126976/194182 (65%)]\tLoss: 0.257934\tGrad Norm: 1.580393\tLR: 0.030000\n",
      "Train Epoch: 1358 [147456/194182 (75%)]\tLoss: 0.259992\tGrad Norm: 1.621169\tLR: 0.030000\n",
      "Train Epoch: 1358 [167936/194182 (85%)]\tLoss: 0.260863\tGrad Norm: 1.844622\tLR: 0.030000\n",
      "Train Epoch: 1358 [188416/194182 (96%)]\tLoss: 0.261007\tGrad Norm: 1.325057\tLR: 0.030000\n",
      "Train set: Average loss: 0.2592\n",
      "Test set: Average loss: 0.2465, Average MAE: 0.3397\n",
      "Train Epoch: 1359 [4096/194182 (2%)]\tLoss: 0.255547\tGrad Norm: 1.412828\tLR: 0.030000\n",
      "Train Epoch: 1359 [24576/194182 (12%)]\tLoss: 0.261034\tGrad Norm: 1.831239\tLR: 0.030000\n",
      "Train Epoch: 1359 [45056/194182 (23%)]\tLoss: 0.256656\tGrad Norm: 1.840288\tLR: 0.030000\n",
      "Train Epoch: 1359 [65536/194182 (33%)]\tLoss: 0.266221\tGrad Norm: 2.066982\tLR: 0.030000\n",
      "Train Epoch: 1359 [86016/194182 (44%)]\tLoss: 0.258737\tGrad Norm: 1.588090\tLR: 0.030000\n",
      "Train Epoch: 1359 [106496/194182 (54%)]\tLoss: 0.265224\tGrad Norm: 1.935349\tLR: 0.030000\n",
      "Train Epoch: 1359 [126976/194182 (65%)]\tLoss: 0.256629\tGrad Norm: 1.640791\tLR: 0.030000\n",
      "Train Epoch: 1359 [147456/194182 (75%)]\tLoss: 0.256951\tGrad Norm: 1.602391\tLR: 0.030000\n",
      "Train Epoch: 1359 [167936/194182 (85%)]\tLoss: 0.260366\tGrad Norm: 1.452738\tLR: 0.030000\n",
      "Train Epoch: 1359 [188416/194182 (96%)]\tLoss: 0.262899\tGrad Norm: 1.759397\tLR: 0.030000\n",
      "Train set: Average loss: 0.2601\n",
      "Test set: Average loss: 0.2516, Average MAE: 0.3376\n",
      "Train Epoch: 1360 [4096/194182 (2%)]\tLoss: 0.262299\tGrad Norm: 1.910894\tLR: 0.030000\n",
      "Train Epoch: 1360 [24576/194182 (12%)]\tLoss: 0.257132\tGrad Norm: 1.449272\tLR: 0.030000\n",
      "Train Epoch: 1360 [45056/194182 (23%)]\tLoss: 0.256968\tGrad Norm: 1.564242\tLR: 0.030000\n",
      "Train Epoch: 1360 [65536/194182 (33%)]\tLoss: 0.262347\tGrad Norm: 1.804347\tLR: 0.030000\n",
      "Train Epoch: 1360 [86016/194182 (44%)]\tLoss: 0.249496\tGrad Norm: 1.307185\tLR: 0.030000\n",
      "Train Epoch: 1360 [106496/194182 (54%)]\tLoss: 0.246112\tGrad Norm: 1.119864\tLR: 0.030000\n",
      "Train Epoch: 1360 [126976/194182 (65%)]\tLoss: 0.263906\tGrad Norm: 1.620855\tLR: 0.030000\n",
      "Train Epoch: 1360 [147456/194182 (75%)]\tLoss: 0.259152\tGrad Norm: 1.579426\tLR: 0.030000\n",
      "Train Epoch: 1360 [167936/194182 (85%)]\tLoss: 0.253953\tGrad Norm: 1.413918\tLR: 0.030000\n",
      "Train Epoch: 1360 [188416/194182 (96%)]\tLoss: 0.256055\tGrad Norm: 1.351728\tLR: 0.030000\n",
      "Train set: Average loss: 0.2578\n",
      "Test set: Average loss: 0.2462, Average MAE: 0.3414\n",
      "Epoch 1360: Mean reward = 0.059 +/- 0.038\n",
      "Train Epoch: 1361 [4096/194182 (2%)]\tLoss: 0.254605\tGrad Norm: 1.369310\tLR: 0.030000\n",
      "Train Epoch: 1361 [24576/194182 (12%)]\tLoss: 0.256016\tGrad Norm: 1.487221\tLR: 0.030000\n",
      "Train Epoch: 1361 [45056/194182 (23%)]\tLoss: 0.249136\tGrad Norm: 1.464216\tLR: 0.030000\n",
      "Train Epoch: 1361 [65536/194182 (33%)]\tLoss: 0.255747\tGrad Norm: 1.500962\tLR: 0.030000\n",
      "Train Epoch: 1361 [86016/194182 (44%)]\tLoss: 0.254770\tGrad Norm: 1.328232\tLR: 0.030000\n",
      "Train Epoch: 1361 [106496/194182 (54%)]\tLoss: 0.259066\tGrad Norm: 1.452301\tLR: 0.030000\n",
      "Train Epoch: 1361 [126976/194182 (65%)]\tLoss: 0.253161\tGrad Norm: 1.273942\tLR: 0.030000\n",
      "Train Epoch: 1361 [147456/194182 (75%)]\tLoss: 0.263100\tGrad Norm: 1.665144\tLR: 0.030000\n",
      "Train Epoch: 1361 [167936/194182 (85%)]\tLoss: 0.246997\tGrad Norm: 1.117924\tLR: 0.030000\n",
      "Train Epoch: 1361 [188416/194182 (96%)]\tLoss: 0.255487\tGrad Norm: 1.562152\tLR: 0.030000\n",
      "Train set: Average loss: 0.2571\n",
      "Test set: Average loss: 0.2493, Average MAE: 0.3430\n",
      "Train Epoch: 1362 [4096/194182 (2%)]\tLoss: 0.255719\tGrad Norm: 1.483717\tLR: 0.030000\n",
      "Train Epoch: 1362 [24576/194182 (12%)]\tLoss: 0.262425\tGrad Norm: 1.929792\tLR: 0.030000\n",
      "Train Epoch: 1362 [45056/194182 (23%)]\tLoss: 0.253482\tGrad Norm: 1.350098\tLR: 0.030000\n",
      "Train Epoch: 1362 [65536/194182 (33%)]\tLoss: 0.256656\tGrad Norm: 1.918213\tLR: 0.030000\n",
      "Train Epoch: 1362 [86016/194182 (44%)]\tLoss: 0.273776\tGrad Norm: 2.373567\tLR: 0.030000\n",
      "Train Epoch: 1362 [106496/194182 (54%)]\tLoss: 0.259929\tGrad Norm: 1.762007\tLR: 0.030000\n",
      "Train Epoch: 1362 [126976/194182 (65%)]\tLoss: 0.252348\tGrad Norm: 1.383267\tLR: 0.030000\n",
      "Train Epoch: 1362 [147456/194182 (75%)]\tLoss: 0.250720\tGrad Norm: 1.124153\tLR: 0.030000\n",
      "Train Epoch: 1362 [167936/194182 (85%)]\tLoss: 0.241633\tGrad Norm: 0.839995\tLR: 0.030000\n",
      "Train Epoch: 1362 [188416/194182 (96%)]\tLoss: 0.252573\tGrad Norm: 1.326422\tLR: 0.030000\n",
      "Train set: Average loss: 0.2588\n",
      "Test set: Average loss: 0.2478, Average MAE: 0.3464\n",
      "Train Epoch: 1363 [4096/194182 (2%)]\tLoss: 0.254636\tGrad Norm: 1.393895\tLR: 0.030000\n",
      "Train Epoch: 1363 [24576/194182 (12%)]\tLoss: 0.260366\tGrad Norm: 1.645200\tLR: 0.030000\n",
      "Train Epoch: 1363 [45056/194182 (23%)]\tLoss: 0.259325\tGrad Norm: 1.625618\tLR: 0.030000\n",
      "Train Epoch: 1363 [65536/194182 (33%)]\tLoss: 0.258386\tGrad Norm: 1.798764\tLR: 0.030000\n",
      "Train Epoch: 1363 [86016/194182 (44%)]\tLoss: 0.256603\tGrad Norm: 1.414134\tLR: 0.030000\n",
      "Train Epoch: 1363 [106496/194182 (54%)]\tLoss: 0.262261\tGrad Norm: 1.692642\tLR: 0.030000\n",
      "Train Epoch: 1363 [126976/194182 (65%)]\tLoss: 0.261828\tGrad Norm: 1.686298\tLR: 0.030000\n",
      "Train Epoch: 1363 [147456/194182 (75%)]\tLoss: 0.257836\tGrad Norm: 1.452623\tLR: 0.030000\n",
      "Train Epoch: 1363 [167936/194182 (85%)]\tLoss: 0.262430\tGrad Norm: 1.858960\tLR: 0.030000\n",
      "Train Epoch: 1363 [188416/194182 (96%)]\tLoss: 0.259839\tGrad Norm: 1.836022\tLR: 0.030000\n",
      "Train set: Average loss: 0.2583\n",
      "Test set: Average loss: 0.2494, Average MAE: 0.3390\n",
      "Train Epoch: 1364 [4096/194182 (2%)]\tLoss: 0.255117\tGrad Norm: 1.670213\tLR: 0.030000\n",
      "Train Epoch: 1364 [24576/194182 (12%)]\tLoss: 0.259762\tGrad Norm: 1.790363\tLR: 0.030000\n",
      "Train Epoch: 1364 [45056/194182 (23%)]\tLoss: 0.252213\tGrad Norm: 1.666336\tLR: 0.030000\n",
      "Train Epoch: 1364 [65536/194182 (33%)]\tLoss: 0.261054\tGrad Norm: 1.665320\tLR: 0.030000\n",
      "Train Epoch: 1364 [86016/194182 (44%)]\tLoss: 0.264365\tGrad Norm: 2.064644\tLR: 0.030000\n",
      "Train Epoch: 1364 [106496/194182 (54%)]\tLoss: 0.265176\tGrad Norm: 1.655342\tLR: 0.030000\n",
      "Train Epoch: 1364 [126976/194182 (65%)]\tLoss: 0.257676\tGrad Norm: 1.465566\tLR: 0.030000\n",
      "Train Epoch: 1364 [147456/194182 (75%)]\tLoss: 0.259189\tGrad Norm: 1.756853\tLR: 0.030000\n",
      "Train Epoch: 1364 [167936/194182 (85%)]\tLoss: 0.258345\tGrad Norm: 1.367558\tLR: 0.030000\n",
      "Train Epoch: 1364 [188416/194182 (96%)]\tLoss: 0.258916\tGrad Norm: 1.790558\tLR: 0.030000\n",
      "Train set: Average loss: 0.2600\n",
      "Test set: Average loss: 0.2540, Average MAE: 0.3536\n",
      "Train Epoch: 1365 [4096/194182 (2%)]\tLoss: 0.254138\tGrad Norm: 1.780161\tLR: 0.030000\n",
      "Train Epoch: 1365 [24576/194182 (12%)]\tLoss: 0.259206\tGrad Norm: 1.464955\tLR: 0.030000\n",
      "Train Epoch: 1365 [45056/194182 (23%)]\tLoss: 0.265174\tGrad Norm: 1.530191\tLR: 0.030000\n",
      "Train Epoch: 1365 [65536/194182 (33%)]\tLoss: 0.268336\tGrad Norm: 2.067223\tLR: 0.030000\n",
      "Train Epoch: 1365 [86016/194182 (44%)]\tLoss: 0.276643\tGrad Norm: 2.177113\tLR: 0.030000\n",
      "Train Epoch: 1365 [106496/194182 (54%)]\tLoss: 0.267502\tGrad Norm: 1.580561\tLR: 0.030000\n",
      "Train Epoch: 1365 [126976/194182 (65%)]\tLoss: 0.254648\tGrad Norm: 1.467853\tLR: 0.030000\n",
      "Train Epoch: 1365 [147456/194182 (75%)]\tLoss: 0.260525\tGrad Norm: 1.446407\tLR: 0.030000\n",
      "Train Epoch: 1365 [167936/194182 (85%)]\tLoss: 0.244298\tGrad Norm: 1.162300\tLR: 0.030000\n",
      "Train Epoch: 1365 [188416/194182 (96%)]\tLoss: 0.258424\tGrad Norm: 1.476951\tLR: 0.030000\n",
      "Train set: Average loss: 0.2583\n",
      "Test set: Average loss: 0.2510, Average MAE: 0.3389\n",
      "Epoch 1365: Mean reward = 0.068 +/- 0.073\n",
      "Train Epoch: 1366 [4096/194182 (2%)]\tLoss: 0.258188\tGrad Norm: 1.701901\tLR: 0.030000\n",
      "Train Epoch: 1366 [24576/194182 (12%)]\tLoss: 0.251911\tGrad Norm: 1.481796\tLR: 0.030000\n",
      "Train Epoch: 1366 [45056/194182 (23%)]\tLoss: 0.255231\tGrad Norm: 1.523914\tLR: 0.030000\n",
      "Train Epoch: 1366 [65536/194182 (33%)]\tLoss: 0.254278\tGrad Norm: 1.164132\tLR: 0.030000\n",
      "Train Epoch: 1366 [86016/194182 (44%)]\tLoss: 0.247534\tGrad Norm: 1.355273\tLR: 0.030000\n",
      "Train Epoch: 1366 [106496/194182 (54%)]\tLoss: 0.256265\tGrad Norm: 1.546447\tLR: 0.030000\n",
      "Train Epoch: 1366 [126976/194182 (65%)]\tLoss: 0.260752\tGrad Norm: 1.416028\tLR: 0.030000\n",
      "Train Epoch: 1366 [147456/194182 (75%)]\tLoss: 0.256144\tGrad Norm: 1.415168\tLR: 0.030000\n",
      "Train Epoch: 1366 [167936/194182 (85%)]\tLoss: 0.264638\tGrad Norm: 2.174015\tLR: 0.030000\n",
      "Train Epoch: 1366 [188416/194182 (96%)]\tLoss: 0.277276\tGrad Norm: 2.478489\tLR: 0.030000\n",
      "Train set: Average loss: 0.2587\n",
      "Test set: Average loss: 0.2772, Average MAE: 0.3535\n",
      "Train Epoch: 1367 [4096/194182 (2%)]\tLoss: 0.290942\tGrad Norm: 6.571485\tLR: 0.030000\n",
      "Train Epoch: 1367 [24576/194182 (12%)]\tLoss: 0.251425\tGrad Norm: 1.425985\tLR: 0.030000\n",
      "Train Epoch: 1367 [45056/194182 (23%)]\tLoss: 0.238766\tGrad Norm: 0.777159\tLR: 0.030000\n",
      "Train Epoch: 1367 [65536/194182 (33%)]\tLoss: 0.246908\tGrad Norm: 1.249203\tLR: 0.030000\n",
      "Train Epoch: 1367 [86016/194182 (44%)]\tLoss: 0.255256\tGrad Norm: 1.437831\tLR: 0.030000\n",
      "Train Epoch: 1367 [106496/194182 (54%)]\tLoss: 0.251735\tGrad Norm: 1.254863\tLR: 0.030000\n",
      "Train Epoch: 1367 [126976/194182 (65%)]\tLoss: 0.251342\tGrad Norm: 1.254788\tLR: 0.030000\n",
      "Train Epoch: 1367 [147456/194182 (75%)]\tLoss: 0.260069\tGrad Norm: 1.536227\tLR: 0.030000\n",
      "Train Epoch: 1367 [167936/194182 (85%)]\tLoss: 0.259847\tGrad Norm: 1.548460\tLR: 0.030000\n",
      "Train Epoch: 1367 [188416/194182 (96%)]\tLoss: 0.264287\tGrad Norm: 1.821949\tLR: 0.030000\n",
      "Train set: Average loss: 0.2570\n",
      "Test set: Average loss: 0.2484, Average MAE: 0.3466\n",
      "Train Epoch: 1368 [4096/194182 (2%)]\tLoss: 0.255968\tGrad Norm: 1.404945\tLR: 0.030000\n",
      "Train Epoch: 1368 [24576/194182 (12%)]\tLoss: 0.253813\tGrad Norm: 1.318061\tLR: 0.030000\n",
      "Train Epoch: 1368 [45056/194182 (23%)]\tLoss: 0.247666\tGrad Norm: 1.431795\tLR: 0.030000\n",
      "Train Epoch: 1368 [65536/194182 (33%)]\tLoss: 0.252628\tGrad Norm: 1.389217\tLR: 0.030000\n",
      "Train Epoch: 1368 [86016/194182 (44%)]\tLoss: 0.252420\tGrad Norm: 1.652352\tLR: 0.030000\n",
      "Train Epoch: 1368 [106496/194182 (54%)]\tLoss: 0.252012\tGrad Norm: 1.594104\tLR: 0.030000\n",
      "Train Epoch: 1368 [126976/194182 (65%)]\tLoss: 0.253912\tGrad Norm: 1.276583\tLR: 0.030000\n",
      "Train Epoch: 1368 [147456/194182 (75%)]\tLoss: 0.262362\tGrad Norm: 1.925920\tLR: 0.030000\n",
      "Train Epoch: 1368 [167936/194182 (85%)]\tLoss: 0.263916\tGrad Norm: 1.999851\tLR: 0.030000\n",
      "Train Epoch: 1368 [188416/194182 (96%)]\tLoss: 0.247522\tGrad Norm: 1.571904\tLR: 0.030000\n",
      "Train set: Average loss: 0.2571\n",
      "Test set: Average loss: 0.2522, Average MAE: 0.3514\n",
      "Train Epoch: 1369 [4096/194182 (2%)]\tLoss: 0.257943\tGrad Norm: 1.642433\tLR: 0.030000\n",
      "Train Epoch: 1369 [24576/194182 (12%)]\tLoss: 0.267368\tGrad Norm: 1.981479\tLR: 0.030000\n",
      "Train Epoch: 1369 [45056/194182 (23%)]\tLoss: 0.257761\tGrad Norm: 2.007850\tLR: 0.030000\n",
      "Train Epoch: 1369 [65536/194182 (33%)]\tLoss: 0.264626\tGrad Norm: 1.871350\tLR: 0.030000\n",
      "Train Epoch: 1369 [86016/194182 (44%)]\tLoss: 0.261952\tGrad Norm: 1.791399\tLR: 0.030000\n",
      "Train Epoch: 1369 [106496/194182 (54%)]\tLoss: 0.254262\tGrad Norm: 1.416324\tLR: 0.030000\n",
      "Train Epoch: 1369 [126976/194182 (65%)]\tLoss: 0.259090\tGrad Norm: 1.464428\tLR: 0.030000\n",
      "Train Epoch: 1369 [147456/194182 (75%)]\tLoss: 0.258178\tGrad Norm: 1.519516\tLR: 0.030000\n",
      "Train Epoch: 1369 [167936/194182 (85%)]\tLoss: 0.259409\tGrad Norm: 1.567162\tLR: 0.030000\n",
      "Train Epoch: 1369 [188416/194182 (96%)]\tLoss: 0.255488\tGrad Norm: 1.459219\tLR: 0.030000\n",
      "Train set: Average loss: 0.2591\n",
      "Test set: Average loss: 0.2523, Average MAE: 0.3484\n",
      "Train Epoch: 1370 [4096/194182 (2%)]\tLoss: 0.255918\tGrad Norm: 1.722430\tLR: 0.030000\n",
      "Train Epoch: 1370 [24576/194182 (12%)]\tLoss: 0.263886\tGrad Norm: 1.786593\tLR: 0.030000\n",
      "Train Epoch: 1370 [45056/194182 (23%)]\tLoss: 0.261572\tGrad Norm: 1.604470\tLR: 0.030000\n",
      "Train Epoch: 1370 [65536/194182 (33%)]\tLoss: 0.253107\tGrad Norm: 1.245853\tLR: 0.030000\n",
      "Train Epoch: 1370 [86016/194182 (44%)]\tLoss: 0.253363\tGrad Norm: 1.394820\tLR: 0.030000\n",
      "Train Epoch: 1370 [106496/194182 (54%)]\tLoss: 0.259016\tGrad Norm: 1.311809\tLR: 0.030000\n",
      "Train Epoch: 1370 [126976/194182 (65%)]\tLoss: 0.255574\tGrad Norm: 1.381307\tLR: 0.030000\n",
      "Train Epoch: 1370 [147456/194182 (75%)]\tLoss: 0.252330\tGrad Norm: 1.377071\tLR: 0.030000\n",
      "Train Epoch: 1370 [167936/194182 (85%)]\tLoss: 0.259029\tGrad Norm: 1.666745\tLR: 0.030000\n",
      "Train Epoch: 1370 [188416/194182 (96%)]\tLoss: 0.259103\tGrad Norm: 1.500198\tLR: 0.030000\n",
      "Train set: Average loss: 0.2563\n",
      "Test set: Average loss: 0.2521, Average MAE: 0.3492\n",
      "Epoch 1370: Mean reward = 0.042 +/- 0.014\n",
      "Train Epoch: 1371 [4096/194182 (2%)]\tLoss: 0.255669\tGrad Norm: 1.635576\tLR: 0.030000\n",
      "Train Epoch: 1371 [24576/194182 (12%)]\tLoss: 0.258942\tGrad Norm: 1.485322\tLR: 0.030000\n",
      "Train Epoch: 1371 [45056/194182 (23%)]\tLoss: 0.260810\tGrad Norm: 1.559662\tLR: 0.030000\n",
      "Train Epoch: 1371 [65536/194182 (33%)]\tLoss: 0.253572\tGrad Norm: 1.816021\tLR: 0.030000\n",
      "Train Epoch: 1371 [86016/194182 (44%)]\tLoss: 0.269474\tGrad Norm: 2.087985\tLR: 0.030000\n",
      "Train Epoch: 1371 [106496/194182 (54%)]\tLoss: 0.254849\tGrad Norm: 1.583887\tLR: 0.030000\n",
      "Train Epoch: 1371 [126976/194182 (65%)]\tLoss: 0.256977\tGrad Norm: 1.460918\tLR: 0.030000\n",
      "Train Epoch: 1371 [147456/194182 (75%)]\tLoss: 0.254751\tGrad Norm: 1.622221\tLR: 0.030000\n",
      "Train Epoch: 1371 [167936/194182 (85%)]\tLoss: 0.256281\tGrad Norm: 2.009439\tLR: 0.030000\n",
      "Train Epoch: 1371 [188416/194182 (96%)]\tLoss: 0.262459\tGrad Norm: 1.761202\tLR: 0.030000\n",
      "Train set: Average loss: 0.2587\n",
      "Test set: Average loss: 0.2488, Average MAE: 0.3409\n",
      "Train Epoch: 1372 [4096/194182 (2%)]\tLoss: 0.257629\tGrad Norm: 1.661099\tLR: 0.030000\n",
      "Train Epoch: 1372 [24576/194182 (12%)]\tLoss: 0.255797\tGrad Norm: 1.561093\tLR: 0.030000\n",
      "Train Epoch: 1372 [45056/194182 (23%)]\tLoss: 0.259943\tGrad Norm: 2.034867\tLR: 0.030000\n",
      "Train Epoch: 1372 [65536/194182 (33%)]\tLoss: 0.259942\tGrad Norm: 1.670615\tLR: 0.030000\n",
      "Train Epoch: 1372 [86016/194182 (44%)]\tLoss: 0.252481\tGrad Norm: 1.521828\tLR: 0.030000\n",
      "Train Epoch: 1372 [106496/194182 (54%)]\tLoss: 0.250010\tGrad Norm: 1.600181\tLR: 0.030000\n",
      "Train Epoch: 1372 [126976/194182 (65%)]\tLoss: 0.260431\tGrad Norm: 1.752854\tLR: 0.030000\n",
      "Train Epoch: 1372 [147456/194182 (75%)]\tLoss: 0.257915\tGrad Norm: 1.418065\tLR: 0.030000\n",
      "Train Epoch: 1372 [167936/194182 (85%)]\tLoss: 0.256826\tGrad Norm: 1.510410\tLR: 0.030000\n",
      "Train Epoch: 1372 [188416/194182 (96%)]\tLoss: 0.267508\tGrad Norm: 1.878024\tLR: 0.030000\n",
      "Train set: Average loss: 0.2575\n",
      "Test set: Average loss: 0.2571, Average MAE: 0.3432\n",
      "Train Epoch: 1373 [4096/194182 (2%)]\tLoss: 0.261214\tGrad Norm: 1.751918\tLR: 0.030000\n",
      "Train Epoch: 1373 [24576/194182 (12%)]\tLoss: 0.259046\tGrad Norm: 1.621000\tLR: 0.030000\n",
      "Train Epoch: 1373 [45056/194182 (23%)]\tLoss: 0.262871\tGrad Norm: 1.717217\tLR: 0.030000\n",
      "Train Epoch: 1373 [65536/194182 (33%)]\tLoss: 0.252195\tGrad Norm: 1.212724\tLR: 0.030000\n",
      "Train Epoch: 1373 [86016/194182 (44%)]\tLoss: 0.252741\tGrad Norm: 1.483027\tLR: 0.030000\n",
      "Train Epoch: 1373 [106496/194182 (54%)]\tLoss: 0.268303\tGrad Norm: 1.881745\tLR: 0.030000\n",
      "Train Epoch: 1373 [126976/194182 (65%)]\tLoss: 0.262156\tGrad Norm: 1.570978\tLR: 0.030000\n",
      "Train Epoch: 1373 [147456/194182 (75%)]\tLoss: 0.250785\tGrad Norm: 1.194028\tLR: 0.030000\n",
      "Train Epoch: 1373 [167936/194182 (85%)]\tLoss: 0.246396\tGrad Norm: 1.092903\tLR: 0.030000\n",
      "Train Epoch: 1373 [188416/194182 (96%)]\tLoss: 0.255469\tGrad Norm: 1.429672\tLR: 0.030000\n",
      "Train set: Average loss: 0.2551\n",
      "Test set: Average loss: 0.2544, Average MAE: 0.3517\n",
      "Train Epoch: 1374 [4096/194182 (2%)]\tLoss: 0.254979\tGrad Norm: 1.621771\tLR: 0.030000\n",
      "Train Epoch: 1374 [24576/194182 (12%)]\tLoss: 0.249106\tGrad Norm: 1.362198\tLR: 0.030000\n",
      "Train Epoch: 1374 [45056/194182 (23%)]\tLoss: 0.259268\tGrad Norm: 1.956598\tLR: 0.030000\n",
      "Train Epoch: 1374 [65536/194182 (33%)]\tLoss: 0.255203\tGrad Norm: 1.657848\tLR: 0.030000\n",
      "Train Epoch: 1374 [86016/194182 (44%)]\tLoss: 0.260997\tGrad Norm: 1.924880\tLR: 0.030000\n",
      "Train Epoch: 1374 [106496/194182 (54%)]\tLoss: 0.264935\tGrad Norm: 1.913572\tLR: 0.030000\n",
      "Train Epoch: 1374 [126976/194182 (65%)]\tLoss: 0.257296\tGrad Norm: 1.668580\tLR: 0.030000\n",
      "Train Epoch: 1374 [147456/194182 (75%)]\tLoss: 0.257965\tGrad Norm: 1.754415\tLR: 0.030000\n",
      "Train Epoch: 1374 [167936/194182 (85%)]\tLoss: 0.258678\tGrad Norm: 1.811134\tLR: 0.030000\n",
      "Train Epoch: 1374 [188416/194182 (96%)]\tLoss: 0.255901\tGrad Norm: 1.588729\tLR: 0.030000\n",
      "Train set: Average loss: 0.2587\n",
      "Test set: Average loss: 0.2514, Average MAE: 0.3527\n",
      "Train Epoch: 1375 [4096/194182 (2%)]\tLoss: 0.257726\tGrad Norm: 1.527750\tLR: 0.030000\n",
      "Train Epoch: 1375 [24576/194182 (12%)]\tLoss: 0.261022\tGrad Norm: 1.576835\tLR: 0.030000\n",
      "Train Epoch: 1375 [45056/194182 (23%)]\tLoss: 0.254918\tGrad Norm: 1.475217\tLR: 0.030000\n",
      "Train Epoch: 1375 [65536/194182 (33%)]\tLoss: 0.251199\tGrad Norm: 1.473748\tLR: 0.030000\n",
      "Train Epoch: 1375 [86016/194182 (44%)]\tLoss: 0.264784\tGrad Norm: 1.924717\tLR: 0.030000\n",
      "Train Epoch: 1375 [106496/194182 (54%)]\tLoss: 0.261339\tGrad Norm: 1.468791\tLR: 0.030000\n",
      "Train Epoch: 1375 [126976/194182 (65%)]\tLoss: 0.248944\tGrad Norm: 1.283676\tLR: 0.030000\n",
      "Train Epoch: 1375 [147456/194182 (75%)]\tLoss: 0.266127\tGrad Norm: 1.633287\tLR: 0.030000\n",
      "Train Epoch: 1375 [167936/194182 (85%)]\tLoss: 0.257180\tGrad Norm: 1.705782\tLR: 0.030000\n",
      "Train Epoch: 1375 [188416/194182 (96%)]\tLoss: 0.257156\tGrad Norm: 1.832785\tLR: 0.030000\n",
      "Train set: Average loss: 0.2566\n",
      "Test set: Average loss: 0.2509, Average MAE: 0.3376\n",
      "Epoch 1375: Mean reward = 0.074 +/- 0.070\n",
      "Train Epoch: 1376 [4096/194182 (2%)]\tLoss: 0.254783\tGrad Norm: 1.717952\tLR: 0.030000\n",
      "Train Epoch: 1376 [24576/194182 (12%)]\tLoss: 0.260522\tGrad Norm: 1.902641\tLR: 0.030000\n",
      "Train Epoch: 1376 [45056/194182 (23%)]\tLoss: 0.252543\tGrad Norm: 1.458755\tLR: 0.030000\n",
      "Train Epoch: 1376 [65536/194182 (33%)]\tLoss: 0.257161\tGrad Norm: 1.878435\tLR: 0.030000\n",
      "Train Epoch: 1376 [86016/194182 (44%)]\tLoss: 0.250081\tGrad Norm: 1.352714\tLR: 0.030000\n",
      "Train Epoch: 1376 [106496/194182 (54%)]\tLoss: 0.252458\tGrad Norm: 1.529743\tLR: 0.030000\n",
      "Train Epoch: 1376 [126976/194182 (65%)]\tLoss: 0.253101\tGrad Norm: 1.460782\tLR: 0.030000\n",
      "Train Epoch: 1376 [147456/194182 (75%)]\tLoss: 0.261476\tGrad Norm: 1.602623\tLR: 0.030000\n",
      "Train Epoch: 1376 [167936/194182 (85%)]\tLoss: 0.249003\tGrad Norm: 1.154827\tLR: 0.030000\n",
      "Train Epoch: 1376 [188416/194182 (96%)]\tLoss: 0.250097\tGrad Norm: 1.312558\tLR: 0.030000\n",
      "Train set: Average loss: 0.2551\n",
      "Test set: Average loss: 0.2597, Average MAE: 0.3582\n",
      "Train Epoch: 1377 [4096/194182 (2%)]\tLoss: 0.263407\tGrad Norm: 2.047898\tLR: 0.030000\n",
      "Train Epoch: 1377 [24576/194182 (12%)]\tLoss: 0.253642\tGrad Norm: 1.662162\tLR: 0.030000\n",
      "Train Epoch: 1377 [45056/194182 (23%)]\tLoss: 0.260001\tGrad Norm: 1.569825\tLR: 0.030000\n",
      "Train Epoch: 1377 [65536/194182 (33%)]\tLoss: 0.249107\tGrad Norm: 1.432786\tLR: 0.030000\n",
      "Train Epoch: 1377 [86016/194182 (44%)]\tLoss: 0.252586\tGrad Norm: 1.578062\tLR: 0.030000\n",
      "Train Epoch: 1377 [106496/194182 (54%)]\tLoss: 0.259908\tGrad Norm: 1.290440\tLR: 0.030000\n",
      "Train Epoch: 1377 [126976/194182 (65%)]\tLoss: 0.250713\tGrad Norm: 1.352697\tLR: 0.030000\n",
      "Train Epoch: 1377 [147456/194182 (75%)]\tLoss: 0.253454\tGrad Norm: 1.600973\tLR: 0.030000\n",
      "Train Epoch: 1377 [167936/194182 (85%)]\tLoss: 0.269833\tGrad Norm: 2.095367\tLR: 0.030000\n",
      "Train Epoch: 1377 [188416/194182 (96%)]\tLoss: 0.269396\tGrad Norm: 2.395257\tLR: 0.030000\n",
      "Train set: Average loss: 0.2573\n",
      "Test set: Average loss: 0.2630, Average MAE: 0.3699\n",
      "Train Epoch: 1378 [4096/194182 (2%)]\tLoss: 0.261263\tGrad Norm: 1.993819\tLR: 0.030000\n",
      "Train Epoch: 1378 [24576/194182 (12%)]\tLoss: 0.251719\tGrad Norm: 1.549468\tLR: 0.030000\n",
      "Train Epoch: 1378 [45056/194182 (23%)]\tLoss: 0.248330\tGrad Norm: 1.333643\tLR: 0.030000\n",
      "Train Epoch: 1378 [65536/194182 (33%)]\tLoss: 0.254734\tGrad Norm: 1.497520\tLR: 0.030000\n",
      "Train Epoch: 1378 [86016/194182 (44%)]\tLoss: 0.247164\tGrad Norm: 1.453322\tLR: 0.030000\n",
      "Train Epoch: 1378 [106496/194182 (54%)]\tLoss: 0.258319\tGrad Norm: 1.798733\tLR: 0.030000\n",
      "Train Epoch: 1378 [126976/194182 (65%)]\tLoss: 0.265308\tGrad Norm: 1.768012\tLR: 0.030000\n",
      "Train Epoch: 1378 [147456/194182 (75%)]\tLoss: 0.254848\tGrad Norm: 1.511749\tLR: 0.030000\n",
      "Train Epoch: 1378 [167936/194182 (85%)]\tLoss: 0.253785\tGrad Norm: 1.245978\tLR: 0.030000\n",
      "Train Epoch: 1378 [188416/194182 (96%)]\tLoss: 0.244508\tGrad Norm: 0.978043\tLR: 0.030000\n",
      "Train set: Average loss: 0.2544\n",
      "Test set: Average loss: 0.2451, Average MAE: 0.3408\n",
      "Train Epoch: 1379 [4096/194182 (2%)]\tLoss: 0.249917\tGrad Norm: 1.417360\tLR: 0.030000\n",
      "Train Epoch: 1379 [24576/194182 (12%)]\tLoss: 0.258607\tGrad Norm: 1.482189\tLR: 0.030000\n",
      "Train Epoch: 1379 [45056/194182 (23%)]\tLoss: 0.263728\tGrad Norm: 1.674431\tLR: 0.030000\n",
      "Train Epoch: 1379 [65536/194182 (33%)]\tLoss: 0.250638\tGrad Norm: 1.411405\tLR: 0.030000\n",
      "Train Epoch: 1379 [86016/194182 (44%)]\tLoss: 0.251210\tGrad Norm: 1.481326\tLR: 0.030000\n",
      "Train Epoch: 1379 [106496/194182 (54%)]\tLoss: 0.246621\tGrad Norm: 1.533402\tLR: 0.030000\n",
      "Train Epoch: 1379 [126976/194182 (65%)]\tLoss: 0.254702\tGrad Norm: 1.727994\tLR: 0.030000\n",
      "Train Epoch: 1379 [147456/194182 (75%)]\tLoss: 0.261540\tGrad Norm: 1.974113\tLR: 0.030000\n",
      "Train Epoch: 1379 [167936/194182 (85%)]\tLoss: 0.258643\tGrad Norm: 1.881918\tLR: 0.030000\n",
      "Train Epoch: 1379 [188416/194182 (96%)]\tLoss: 0.259855\tGrad Norm: 1.783688\tLR: 0.030000\n",
      "Train set: Average loss: 0.2563\n",
      "Test set: Average loss: 0.2566, Average MAE: 0.3586\n",
      "Train Epoch: 1380 [4096/194182 (2%)]\tLoss: 0.267529\tGrad Norm: 1.992192\tLR: 0.030000\n",
      "Train Epoch: 1380 [24576/194182 (12%)]\tLoss: 0.260804\tGrad Norm: 1.557183\tLR: 0.030000\n",
      "Train Epoch: 1380 [45056/194182 (23%)]\tLoss: 0.247189\tGrad Norm: 1.245667\tLR: 0.030000\n",
      "Train Epoch: 1380 [65536/194182 (33%)]\tLoss: 0.256035\tGrad Norm: 1.535269\tLR: 0.030000\n",
      "Train Epoch: 1380 [86016/194182 (44%)]\tLoss: 0.261895\tGrad Norm: 1.768434\tLR: 0.030000\n",
      "Train Epoch: 1380 [106496/194182 (54%)]\tLoss: 0.249922\tGrad Norm: 1.376875\tLR: 0.030000\n",
      "Train Epoch: 1380 [126976/194182 (65%)]\tLoss: 0.249611\tGrad Norm: 1.360562\tLR: 0.030000\n",
      "Train Epoch: 1380 [147456/194182 (75%)]\tLoss: 0.255812\tGrad Norm: 1.352509\tLR: 0.030000\n",
      "Train Epoch: 1380 [167936/194182 (85%)]\tLoss: 0.255857\tGrad Norm: 1.503217\tLR: 0.030000\n",
      "Train Epoch: 1380 [188416/194182 (96%)]\tLoss: 0.255981\tGrad Norm: 1.443733\tLR: 0.030000\n",
      "Train set: Average loss: 0.2542\n",
      "Test set: Average loss: 0.2449, Average MAE: 0.3426\n",
      "Epoch 1380: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 1381 [4096/194182 (2%)]\tLoss: 0.251173\tGrad Norm: 1.304609\tLR: 0.030000\n",
      "Train Epoch: 1381 [24576/194182 (12%)]\tLoss: 0.261777\tGrad Norm: 1.609545\tLR: 0.030000\n",
      "Train Epoch: 1381 [45056/194182 (23%)]\tLoss: 0.253883\tGrad Norm: 1.559602\tLR: 0.030000\n",
      "Train Epoch: 1381 [65536/194182 (33%)]\tLoss: 0.254168\tGrad Norm: 1.685866\tLR: 0.030000\n",
      "Train Epoch: 1381 [86016/194182 (44%)]\tLoss: 0.255966\tGrad Norm: 1.626520\tLR: 0.030000\n",
      "Train Epoch: 1381 [106496/194182 (54%)]\tLoss: 0.262601\tGrad Norm: 2.333590\tLR: 0.030000\n",
      "Train Epoch: 1381 [126976/194182 (65%)]\tLoss: 0.267086\tGrad Norm: 2.002584\tLR: 0.030000\n",
      "Train Epoch: 1381 [147456/194182 (75%)]\tLoss: 0.251397\tGrad Norm: 1.371292\tLR: 0.030000\n",
      "Train Epoch: 1381 [167936/194182 (85%)]\tLoss: 0.256249\tGrad Norm: 1.614483\tLR: 0.030000\n",
      "Train Epoch: 1381 [188416/194182 (96%)]\tLoss: 0.248800\tGrad Norm: 1.564154\tLR: 0.030000\n",
      "Train set: Average loss: 0.2559\n",
      "Test set: Average loss: 0.2515, Average MAE: 0.3532\n",
      "Train Epoch: 1382 [4096/194182 (2%)]\tLoss: 0.255739\tGrad Norm: 1.552680\tLR: 0.030000\n",
      "Train Epoch: 1382 [24576/194182 (12%)]\tLoss: 0.265089\tGrad Norm: 2.019731\tLR: 0.030000\n",
      "Train Epoch: 1382 [45056/194182 (23%)]\tLoss: 0.256298\tGrad Norm: 1.868947\tLR: 0.030000\n",
      "Train Epoch: 1382 [65536/194182 (33%)]\tLoss: 0.259259\tGrad Norm: 1.768405\tLR: 0.030000\n",
      "Train Epoch: 1382 [86016/194182 (44%)]\tLoss: 0.253252\tGrad Norm: 1.579105\tLR: 0.030000\n",
      "Train Epoch: 1382 [106496/194182 (54%)]\tLoss: 0.244399\tGrad Norm: 1.300847\tLR: 0.030000\n",
      "Train Epoch: 1382 [126976/194182 (65%)]\tLoss: 0.258082\tGrad Norm: 1.645921\tLR: 0.030000\n",
      "Train Epoch: 1382 [147456/194182 (75%)]\tLoss: 0.256746\tGrad Norm: 1.485327\tLR: 0.030000\n",
      "Train Epoch: 1382 [167936/194182 (85%)]\tLoss: 0.258377\tGrad Norm: 1.462718\tLR: 0.030000\n",
      "Train Epoch: 1382 [188416/194182 (96%)]\tLoss: 0.256191\tGrad Norm: 1.575571\tLR: 0.030000\n",
      "Train set: Average loss: 0.2558\n",
      "Test set: Average loss: 0.2459, Average MAE: 0.3452\n",
      "Train Epoch: 1383 [4096/194182 (2%)]\tLoss: 0.248525\tGrad Norm: 1.123491\tLR: 0.030000\n",
      "Train Epoch: 1383 [24576/194182 (12%)]\tLoss: 0.251189\tGrad Norm: 1.197840\tLR: 0.030000\n",
      "Train Epoch: 1383 [45056/194182 (23%)]\tLoss: 0.248707\tGrad Norm: 1.398120\tLR: 0.030000\n",
      "Train Epoch: 1383 [65536/194182 (33%)]\tLoss: 0.261075\tGrad Norm: 1.607300\tLR: 0.030000\n",
      "Train Epoch: 1383 [86016/194182 (44%)]\tLoss: 0.255493\tGrad Norm: 1.414477\tLR: 0.030000\n",
      "Train Epoch: 1383 [106496/194182 (54%)]\tLoss: 0.252343\tGrad Norm: 1.660653\tLR: 0.030000\n",
      "Train Epoch: 1383 [126976/194182 (65%)]\tLoss: 0.263529\tGrad Norm: 1.757919\tLR: 0.030000\n",
      "Train Epoch: 1383 [147456/194182 (75%)]\tLoss: 0.261207\tGrad Norm: 1.651350\tLR: 0.030000\n",
      "Train Epoch: 1383 [167936/194182 (85%)]\tLoss: 0.244344\tGrad Norm: 1.391691\tLR: 0.030000\n",
      "Train Epoch: 1383 [188416/194182 (96%)]\tLoss: 0.247349\tGrad Norm: 1.445156\tLR: 0.030000\n",
      "Train set: Average loss: 0.2538\n",
      "Test set: Average loss: 0.2456, Average MAE: 0.3381\n",
      "Train Epoch: 1384 [4096/194182 (2%)]\tLoss: 0.255172\tGrad Norm: 1.385909\tLR: 0.030000\n",
      "Train Epoch: 1384 [24576/194182 (12%)]\tLoss: 0.260370\tGrad Norm: 1.695634\tLR: 0.030000\n",
      "Train Epoch: 1384 [45056/194182 (23%)]\tLoss: 0.253253\tGrad Norm: 1.475621\tLR: 0.030000\n",
      "Train Epoch: 1384 [65536/194182 (33%)]\tLoss: 0.259079\tGrad Norm: 2.167477\tLR: 0.030000\n",
      "Train Epoch: 1384 [86016/194182 (44%)]\tLoss: 0.250633\tGrad Norm: 1.856055\tLR: 0.030000\n",
      "Train Epoch: 1384 [106496/194182 (54%)]\tLoss: 0.262290\tGrad Norm: 2.015122\tLR: 0.030000\n",
      "Train Epoch: 1384 [126976/194182 (65%)]\tLoss: 0.265868\tGrad Norm: 1.938106\tLR: 0.030000\n",
      "Train Epoch: 1384 [147456/194182 (75%)]\tLoss: 0.253893\tGrad Norm: 1.530190\tLR: 0.030000\n",
      "Train Epoch: 1384 [167936/194182 (85%)]\tLoss: 0.253592\tGrad Norm: 1.456626\tLR: 0.030000\n",
      "Train Epoch: 1384 [188416/194182 (96%)]\tLoss: 0.259313\tGrad Norm: 1.772225\tLR: 0.030000\n",
      "Train set: Average loss: 0.2561\n",
      "Test set: Average loss: 0.2493, Average MAE: 0.3527\n",
      "Train Epoch: 1385 [4096/194182 (2%)]\tLoss: 0.244419\tGrad Norm: 1.370602\tLR: 0.030000\n",
      "Train Epoch: 1385 [24576/194182 (12%)]\tLoss: 0.260686\tGrad Norm: 1.417927\tLR: 0.030000\n",
      "Train Epoch: 1385 [45056/194182 (23%)]\tLoss: 0.245757\tGrad Norm: 1.054071\tLR: 0.030000\n",
      "Train Epoch: 1385 [65536/194182 (33%)]\tLoss: 0.249714\tGrad Norm: 1.271970\tLR: 0.030000\n",
      "Train Epoch: 1385 [86016/194182 (44%)]\tLoss: 0.260509\tGrad Norm: 1.585744\tLR: 0.030000\n",
      "Train Epoch: 1385 [106496/194182 (54%)]\tLoss: 0.253930\tGrad Norm: 1.671796\tLR: 0.030000\n",
      "Train Epoch: 1385 [126976/194182 (65%)]\tLoss: 0.266342\tGrad Norm: 1.773524\tLR: 0.030000\n",
      "Train Epoch: 1385 [147456/194182 (75%)]\tLoss: 0.252194\tGrad Norm: 1.314803\tLR: 0.030000\n",
      "Train Epoch: 1385 [167936/194182 (85%)]\tLoss: 0.254895\tGrad Norm: 1.727182\tLR: 0.030000\n",
      "Train Epoch: 1385 [188416/194182 (96%)]\tLoss: 0.255051\tGrad Norm: 1.722945\tLR: 0.030000\n",
      "Train set: Average loss: 0.2544\n",
      "Test set: Average loss: 0.2574, Average MAE: 0.3597\n",
      "Epoch 1385: Mean reward = 0.037 +/- 0.019\n",
      "Train Epoch: 1386 [4096/194182 (2%)]\tLoss: 0.262908\tGrad Norm: 2.026254\tLR: 0.030000\n",
      "Train Epoch: 1386 [24576/194182 (12%)]\tLoss: 0.253369\tGrad Norm: 1.781762\tLR: 0.030000\n",
      "Train Epoch: 1386 [45056/194182 (23%)]\tLoss: 0.257912\tGrad Norm: 1.802482\tLR: 0.030000\n",
      "Train Epoch: 1386 [65536/194182 (33%)]\tLoss: 0.249281\tGrad Norm: 1.379648\tLR: 0.030000\n",
      "Train Epoch: 1386 [86016/194182 (44%)]\tLoss: 0.252664\tGrad Norm: 1.574574\tLR: 0.030000\n",
      "Train Epoch: 1386 [106496/194182 (54%)]\tLoss: 0.251524\tGrad Norm: 1.444643\tLR: 0.030000\n",
      "Train Epoch: 1386 [126976/194182 (65%)]\tLoss: 0.249831\tGrad Norm: 1.565657\tLR: 0.030000\n",
      "Train Epoch: 1386 [147456/194182 (75%)]\tLoss: 0.256169\tGrad Norm: 1.679937\tLR: 0.030000\n",
      "Train Epoch: 1386 [167936/194182 (85%)]\tLoss: 0.263006\tGrad Norm: 2.177341\tLR: 0.030000\n",
      "Train Epoch: 1386 [188416/194182 (96%)]\tLoss: 0.259279\tGrad Norm: 1.670371\tLR: 0.030000\n",
      "Train set: Average loss: 0.2560\n",
      "Test set: Average loss: 0.2477, Average MAE: 0.3519\n",
      "Train Epoch: 1387 [4096/194182 (2%)]\tLoss: 0.253901\tGrad Norm: 1.502684\tLR: 0.030000\n",
      "Train Epoch: 1387 [24576/194182 (12%)]\tLoss: 0.257503\tGrad Norm: 1.689779\tLR: 0.030000\n",
      "Train Epoch: 1387 [45056/194182 (23%)]\tLoss: 0.257479\tGrad Norm: 1.681828\tLR: 0.030000\n",
      "Train Epoch: 1387 [65536/194182 (33%)]\tLoss: 0.253438\tGrad Norm: 1.435688\tLR: 0.030000\n",
      "Train Epoch: 1387 [86016/194182 (44%)]\tLoss: 0.253620\tGrad Norm: 1.893346\tLR: 0.030000\n",
      "Train Epoch: 1387 [106496/194182 (54%)]\tLoss: 0.254798\tGrad Norm: 1.637393\tLR: 0.030000\n",
      "Train Epoch: 1387 [126976/194182 (65%)]\tLoss: 0.251271\tGrad Norm: 1.516136\tLR: 0.030000\n",
      "Train Epoch: 1387 [147456/194182 (75%)]\tLoss: 0.261730\tGrad Norm: 1.486087\tLR: 0.030000\n",
      "Train Epoch: 1387 [167936/194182 (85%)]\tLoss: 0.252419\tGrad Norm: 1.545601\tLR: 0.030000\n",
      "Train Epoch: 1387 [188416/194182 (96%)]\tLoss: 0.250385\tGrad Norm: 1.256840\tLR: 0.030000\n",
      "Train set: Average loss: 0.2547\n",
      "Test set: Average loss: 0.2566, Average MAE: 0.3510\n",
      "Train Epoch: 1388 [4096/194182 (2%)]\tLoss: 0.258908\tGrad Norm: 1.905421\tLR: 0.030000\n",
      "Train Epoch: 1388 [24576/194182 (12%)]\tLoss: 0.253323\tGrad Norm: 1.613615\tLR: 0.030000\n",
      "Train Epoch: 1388 [45056/194182 (23%)]\tLoss: 0.253422\tGrad Norm: 1.405291\tLR: 0.030000\n",
      "Train Epoch: 1388 [65536/194182 (33%)]\tLoss: 0.248748\tGrad Norm: 1.310915\tLR: 0.030000\n",
      "Train Epoch: 1388 [86016/194182 (44%)]\tLoss: 0.244768\tGrad Norm: 1.165750\tLR: 0.030000\n",
      "Train Epoch: 1388 [106496/194182 (54%)]\tLoss: 0.263222\tGrad Norm: 1.530519\tLR: 0.030000\n",
      "Train Epoch: 1388 [126976/194182 (65%)]\tLoss: 0.256999\tGrad Norm: 1.520161\tLR: 0.030000\n",
      "Train Epoch: 1388 [147456/194182 (75%)]\tLoss: 0.265036\tGrad Norm: 2.202021\tLR: 0.030000\n",
      "Train Epoch: 1388 [167936/194182 (85%)]\tLoss: 0.265694\tGrad Norm: 2.024627\tLR: 0.030000\n",
      "Train Epoch: 1388 [188416/194182 (96%)]\tLoss: 0.262752\tGrad Norm: 1.715789\tLR: 0.030000\n",
      "Train set: Average loss: 0.2553\n",
      "Test set: Average loss: 0.2438, Average MAE: 0.3417\n",
      "Train Epoch: 1389 [4096/194182 (2%)]\tLoss: 0.249399\tGrad Norm: 1.125834\tLR: 0.030000\n",
      "Train Epoch: 1389 [24576/194182 (12%)]\tLoss: 0.252830\tGrad Norm: 1.580625\tLR: 0.030000\n",
      "Train Epoch: 1389 [45056/194182 (23%)]\tLoss: 0.250328\tGrad Norm: 1.509471\tLR: 0.030000\n",
      "Train Epoch: 1389 [65536/194182 (33%)]\tLoss: 0.254758\tGrad Norm: 1.654589\tLR: 0.030000\n",
      "Train Epoch: 1389 [86016/194182 (44%)]\tLoss: 0.253707\tGrad Norm: 1.656171\tLR: 0.030000\n",
      "Train Epoch: 1389 [106496/194182 (54%)]\tLoss: 0.259311\tGrad Norm: 1.610293\tLR: 0.030000\n",
      "Train Epoch: 1389 [126976/194182 (65%)]\tLoss: 0.258782\tGrad Norm: 1.639132\tLR: 0.030000\n",
      "Train Epoch: 1389 [147456/194182 (75%)]\tLoss: 0.256717\tGrad Norm: 1.389482\tLR: 0.030000\n",
      "Train Epoch: 1389 [167936/194182 (85%)]\tLoss: 0.256488\tGrad Norm: 1.571652\tLR: 0.030000\n",
      "Train Epoch: 1389 [188416/194182 (96%)]\tLoss: 0.247755\tGrad Norm: 1.552631\tLR: 0.030000\n",
      "Train set: Average loss: 0.2543\n",
      "Test set: Average loss: 0.2458, Average MAE: 0.3392\n",
      "Train Epoch: 1390 [4096/194182 (2%)]\tLoss: 0.253749\tGrad Norm: 1.321403\tLR: 0.030000\n",
      "Train Epoch: 1390 [24576/194182 (12%)]\tLoss: 0.249574\tGrad Norm: 1.271277\tLR: 0.030000\n",
      "Train Epoch: 1390 [45056/194182 (23%)]\tLoss: 0.253760\tGrad Norm: 1.862568\tLR: 0.030000\n",
      "Train Epoch: 1390 [65536/194182 (33%)]\tLoss: 0.253541\tGrad Norm: 1.594447\tLR: 0.030000\n",
      "Train Epoch: 1390 [86016/194182 (44%)]\tLoss: 0.250448\tGrad Norm: 1.305028\tLR: 0.030000\n",
      "Train Epoch: 1390 [106496/194182 (54%)]\tLoss: 0.253160\tGrad Norm: 1.670689\tLR: 0.030000\n",
      "Train Epoch: 1390 [126976/194182 (65%)]\tLoss: 0.252482\tGrad Norm: 1.680153\tLR: 0.030000\n",
      "Train Epoch: 1390 [147456/194182 (75%)]\tLoss: 0.245252\tGrad Norm: 1.609902\tLR: 0.030000\n",
      "Train Epoch: 1390 [167936/194182 (85%)]\tLoss: 0.263727\tGrad Norm: 1.848137\tLR: 0.030000\n",
      "Train Epoch: 1390 [188416/194182 (96%)]\tLoss: 0.245887\tGrad Norm: 1.629233\tLR: 0.030000\n",
      "Train set: Average loss: 0.2536\n",
      "Test set: Average loss: 0.2432, Average MAE: 0.3438\n",
      "Epoch 1390: Mean reward = 0.078 +/- 0.071\n",
      "Train Epoch: 1391 [4096/194182 (2%)]\tLoss: 0.245618\tGrad Norm: 1.049960\tLR: 0.030000\n",
      "Train Epoch: 1391 [24576/194182 (12%)]\tLoss: 0.258647\tGrad Norm: 1.716333\tLR: 0.030000\n",
      "Train Epoch: 1391 [45056/194182 (23%)]\tLoss: 0.267437\tGrad Norm: 2.167797\tLR: 0.030000\n",
      "Train Epoch: 1391 [65536/194182 (33%)]\tLoss: 0.249790\tGrad Norm: 1.553330\tLR: 0.030000\n",
      "Train Epoch: 1391 [86016/194182 (44%)]\tLoss: 0.252899\tGrad Norm: 1.686342\tLR: 0.030000\n",
      "Train Epoch: 1391 [106496/194182 (54%)]\tLoss: 0.259179\tGrad Norm: 1.532171\tLR: 0.030000\n",
      "Train Epoch: 1391 [126976/194182 (65%)]\tLoss: 0.259492\tGrad Norm: 1.520909\tLR: 0.030000\n",
      "Train Epoch: 1391 [147456/194182 (75%)]\tLoss: 0.252800\tGrad Norm: 1.686736\tLR: 0.030000\n",
      "Train Epoch: 1391 [167936/194182 (85%)]\tLoss: 0.251029\tGrad Norm: 1.552549\tLR: 0.030000\n",
      "Train Epoch: 1391 [188416/194182 (96%)]\tLoss: 0.259441\tGrad Norm: 1.437013\tLR: 0.030000\n",
      "Train set: Average loss: 0.2546\n",
      "Test set: Average loss: 0.2459, Average MAE: 0.3453\n",
      "Train Epoch: 1392 [4096/194182 (2%)]\tLoss: 0.247710\tGrad Norm: 1.203191\tLR: 0.030000\n",
      "Train Epoch: 1392 [24576/194182 (12%)]\tLoss: 0.251731\tGrad Norm: 1.352650\tLR: 0.030000\n",
      "Train Epoch: 1392 [45056/194182 (23%)]\tLoss: 0.256375\tGrad Norm: 1.528553\tLR: 0.030000\n",
      "Train Epoch: 1392 [65536/194182 (33%)]\tLoss: 0.254920\tGrad Norm: 1.655731\tLR: 0.030000\n",
      "Train Epoch: 1392 [86016/194182 (44%)]\tLoss: 0.253186\tGrad Norm: 1.351841\tLR: 0.030000\n",
      "Train Epoch: 1392 [106496/194182 (54%)]\tLoss: 0.254335\tGrad Norm: 1.488086\tLR: 0.030000\n",
      "Train Epoch: 1392 [126976/194182 (65%)]\tLoss: 0.255717\tGrad Norm: 1.735944\tLR: 0.030000\n",
      "Train Epoch: 1392 [147456/194182 (75%)]\tLoss: 0.250638\tGrad Norm: 1.870857\tLR: 0.030000\n",
      "Train Epoch: 1392 [167936/194182 (85%)]\tLoss: 0.265119\tGrad Norm: 2.002967\tLR: 0.030000\n",
      "Train Epoch: 1392 [188416/194182 (96%)]\tLoss: 0.276696\tGrad Norm: 5.868082\tLR: 0.030000\n",
      "Train set: Average loss: 0.2553\n",
      "Test set: Average loss: 0.2555, Average MAE: 0.3627\n",
      "Train Epoch: 1393 [4096/194182 (2%)]\tLoss: 0.255307\tGrad Norm: 1.718916\tLR: 0.030000\n",
      "Train Epoch: 1393 [24576/194182 (12%)]\tLoss: 0.252887\tGrad Norm: 1.696963\tLR: 0.030000\n",
      "Train Epoch: 1393 [45056/194182 (23%)]\tLoss: 0.254001\tGrad Norm: 1.660952\tLR: 0.030000\n",
      "Train Epoch: 1393 [65536/194182 (33%)]\tLoss: 0.258883\tGrad Norm: 2.064243\tLR: 0.030000\n",
      "Train Epoch: 1393 [86016/194182 (44%)]\tLoss: 0.254606\tGrad Norm: 1.566592\tLR: 0.030000\n",
      "Train Epoch: 1393 [106496/194182 (54%)]\tLoss: 0.248604\tGrad Norm: 1.436705\tLR: 0.030000\n",
      "Train Epoch: 1393 [126976/194182 (65%)]\tLoss: 0.254289\tGrad Norm: 1.501809\tLR: 0.030000\n",
      "Train Epoch: 1393 [147456/194182 (75%)]\tLoss: 0.254109\tGrad Norm: 1.649274\tLR: 0.030000\n",
      "Train Epoch: 1393 [167936/194182 (85%)]\tLoss: 0.255869\tGrad Norm: 1.702477\tLR: 0.030000\n",
      "Train Epoch: 1393 [188416/194182 (96%)]\tLoss: 0.253792\tGrad Norm: 1.086739\tLR: 0.030000\n",
      "Train set: Average loss: 0.2545\n",
      "Test set: Average loss: 0.2433, Average MAE: 0.3410\n",
      "Train Epoch: 1394 [4096/194182 (2%)]\tLoss: 0.248314\tGrad Norm: 1.194977\tLR: 0.030000\n",
      "Train Epoch: 1394 [24576/194182 (12%)]\tLoss: 0.250125\tGrad Norm: 1.428631\tLR: 0.030000\n",
      "Train Epoch: 1394 [45056/194182 (23%)]\tLoss: 0.254280\tGrad Norm: 1.585914\tLR: 0.030000\n",
      "Train Epoch: 1394 [65536/194182 (33%)]\tLoss: 0.253898\tGrad Norm: 1.500667\tLR: 0.030000\n",
      "Train Epoch: 1394 [86016/194182 (44%)]\tLoss: 0.252651\tGrad Norm: 1.391795\tLR: 0.030000\n",
      "Train Epoch: 1394 [106496/194182 (54%)]\tLoss: 0.249372\tGrad Norm: 1.348926\tLR: 0.030000\n",
      "Train Epoch: 1394 [126976/194182 (65%)]\tLoss: 0.253903\tGrad Norm: 1.479938\tLR: 0.030000\n",
      "Train Epoch: 1394 [147456/194182 (75%)]\tLoss: 0.262419\tGrad Norm: 2.046553\tLR: 0.030000\n",
      "Train Epoch: 1394 [167936/194182 (85%)]\tLoss: 0.250157\tGrad Norm: 1.625691\tLR: 0.030000\n",
      "Train Epoch: 1394 [188416/194182 (96%)]\tLoss: 0.244366\tGrad Norm: 1.322945\tLR: 0.030000\n",
      "Train set: Average loss: 0.2527\n",
      "Test set: Average loss: 0.2491, Average MAE: 0.3376\n",
      "Train Epoch: 1395 [4096/194182 (2%)]\tLoss: 0.257002\tGrad Norm: 1.582219\tLR: 0.030000\n",
      "Train Epoch: 1395 [24576/194182 (12%)]\tLoss: 0.247465\tGrad Norm: 1.542091\tLR: 0.030000\n",
      "Train Epoch: 1395 [45056/194182 (23%)]\tLoss: 0.253592\tGrad Norm: 1.730080\tLR: 0.030000\n",
      "Train Epoch: 1395 [65536/194182 (33%)]\tLoss: 0.249805\tGrad Norm: 1.953864\tLR: 0.030000\n",
      "Train Epoch: 1395 [86016/194182 (44%)]\tLoss: 0.252444\tGrad Norm: 1.844776\tLR: 0.030000\n",
      "Train Epoch: 1395 [106496/194182 (54%)]\tLoss: 0.257532\tGrad Norm: 1.817022\tLR: 0.030000\n",
      "Train Epoch: 1395 [126976/194182 (65%)]\tLoss: 0.251068\tGrad Norm: 1.431880\tLR: 0.030000\n",
      "Train Epoch: 1395 [147456/194182 (75%)]\tLoss: 0.249410\tGrad Norm: 1.318083\tLR: 0.030000\n",
      "Train Epoch: 1395 [167936/194182 (85%)]\tLoss: 0.253099\tGrad Norm: 1.559156\tLR: 0.030000\n",
      "Train Epoch: 1395 [188416/194182 (96%)]\tLoss: 0.255044\tGrad Norm: 1.634006\tLR: 0.030000\n",
      "Train set: Average loss: 0.2541\n",
      "Test set: Average loss: 0.2497, Average MAE: 0.3394\n",
      "Epoch 1395: Mean reward = 0.059 +/- 0.057\n",
      "Train Epoch: 1396 [4096/194182 (2%)]\tLoss: 0.254250\tGrad Norm: 1.603624\tLR: 0.030000\n",
      "Train Epoch: 1396 [24576/194182 (12%)]\tLoss: 0.249499\tGrad Norm: 1.558065\tLR: 0.030000\n",
      "Train Epoch: 1396 [45056/194182 (23%)]\tLoss: 0.248080\tGrad Norm: 1.360547\tLR: 0.030000\n",
      "Train Epoch: 1396 [65536/194182 (33%)]\tLoss: 0.248957\tGrad Norm: 1.468621\tLR: 0.030000\n",
      "Train Epoch: 1396 [86016/194182 (44%)]\tLoss: 0.250376\tGrad Norm: 1.535878\tLR: 0.030000\n",
      "Train Epoch: 1396 [106496/194182 (54%)]\tLoss: 0.262373\tGrad Norm: 1.580218\tLR: 0.030000\n",
      "Train Epoch: 1396 [126976/194182 (65%)]\tLoss: 0.256552\tGrad Norm: 1.713959\tLR: 0.030000\n",
      "Train Epoch: 1396 [147456/194182 (75%)]\tLoss: 0.252815\tGrad Norm: 1.541767\tLR: 0.030000\n",
      "Train Epoch: 1396 [167936/194182 (85%)]\tLoss: 0.260373\tGrad Norm: 1.764389\tLR: 0.030000\n",
      "Train Epoch: 1396 [188416/194182 (96%)]\tLoss: 0.252150\tGrad Norm: 1.472429\tLR: 0.030000\n",
      "Train set: Average loss: 0.2530\n",
      "Test set: Average loss: 0.2514, Average MAE: 0.3345\n",
      "Train Epoch: 1397 [4096/194182 (2%)]\tLoss: 0.253230\tGrad Norm: 1.818487\tLR: 0.030000\n",
      "Train Epoch: 1397 [24576/194182 (12%)]\tLoss: 0.253983\tGrad Norm: 1.804237\tLR: 0.030000\n",
      "Train Epoch: 1397 [45056/194182 (23%)]\tLoss: 0.256204\tGrad Norm: 1.623222\tLR: 0.030000\n",
      "Train Epoch: 1397 [65536/194182 (33%)]\tLoss: 0.250129\tGrad Norm: 1.759818\tLR: 0.030000\n",
      "Train Epoch: 1397 [86016/194182 (44%)]\tLoss: 0.250723\tGrad Norm: 1.627813\tLR: 0.030000\n",
      "Train Epoch: 1397 [106496/194182 (54%)]\tLoss: 0.250844\tGrad Norm: 1.442825\tLR: 0.030000\n",
      "Train Epoch: 1397 [126976/194182 (65%)]\tLoss: 0.253976\tGrad Norm: 1.579984\tLR: 0.030000\n",
      "Train Epoch: 1397 [147456/194182 (75%)]\tLoss: 0.256184\tGrad Norm: 1.447631\tLR: 0.030000\n",
      "Train Epoch: 1397 [167936/194182 (85%)]\tLoss: 0.245603\tGrad Norm: 1.404312\tLR: 0.030000\n",
      "Train Epoch: 1397 [188416/194182 (96%)]\tLoss: 0.257777\tGrad Norm: 1.993206\tLR: 0.030000\n",
      "Train set: Average loss: 0.2534\n",
      "Test set: Average loss: 0.2514, Average MAE: 0.3546\n",
      "Train Epoch: 1398 [4096/194182 (2%)]\tLoss: 0.253164\tGrad Norm: 1.748438\tLR: 0.030000\n",
      "Train Epoch: 1398 [24576/194182 (12%)]\tLoss: 0.262838\tGrad Norm: 1.868481\tLR: 0.030000\n",
      "Train Epoch: 1398 [45056/194182 (23%)]\tLoss: 0.257946\tGrad Norm: 1.728116\tLR: 0.030000\n",
      "Train Epoch: 1398 [65536/194182 (33%)]\tLoss: 0.248921\tGrad Norm: 1.385607\tLR: 0.030000\n",
      "Train Epoch: 1398 [86016/194182 (44%)]\tLoss: 0.254277\tGrad Norm: 1.687857\tLR: 0.030000\n",
      "Train Epoch: 1398 [106496/194182 (54%)]\tLoss: 0.255079\tGrad Norm: 1.457996\tLR: 0.030000\n",
      "Train Epoch: 1398 [126976/194182 (65%)]\tLoss: 0.242287\tGrad Norm: 1.204269\tLR: 0.030000\n",
      "Train Epoch: 1398 [147456/194182 (75%)]\tLoss: 0.250541\tGrad Norm: 1.326631\tLR: 0.030000\n",
      "Train Epoch: 1398 [167936/194182 (85%)]\tLoss: 0.255677\tGrad Norm: 1.389845\tLR: 0.030000\n",
      "Train Epoch: 1398 [188416/194182 (96%)]\tLoss: 0.266893\tGrad Norm: 1.931241\tLR: 0.030000\n",
      "Train set: Average loss: 0.2526\n",
      "Test set: Average loss: 0.2505, Average MAE: 0.3481\n",
      "Train Epoch: 1399 [4096/194182 (2%)]\tLoss: 0.254929\tGrad Norm: 1.467270\tLR: 0.030000\n",
      "Train Epoch: 1399 [24576/194182 (12%)]\tLoss: 0.249058\tGrad Norm: 1.408729\tLR: 0.030000\n",
      "Train Epoch: 1399 [45056/194182 (23%)]\tLoss: 0.253276\tGrad Norm: 1.429770\tLR: 0.030000\n",
      "Train Epoch: 1399 [65536/194182 (33%)]\tLoss: 0.246852\tGrad Norm: 1.072080\tLR: 0.030000\n",
      "Train Epoch: 1399 [86016/194182 (44%)]\tLoss: 0.247230\tGrad Norm: 1.632027\tLR: 0.030000\n",
      "Train Epoch: 1399 [106496/194182 (54%)]\tLoss: 0.262794\tGrad Norm: 1.861949\tLR: 0.030000\n",
      "Train Epoch: 1399 [126976/194182 (65%)]\tLoss: 0.251735\tGrad Norm: 1.557676\tLR: 0.030000\n",
      "Train Epoch: 1399 [147456/194182 (75%)]\tLoss: 0.251147\tGrad Norm: 1.691615\tLR: 0.030000\n",
      "Train Epoch: 1399 [167936/194182 (85%)]\tLoss: 0.251273\tGrad Norm: 1.671075\tLR: 0.030000\n",
      "Train Epoch: 1399 [188416/194182 (96%)]\tLoss: 0.258474\tGrad Norm: 1.907357\tLR: 0.030000\n",
      "Train set: Average loss: 0.2521\n",
      "Test set: Average loss: 0.2475, Average MAE: 0.3365\n",
      "Train Epoch: 1400 [4096/194182 (2%)]\tLoss: 0.249671\tGrad Norm: 1.577444\tLR: 0.030000\n",
      "Train Epoch: 1400 [24576/194182 (12%)]\tLoss: 0.256259\tGrad Norm: 1.735303\tLR: 0.030000\n",
      "Train Epoch: 1400 [45056/194182 (23%)]\tLoss: 0.256053\tGrad Norm: 1.580140\tLR: 0.030000\n",
      "Train Epoch: 1400 [65536/194182 (33%)]\tLoss: 0.249248\tGrad Norm: 1.597806\tLR: 0.030000\n",
      "Train Epoch: 1400 [86016/194182 (44%)]\tLoss: 0.250704\tGrad Norm: 1.473181\tLR: 0.030000\n",
      "Train Epoch: 1400 [106496/194182 (54%)]\tLoss: 0.250406\tGrad Norm: 1.655180\tLR: 0.030000\n",
      "Train Epoch: 1400 [126976/194182 (65%)]\tLoss: 0.256876\tGrad Norm: 1.615647\tLR: 0.030000\n",
      "Train Epoch: 1400 [147456/194182 (75%)]\tLoss: 0.249662\tGrad Norm: 1.650085\tLR: 0.030000\n",
      "Train Epoch: 1400 [167936/194182 (85%)]\tLoss: 0.254469\tGrad Norm: 1.554251\tLR: 0.030000\n",
      "Train Epoch: 1400 [188416/194182 (96%)]\tLoss: 0.251838\tGrad Norm: 1.339276\tLR: 0.030000\n",
      "Train set: Average loss: 0.2514\n",
      "Test set: Average loss: 0.2515, Average MAE: 0.3570\n",
      "Epoch 1400: Mean reward = 0.052 +/- 0.017\n",
      "Train Epoch: 1401 [4096/194182 (2%)]\tLoss: 0.250614\tGrad Norm: 1.794602\tLR: 0.030000\n",
      "Train Epoch: 1401 [24576/194182 (12%)]\tLoss: 0.242375\tGrad Norm: 1.308571\tLR: 0.030000\n",
      "Train Epoch: 1401 [45056/194182 (23%)]\tLoss: 0.242696\tGrad Norm: 1.303770\tLR: 0.030000\n",
      "Train Epoch: 1401 [65536/194182 (33%)]\tLoss: 0.254694\tGrad Norm: 1.738251\tLR: 0.030000\n",
      "Train Epoch: 1401 [86016/194182 (44%)]\tLoss: 0.258740\tGrad Norm: 1.449659\tLR: 0.030000\n",
      "Train Epoch: 1401 [106496/194182 (54%)]\tLoss: 0.249348\tGrad Norm: 1.309609\tLR: 0.030000\n",
      "Train Epoch: 1401 [126976/194182 (65%)]\tLoss: 0.252965\tGrad Norm: 1.517554\tLR: 0.030000\n",
      "Train Epoch: 1401 [147456/194182 (75%)]\tLoss: 0.256505\tGrad Norm: 1.768903\tLR: 0.030000\n",
      "Train Epoch: 1401 [167936/194182 (85%)]\tLoss: 0.257984\tGrad Norm: 1.642323\tLR: 0.030000\n",
      "Train Epoch: 1401 [188416/194182 (96%)]\tLoss: 0.253759\tGrad Norm: 1.781854\tLR: 0.030000\n",
      "Train set: Average loss: 0.2531\n",
      "Test set: Average loss: 0.2476, Average MAE: 0.3370\n",
      "Train Epoch: 1402 [4096/194182 (2%)]\tLoss: 0.253235\tGrad Norm: 1.551114\tLR: 0.030000\n",
      "Train Epoch: 1402 [24576/194182 (12%)]\tLoss: 0.246450\tGrad Norm: 1.384720\tLR: 0.030000\n",
      "Train Epoch: 1402 [45056/194182 (23%)]\tLoss: 0.257046\tGrad Norm: 1.981366\tLR: 0.030000\n",
      "Train Epoch: 1402 [65536/194182 (33%)]\tLoss: 0.253661\tGrad Norm: 1.907333\tLR: 0.030000\n",
      "Train Epoch: 1402 [86016/194182 (44%)]\tLoss: 0.257290\tGrad Norm: 1.921757\tLR: 0.030000\n",
      "Train Epoch: 1402 [106496/194182 (54%)]\tLoss: 0.251529\tGrad Norm: 1.659333\tLR: 0.030000\n",
      "Train Epoch: 1402 [126976/194182 (65%)]\tLoss: 0.260127\tGrad Norm: 2.109223\tLR: 0.030000\n",
      "Train Epoch: 1402 [147456/194182 (75%)]\tLoss: 0.249736\tGrad Norm: 1.486356\tLR: 0.030000\n",
      "Train Epoch: 1402 [167936/194182 (85%)]\tLoss: 0.264696\tGrad Norm: 1.992363\tLR: 0.030000\n",
      "Train Epoch: 1402 [188416/194182 (96%)]\tLoss: 0.256452\tGrad Norm: 2.018135\tLR: 0.030000\n",
      "Train set: Average loss: 0.2545\n",
      "Test set: Average loss: 0.2510, Average MAE: 0.3404\n",
      "Train Epoch: 1403 [4096/194182 (2%)]\tLoss: 0.259658\tGrad Norm: 1.807708\tLR: 0.030000\n",
      "Train Epoch: 1403 [24576/194182 (12%)]\tLoss: 0.247387\tGrad Norm: 1.144852\tLR: 0.030000\n",
      "Train Epoch: 1403 [45056/194182 (23%)]\tLoss: 0.245252\tGrad Norm: 1.466571\tLR: 0.030000\n",
      "Train Epoch: 1403 [65536/194182 (33%)]\tLoss: 0.248710\tGrad Norm: 1.253490\tLR: 0.030000\n",
      "Train Epoch: 1403 [86016/194182 (44%)]\tLoss: 0.250404\tGrad Norm: 1.263314\tLR: 0.030000\n",
      "Train Epoch: 1403 [106496/194182 (54%)]\tLoss: 0.240434\tGrad Norm: 1.200285\tLR: 0.030000\n",
      "Train Epoch: 1403 [126976/194182 (65%)]\tLoss: 0.249161\tGrad Norm: 1.455811\tLR: 0.030000\n",
      "Train Epoch: 1403 [147456/194182 (75%)]\tLoss: 0.250389\tGrad Norm: 1.392010\tLR: 0.030000\n",
      "Train Epoch: 1403 [167936/194182 (85%)]\tLoss: 0.252495\tGrad Norm: 1.512754\tLR: 0.030000\n",
      "Train Epoch: 1403 [188416/194182 (96%)]\tLoss: 0.254298\tGrad Norm: 1.822770\tLR: 0.030000\n",
      "Train set: Average loss: 0.2495\n",
      "Test set: Average loss: 0.2582, Average MAE: 0.3598\n",
      "Train Epoch: 1404 [4096/194182 (2%)]\tLoss: 0.253817\tGrad Norm: 1.879529\tLR: 0.030000\n",
      "Train Epoch: 1404 [24576/194182 (12%)]\tLoss: 0.256860\tGrad Norm: 1.650412\tLR: 0.030000\n",
      "Train Epoch: 1404 [45056/194182 (23%)]\tLoss: 0.252082\tGrad Norm: 1.787148\tLR: 0.030000\n",
      "Train Epoch: 1404 [65536/194182 (33%)]\tLoss: 0.259756\tGrad Norm: 1.897015\tLR: 0.030000\n",
      "Train Epoch: 1404 [86016/194182 (44%)]\tLoss: 0.255943\tGrad Norm: 1.720927\tLR: 0.030000\n",
      "Train Epoch: 1404 [106496/194182 (54%)]\tLoss: 0.244782\tGrad Norm: 1.338672\tLR: 0.030000\n",
      "Train Epoch: 1404 [126976/194182 (65%)]\tLoss: 0.251187\tGrad Norm: 1.722367\tLR: 0.030000\n",
      "Train Epoch: 1404 [147456/194182 (75%)]\tLoss: 0.244176\tGrad Norm: 1.539371\tLR: 0.030000\n",
      "Train Epoch: 1404 [167936/194182 (85%)]\tLoss: 0.259274\tGrad Norm: 1.787533\tLR: 0.030000\n",
      "Train Epoch: 1404 [188416/194182 (96%)]\tLoss: 0.255004\tGrad Norm: 1.740076\tLR: 0.030000\n",
      "Train set: Average loss: 0.2537\n",
      "Test set: Average loss: 0.2466, Average MAE: 0.3434\n",
      "Train Epoch: 1405 [4096/194182 (2%)]\tLoss: 0.246048\tGrad Norm: 1.281249\tLR: 0.030000\n",
      "Train Epoch: 1405 [24576/194182 (12%)]\tLoss: 0.252034\tGrad Norm: 1.385299\tLR: 0.030000\n",
      "Train Epoch: 1405 [45056/194182 (23%)]\tLoss: 0.246734\tGrad Norm: 1.458072\tLR: 0.030000\n",
      "Train Epoch: 1405 [65536/194182 (33%)]\tLoss: 0.248749\tGrad Norm: 1.869781\tLR: 0.030000\n",
      "Train Epoch: 1405 [86016/194182 (44%)]\tLoss: 0.246209\tGrad Norm: 1.836537\tLR: 0.030000\n",
      "Train Epoch: 1405 [106496/194182 (54%)]\tLoss: 0.247826\tGrad Norm: 1.668582\tLR: 0.030000\n",
      "Train Epoch: 1405 [126976/194182 (65%)]\tLoss: 0.247491\tGrad Norm: 1.584705\tLR: 0.030000\n",
      "Train Epoch: 1405 [147456/194182 (75%)]\tLoss: 0.257838\tGrad Norm: 1.753777\tLR: 0.030000\n",
      "Train Epoch: 1405 [167936/194182 (85%)]\tLoss: 0.251374\tGrad Norm: 1.542738\tLR: 0.030000\n",
      "Train Epoch: 1405 [188416/194182 (96%)]\tLoss: 0.255096\tGrad Norm: 1.669382\tLR: 0.030000\n",
      "Train set: Average loss: 0.2524\n",
      "Test set: Average loss: 0.2475, Average MAE: 0.3428\n",
      "Epoch 1405: Mean reward = 0.091 +/- 0.072\n",
      "Train Epoch: 1406 [4096/194182 (2%)]\tLoss: 0.249346\tGrad Norm: 1.360341\tLR: 0.030000\n",
      "Train Epoch: 1406 [24576/194182 (12%)]\tLoss: 0.256563\tGrad Norm: 1.642642\tLR: 0.030000\n",
      "Train Epoch: 1406 [45056/194182 (23%)]\tLoss: 0.249516\tGrad Norm: 1.404870\tLR: 0.030000\n",
      "Train Epoch: 1406 [65536/194182 (33%)]\tLoss: 0.249278\tGrad Norm: 1.503857\tLR: 0.030000\n",
      "Train Epoch: 1406 [86016/194182 (44%)]\tLoss: 0.259417\tGrad Norm: 1.759877\tLR: 0.030000\n",
      "Train Epoch: 1406 [106496/194182 (54%)]\tLoss: 0.247024\tGrad Norm: 1.382201\tLR: 0.030000\n",
      "Train Epoch: 1406 [126976/194182 (65%)]\tLoss: 0.255387\tGrad Norm: 1.355512\tLR: 0.030000\n",
      "Train Epoch: 1406 [147456/194182 (75%)]\tLoss: 0.257677\tGrad Norm: 1.594401\tLR: 0.030000\n",
      "Train Epoch: 1406 [167936/194182 (85%)]\tLoss: 0.242193\tGrad Norm: 1.234557\tLR: 0.030000\n",
      "Train Epoch: 1406 [188416/194182 (96%)]\tLoss: 0.254942\tGrad Norm: 1.686983\tLR: 0.030000\n",
      "Train set: Average loss: 0.2508\n",
      "Test set: Average loss: 0.2471, Average MAE: 0.3384\n",
      "Train Epoch: 1407 [4096/194182 (2%)]\tLoss: 0.250764\tGrad Norm: 1.467167\tLR: 0.030000\n",
      "Train Epoch: 1407 [24576/194182 (12%)]\tLoss: 0.253592\tGrad Norm: 1.832932\tLR: 0.030000\n",
      "Train Epoch: 1407 [45056/194182 (23%)]\tLoss: 0.250872\tGrad Norm: 1.676639\tLR: 0.030000\n",
      "Train Epoch: 1407 [65536/194182 (33%)]\tLoss: 0.251018\tGrad Norm: 1.780140\tLR: 0.030000\n",
      "Train Epoch: 1407 [86016/194182 (44%)]\tLoss: 0.250157\tGrad Norm: 1.620392\tLR: 0.030000\n",
      "Train Epoch: 1407 [106496/194182 (54%)]\tLoss: 0.266497\tGrad Norm: 2.197774\tLR: 0.030000\n",
      "Train Epoch: 1407 [126976/194182 (65%)]\tLoss: 0.250721\tGrad Norm: 1.362445\tLR: 0.030000\n",
      "Train Epoch: 1407 [147456/194182 (75%)]\tLoss: 0.256815\tGrad Norm: 1.695657\tLR: 0.030000\n",
      "Train Epoch: 1407 [167936/194182 (85%)]\tLoss: 0.246436\tGrad Norm: 1.231214\tLR: 0.030000\n",
      "Train Epoch: 1407 [188416/194182 (96%)]\tLoss: 0.246250\tGrad Norm: 1.372980\tLR: 0.030000\n",
      "Train set: Average loss: 0.2519\n",
      "Test set: Average loss: 0.2486, Average MAE: 0.3541\n",
      "Train Epoch: 1408 [4096/194182 (2%)]\tLoss: 0.241311\tGrad Norm: 1.473723\tLR: 0.030000\n",
      "Train Epoch: 1408 [24576/194182 (12%)]\tLoss: 0.249433\tGrad Norm: 1.612808\tLR: 0.030000\n",
      "Train Epoch: 1408 [45056/194182 (23%)]\tLoss: 0.254608\tGrad Norm: 1.522459\tLR: 0.030000\n",
      "Train Epoch: 1408 [65536/194182 (33%)]\tLoss: 0.252901\tGrad Norm: 1.815841\tLR: 0.030000\n",
      "Train Epoch: 1408 [86016/194182 (44%)]\tLoss: 0.255748\tGrad Norm: 2.077607\tLR: 0.030000\n",
      "Train Epoch: 1408 [106496/194182 (54%)]\tLoss: 0.258557\tGrad Norm: 1.840470\tLR: 0.030000\n",
      "Train Epoch: 1408 [126976/194182 (65%)]\tLoss: 0.244900\tGrad Norm: 1.338585\tLR: 0.030000\n",
      "Train Epoch: 1408 [147456/194182 (75%)]\tLoss: 0.259033\tGrad Norm: 1.690871\tLR: 0.030000\n",
      "Train Epoch: 1408 [167936/194182 (85%)]\tLoss: 0.255508\tGrad Norm: 1.876007\tLR: 0.030000\n",
      "Train Epoch: 1408 [188416/194182 (96%)]\tLoss: 0.256090\tGrad Norm: 1.536539\tLR: 0.030000\n",
      "Train set: Average loss: 0.2524\n",
      "Test set: Average loss: 0.2488, Average MAE: 0.3461\n",
      "Train Epoch: 1409 [4096/194182 (2%)]\tLoss: 0.251199\tGrad Norm: 1.493866\tLR: 0.030000\n",
      "Train Epoch: 1409 [24576/194182 (12%)]\tLoss: 0.250334\tGrad Norm: 1.595412\tLR: 0.030000\n",
      "Train Epoch: 1409 [45056/194182 (23%)]\tLoss: 0.250012\tGrad Norm: 1.095457\tLR: 0.030000\n",
      "Train Epoch: 1409 [65536/194182 (33%)]\tLoss: 0.248879\tGrad Norm: 1.423623\tLR: 0.030000\n",
      "Train Epoch: 1409 [86016/194182 (44%)]\tLoss: 0.245570\tGrad Norm: 1.593418\tLR: 0.030000\n",
      "Train Epoch: 1409 [106496/194182 (54%)]\tLoss: 0.259340\tGrad Norm: 2.273179\tLR: 0.030000\n",
      "Train Epoch: 1409 [126976/194182 (65%)]\tLoss: 0.259450\tGrad Norm: 2.089693\tLR: 0.030000\n",
      "Train Epoch: 1409 [147456/194182 (75%)]\tLoss: 0.255398\tGrad Norm: 1.746707\tLR: 0.030000\n",
      "Train Epoch: 1409 [167936/194182 (85%)]\tLoss: 0.247051\tGrad Norm: 1.402979\tLR: 0.030000\n",
      "Train Epoch: 1409 [188416/194182 (96%)]\tLoss: 0.245561\tGrad Norm: 1.413776\tLR: 0.030000\n",
      "Train set: Average loss: 0.2513\n",
      "Test set: Average loss: 0.2611, Average MAE: 0.3659\n",
      "Train Epoch: 1410 [4096/194182 (2%)]\tLoss: 0.254919\tGrad Norm: 2.157123\tLR: 0.030000\n",
      "Train Epoch: 1410 [24576/194182 (12%)]\tLoss: 0.255115\tGrad Norm: 2.039323\tLR: 0.030000\n",
      "Train Epoch: 1410 [45056/194182 (23%)]\tLoss: 0.247125\tGrad Norm: 1.811802\tLR: 0.030000\n",
      "Train Epoch: 1410 [65536/194182 (33%)]\tLoss: 0.249361\tGrad Norm: 1.459757\tLR: 0.030000\n",
      "Train Epoch: 1410 [86016/194182 (44%)]\tLoss: 0.244395\tGrad Norm: 1.598123\tLR: 0.030000\n",
      "Train Epoch: 1410 [106496/194182 (54%)]\tLoss: 0.252601\tGrad Norm: 1.462269\tLR: 0.030000\n",
      "Train Epoch: 1410 [126976/194182 (65%)]\tLoss: 0.255619\tGrad Norm: 1.487896\tLR: 0.030000\n",
      "Train Epoch: 1410 [147456/194182 (75%)]\tLoss: 0.253659\tGrad Norm: 1.501232\tLR: 0.030000\n",
      "Train Epoch: 1410 [167936/194182 (85%)]\tLoss: 0.246518\tGrad Norm: 1.540410\tLR: 0.030000\n",
      "Train Epoch: 1410 [188416/194182 (96%)]\tLoss: 0.252554\tGrad Norm: 1.603770\tLR: 0.030000\n",
      "Train set: Average loss: 0.2523\n",
      "Test set: Average loss: 0.2467, Average MAE: 0.3447\n",
      "Epoch 1410: Mean reward = 0.057 +/- 0.033\n",
      "Train Epoch: 1411 [4096/194182 (2%)]\tLoss: 0.250422\tGrad Norm: 1.364260\tLR: 0.030000\n",
      "Train Epoch: 1411 [24576/194182 (12%)]\tLoss: 0.246003\tGrad Norm: 1.206087\tLR: 0.030000\n",
      "Train Epoch: 1411 [45056/194182 (23%)]\tLoss: 0.249306\tGrad Norm: 1.457413\tLR: 0.030000\n",
      "Train Epoch: 1411 [65536/194182 (33%)]\tLoss: 0.245757\tGrad Norm: 1.542843\tLR: 0.030000\n",
      "Train Epoch: 1411 [86016/194182 (44%)]\tLoss: 0.256593\tGrad Norm: 1.709961\tLR: 0.030000\n",
      "Train Epoch: 1411 [106496/194182 (54%)]\tLoss: 0.249041\tGrad Norm: 1.393084\tLR: 0.030000\n",
      "Train Epoch: 1411 [126976/194182 (65%)]\tLoss: 0.251044\tGrad Norm: 1.730970\tLR: 0.030000\n",
      "Train Epoch: 1411 [147456/194182 (75%)]\tLoss: 0.246916\tGrad Norm: 1.573691\tLR: 0.030000\n",
      "Train Epoch: 1411 [167936/194182 (85%)]\tLoss: 0.253730\tGrad Norm: 1.967555\tLR: 0.030000\n",
      "Train Epoch: 1411 [188416/194182 (96%)]\tLoss: 0.259245\tGrad Norm: 2.220141\tLR: 0.030000\n",
      "Train set: Average loss: 0.2509\n",
      "Test set: Average loss: 0.2659, Average MAE: 0.3690\n",
      "Train Epoch: 1412 [4096/194182 (2%)]\tLoss: 0.266115\tGrad Norm: 2.279132\tLR: 0.030000\n",
      "Train Epoch: 1412 [24576/194182 (12%)]\tLoss: 0.246651\tGrad Norm: 1.484247\tLR: 0.030000\n",
      "Train Epoch: 1412 [45056/194182 (23%)]\tLoss: 0.245702\tGrad Norm: 1.322204\tLR: 0.030000\n",
      "Train Epoch: 1412 [65536/194182 (33%)]\tLoss: 0.242339\tGrad Norm: 1.363221\tLR: 0.030000\n",
      "Train Epoch: 1412 [86016/194182 (44%)]\tLoss: 0.249236\tGrad Norm: 1.236902\tLR: 0.030000\n",
      "Train Epoch: 1412 [106496/194182 (54%)]\tLoss: 0.254893\tGrad Norm: 1.536473\tLR: 0.030000\n",
      "Train Epoch: 1412 [126976/194182 (65%)]\tLoss: 0.246108\tGrad Norm: 1.380348\tLR: 0.030000\n",
      "Train Epoch: 1412 [147456/194182 (75%)]\tLoss: 0.247472\tGrad Norm: 1.442945\tLR: 0.030000\n",
      "Train Epoch: 1412 [167936/194182 (85%)]\tLoss: 0.246575\tGrad Norm: 1.536667\tLR: 0.030000\n",
      "Train Epoch: 1412 [188416/194182 (96%)]\tLoss: 0.257817\tGrad Norm: 2.015129\tLR: 0.030000\n",
      "Train set: Average loss: 0.2508\n",
      "Test set: Average loss: 0.2595, Average MAE: 0.3506\n",
      "Train Epoch: 1413 [4096/194182 (2%)]\tLoss: 0.255124\tGrad Norm: 2.007913\tLR: 0.030000\n",
      "Train Epoch: 1413 [24576/194182 (12%)]\tLoss: 0.261045\tGrad Norm: 2.021965\tLR: 0.030000\n",
      "Train Epoch: 1413 [45056/194182 (23%)]\tLoss: 0.246621\tGrad Norm: 1.754652\tLR: 0.030000\n",
      "Train Epoch: 1413 [65536/194182 (33%)]\tLoss: 0.254673\tGrad Norm: 1.781804\tLR: 0.030000\n",
      "Train Epoch: 1413 [86016/194182 (44%)]\tLoss: 0.255114\tGrad Norm: 1.782106\tLR: 0.030000\n",
      "Train Epoch: 1413 [106496/194182 (54%)]\tLoss: 0.244379\tGrad Norm: 1.357221\tLR: 0.030000\n",
      "Train Epoch: 1413 [126976/194182 (65%)]\tLoss: 0.249231\tGrad Norm: 1.577720\tLR: 0.030000\n",
      "Train Epoch: 1413 [147456/194182 (75%)]\tLoss: 0.253875\tGrad Norm: 1.626154\tLR: 0.030000\n",
      "Train Epoch: 1413 [167936/194182 (85%)]\tLoss: 0.248511\tGrad Norm: 1.680305\tLR: 0.030000\n",
      "Train Epoch: 1413 [188416/194182 (96%)]\tLoss: 0.247744\tGrad Norm: 1.440792\tLR: 0.030000\n",
      "Train set: Average loss: 0.2507\n",
      "Test set: Average loss: 0.2464, Average MAE: 0.3385\n",
      "Train Epoch: 1414 [4096/194182 (2%)]\tLoss: 0.245576\tGrad Norm: 1.389717\tLR: 0.030000\n",
      "Train Epoch: 1414 [24576/194182 (12%)]\tLoss: 0.248811\tGrad Norm: 1.573221\tLR: 0.030000\n",
      "Train Epoch: 1414 [45056/194182 (23%)]\tLoss: 0.248228\tGrad Norm: 1.362566\tLR: 0.030000\n",
      "Train Epoch: 1414 [65536/194182 (33%)]\tLoss: 0.247835\tGrad Norm: 1.369323\tLR: 0.030000\n",
      "Train Epoch: 1414 [86016/194182 (44%)]\tLoss: 0.252453\tGrad Norm: 1.477837\tLR: 0.030000\n",
      "Train Epoch: 1414 [106496/194182 (54%)]\tLoss: 0.255411\tGrad Norm: 1.379201\tLR: 0.030000\n",
      "Train Epoch: 1414 [126976/194182 (65%)]\tLoss: 0.249179\tGrad Norm: 1.599535\tLR: 0.030000\n",
      "Train Epoch: 1414 [147456/194182 (75%)]\tLoss: 0.255973\tGrad Norm: 1.559886\tLR: 0.030000\n",
      "Train Epoch: 1414 [167936/194182 (85%)]\tLoss: 0.248657\tGrad Norm: 1.570449\tLR: 0.030000\n",
      "Train Epoch: 1414 [188416/194182 (96%)]\tLoss: 0.256470\tGrad Norm: 1.668657\tLR: 0.030000\n",
      "Train set: Average loss: 0.2494\n",
      "Test set: Average loss: 0.2531, Average MAE: 0.3435\n",
      "Train Epoch: 1415 [4096/194182 (2%)]\tLoss: 0.256809\tGrad Norm: 1.908975\tLR: 0.030000\n",
      "Train Epoch: 1415 [24576/194182 (12%)]\tLoss: 0.251303\tGrad Norm: 1.738530\tLR: 0.030000\n",
      "Train Epoch: 1415 [45056/194182 (23%)]\tLoss: 0.254617\tGrad Norm: 1.526778\tLR: 0.030000\n",
      "Train Epoch: 1415 [65536/194182 (33%)]\tLoss: 0.253859\tGrad Norm: 1.527672\tLR: 0.030000\n",
      "Train Epoch: 1415 [86016/194182 (44%)]\tLoss: 0.252596\tGrad Norm: 1.480183\tLR: 0.030000\n",
      "Train Epoch: 1415 [106496/194182 (54%)]\tLoss: 0.252319\tGrad Norm: 1.565173\tLR: 0.030000\n",
      "Train Epoch: 1415 [126976/194182 (65%)]\tLoss: 0.260967\tGrad Norm: 1.921953\tLR: 0.030000\n",
      "Train Epoch: 1415 [147456/194182 (75%)]\tLoss: 0.261557\tGrad Norm: 2.178062\tLR: 0.030000\n",
      "Train Epoch: 1415 [167936/194182 (85%)]\tLoss: 0.243052\tGrad Norm: 1.524560\tLR: 0.030000\n",
      "Train Epoch: 1415 [188416/194182 (96%)]\tLoss: 0.264906\tGrad Norm: 2.042091\tLR: 0.030000\n",
      "Train set: Average loss: 0.2530\n",
      "Test set: Average loss: 0.2525, Average MAE: 0.3388\n",
      "Epoch 1415: Mean reward = 0.044 +/- 0.045\n",
      "Train Epoch: 1416 [4096/194182 (2%)]\tLoss: 0.252553\tGrad Norm: 1.819462\tLR: 0.030000\n",
      "Train Epoch: 1416 [24576/194182 (12%)]\tLoss: 0.253047\tGrad Norm: 1.816594\tLR: 0.030000\n",
      "Train Epoch: 1416 [45056/194182 (23%)]\tLoss: 0.254181\tGrad Norm: 1.840932\tLR: 0.030000\n",
      "Train Epoch: 1416 [65536/194182 (33%)]\tLoss: 0.252788\tGrad Norm: 1.353442\tLR: 0.030000\n",
      "Train Epoch: 1416 [86016/194182 (44%)]\tLoss: 0.247611\tGrad Norm: 1.435773\tLR: 0.030000\n",
      "Train Epoch: 1416 [106496/194182 (54%)]\tLoss: 0.256398\tGrad Norm: 1.730774\tLR: 0.030000\n",
      "Train Epoch: 1416 [126976/194182 (65%)]\tLoss: 0.250402\tGrad Norm: 1.551300\tLR: 0.030000\n",
      "Train Epoch: 1416 [147456/194182 (75%)]\tLoss: 0.245717\tGrad Norm: 1.680688\tLR: 0.030000\n",
      "Train Epoch: 1416 [167936/194182 (85%)]\tLoss: 0.248149\tGrad Norm: 1.467440\tLR: 0.030000\n",
      "Train Epoch: 1416 [188416/194182 (96%)]\tLoss: 0.236314\tGrad Norm: 1.062922\tLR: 0.030000\n",
      "Train set: Average loss: 0.2505\n",
      "Test set: Average loss: 0.2471, Average MAE: 0.3395\n",
      "Train Epoch: 1417 [4096/194182 (2%)]\tLoss: 0.251788\tGrad Norm: 1.463186\tLR: 0.030000\n",
      "Train Epoch: 1417 [24576/194182 (12%)]\tLoss: 0.246013\tGrad Norm: 1.346496\tLR: 0.030000\n",
      "Train Epoch: 1417 [45056/194182 (23%)]\tLoss: 0.247405\tGrad Norm: 1.339360\tLR: 0.030000\n",
      "Train Epoch: 1417 [65536/194182 (33%)]\tLoss: 0.255861\tGrad Norm: 1.805288\tLR: 0.030000\n",
      "Train Epoch: 1417 [86016/194182 (44%)]\tLoss: 0.253558\tGrad Norm: 1.704067\tLR: 0.030000\n",
      "Train Epoch: 1417 [106496/194182 (54%)]\tLoss: 0.241314\tGrad Norm: 1.281500\tLR: 0.030000\n",
      "Train Epoch: 1417 [126976/194182 (65%)]\tLoss: 0.255264\tGrad Norm: 1.831398\tLR: 0.030000\n",
      "Train Epoch: 1417 [147456/194182 (75%)]\tLoss: 0.251099\tGrad Norm: 1.861910\tLR: 0.030000\n",
      "Train Epoch: 1417 [167936/194182 (85%)]\tLoss: 0.252054\tGrad Norm: 1.812128\tLR: 0.030000\n",
      "Train Epoch: 1417 [188416/194182 (96%)]\tLoss: 0.249685\tGrad Norm: 1.597278\tLR: 0.030000\n",
      "Train set: Average loss: 0.2507\n",
      "Test set: Average loss: 0.2560, Average MAE: 0.3608\n",
      "Train Epoch: 1418 [4096/194182 (2%)]\tLoss: 0.254959\tGrad Norm: 1.900678\tLR: 0.030000\n",
      "Train Epoch: 1418 [24576/194182 (12%)]\tLoss: 0.245109\tGrad Norm: 1.681838\tLR: 0.030000\n",
      "Train Epoch: 1418 [45056/194182 (23%)]\tLoss: 0.256437\tGrad Norm: 1.845534\tLR: 0.030000\n",
      "Train Epoch: 1418 [65536/194182 (33%)]\tLoss: 0.250180\tGrad Norm: 1.607755\tLR: 0.030000\n",
      "Train Epoch: 1418 [86016/194182 (44%)]\tLoss: 0.246529\tGrad Norm: 1.562687\tLR: 0.030000\n",
      "Train Epoch: 1418 [106496/194182 (54%)]\tLoss: 0.247848\tGrad Norm: 1.215306\tLR: 0.030000\n",
      "Train Epoch: 1418 [126976/194182 (65%)]\tLoss: 0.252264\tGrad Norm: 1.442193\tLR: 0.030000\n",
      "Train Epoch: 1418 [147456/194182 (75%)]\tLoss: 0.252781\tGrad Norm: 1.724122\tLR: 0.030000\n",
      "Train Epoch: 1418 [167936/194182 (85%)]\tLoss: 0.251433\tGrad Norm: 1.830493\tLR: 0.030000\n",
      "Train Epoch: 1418 [188416/194182 (96%)]\tLoss: 0.252156\tGrad Norm: 1.463177\tLR: 0.030000\n",
      "Train set: Average loss: 0.2506\n",
      "Test set: Average loss: 0.2524, Average MAE: 0.3407\n",
      "Train Epoch: 1419 [4096/194182 (2%)]\tLoss: 0.257731\tGrad Norm: 1.967700\tLR: 0.030000\n",
      "Train Epoch: 1419 [24576/194182 (12%)]\tLoss: 0.244176\tGrad Norm: 1.461971\tLR: 0.030000\n",
      "Train Epoch: 1419 [45056/194182 (23%)]\tLoss: 0.243551\tGrad Norm: 1.443061\tLR: 0.030000\n",
      "Train Epoch: 1419 [65536/194182 (33%)]\tLoss: 0.243625\tGrad Norm: 1.277386\tLR: 0.030000\n",
      "Train Epoch: 1419 [86016/194182 (44%)]\tLoss: 0.248734\tGrad Norm: 1.246987\tLR: 0.030000\n",
      "Train Epoch: 1419 [106496/194182 (54%)]\tLoss: 0.243059\tGrad Norm: 1.424605\tLR: 0.030000\n",
      "Train Epoch: 1419 [126976/194182 (65%)]\tLoss: 0.258179\tGrad Norm: 1.778090\tLR: 0.030000\n",
      "Train Epoch: 1419 [147456/194182 (75%)]\tLoss: 0.250797\tGrad Norm: 1.638976\tLR: 0.030000\n",
      "Train Epoch: 1419 [167936/194182 (85%)]\tLoss: 0.255874\tGrad Norm: 1.909867\tLR: 0.030000\n",
      "Train Epoch: 1419 [188416/194182 (96%)]\tLoss: 0.254938\tGrad Norm: 1.832611\tLR: 0.030000\n",
      "Train set: Average loss: 0.2490\n",
      "Test set: Average loss: 0.2537, Average MAE: 0.3537\n",
      "Train Epoch: 1420 [4096/194182 (2%)]\tLoss: 0.247667\tGrad Norm: 1.563550\tLR: 0.030000\n",
      "Train Epoch: 1420 [24576/194182 (12%)]\tLoss: 0.242548\tGrad Norm: 1.241323\tLR: 0.030000\n",
      "Train Epoch: 1420 [45056/194182 (23%)]\tLoss: 0.245104\tGrad Norm: 1.545371\tLR: 0.030000\n",
      "Train Epoch: 1420 [65536/194182 (33%)]\tLoss: 0.258913\tGrad Norm: 1.970600\tLR: 0.030000\n",
      "Train Epoch: 1420 [86016/194182 (44%)]\tLoss: 0.251611\tGrad Norm: 1.501821\tLR: 0.030000\n",
      "Train Epoch: 1420 [106496/194182 (54%)]\tLoss: 0.256575\tGrad Norm: 1.581535\tLR: 0.030000\n",
      "Train Epoch: 1420 [126976/194182 (65%)]\tLoss: 0.255312\tGrad Norm: 1.662141\tLR: 0.030000\n",
      "Train Epoch: 1420 [147456/194182 (75%)]\tLoss: 0.251384\tGrad Norm: 1.732430\tLR: 0.030000\n",
      "Train Epoch: 1420 [167936/194182 (85%)]\tLoss: 0.253559\tGrad Norm: 1.446498\tLR: 0.030000\n",
      "Train Epoch: 1420 [188416/194182 (96%)]\tLoss: 0.247720\tGrad Norm: 1.503873\tLR: 0.030000\n",
      "Train set: Average loss: 0.2491\n",
      "Test set: Average loss: 0.2524, Average MAE: 0.3492\n",
      "Epoch 1420: Mean reward = 0.065 +/- 0.070\n",
      "Train Epoch: 1421 [4096/194182 (2%)]\tLoss: 0.248774\tGrad Norm: 1.632151\tLR: 0.030000\n",
      "Train Epoch: 1421 [24576/194182 (12%)]\tLoss: 0.259220\tGrad Norm: 1.852850\tLR: 0.030000\n",
      "Train Epoch: 1421 [45056/194182 (23%)]\tLoss: 0.240330\tGrad Norm: 1.175905\tLR: 0.030000\n",
      "Train Epoch: 1421 [65536/194182 (33%)]\tLoss: 0.251470\tGrad Norm: 1.557697\tLR: 0.030000\n",
      "Train Epoch: 1421 [86016/194182 (44%)]\tLoss: 0.249695\tGrad Norm: 1.608163\tLR: 0.030000\n",
      "Train Epoch: 1421 [106496/194182 (54%)]\tLoss: 0.256682\tGrad Norm: 2.077340\tLR: 0.030000\n",
      "Train Epoch: 1421 [126976/194182 (65%)]\tLoss: 0.257045\tGrad Norm: 1.867336\tLR: 0.030000\n",
      "Train Epoch: 1421 [147456/194182 (75%)]\tLoss: 0.248593\tGrad Norm: 1.959512\tLR: 0.030000\n",
      "Train Epoch: 1421 [167936/194182 (85%)]\tLoss: 0.250514\tGrad Norm: 1.926537\tLR: 0.030000\n",
      "Train Epoch: 1421 [188416/194182 (96%)]\tLoss: 0.259543\tGrad Norm: 2.102533\tLR: 0.030000\n",
      "Train set: Average loss: 0.2522\n",
      "Test set: Average loss: 0.2564, Average MAE: 0.3602\n",
      "Train Epoch: 1422 [4096/194182 (2%)]\tLoss: 0.251674\tGrad Norm: 1.727929\tLR: 0.030000\n",
      "Train Epoch: 1422 [24576/194182 (12%)]\tLoss: 0.250801\tGrad Norm: 1.579428\tLR: 0.030000\n",
      "Train Epoch: 1422 [45056/194182 (23%)]\tLoss: 0.251697\tGrad Norm: 1.709833\tLR: 0.030000\n",
      "Train Epoch: 1422 [65536/194182 (33%)]\tLoss: 0.253418\tGrad Norm: 1.598791\tLR: 0.030000\n",
      "Train Epoch: 1422 [86016/194182 (44%)]\tLoss: 0.251226\tGrad Norm: 1.533737\tLR: 0.030000\n",
      "Train Epoch: 1422 [106496/194182 (54%)]\tLoss: 0.253286\tGrad Norm: 1.506787\tLR: 0.030000\n",
      "Train Epoch: 1422 [126976/194182 (65%)]\tLoss: 0.251314\tGrad Norm: 1.492067\tLR: 0.030000\n",
      "Train Epoch: 1422 [147456/194182 (75%)]\tLoss: 0.239531\tGrad Norm: 1.167458\tLR: 0.030000\n",
      "Train Epoch: 1422 [167936/194182 (85%)]\tLoss: 0.244816\tGrad Norm: 1.427642\tLR: 0.030000\n",
      "Train Epoch: 1422 [188416/194182 (96%)]\tLoss: 0.246238\tGrad Norm: 1.380401\tLR: 0.030000\n",
      "Train set: Average loss: 0.2487\n",
      "Test set: Average loss: 0.2512, Average MAE: 0.3487\n",
      "Train Epoch: 1423 [4096/194182 (2%)]\tLoss: 0.250904\tGrad Norm: 1.619983\tLR: 0.030000\n",
      "Train Epoch: 1423 [24576/194182 (12%)]\tLoss: 0.245471\tGrad Norm: 1.478868\tLR: 0.030000\n",
      "Train Epoch: 1423 [45056/194182 (23%)]\tLoss: 0.240832\tGrad Norm: 1.189145\tLR: 0.030000\n",
      "Train Epoch: 1423 [65536/194182 (33%)]\tLoss: 0.248128\tGrad Norm: 1.396590\tLR: 0.030000\n",
      "Train Epoch: 1423 [86016/194182 (44%)]\tLoss: 0.247266\tGrad Norm: 1.591676\tLR: 0.030000\n",
      "Train Epoch: 1423 [106496/194182 (54%)]\tLoss: 0.247195\tGrad Norm: 1.488170\tLR: 0.030000\n",
      "Train Epoch: 1423 [126976/194182 (65%)]\tLoss: 0.257521\tGrad Norm: 1.760536\tLR: 0.030000\n",
      "Train Epoch: 1423 [147456/194182 (75%)]\tLoss: 0.251802\tGrad Norm: 1.591515\tLR: 0.030000\n",
      "Train Epoch: 1423 [167936/194182 (85%)]\tLoss: 0.251729\tGrad Norm: 1.816064\tLR: 0.030000\n",
      "Train Epoch: 1423 [188416/194182 (96%)]\tLoss: 0.249334\tGrad Norm: 1.689689\tLR: 0.030000\n",
      "Train set: Average loss: 0.2491\n",
      "Test set: Average loss: 0.2545, Average MAE: 0.3574\n",
      "Train Epoch: 1424 [4096/194182 (2%)]\tLoss: 0.248183\tGrad Norm: 1.705292\tLR: 0.030000\n",
      "Train Epoch: 1424 [24576/194182 (12%)]\tLoss: 0.249241\tGrad Norm: 1.856741\tLR: 0.030000\n",
      "Train Epoch: 1424 [45056/194182 (23%)]\tLoss: 0.260477\tGrad Norm: 2.208006\tLR: 0.030000\n",
      "Train Epoch: 1424 [65536/194182 (33%)]\tLoss: 0.249479\tGrad Norm: 1.890245\tLR: 0.030000\n",
      "Train Epoch: 1424 [86016/194182 (44%)]\tLoss: 0.252068\tGrad Norm: 1.870554\tLR: 0.030000\n",
      "Train Epoch: 1424 [106496/194182 (54%)]\tLoss: 0.243994\tGrad Norm: 1.457233\tLR: 0.030000\n",
      "Train Epoch: 1424 [126976/194182 (65%)]\tLoss: 0.239658\tGrad Norm: 1.555457\tLR: 0.030000\n",
      "Train Epoch: 1424 [147456/194182 (75%)]\tLoss: 0.249516\tGrad Norm: 1.721165\tLR: 0.030000\n",
      "Train Epoch: 1424 [167936/194182 (85%)]\tLoss: 0.255498\tGrad Norm: 1.918070\tLR: 0.030000\n",
      "Train Epoch: 1424 [188416/194182 (96%)]\tLoss: 0.247241\tGrad Norm: 1.572472\tLR: 0.030000\n",
      "Train set: Average loss: 0.2513\n",
      "Test set: Average loss: 0.2553, Average MAE: 0.3570\n",
      "Train Epoch: 1425 [4096/194182 (2%)]\tLoss: 0.253878\tGrad Norm: 1.717961\tLR: 0.030000\n",
      "Train Epoch: 1425 [24576/194182 (12%)]\tLoss: 0.245437\tGrad Norm: 1.215195\tLR: 0.030000\n",
      "Train Epoch: 1425 [45056/194182 (23%)]\tLoss: 0.249877\tGrad Norm: 1.658348\tLR: 0.030000\n",
      "Train Epoch: 1425 [65536/194182 (33%)]\tLoss: 0.251381\tGrad Norm: 1.745049\tLR: 0.030000\n",
      "Train Epoch: 1425 [86016/194182 (44%)]\tLoss: 0.249795\tGrad Norm: 1.430491\tLR: 0.030000\n",
      "Train Epoch: 1425 [106496/194182 (54%)]\tLoss: 0.249839\tGrad Norm: 1.671636\tLR: 0.030000\n",
      "Train Epoch: 1425 [126976/194182 (65%)]\tLoss: 0.251048\tGrad Norm: 1.704372\tLR: 0.030000\n",
      "Train Epoch: 1425 [147456/194182 (75%)]\tLoss: 0.250447\tGrad Norm: 1.598287\tLR: 0.030000\n",
      "Train Epoch: 1425 [167936/194182 (85%)]\tLoss: 0.249854\tGrad Norm: 1.637797\tLR: 0.030000\n",
      "Train Epoch: 1425 [188416/194182 (96%)]\tLoss: 0.250399\tGrad Norm: 1.524419\tLR: 0.030000\n",
      "Train set: Average loss: 0.2490\n",
      "Test set: Average loss: 0.2521, Average MAE: 0.3503\n",
      "Epoch 1425: Mean reward = 0.044 +/- 0.031\n",
      "Train Epoch: 1426 [4096/194182 (2%)]\tLoss: 0.248059\tGrad Norm: 1.615467\tLR: 0.030000\n",
      "Train Epoch: 1426 [24576/194182 (12%)]\tLoss: 0.254616\tGrad Norm: 1.714549\tLR: 0.030000\n",
      "Train Epoch: 1426 [45056/194182 (23%)]\tLoss: 0.245247\tGrad Norm: 1.555346\tLR: 0.030000\n",
      "Train Epoch: 1426 [65536/194182 (33%)]\tLoss: 0.246801\tGrad Norm: 1.761196\tLR: 0.030000\n",
      "Train Epoch: 1426 [86016/194182 (44%)]\tLoss: 0.255459\tGrad Norm: 1.733946\tLR: 0.030000\n",
      "Train Epoch: 1426 [106496/194182 (54%)]\tLoss: 0.245218\tGrad Norm: 1.414724\tLR: 0.030000\n",
      "Train Epoch: 1426 [126976/194182 (65%)]\tLoss: 0.251575\tGrad Norm: 1.710034\tLR: 0.030000\n",
      "Train Epoch: 1426 [147456/194182 (75%)]\tLoss: 0.245055\tGrad Norm: 1.647762\tLR: 0.030000\n",
      "Train Epoch: 1426 [167936/194182 (85%)]\tLoss: 0.249821\tGrad Norm: 1.731644\tLR: 0.030000\n",
      "Train Epoch: 1426 [188416/194182 (96%)]\tLoss: 0.257714\tGrad Norm: 1.814443\tLR: 0.030000\n",
      "Train set: Average loss: 0.2496\n",
      "Test set: Average loss: 0.2552, Average MAE: 0.3576\n",
      "Train Epoch: 1427 [4096/194182 (2%)]\tLoss: 0.252828\tGrad Norm: 1.680708\tLR: 0.030000\n",
      "Train Epoch: 1427 [24576/194182 (12%)]\tLoss: 0.255340\tGrad Norm: 1.607566\tLR: 0.030000\n",
      "Train Epoch: 1427 [45056/194182 (23%)]\tLoss: 0.250197\tGrad Norm: 1.886942\tLR: 0.030000\n",
      "Train Epoch: 1427 [65536/194182 (33%)]\tLoss: 0.257805\tGrad Norm: 2.124875\tLR: 0.030000\n",
      "Train Epoch: 1427 [86016/194182 (44%)]\tLoss: 0.259300\tGrad Norm: 1.858850\tLR: 0.030000\n",
      "Train Epoch: 1427 [106496/194182 (54%)]\tLoss: 0.251973\tGrad Norm: 1.810773\tLR: 0.030000\n",
      "Train Epoch: 1427 [126976/194182 (65%)]\tLoss: 0.256744\tGrad Norm: 1.969191\tLR: 0.030000\n",
      "Train Epoch: 1427 [147456/194182 (75%)]\tLoss: 0.247291\tGrad Norm: 1.431840\tLR: 0.030000\n",
      "Train Epoch: 1427 [167936/194182 (85%)]\tLoss: 0.250784\tGrad Norm: 1.401466\tLR: 0.030000\n",
      "Train Epoch: 1427 [188416/194182 (96%)]\tLoss: 0.245090\tGrad Norm: 1.575992\tLR: 0.030000\n",
      "Train set: Average loss: 0.2509\n",
      "Test set: Average loss: 0.2521, Average MAE: 0.3482\n",
      "Train Epoch: 1428 [4096/194182 (2%)]\tLoss: 0.249052\tGrad Norm: 1.574243\tLR: 0.030000\n",
      "Train Epoch: 1428 [24576/194182 (12%)]\tLoss: 0.237601\tGrad Norm: 0.919749\tLR: 0.030000\n",
      "Train Epoch: 1428 [45056/194182 (23%)]\tLoss: 0.245047\tGrad Norm: 1.319301\tLR: 0.030000\n",
      "Train Epoch: 1428 [65536/194182 (33%)]\tLoss: 0.243682\tGrad Norm: 1.429034\tLR: 0.030000\n",
      "Train Epoch: 1428 [86016/194182 (44%)]\tLoss: 0.244246\tGrad Norm: 1.502863\tLR: 0.030000\n",
      "Train Epoch: 1428 [106496/194182 (54%)]\tLoss: 0.252679\tGrad Norm: 1.719682\tLR: 0.030000\n",
      "Train Epoch: 1428 [126976/194182 (65%)]\tLoss: 0.249555\tGrad Norm: 1.667688\tLR: 0.030000\n",
      "Train Epoch: 1428 [147456/194182 (75%)]\tLoss: 0.251384\tGrad Norm: 1.684730\tLR: 0.030000\n",
      "Train Epoch: 1428 [167936/194182 (85%)]\tLoss: 0.245201\tGrad Norm: 1.645094\tLR: 0.030000\n",
      "Train Epoch: 1428 [188416/194182 (96%)]\tLoss: 0.251969\tGrad Norm: 1.752446\tLR: 0.030000\n",
      "Train set: Average loss: 0.2471\n",
      "Test set: Average loss: 0.2557, Average MAE: 0.3615\n",
      "Train Epoch: 1429 [4096/194182 (2%)]\tLoss: 0.254239\tGrad Norm: 1.696666\tLR: 0.030000\n",
      "Train Epoch: 1429 [24576/194182 (12%)]\tLoss: 0.251513\tGrad Norm: 1.917022\tLR: 0.030000\n",
      "Train Epoch: 1429 [45056/194182 (23%)]\tLoss: 0.252232\tGrad Norm: 1.734190\tLR: 0.030000\n",
      "Train Epoch: 1429 [65536/194182 (33%)]\tLoss: 0.237837\tGrad Norm: 1.221263\tLR: 0.030000\n",
      "Train Epoch: 1429 [86016/194182 (44%)]\tLoss: 0.235605\tGrad Norm: 1.127642\tLR: 0.030000\n",
      "Train Epoch: 1429 [106496/194182 (54%)]\tLoss: 0.246752\tGrad Norm: 1.521475\tLR: 0.030000\n",
      "Train Epoch: 1429 [126976/194182 (65%)]\tLoss: 0.250848\tGrad Norm: 1.532455\tLR: 0.030000\n",
      "Train Epoch: 1429 [147456/194182 (75%)]\tLoss: 0.249430\tGrad Norm: 1.740869\tLR: 0.030000\n",
      "Train Epoch: 1429 [167936/194182 (85%)]\tLoss: 0.251407\tGrad Norm: 2.071632\tLR: 0.030000\n",
      "Train Epoch: 1429 [188416/194182 (96%)]\tLoss: 0.250031\tGrad Norm: 1.533254\tLR: 0.030000\n",
      "Train set: Average loss: 0.2483\n",
      "Test set: Average loss: 0.2459, Average MAE: 0.3400\n",
      "Train Epoch: 1430 [4096/194182 (2%)]\tLoss: 0.239584\tGrad Norm: 1.364023\tLR: 0.030000\n",
      "Train Epoch: 1430 [24576/194182 (12%)]\tLoss: 0.250711\tGrad Norm: 1.773632\tLR: 0.030000\n",
      "Train Epoch: 1430 [45056/194182 (23%)]\tLoss: 0.254592\tGrad Norm: 1.910102\tLR: 0.030000\n",
      "Train Epoch: 1430 [65536/194182 (33%)]\tLoss: 0.247775\tGrad Norm: 1.626682\tLR: 0.030000\n",
      "Train Epoch: 1430 [86016/194182 (44%)]\tLoss: 0.240025\tGrad Norm: 1.129190\tLR: 0.030000\n",
      "Train Epoch: 1430 [106496/194182 (54%)]\tLoss: 0.244139\tGrad Norm: 1.458931\tLR: 0.030000\n",
      "Train Epoch: 1430 [126976/194182 (65%)]\tLoss: 0.247707\tGrad Norm: 1.361123\tLR: 0.030000\n",
      "Train Epoch: 1430 [147456/194182 (75%)]\tLoss: 0.244738\tGrad Norm: 1.500842\tLR: 0.030000\n",
      "Train Epoch: 1430 [167936/194182 (85%)]\tLoss: 0.249843\tGrad Norm: 1.591156\tLR: 0.030000\n",
      "Train Epoch: 1430 [188416/194182 (96%)]\tLoss: 0.257564\tGrad Norm: 2.145049\tLR: 0.030000\n",
      "Train set: Average loss: 0.2484\n",
      "Test set: Average loss: 0.2555, Average MAE: 0.3610\n",
      "Epoch 1430: Mean reward = 0.083 +/- 0.076\n",
      "Train Epoch: 1431 [4096/194182 (2%)]\tLoss: 0.252815\tGrad Norm: 1.782856\tLR: 0.030000\n",
      "Train Epoch: 1431 [24576/194182 (12%)]\tLoss: 0.241985\tGrad Norm: 1.869331\tLR: 0.030000\n",
      "Train Epoch: 1431 [45056/194182 (23%)]\tLoss: 0.244881\tGrad Norm: 1.678878\tLR: 0.030000\n",
      "Train Epoch: 1431 [65536/194182 (33%)]\tLoss: 0.245543\tGrad Norm: 1.632970\tLR: 0.030000\n",
      "Train Epoch: 1431 [86016/194182 (44%)]\tLoss: 0.244728\tGrad Norm: 1.437710\tLR: 0.030000\n",
      "Train Epoch: 1431 [106496/194182 (54%)]\tLoss: 0.253672\tGrad Norm: 1.827813\tLR: 0.030000\n",
      "Train Epoch: 1431 [126976/194182 (65%)]\tLoss: 0.245172\tGrad Norm: 1.450147\tLR: 0.030000\n",
      "Train Epoch: 1431 [147456/194182 (75%)]\tLoss: 0.243698\tGrad Norm: 1.561414\tLR: 0.030000\n",
      "Train Epoch: 1431 [167936/194182 (85%)]\tLoss: 0.247442\tGrad Norm: 1.135827\tLR: 0.030000\n",
      "Train Epoch: 1431 [188416/194182 (96%)]\tLoss: 0.240573\tGrad Norm: 1.341003\tLR: 0.030000\n",
      "Train set: Average loss: 0.2473\n",
      "Test set: Average loss: 0.2526, Average MAE: 0.3515\n",
      "Train Epoch: 1432 [4096/194182 (2%)]\tLoss: 0.248853\tGrad Norm: 1.690932\tLR: 0.030000\n",
      "Train Epoch: 1432 [24576/194182 (12%)]\tLoss: 0.245033\tGrad Norm: 1.628194\tLR: 0.030000\n",
      "Train Epoch: 1432 [45056/194182 (23%)]\tLoss: 0.249389\tGrad Norm: 1.467826\tLR: 0.030000\n",
      "Train Epoch: 1432 [65536/194182 (33%)]\tLoss: 0.246749\tGrad Norm: 1.634274\tLR: 0.030000\n",
      "Train Epoch: 1432 [86016/194182 (44%)]\tLoss: 0.252751\tGrad Norm: 1.759255\tLR: 0.030000\n",
      "Train Epoch: 1432 [106496/194182 (54%)]\tLoss: 0.253103\tGrad Norm: 1.742703\tLR: 0.030000\n",
      "Train Epoch: 1432 [126976/194182 (65%)]\tLoss: 0.256171\tGrad Norm: 1.775768\tLR: 0.030000\n",
      "Train Epoch: 1432 [147456/194182 (75%)]\tLoss: 0.248499\tGrad Norm: 1.470271\tLR: 0.030000\n",
      "Train Epoch: 1432 [167936/194182 (85%)]\tLoss: 0.248769\tGrad Norm: 1.391535\tLR: 0.030000\n",
      "Train Epoch: 1432 [188416/194182 (96%)]\tLoss: 0.248545\tGrad Norm: 1.435842\tLR: 0.030000\n",
      "Train set: Average loss: 0.2474\n",
      "Test set: Average loss: 0.2486, Average MAE: 0.3406\n",
      "Train Epoch: 1433 [4096/194182 (2%)]\tLoss: 0.245744\tGrad Norm: 1.475472\tLR: 0.030000\n",
      "Train Epoch: 1433 [24576/194182 (12%)]\tLoss: 0.247973\tGrad Norm: 1.574366\tLR: 0.030000\n",
      "Train Epoch: 1433 [45056/194182 (23%)]\tLoss: 0.242346\tGrad Norm: 2.014768\tLR: 0.030000\n",
      "Train Epoch: 1433 [65536/194182 (33%)]\tLoss: 0.255655\tGrad Norm: 2.068224\tLR: 0.030000\n",
      "Train Epoch: 1433 [86016/194182 (44%)]\tLoss: 0.259651\tGrad Norm: 2.143523\tLR: 0.030000\n",
      "Train Epoch: 1433 [106496/194182 (54%)]\tLoss: 0.254841\tGrad Norm: 1.891091\tLR: 0.030000\n",
      "Train Epoch: 1433 [126976/194182 (65%)]\tLoss: 0.249677\tGrad Norm: 1.796002\tLR: 0.030000\n",
      "Train Epoch: 1433 [147456/194182 (75%)]\tLoss: 0.249011\tGrad Norm: 1.480348\tLR: 0.030000\n",
      "Train Epoch: 1433 [167936/194182 (85%)]\tLoss: 0.246520\tGrad Norm: 1.439248\tLR: 0.030000\n",
      "Train Epoch: 1433 [188416/194182 (96%)]\tLoss: 0.252663\tGrad Norm: 1.799009\tLR: 0.030000\n",
      "Train set: Average loss: 0.2510\n",
      "Test set: Average loss: 0.2505, Average MAE: 0.3410\n",
      "Train Epoch: 1434 [4096/194182 (2%)]\tLoss: 0.256124\tGrad Norm: 1.622831\tLR: 0.030000\n",
      "Train Epoch: 1434 [24576/194182 (12%)]\tLoss: 0.248855\tGrad Norm: 1.352325\tLR: 0.030000\n",
      "Train Epoch: 1434 [45056/194182 (23%)]\tLoss: 0.242464\tGrad Norm: 1.292484\tLR: 0.030000\n",
      "Train Epoch: 1434 [65536/194182 (33%)]\tLoss: 0.244357\tGrad Norm: 0.978518\tLR: 0.030000\n",
      "Train Epoch: 1434 [86016/194182 (44%)]\tLoss: 0.245711\tGrad Norm: 1.449968\tLR: 0.030000\n",
      "Train Epoch: 1434 [106496/194182 (54%)]\tLoss: 0.245395\tGrad Norm: 1.614875\tLR: 0.030000\n",
      "Train Epoch: 1434 [126976/194182 (65%)]\tLoss: 0.254185\tGrad Norm: 1.696687\tLR: 0.030000\n",
      "Train Epoch: 1434 [147456/194182 (75%)]\tLoss: 0.255447\tGrad Norm: 2.098732\tLR: 0.030000\n",
      "Train Epoch: 1434 [167936/194182 (85%)]\tLoss: 0.252300\tGrad Norm: 1.794072\tLR: 0.030000\n",
      "Train Epoch: 1434 [188416/194182 (96%)]\tLoss: 0.251378\tGrad Norm: 1.927744\tLR: 0.030000\n",
      "Train set: Average loss: 0.2473\n",
      "Test set: Average loss: 0.2534, Average MAE: 0.3554\n",
      "Train Epoch: 1435 [4096/194182 (2%)]\tLoss: 0.247446\tGrad Norm: 1.808280\tLR: 0.030000\n",
      "Train Epoch: 1435 [24576/194182 (12%)]\tLoss: 0.250817\tGrad Norm: 1.937552\tLR: 0.030000\n",
      "Train Epoch: 1435 [45056/194182 (23%)]\tLoss: 0.248929\tGrad Norm: 1.610377\tLR: 0.030000\n",
      "Train Epoch: 1435 [65536/194182 (33%)]\tLoss: 0.245276\tGrad Norm: 1.636549\tLR: 0.030000\n",
      "Train Epoch: 1435 [86016/194182 (44%)]\tLoss: 0.241092\tGrad Norm: 1.277491\tLR: 0.030000\n",
      "Train Epoch: 1435 [106496/194182 (54%)]\tLoss: 0.254583\tGrad Norm: 1.778954\tLR: 0.030000\n",
      "Train Epoch: 1435 [126976/194182 (65%)]\tLoss: 0.248215\tGrad Norm: 1.616019\tLR: 0.030000\n",
      "Train Epoch: 1435 [147456/194182 (75%)]\tLoss: 0.244257\tGrad Norm: 1.341231\tLR: 0.030000\n",
      "Train Epoch: 1435 [167936/194182 (85%)]\tLoss: 0.245650\tGrad Norm: 1.683360\tLR: 0.030000\n",
      "Train Epoch: 1435 [188416/194182 (96%)]\tLoss: 0.246131\tGrad Norm: 1.615419\tLR: 0.030000\n",
      "Train set: Average loss: 0.2475\n",
      "Test set: Average loss: 0.2548, Average MAE: 0.3403\n",
      "Epoch 1435: Mean reward = 0.046 +/- 0.035\n",
      "Train Epoch: 1436 [4096/194182 (2%)]\tLoss: 0.254321\tGrad Norm: 1.949904\tLR: 0.030000\n",
      "Train Epoch: 1436 [24576/194182 (12%)]\tLoss: 0.251664\tGrad Norm: 1.604527\tLR: 0.030000\n",
      "Train Epoch: 1436 [45056/194182 (23%)]\tLoss: 0.241160\tGrad Norm: 1.277145\tLR: 0.030000\n",
      "Train Epoch: 1436 [65536/194182 (33%)]\tLoss: 0.248029\tGrad Norm: 1.550157\tLR: 0.030000\n",
      "Train Epoch: 1436 [86016/194182 (44%)]\tLoss: 0.250394\tGrad Norm: 1.872300\tLR: 0.030000\n",
      "Train Epoch: 1436 [106496/194182 (54%)]\tLoss: 0.234999\tGrad Norm: 1.288776\tLR: 0.030000\n",
      "Train Epoch: 1436 [126976/194182 (65%)]\tLoss: 0.240104\tGrad Norm: 1.519359\tLR: 0.030000\n",
      "Train Epoch: 1436 [147456/194182 (75%)]\tLoss: 0.249815\tGrad Norm: 1.540829\tLR: 0.030000\n",
      "Train Epoch: 1436 [167936/194182 (85%)]\tLoss: 0.246829\tGrad Norm: 1.574136\tLR: 0.030000\n",
      "Train Epoch: 1436 [188416/194182 (96%)]\tLoss: 0.243655\tGrad Norm: 1.280698\tLR: 0.030000\n",
      "Train set: Average loss: 0.2462\n",
      "Test set: Average loss: 0.2495, Average MAE: 0.3512\n",
      "Train Epoch: 1437 [4096/194182 (2%)]\tLoss: 0.244365\tGrad Norm: 1.545960\tLR: 0.030000\n",
      "Train Epoch: 1437 [24576/194182 (12%)]\tLoss: 0.256570\tGrad Norm: 1.907313\tLR: 0.030000\n",
      "Train Epoch: 1437 [45056/194182 (23%)]\tLoss: 0.248835\tGrad Norm: 1.580237\tLR: 0.030000\n",
      "Train Epoch: 1437 [65536/194182 (33%)]\tLoss: 0.252275\tGrad Norm: 1.807986\tLR: 0.030000\n",
      "Train Epoch: 1437 [86016/194182 (44%)]\tLoss: 0.257386\tGrad Norm: 1.886988\tLR: 0.030000\n",
      "Train Epoch: 1437 [106496/194182 (54%)]\tLoss: 0.245107\tGrad Norm: 1.621214\tLR: 0.030000\n",
      "Train Epoch: 1437 [126976/194182 (65%)]\tLoss: 0.243221\tGrad Norm: 1.545294\tLR: 0.030000\n",
      "Train Epoch: 1437 [147456/194182 (75%)]\tLoss: 0.249061\tGrad Norm: 1.567001\tLR: 0.030000\n",
      "Train Epoch: 1437 [167936/194182 (85%)]\tLoss: 0.245411\tGrad Norm: 1.468689\tLR: 0.030000\n",
      "Train Epoch: 1437 [188416/194182 (96%)]\tLoss: 0.251147\tGrad Norm: 1.599456\tLR: 0.030000\n",
      "Train set: Average loss: 0.2482\n",
      "Test set: Average loss: 0.2503, Average MAE: 0.3412\n",
      "Train Epoch: 1438 [4096/194182 (2%)]\tLoss: 0.245778\tGrad Norm: 1.617484\tLR: 0.030000\n",
      "Train Epoch: 1438 [24576/194182 (12%)]\tLoss: 0.249570\tGrad Norm: 1.497220\tLR: 0.030000\n",
      "Train Epoch: 1438 [45056/194182 (23%)]\tLoss: 0.249956\tGrad Norm: 1.588627\tLR: 0.030000\n",
      "Train Epoch: 1438 [65536/194182 (33%)]\tLoss: 0.242695\tGrad Norm: 1.583862\tLR: 0.030000\n",
      "Train Epoch: 1438 [86016/194182 (44%)]\tLoss: 0.241695\tGrad Norm: 1.459257\tLR: 0.030000\n",
      "Train Epoch: 1438 [106496/194182 (54%)]\tLoss: 0.245565\tGrad Norm: 1.599977\tLR: 0.030000\n",
      "Train Epoch: 1438 [126976/194182 (65%)]\tLoss: 0.249764\tGrad Norm: 1.845141\tLR: 0.030000\n",
      "Train Epoch: 1438 [147456/194182 (75%)]\tLoss: 0.248379\tGrad Norm: 1.574425\tLR: 0.030000\n",
      "Train Epoch: 1438 [167936/194182 (85%)]\tLoss: 0.244973\tGrad Norm: 1.442613\tLR: 0.030000\n",
      "Train Epoch: 1438 [188416/194182 (96%)]\tLoss: 0.246986\tGrad Norm: 2.036812\tLR: 0.030000\n",
      "Train set: Average loss: 0.2472\n",
      "Test set: Average loss: 0.2549, Average MAE: 0.3515\n",
      "Train Epoch: 1439 [4096/194182 (2%)]\tLoss: 0.252100\tGrad Norm: 1.869283\tLR: 0.030000\n",
      "Train Epoch: 1439 [24576/194182 (12%)]\tLoss: 0.242447\tGrad Norm: 1.409307\tLR: 0.030000\n",
      "Train Epoch: 1439 [45056/194182 (23%)]\tLoss: 0.242840\tGrad Norm: 1.464794\tLR: 0.030000\n",
      "Train Epoch: 1439 [65536/194182 (33%)]\tLoss: 0.240840\tGrad Norm: 1.815015\tLR: 0.030000\n",
      "Train Epoch: 1439 [86016/194182 (44%)]\tLoss: 0.256151\tGrad Norm: 2.201223\tLR: 0.030000\n",
      "Train Epoch: 1439 [106496/194182 (54%)]\tLoss: 0.247897\tGrad Norm: 1.748565\tLR: 0.030000\n",
      "Train Epoch: 1439 [126976/194182 (65%)]\tLoss: 0.248460\tGrad Norm: 1.563785\tLR: 0.030000\n",
      "Train Epoch: 1439 [147456/194182 (75%)]\tLoss: 0.246909\tGrad Norm: 1.232854\tLR: 0.030000\n",
      "Train Epoch: 1439 [167936/194182 (85%)]\tLoss: 0.251102\tGrad Norm: 1.552779\tLR: 0.030000\n",
      "Train Epoch: 1439 [188416/194182 (96%)]\tLoss: 0.248362\tGrad Norm: 1.979657\tLR: 0.030000\n",
      "Train set: Average loss: 0.2471\n",
      "Test set: Average loss: 0.2585, Average MAE: 0.3426\n",
      "Train Epoch: 1440 [4096/194182 (2%)]\tLoss: 0.265909\tGrad Norm: 2.133314\tLR: 0.030000\n",
      "Train Epoch: 1440 [24576/194182 (12%)]\tLoss: 0.246719\tGrad Norm: 1.809508\tLR: 0.030000\n",
      "Train Epoch: 1440 [45056/194182 (23%)]\tLoss: 0.239639\tGrad Norm: 1.439781\tLR: 0.030000\n",
      "Train Epoch: 1440 [65536/194182 (33%)]\tLoss: 0.245017\tGrad Norm: 1.802455\tLR: 0.030000\n",
      "Train Epoch: 1440 [86016/194182 (44%)]\tLoss: 0.245796\tGrad Norm: 1.430051\tLR: 0.030000\n",
      "Train Epoch: 1440 [106496/194182 (54%)]\tLoss: 0.243316\tGrad Norm: 1.538858\tLR: 0.030000\n",
      "Train Epoch: 1440 [126976/194182 (65%)]\tLoss: 0.245986\tGrad Norm: 1.148659\tLR: 0.030000\n",
      "Train Epoch: 1440 [147456/194182 (75%)]\tLoss: 0.246307\tGrad Norm: 1.579402\tLR: 0.030000\n",
      "Train Epoch: 1440 [167936/194182 (85%)]\tLoss: 0.254172\tGrad Norm: 2.082524\tLR: 0.030000\n",
      "Train Epoch: 1440 [188416/194182 (96%)]\tLoss: 0.245321\tGrad Norm: 1.465912\tLR: 0.030000\n",
      "Train set: Average loss: 0.2467\n",
      "Test set: Average loss: 0.2426, Average MAE: 0.3408\n",
      "Epoch 1440: Mean reward = 0.044 +/- 0.032\n",
      "Train Epoch: 1441 [4096/194182 (2%)]\tLoss: 0.233884\tGrad Norm: 0.885194\tLR: 0.030000\n",
      "Train Epoch: 1441 [24576/194182 (12%)]\tLoss: 0.236923\tGrad Norm: 1.019370\tLR: 0.030000\n",
      "Train Epoch: 1441 [45056/194182 (23%)]\tLoss: 0.246119\tGrad Norm: 1.680598\tLR: 0.030000\n",
      "Train Epoch: 1441 [65536/194182 (33%)]\tLoss: 0.241299\tGrad Norm: 1.610904\tLR: 0.030000\n",
      "Train Epoch: 1441 [86016/194182 (44%)]\tLoss: 0.247748\tGrad Norm: 1.395943\tLR: 0.030000\n",
      "Train Epoch: 1441 [106496/194182 (54%)]\tLoss: 0.253409\tGrad Norm: 2.095282\tLR: 0.030000\n",
      "Train Epoch: 1441 [126976/194182 (65%)]\tLoss: 0.242486\tGrad Norm: 1.489952\tLR: 0.030000\n",
      "Train Epoch: 1441 [147456/194182 (75%)]\tLoss: 0.245054\tGrad Norm: 1.662755\tLR: 0.030000\n",
      "Train Epoch: 1441 [167936/194182 (85%)]\tLoss: 0.249095\tGrad Norm: 1.606687\tLR: 0.030000\n",
      "Train Epoch: 1441 [188416/194182 (96%)]\tLoss: 0.251601\tGrad Norm: 1.660231\tLR: 0.030000\n",
      "Train set: Average loss: 0.2451\n",
      "Test set: Average loss: 0.2500, Average MAE: 0.3452\n",
      "Train Epoch: 1442 [4096/194182 (2%)]\tLoss: 0.243788\tGrad Norm: 1.483898\tLR: 0.030000\n",
      "Train Epoch: 1442 [24576/194182 (12%)]\tLoss: 0.243496\tGrad Norm: 1.347024\tLR: 0.030000\n",
      "Train Epoch: 1442 [45056/194182 (23%)]\tLoss: 0.256200\tGrad Norm: 1.688371\tLR: 0.030000\n",
      "Train Epoch: 1442 [65536/194182 (33%)]\tLoss: 0.255326\tGrad Norm: 1.842272\tLR: 0.030000\n",
      "Train Epoch: 1442 [86016/194182 (44%)]\tLoss: 0.249453\tGrad Norm: 1.427079\tLR: 0.030000\n",
      "Train Epoch: 1442 [106496/194182 (54%)]\tLoss: 0.244447\tGrad Norm: 1.277410\tLR: 0.030000\n",
      "Train Epoch: 1442 [126976/194182 (65%)]\tLoss: 0.251622\tGrad Norm: 1.765923\tLR: 0.030000\n",
      "Train Epoch: 1442 [147456/194182 (75%)]\tLoss: 0.253859\tGrad Norm: 1.922428\tLR: 0.030000\n",
      "Train Epoch: 1442 [167936/194182 (85%)]\tLoss: 0.250375\tGrad Norm: 1.662813\tLR: 0.030000\n",
      "Train Epoch: 1442 [188416/194182 (96%)]\tLoss: 0.248543\tGrad Norm: 1.517780\tLR: 0.030000\n",
      "Train set: Average loss: 0.2472\n",
      "Test set: Average loss: 0.2517, Average MAE: 0.3365\n",
      "Train Epoch: 1443 [4096/194182 (2%)]\tLoss: 0.242517\tGrad Norm: 1.890881\tLR: 0.030000\n",
      "Train Epoch: 1443 [24576/194182 (12%)]\tLoss: 0.253218\tGrad Norm: 1.866648\tLR: 0.030000\n",
      "Train Epoch: 1443 [45056/194182 (23%)]\tLoss: 0.254921\tGrad Norm: 2.077996\tLR: 0.030000\n",
      "Train Epoch: 1443 [65536/194182 (33%)]\tLoss: 0.248426\tGrad Norm: 1.560523\tLR: 0.030000\n",
      "Train Epoch: 1443 [86016/194182 (44%)]\tLoss: 0.249256\tGrad Norm: 1.654978\tLR: 0.030000\n",
      "Train Epoch: 1443 [106496/194182 (54%)]\tLoss: 0.254521\tGrad Norm: 1.906691\tLR: 0.030000\n",
      "Train Epoch: 1443 [126976/194182 (65%)]\tLoss: 0.247505\tGrad Norm: 1.388407\tLR: 0.030000\n",
      "Train Epoch: 1443 [147456/194182 (75%)]\tLoss: 0.243785\tGrad Norm: 1.441293\tLR: 0.030000\n",
      "Train Epoch: 1443 [167936/194182 (85%)]\tLoss: 0.243920\tGrad Norm: 1.595986\tLR: 0.030000\n",
      "Train Epoch: 1443 [188416/194182 (96%)]\tLoss: 0.239126\tGrad Norm: 1.314729\tLR: 0.030000\n",
      "Train set: Average loss: 0.2465\n",
      "Test set: Average loss: 0.2467, Average MAE: 0.3399\n",
      "Train Epoch: 1444 [4096/194182 (2%)]\tLoss: 0.238463\tGrad Norm: 1.433335\tLR: 0.030000\n",
      "Train Epoch: 1444 [24576/194182 (12%)]\tLoss: 0.243619\tGrad Norm: 1.646488\tLR: 0.030000\n",
      "Train Epoch: 1444 [45056/194182 (23%)]\tLoss: 0.250885\tGrad Norm: 1.761619\tLR: 0.030000\n",
      "Train Epoch: 1444 [65536/194182 (33%)]\tLoss: 0.249555\tGrad Norm: 1.631167\tLR: 0.030000\n",
      "Train Epoch: 1444 [86016/194182 (44%)]\tLoss: 0.240440\tGrad Norm: 1.411506\tLR: 0.030000\n",
      "Train Epoch: 1444 [106496/194182 (54%)]\tLoss: 0.248200\tGrad Norm: 1.552214\tLR: 0.030000\n",
      "Train Epoch: 1444 [126976/194182 (65%)]\tLoss: 0.246610\tGrad Norm: 1.872620\tLR: 0.030000\n",
      "Train Epoch: 1444 [147456/194182 (75%)]\tLoss: 0.267223\tGrad Norm: 2.144645\tLR: 0.030000\n",
      "Train Epoch: 1444 [167936/194182 (85%)]\tLoss: 0.245262\tGrad Norm: 1.487226\tLR: 0.030000\n",
      "Train Epoch: 1444 [188416/194182 (96%)]\tLoss: 0.236562\tGrad Norm: 1.412466\tLR: 0.030000\n",
      "Train set: Average loss: 0.2461\n",
      "Test set: Average loss: 0.2466, Average MAE: 0.3357\n",
      "Train Epoch: 1445 [4096/194182 (2%)]\tLoss: 0.241657\tGrad Norm: 1.299488\tLR: 0.030000\n",
      "Train Epoch: 1445 [24576/194182 (12%)]\tLoss: 0.241201\tGrad Norm: 1.488651\tLR: 0.030000\n",
      "Train Epoch: 1445 [45056/194182 (23%)]\tLoss: 0.245857\tGrad Norm: 1.730719\tLR: 0.030000\n",
      "Train Epoch: 1445 [65536/194182 (33%)]\tLoss: 0.242109\tGrad Norm: 1.600423\tLR: 0.030000\n",
      "Train Epoch: 1445 [86016/194182 (44%)]\tLoss: 0.247912\tGrad Norm: 1.583039\tLR: 0.030000\n",
      "Train Epoch: 1445 [106496/194182 (54%)]\tLoss: 0.242124\tGrad Norm: 1.649064\tLR: 0.030000\n",
      "Train Epoch: 1445 [126976/194182 (65%)]\tLoss: 0.244123\tGrad Norm: 1.522972\tLR: 0.030000\n",
      "Train Epoch: 1445 [147456/194182 (75%)]\tLoss: 0.255882\tGrad Norm: 1.632904\tLR: 0.030000\n",
      "Train Epoch: 1445 [167936/194182 (85%)]\tLoss: 0.250338\tGrad Norm: 1.610053\tLR: 0.030000\n",
      "Train Epoch: 1445 [188416/194182 (96%)]\tLoss: 0.250581\tGrad Norm: 1.610748\tLR: 0.030000\n",
      "Train set: Average loss: 0.2461\n",
      "Test set: Average loss: 0.2502, Average MAE: 0.3366\n",
      "Epoch 1445: Mean reward = 0.067 +/- 0.062\n",
      "Train Epoch: 1446 [4096/194182 (2%)]\tLoss: 0.247714\tGrad Norm: 1.674866\tLR: 0.030000\n",
      "Train Epoch: 1446 [24576/194182 (12%)]\tLoss: 0.251727\tGrad Norm: 1.671187\tLR: 0.030000\n",
      "Train Epoch: 1446 [45056/194182 (23%)]\tLoss: 0.245533\tGrad Norm: 1.606694\tLR: 0.030000\n",
      "Train Epoch: 1446 [65536/194182 (33%)]\tLoss: 0.233311\tGrad Norm: 1.132835\tLR: 0.030000\n",
      "Train Epoch: 1446 [86016/194182 (44%)]\tLoss: 0.242263\tGrad Norm: 1.295555\tLR: 0.030000\n",
      "Train Epoch: 1446 [106496/194182 (54%)]\tLoss: 0.240304\tGrad Norm: 1.468593\tLR: 0.030000\n",
      "Train Epoch: 1446 [126976/194182 (65%)]\tLoss: 0.250757\tGrad Norm: 1.491960\tLR: 0.030000\n",
      "Train Epoch: 1446 [147456/194182 (75%)]\tLoss: 0.245552\tGrad Norm: 1.770471\tLR: 0.030000\n",
      "Train Epoch: 1446 [167936/194182 (85%)]\tLoss: 0.243629\tGrad Norm: 1.688601\tLR: 0.030000\n",
      "Train Epoch: 1446 [188416/194182 (96%)]\tLoss: 0.248746\tGrad Norm: 1.655056\tLR: 0.030000\n",
      "Train set: Average loss: 0.2454\n",
      "Test set: Average loss: 0.2687, Average MAE: 0.3702\n",
      "Train Epoch: 1447 [4096/194182 (2%)]\tLoss: 0.258743\tGrad Norm: 2.327648\tLR: 0.030000\n",
      "Train Epoch: 1447 [24576/194182 (12%)]\tLoss: 0.246612\tGrad Norm: 1.693683\tLR: 0.030000\n",
      "Train Epoch: 1447 [45056/194182 (23%)]\tLoss: 0.241019\tGrad Norm: 1.453639\tLR: 0.030000\n",
      "Train Epoch: 1447 [65536/194182 (33%)]\tLoss: 0.246528\tGrad Norm: 1.669741\tLR: 0.030000\n",
      "Train Epoch: 1447 [86016/194182 (44%)]\tLoss: 0.242220\tGrad Norm: 1.389113\tLR: 0.030000\n",
      "Train Epoch: 1447 [106496/194182 (54%)]\tLoss: 0.250764\tGrad Norm: 2.139691\tLR: 0.030000\n",
      "Train Epoch: 1447 [126976/194182 (65%)]\tLoss: 0.246138\tGrad Norm: 1.944825\tLR: 0.030000\n",
      "Train Epoch: 1447 [147456/194182 (75%)]\tLoss: 0.252705\tGrad Norm: 2.054055\tLR: 0.030000\n",
      "Train Epoch: 1447 [167936/194182 (85%)]\tLoss: 0.252203\tGrad Norm: 1.780457\tLR: 0.030000\n",
      "Train Epoch: 1447 [188416/194182 (96%)]\tLoss: 0.235123\tGrad Norm: 1.181021\tLR: 0.030000\n",
      "Train set: Average loss: 0.2473\n",
      "Test set: Average loss: 0.2479, Average MAE: 0.3457\n",
      "Train Epoch: 1448 [4096/194182 (2%)]\tLoss: 0.237681\tGrad Norm: 1.315089\tLR: 0.030000\n",
      "Train Epoch: 1448 [24576/194182 (12%)]\tLoss: 0.243906\tGrad Norm: 1.411428\tLR: 0.030000\n",
      "Train Epoch: 1448 [45056/194182 (23%)]\tLoss: 0.243010\tGrad Norm: 1.569771\tLR: 0.030000\n",
      "Train Epoch: 1448 [65536/194182 (33%)]\tLoss: 0.240479\tGrad Norm: 1.683246\tLR: 0.030000\n",
      "Train Epoch: 1448 [86016/194182 (44%)]\tLoss: 0.242010\tGrad Norm: 1.691702\tLR: 0.030000\n",
      "Train Epoch: 1448 [106496/194182 (54%)]\tLoss: 0.244973\tGrad Norm: 1.669200\tLR: 0.030000\n",
      "Train Epoch: 1448 [126976/194182 (65%)]\tLoss: 0.241811\tGrad Norm: 1.487621\tLR: 0.030000\n",
      "Train Epoch: 1448 [147456/194182 (75%)]\tLoss: 0.237169\tGrad Norm: 1.305145\tLR: 0.030000\n",
      "Train Epoch: 1448 [167936/194182 (85%)]\tLoss: 0.250344\tGrad Norm: 1.516162\tLR: 0.030000\n",
      "Train Epoch: 1448 [188416/194182 (96%)]\tLoss: 0.250024\tGrad Norm: 1.728735\tLR: 0.030000\n",
      "Train set: Average loss: 0.2457\n",
      "Test set: Average loss: 0.2583, Average MAE: 0.3569\n",
      "Train Epoch: 1449 [4096/194182 (2%)]\tLoss: 0.246408\tGrad Norm: 1.871086\tLR: 0.030000\n",
      "Train Epoch: 1449 [24576/194182 (12%)]\tLoss: 0.251580\tGrad Norm: 1.739551\tLR: 0.030000\n",
      "Train Epoch: 1449 [45056/194182 (23%)]\tLoss: 0.259992\tGrad Norm: 1.955405\tLR: 0.030000\n",
      "Train Epoch: 1449 [65536/194182 (33%)]\tLoss: 0.237603\tGrad Norm: 1.256899\tLR: 0.030000\n",
      "Train Epoch: 1449 [86016/194182 (44%)]\tLoss: 0.249807\tGrad Norm: 1.929878\tLR: 0.030000\n",
      "Train Epoch: 1449 [106496/194182 (54%)]\tLoss: 0.248490\tGrad Norm: 1.680867\tLR: 0.030000\n",
      "Train Epoch: 1449 [126976/194182 (65%)]\tLoss: 0.248756\tGrad Norm: 1.536780\tLR: 0.030000\n",
      "Train Epoch: 1449 [147456/194182 (75%)]\tLoss: 0.248191\tGrad Norm: 1.596817\tLR: 0.030000\n",
      "Train Epoch: 1449 [167936/194182 (85%)]\tLoss: 0.254324\tGrad Norm: 2.129741\tLR: 0.030000\n",
      "Train Epoch: 1449 [188416/194182 (96%)]\tLoss: 0.245638\tGrad Norm: 1.433156\tLR: 0.030000\n",
      "Train set: Average loss: 0.2466\n",
      "Test set: Average loss: 0.2515, Average MAE: 0.3561\n",
      "Train Epoch: 1450 [4096/194182 (2%)]\tLoss: 0.239486\tGrad Norm: 1.546359\tLR: 0.030000\n",
      "Train Epoch: 1450 [24576/194182 (12%)]\tLoss: 0.242840\tGrad Norm: 2.092393\tLR: 0.030000\n",
      "Train Epoch: 1450 [45056/194182 (23%)]\tLoss: 0.260616\tGrad Norm: 2.067267\tLR: 0.030000\n",
      "Train Epoch: 1450 [65536/194182 (33%)]\tLoss: 0.245820\tGrad Norm: 1.988292\tLR: 0.030000\n",
      "Train Epoch: 1450 [86016/194182 (44%)]\tLoss: 0.241623\tGrad Norm: 1.851292\tLR: 0.030000\n",
      "Train Epoch: 1450 [106496/194182 (54%)]\tLoss: 0.241137\tGrad Norm: 1.509258\tLR: 0.030000\n",
      "Train Epoch: 1450 [126976/194182 (65%)]\tLoss: 0.239721\tGrad Norm: 1.353657\tLR: 0.030000\n",
      "Train Epoch: 1450 [147456/194182 (75%)]\tLoss: 0.249952\tGrad Norm: 1.580861\tLR: 0.030000\n",
      "Train Epoch: 1450 [167936/194182 (85%)]\tLoss: 0.250776\tGrad Norm: 1.653215\tLR: 0.030000\n",
      "Train Epoch: 1450 [188416/194182 (96%)]\tLoss: 0.247806\tGrad Norm: 1.529567\tLR: 0.030000\n",
      "Train set: Average loss: 0.2479\n",
      "Test set: Average loss: 0.2509, Average MAE: 0.3404\n",
      "Epoch 1450: Mean reward = 0.069 +/- 0.073\n",
      "Train Epoch: 1451 [4096/194182 (2%)]\tLoss: 0.244490\tGrad Norm: 1.512315\tLR: 0.030000\n",
      "Train Epoch: 1451 [24576/194182 (12%)]\tLoss: 0.243183\tGrad Norm: 1.302600\tLR: 0.030000\n",
      "Train Epoch: 1451 [45056/194182 (23%)]\tLoss: 0.230456\tGrad Norm: 1.172421\tLR: 0.030000\n",
      "Train Epoch: 1451 [65536/194182 (33%)]\tLoss: 0.247830\tGrad Norm: 1.577018\tLR: 0.030000\n",
      "Train Epoch: 1451 [86016/194182 (44%)]\tLoss: 0.255066\tGrad Norm: 2.020070\tLR: 0.030000\n",
      "Train Epoch: 1451 [106496/194182 (54%)]\tLoss: 0.244468\tGrad Norm: 1.497599\tLR: 0.030000\n",
      "Train Epoch: 1451 [126976/194182 (65%)]\tLoss: 0.237771\tGrad Norm: 1.365160\tLR: 0.030000\n",
      "Train Epoch: 1451 [147456/194182 (75%)]\tLoss: 0.248957\tGrad Norm: 1.942597\tLR: 0.030000\n",
      "Train Epoch: 1451 [167936/194182 (85%)]\tLoss: 0.253759\tGrad Norm: 2.019831\tLR: 0.030000\n",
      "Train Epoch: 1451 [188416/194182 (96%)]\tLoss: 0.246043\tGrad Norm: 1.728509\tLR: 0.030000\n",
      "Train set: Average loss: 0.2452\n",
      "Test set: Average loss: 0.2449, Average MAE: 0.3376\n",
      "Train Epoch: 1452 [4096/194182 (2%)]\tLoss: 0.242178\tGrad Norm: 1.245815\tLR: 0.030000\n",
      "Train Epoch: 1452 [24576/194182 (12%)]\tLoss: 0.238370\tGrad Norm: 1.352014\tLR: 0.030000\n",
      "Train Epoch: 1452 [45056/194182 (23%)]\tLoss: 0.243537\tGrad Norm: 1.541329\tLR: 0.030000\n",
      "Train Epoch: 1452 [65536/194182 (33%)]\tLoss: 0.246544\tGrad Norm: 1.741463\tLR: 0.030000\n",
      "Train Epoch: 1452 [86016/194182 (44%)]\tLoss: 0.241522\tGrad Norm: 1.610533\tLR: 0.030000\n",
      "Train Epoch: 1452 [106496/194182 (54%)]\tLoss: 0.240320\tGrad Norm: 1.344878\tLR: 0.030000\n",
      "Train Epoch: 1452 [126976/194182 (65%)]\tLoss: 0.247895\tGrad Norm: 1.569342\tLR: 0.030000\n",
      "Train Epoch: 1452 [147456/194182 (75%)]\tLoss: 0.245612\tGrad Norm: 1.626937\tLR: 0.030000\n",
      "Train Epoch: 1452 [167936/194182 (85%)]\tLoss: 0.246528\tGrad Norm: 1.684020\tLR: 0.030000\n",
      "Train Epoch: 1452 [188416/194182 (96%)]\tLoss: 0.239467\tGrad Norm: 1.645656\tLR: 0.030000\n",
      "Train set: Average loss: 0.2437\n",
      "Test set: Average loss: 0.2558, Average MAE: 0.3386\n",
      "Train Epoch: 1453 [4096/194182 (2%)]\tLoss: 0.253089\tGrad Norm: 1.956785\tLR: 0.030000\n",
      "Train Epoch: 1453 [24576/194182 (12%)]\tLoss: 0.244266\tGrad Norm: 1.876903\tLR: 0.030000\n",
      "Train Epoch: 1453 [45056/194182 (23%)]\tLoss: 0.248033\tGrad Norm: 1.914294\tLR: 0.030000\n",
      "Train Epoch: 1453 [65536/194182 (33%)]\tLoss: 0.241466\tGrad Norm: 1.516117\tLR: 0.030000\n",
      "Train Epoch: 1453 [86016/194182 (44%)]\tLoss: 0.253513\tGrad Norm: 1.668865\tLR: 0.030000\n",
      "Train Epoch: 1453 [106496/194182 (54%)]\tLoss: 0.245568\tGrad Norm: 1.529579\tLR: 0.030000\n",
      "Train Epoch: 1453 [126976/194182 (65%)]\tLoss: 0.238001\tGrad Norm: 1.566660\tLR: 0.030000\n",
      "Train Epoch: 1453 [147456/194182 (75%)]\tLoss: 0.247879\tGrad Norm: 1.704318\tLR: 0.030000\n",
      "Train Epoch: 1453 [167936/194182 (85%)]\tLoss: 0.249303\tGrad Norm: 1.635704\tLR: 0.030000\n",
      "Train Epoch: 1453 [188416/194182 (96%)]\tLoss: 0.242553\tGrad Norm: 1.560647\tLR: 0.030000\n",
      "Train set: Average loss: 0.2461\n",
      "Test set: Average loss: 0.2507, Average MAE: 0.3485\n",
      "Train Epoch: 1454 [4096/194182 (2%)]\tLoss: 0.236259\tGrad Norm: 1.545480\tLR: 0.030000\n",
      "Train Epoch: 1454 [24576/194182 (12%)]\tLoss: 0.239683\tGrad Norm: 1.395584\tLR: 0.030000\n",
      "Train Epoch: 1454 [45056/194182 (23%)]\tLoss: 0.247320\tGrad Norm: 1.680317\tLR: 0.030000\n",
      "Train Epoch: 1454 [65536/194182 (33%)]\tLoss: 0.243874\tGrad Norm: 1.571009\tLR: 0.030000\n",
      "Train Epoch: 1454 [86016/194182 (44%)]\tLoss: 0.244524\tGrad Norm: 1.644257\tLR: 0.030000\n",
      "Train Epoch: 1454 [106496/194182 (54%)]\tLoss: 0.247098\tGrad Norm: 1.510408\tLR: 0.030000\n",
      "Train Epoch: 1454 [126976/194182 (65%)]\tLoss: 0.254433\tGrad Norm: 1.954295\tLR: 0.030000\n",
      "Train Epoch: 1454 [147456/194182 (75%)]\tLoss: 0.247962\tGrad Norm: 1.844631\tLR: 0.030000\n",
      "Train Epoch: 1454 [167936/194182 (85%)]\tLoss: 0.242370\tGrad Norm: 1.736880\tLR: 0.030000\n",
      "Train Epoch: 1454 [188416/194182 (96%)]\tLoss: 0.247789\tGrad Norm: 2.285346\tLR: 0.030000\n",
      "Train set: Average loss: 0.2461\n",
      "Test set: Average loss: 0.2524, Average MAE: 0.3542\n",
      "Train Epoch: 1455 [4096/194182 (2%)]\tLoss: 0.241548\tGrad Norm: 1.648460\tLR: 0.030000\n",
      "Train Epoch: 1455 [24576/194182 (12%)]\tLoss: 0.249817\tGrad Norm: 1.621497\tLR: 0.030000\n",
      "Train Epoch: 1455 [45056/194182 (23%)]\tLoss: 0.238959\tGrad Norm: 1.635343\tLR: 0.030000\n",
      "Train Epoch: 1455 [65536/194182 (33%)]\tLoss: 0.252540\tGrad Norm: 1.829027\tLR: 0.030000\n",
      "Train Epoch: 1455 [86016/194182 (44%)]\tLoss: 0.242212\tGrad Norm: 1.514646\tLR: 0.030000\n",
      "Train Epoch: 1455 [106496/194182 (54%)]\tLoss: 0.250897\tGrad Norm: 1.552282\tLR: 0.030000\n",
      "Train Epoch: 1455 [126976/194182 (65%)]\tLoss: 0.245254\tGrad Norm: 1.470682\tLR: 0.030000\n",
      "Train Epoch: 1455 [147456/194182 (75%)]\tLoss: 0.245544\tGrad Norm: 1.693257\tLR: 0.030000\n",
      "Train Epoch: 1455 [167936/194182 (85%)]\tLoss: 0.242413\tGrad Norm: 1.370220\tLR: 0.030000\n",
      "Train Epoch: 1455 [188416/194182 (96%)]\tLoss: 0.238100\tGrad Norm: 1.309472\tLR: 0.030000\n",
      "Train set: Average loss: 0.2443\n",
      "Test set: Average loss: 0.2484, Average MAE: 0.3464\n",
      "Epoch 1455: Mean reward = 0.052 +/- 0.017\n",
      "Train Epoch: 1456 [4096/194182 (2%)]\tLoss: 0.243711\tGrad Norm: 1.365529\tLR: 0.030000\n",
      "Train Epoch: 1456 [24576/194182 (12%)]\tLoss: 0.243242\tGrad Norm: 1.862183\tLR: 0.030000\n",
      "Train Epoch: 1456 [45056/194182 (23%)]\tLoss: 0.238397\tGrad Norm: 1.496142\tLR: 0.030000\n",
      "Train Epoch: 1456 [65536/194182 (33%)]\tLoss: 0.237195\tGrad Norm: 1.444492\tLR: 0.030000\n",
      "Train Epoch: 1456 [86016/194182 (44%)]\tLoss: 0.248958\tGrad Norm: 1.823468\tLR: 0.030000\n",
      "Train Epoch: 1456 [106496/194182 (54%)]\tLoss: 0.247536\tGrad Norm: 2.019128\tLR: 0.030000\n",
      "Train Epoch: 1456 [126976/194182 (65%)]\tLoss: 0.242173\tGrad Norm: 1.687225\tLR: 0.030000\n",
      "Train Epoch: 1456 [147456/194182 (75%)]\tLoss: 0.250213\tGrad Norm: 1.665657\tLR: 0.030000\n",
      "Train Epoch: 1456 [167936/194182 (85%)]\tLoss: 0.239629\tGrad Norm: 1.096020\tLR: 0.030000\n",
      "Train Epoch: 1456 [188416/194182 (96%)]\tLoss: 0.236569\tGrad Norm: 1.285256\tLR: 0.030000\n",
      "Train set: Average loss: 0.2438\n",
      "Test set: Average loss: 0.2523, Average MAE: 0.3533\n",
      "Train Epoch: 1457 [4096/194182 (2%)]\tLoss: 0.239521\tGrad Norm: 1.624731\tLR: 0.030000\n",
      "Train Epoch: 1457 [24576/194182 (12%)]\tLoss: 0.242871\tGrad Norm: 1.573851\tLR: 0.030000\n",
      "Train Epoch: 1457 [45056/194182 (23%)]\tLoss: 0.252166\tGrad Norm: 1.847793\tLR: 0.030000\n",
      "Train Epoch: 1457 [65536/194182 (33%)]\tLoss: 0.245546\tGrad Norm: 1.605558\tLR: 0.030000\n",
      "Train Epoch: 1457 [86016/194182 (44%)]\tLoss: 0.235665\tGrad Norm: 1.502191\tLR: 0.030000\n",
      "Train Epoch: 1457 [106496/194182 (54%)]\tLoss: 0.244152\tGrad Norm: 1.516218\tLR: 0.030000\n",
      "Train Epoch: 1457 [126976/194182 (65%)]\tLoss: 0.245920\tGrad Norm: 1.671717\tLR: 0.030000\n",
      "Train Epoch: 1457 [147456/194182 (75%)]\tLoss: 0.250153\tGrad Norm: 1.953620\tLR: 0.030000\n",
      "Train Epoch: 1457 [167936/194182 (85%)]\tLoss: 0.251415\tGrad Norm: 1.846227\tLR: 0.030000\n",
      "Train Epoch: 1457 [188416/194182 (96%)]\tLoss: 0.248272\tGrad Norm: 1.606409\tLR: 0.030000\n",
      "Train set: Average loss: 0.2465\n",
      "Test set: Average loss: 0.2502, Average MAE: 0.3510\n",
      "Train Epoch: 1458 [4096/194182 (2%)]\tLoss: 0.238949\tGrad Norm: 1.447337\tLR: 0.030000\n",
      "Train Epoch: 1458 [24576/194182 (12%)]\tLoss: 0.236004\tGrad Norm: 1.436846\tLR: 0.030000\n",
      "Train Epoch: 1458 [45056/194182 (23%)]\tLoss: 0.249183\tGrad Norm: 1.624445\tLR: 0.030000\n",
      "Train Epoch: 1458 [65536/194182 (33%)]\tLoss: 0.242567\tGrad Norm: 1.684664\tLR: 0.030000\n",
      "Train Epoch: 1458 [86016/194182 (44%)]\tLoss: 0.242013\tGrad Norm: 1.435409\tLR: 0.030000\n",
      "Train Epoch: 1458 [106496/194182 (54%)]\tLoss: 0.239665\tGrad Norm: 1.633054\tLR: 0.030000\n",
      "Train Epoch: 1458 [126976/194182 (65%)]\tLoss: 0.242449\tGrad Norm: 1.495593\tLR: 0.030000\n",
      "Train Epoch: 1458 [147456/194182 (75%)]\tLoss: 0.241604\tGrad Norm: 1.459184\tLR: 0.030000\n",
      "Train Epoch: 1458 [167936/194182 (85%)]\tLoss: 0.248006\tGrad Norm: 1.788508\tLR: 0.030000\n",
      "Train Epoch: 1458 [188416/194182 (96%)]\tLoss: 0.247741\tGrad Norm: 2.111973\tLR: 0.030000\n",
      "Train set: Average loss: 0.2442\n",
      "Test set: Average loss: 0.2593, Average MAE: 0.3657\n",
      "Train Epoch: 1459 [4096/194182 (2%)]\tLoss: 0.250796\tGrad Norm: 2.093820\tLR: 0.030000\n",
      "Train Epoch: 1459 [24576/194182 (12%)]\tLoss: 0.238888\tGrad Norm: 1.799292\tLR: 0.030000\n",
      "Train Epoch: 1459 [45056/194182 (23%)]\tLoss: 0.239378\tGrad Norm: 1.448442\tLR: 0.030000\n",
      "Train Epoch: 1459 [65536/194182 (33%)]\tLoss: 0.248627\tGrad Norm: 1.653475\tLR: 0.030000\n",
      "Train Epoch: 1459 [86016/194182 (44%)]\tLoss: 0.234384\tGrad Norm: 0.999031\tLR: 0.030000\n",
      "Train Epoch: 1459 [106496/194182 (54%)]\tLoss: 0.235957\tGrad Norm: 0.872810\tLR: 0.030000\n",
      "Train Epoch: 1459 [126976/194182 (65%)]\tLoss: 0.244591\tGrad Norm: 1.568588\tLR: 0.030000\n",
      "Train Epoch: 1459 [147456/194182 (75%)]\tLoss: 0.241578\tGrad Norm: 1.792441\tLR: 0.030000\n",
      "Train Epoch: 1459 [167936/194182 (85%)]\tLoss: 0.248048\tGrad Norm: 1.480992\tLR: 0.030000\n",
      "Train Epoch: 1459 [188416/194182 (96%)]\tLoss: 0.240715\tGrad Norm: 1.284288\tLR: 0.030000\n",
      "Train set: Average loss: 0.2430\n",
      "Test set: Average loss: 0.2475, Average MAE: 0.3461\n",
      "Train Epoch: 1460 [4096/194182 (2%)]\tLoss: 0.238648\tGrad Norm: 1.264032\tLR: 0.030000\n",
      "Train Epoch: 1460 [24576/194182 (12%)]\tLoss: 0.242302\tGrad Norm: 1.424964\tLR: 0.030000\n",
      "Train Epoch: 1460 [45056/194182 (23%)]\tLoss: 0.246059\tGrad Norm: 1.723067\tLR: 0.030000\n",
      "Train Epoch: 1460 [65536/194182 (33%)]\tLoss: 0.241801\tGrad Norm: 1.470495\tLR: 0.030000\n",
      "Train Epoch: 1460 [86016/194182 (44%)]\tLoss: 0.243102\tGrad Norm: 1.651042\tLR: 0.030000\n",
      "Train Epoch: 1460 [106496/194182 (54%)]\tLoss: 0.245182\tGrad Norm: 1.719355\tLR: 0.030000\n",
      "Train Epoch: 1460 [126976/194182 (65%)]\tLoss: 0.248868\tGrad Norm: 1.836089\tLR: 0.030000\n",
      "Train Epoch: 1460 [147456/194182 (75%)]\tLoss: 0.243093\tGrad Norm: 1.844089\tLR: 0.030000\n",
      "Train Epoch: 1460 [167936/194182 (85%)]\tLoss: 0.244999\tGrad Norm: 1.682219\tLR: 0.030000\n",
      "Train Epoch: 1460 [188416/194182 (96%)]\tLoss: 0.243481\tGrad Norm: 1.592497\tLR: 0.030000\n",
      "Train set: Average loss: 0.2444\n",
      "Test set: Average loss: 0.2544, Average MAE: 0.3390\n",
      "Epoch 1460: Mean reward = 0.071 +/- 0.064\n",
      "Train Epoch: 1461 [4096/194182 (2%)]\tLoss: 0.253616\tGrad Norm: 2.214438\tLR: 0.030000\n",
      "Train Epoch: 1461 [24576/194182 (12%)]\tLoss: 0.236464\tGrad Norm: 1.124169\tLR: 0.030000\n",
      "Train Epoch: 1461 [45056/194182 (23%)]\tLoss: 0.228054\tGrad Norm: 1.046915\tLR: 0.030000\n",
      "Train Epoch: 1461 [65536/194182 (33%)]\tLoss: 0.248010\tGrad Norm: 1.471330\tLR: 0.030000\n",
      "Train Epoch: 1461 [86016/194182 (44%)]\tLoss: 0.242593\tGrad Norm: 1.609923\tLR: 0.030000\n",
      "Train Epoch: 1461 [106496/194182 (54%)]\tLoss: 0.253858\tGrad Norm: 1.766465\tLR: 0.030000\n",
      "Train Epoch: 1461 [126976/194182 (65%)]\tLoss: 0.252131\tGrad Norm: 2.083600\tLR: 0.030000\n",
      "Train Epoch: 1461 [147456/194182 (75%)]\tLoss: 0.254022\tGrad Norm: 1.945845\tLR: 0.030000\n",
      "Train Epoch: 1461 [167936/194182 (85%)]\tLoss: 0.244370\tGrad Norm: 1.652268\tLR: 0.030000\n",
      "Train Epoch: 1461 [188416/194182 (96%)]\tLoss: 0.244952\tGrad Norm: 1.667524\tLR: 0.030000\n",
      "Train set: Average loss: 0.2455\n",
      "Test set: Average loss: 0.2562, Average MAE: 0.3572\n",
      "Train Epoch: 1462 [4096/194182 (2%)]\tLoss: 0.249698\tGrad Norm: 1.843349\tLR: 0.030000\n",
      "Train Epoch: 1462 [24576/194182 (12%)]\tLoss: 0.245346\tGrad Norm: 1.612903\tLR: 0.030000\n",
      "Train Epoch: 1462 [45056/194182 (23%)]\tLoss: 0.234216\tGrad Norm: 1.419286\tLR: 0.030000\n",
      "Train Epoch: 1462 [65536/194182 (33%)]\tLoss: 0.250472\tGrad Norm: 1.881427\tLR: 0.030000\n",
      "Train Epoch: 1462 [86016/194182 (44%)]\tLoss: 0.241167\tGrad Norm: 1.506879\tLR: 0.030000\n",
      "Train Epoch: 1462 [106496/194182 (54%)]\tLoss: 0.245557\tGrad Norm: 1.499261\tLR: 0.030000\n",
      "Train Epoch: 1462 [126976/194182 (65%)]\tLoss: 0.243272\tGrad Norm: 1.519468\tLR: 0.030000\n",
      "Train Epoch: 1462 [147456/194182 (75%)]\tLoss: 0.245052\tGrad Norm: 1.537081\tLR: 0.030000\n",
      "Train Epoch: 1462 [167936/194182 (85%)]\tLoss: 0.253613\tGrad Norm: 1.608579\tLR: 0.030000\n",
      "Train Epoch: 1462 [188416/194182 (96%)]\tLoss: 0.240416\tGrad Norm: 1.583211\tLR: 0.030000\n",
      "Train set: Average loss: 0.2438\n",
      "Test set: Average loss: 0.2561, Average MAE: 0.3529\n",
      "Train Epoch: 1463 [4096/194182 (2%)]\tLoss: 0.244113\tGrad Norm: 1.789584\tLR: 0.030000\n",
      "Train Epoch: 1463 [24576/194182 (12%)]\tLoss: 0.240229\tGrad Norm: 1.485275\tLR: 0.030000\n",
      "Train Epoch: 1463 [45056/194182 (23%)]\tLoss: 0.249862\tGrad Norm: 1.923925\tLR: 0.030000\n",
      "Train Epoch: 1463 [65536/194182 (33%)]\tLoss: 0.241976\tGrad Norm: 1.391612\tLR: 0.030000\n",
      "Train Epoch: 1463 [86016/194182 (44%)]\tLoss: 0.239288\tGrad Norm: 1.545039\tLR: 0.030000\n",
      "Train Epoch: 1463 [106496/194182 (54%)]\tLoss: 0.236196\tGrad Norm: 1.190066\tLR: 0.030000\n",
      "Train Epoch: 1463 [126976/194182 (65%)]\tLoss: 0.249217\tGrad Norm: 1.964321\tLR: 0.030000\n",
      "Train Epoch: 1463 [147456/194182 (75%)]\tLoss: 0.245027\tGrad Norm: 1.727780\tLR: 0.030000\n",
      "Train Epoch: 1463 [167936/194182 (85%)]\tLoss: 0.247674\tGrad Norm: 1.741468\tLR: 0.030000\n",
      "Train Epoch: 1463 [188416/194182 (96%)]\tLoss: 0.245980\tGrad Norm: 1.696318\tLR: 0.030000\n",
      "Train set: Average loss: 0.2444\n",
      "Test set: Average loss: 0.2508, Average MAE: 0.3431\n",
      "Train Epoch: 1464 [4096/194182 (2%)]\tLoss: 0.236524\tGrad Norm: 1.416783\tLR: 0.030000\n",
      "Train Epoch: 1464 [24576/194182 (12%)]\tLoss: 0.239470\tGrad Norm: 1.606628\tLR: 0.030000\n",
      "Train Epoch: 1464 [45056/194182 (23%)]\tLoss: 0.243074\tGrad Norm: 1.737451\tLR: 0.030000\n",
      "Train Epoch: 1464 [65536/194182 (33%)]\tLoss: 0.246000\tGrad Norm: 1.827203\tLR: 0.030000\n",
      "Train Epoch: 1464 [86016/194182 (44%)]\tLoss: 0.247327\tGrad Norm: 1.628617\tLR: 0.030000\n",
      "Train Epoch: 1464 [106496/194182 (54%)]\tLoss: 0.250358\tGrad Norm: 2.057159\tLR: 0.030000\n",
      "Train Epoch: 1464 [126976/194182 (65%)]\tLoss: 0.252775\tGrad Norm: 2.066653\tLR: 0.030000\n",
      "Train Epoch: 1464 [147456/194182 (75%)]\tLoss: 0.244288\tGrad Norm: 1.754678\tLR: 0.030000\n",
      "Train Epoch: 1464 [167936/194182 (85%)]\tLoss: 0.237902\tGrad Norm: 1.422466\tLR: 0.030000\n",
      "Train Epoch: 1464 [188416/194182 (96%)]\tLoss: 0.243718\tGrad Norm: 1.504677\tLR: 0.030000\n",
      "Train set: Average loss: 0.2451\n",
      "Test set: Average loss: 0.2488, Average MAE: 0.3430\n",
      "Train Epoch: 1465 [4096/194182 (2%)]\tLoss: 0.242529\tGrad Norm: 1.485265\tLR: 0.030000\n",
      "Train Epoch: 1465 [24576/194182 (12%)]\tLoss: 0.242140\tGrad Norm: 1.777528\tLR: 0.030000\n",
      "Train Epoch: 1465 [45056/194182 (23%)]\tLoss: 0.240066\tGrad Norm: 1.445120\tLR: 0.030000\n",
      "Train Epoch: 1465 [65536/194182 (33%)]\tLoss: 0.249286\tGrad Norm: 1.781143\tLR: 0.030000\n",
      "Train Epoch: 1465 [86016/194182 (44%)]\tLoss: 0.239067\tGrad Norm: 1.670114\tLR: 0.030000\n",
      "Train Epoch: 1465 [106496/194182 (54%)]\tLoss: 0.243630\tGrad Norm: 1.726307\tLR: 0.030000\n",
      "Train Epoch: 1465 [126976/194182 (65%)]\tLoss: 0.245014\tGrad Norm: 1.673802\tLR: 0.030000\n",
      "Train Epoch: 1465 [147456/194182 (75%)]\tLoss: 0.241635\tGrad Norm: 1.408529\tLR: 0.030000\n",
      "Train Epoch: 1465 [167936/194182 (85%)]\tLoss: 0.249955\tGrad Norm: 1.402946\tLR: 0.030000\n",
      "Train Epoch: 1465 [188416/194182 (96%)]\tLoss: 0.248521\tGrad Norm: 1.755936\tLR: 0.030000\n",
      "Train set: Average loss: 0.2442\n",
      "Test set: Average loss: 0.2556, Average MAE: 0.3464\n",
      "Epoch 1465: Mean reward = 0.057 +/- 0.067\n",
      "Train Epoch: 1466 [4096/194182 (2%)]\tLoss: 0.249656\tGrad Norm: 1.734837\tLR: 0.030000\n",
      "Train Epoch: 1466 [24576/194182 (12%)]\tLoss: 0.237264\tGrad Norm: 1.477613\tLR: 0.030000\n",
      "Train Epoch: 1466 [45056/194182 (23%)]\tLoss: 0.241731\tGrad Norm: 1.444595\tLR: 0.030000\n",
      "Train Epoch: 1466 [65536/194182 (33%)]\tLoss: 0.237392\tGrad Norm: 1.289456\tLR: 0.030000\n",
      "Train Epoch: 1466 [86016/194182 (44%)]\tLoss: 0.240207\tGrad Norm: 1.531119\tLR: 0.030000\n",
      "Train Epoch: 1466 [106496/194182 (54%)]\tLoss: 0.238746\tGrad Norm: 1.458911\tLR: 0.030000\n",
      "Train Epoch: 1466 [126976/194182 (65%)]\tLoss: 0.248193\tGrad Norm: 1.860376\tLR: 0.030000\n",
      "Train Epoch: 1466 [147456/194182 (75%)]\tLoss: 0.242832\tGrad Norm: 1.776513\tLR: 0.030000\n",
      "Train Epoch: 1466 [167936/194182 (85%)]\tLoss: 0.243692\tGrad Norm: 1.924665\tLR: 0.030000\n",
      "Train Epoch: 1466 [188416/194182 (96%)]\tLoss: 0.250421\tGrad Norm: 2.078175\tLR: 0.030000\n",
      "Train set: Average loss: 0.2427\n",
      "Test set: Average loss: 0.2503, Average MAE: 0.3556\n",
      "Train Epoch: 1467 [4096/194182 (2%)]\tLoss: 0.236171\tGrad Norm: 1.408265\tLR: 0.030000\n",
      "Train Epoch: 1467 [24576/194182 (12%)]\tLoss: 0.235398\tGrad Norm: 1.562437\tLR: 0.030000\n",
      "Train Epoch: 1467 [45056/194182 (23%)]\tLoss: 0.243486\tGrad Norm: 1.576817\tLR: 0.030000\n",
      "Train Epoch: 1467 [65536/194182 (33%)]\tLoss: 0.249473\tGrad Norm: 1.871314\tLR: 0.030000\n",
      "Train Epoch: 1467 [86016/194182 (44%)]\tLoss: 0.243896\tGrad Norm: 1.702795\tLR: 0.030000\n",
      "Train Epoch: 1467 [106496/194182 (54%)]\tLoss: 0.249388\tGrad Norm: 1.760540\tLR: 0.030000\n",
      "Train Epoch: 1467 [126976/194182 (65%)]\tLoss: 0.242483\tGrad Norm: 1.584884\tLR: 0.030000\n",
      "Train Epoch: 1467 [147456/194182 (75%)]\tLoss: 0.242909\tGrad Norm: 1.307464\tLR: 0.030000\n",
      "Train Epoch: 1467 [167936/194182 (85%)]\tLoss: 0.244504\tGrad Norm: 1.521196\tLR: 0.030000\n",
      "Train Epoch: 1467 [188416/194182 (96%)]\tLoss: 0.236477\tGrad Norm: 1.477447\tLR: 0.030000\n",
      "Train set: Average loss: 0.2424\n",
      "Test set: Average loss: 0.2526, Average MAE: 0.3475\n",
      "Train Epoch: 1468 [4096/194182 (2%)]\tLoss: 0.241353\tGrad Norm: 1.590593\tLR: 0.030000\n",
      "Train Epoch: 1468 [24576/194182 (12%)]\tLoss: 0.242577\tGrad Norm: 1.664754\tLR: 0.030000\n",
      "Train Epoch: 1468 [45056/194182 (23%)]\tLoss: 0.248236\tGrad Norm: 1.678594\tLR: 0.030000\n",
      "Train Epoch: 1468 [65536/194182 (33%)]\tLoss: 0.247725\tGrad Norm: 1.502036\tLR: 0.030000\n",
      "Train Epoch: 1468 [86016/194182 (44%)]\tLoss: 0.248440\tGrad Norm: 1.775099\tLR: 0.030000\n",
      "Train Epoch: 1468 [106496/194182 (54%)]\tLoss: 0.243424\tGrad Norm: 1.263117\tLR: 0.030000\n",
      "Train Epoch: 1468 [126976/194182 (65%)]\tLoss: 0.245192\tGrad Norm: 1.734403\tLR: 0.030000\n",
      "Train Epoch: 1468 [147456/194182 (75%)]\tLoss: 0.244255\tGrad Norm: 1.993046\tLR: 0.030000\n",
      "Train Epoch: 1468 [167936/194182 (85%)]\tLoss: 0.248617\tGrad Norm: 2.117184\tLR: 0.030000\n",
      "Train Epoch: 1468 [188416/194182 (96%)]\tLoss: 0.237576\tGrad Norm: 1.704922\tLR: 0.030000\n",
      "Train set: Average loss: 0.2455\n",
      "Test set: Average loss: 0.2546, Average MAE: 0.3409\n",
      "Train Epoch: 1469 [4096/194182 (2%)]\tLoss: 0.247439\tGrad Norm: 2.015434\tLR: 0.030000\n",
      "Train Epoch: 1469 [24576/194182 (12%)]\tLoss: 0.242316\tGrad Norm: 1.504667\tLR: 0.030000\n",
      "Train Epoch: 1469 [45056/194182 (23%)]\tLoss: 0.249451\tGrad Norm: 1.924469\tLR: 0.030000\n",
      "Train Epoch: 1469 [65536/194182 (33%)]\tLoss: 0.240875\tGrad Norm: 1.813416\tLR: 0.030000\n",
      "Train Epoch: 1469 [86016/194182 (44%)]\tLoss: 0.243480\tGrad Norm: 1.738382\tLR: 0.030000\n",
      "Train Epoch: 1469 [106496/194182 (54%)]\tLoss: 0.247217\tGrad Norm: 1.677309\tLR: 0.030000\n",
      "Train Epoch: 1469 [126976/194182 (65%)]\tLoss: 0.241532\tGrad Norm: 1.400920\tLR: 0.030000\n",
      "Train Epoch: 1469 [147456/194182 (75%)]\tLoss: 0.252835\tGrad Norm: 1.781092\tLR: 0.030000\n",
      "Train Epoch: 1469 [167936/194182 (85%)]\tLoss: 0.237026\tGrad Norm: 1.438983\tLR: 0.030000\n",
      "Train Epoch: 1469 [188416/194182 (96%)]\tLoss: 0.241307\tGrad Norm: 1.701077\tLR: 0.030000\n",
      "Train set: Average loss: 0.2420\n",
      "Test set: Average loss: 0.2473, Average MAE: 0.3448\n",
      "Train Epoch: 1470 [4096/194182 (2%)]\tLoss: 0.236185\tGrad Norm: 1.335038\tLR: 0.030000\n",
      "Train Epoch: 1470 [24576/194182 (12%)]\tLoss: 0.235704\tGrad Norm: 1.547630\tLR: 0.030000\n",
      "Train Epoch: 1470 [45056/194182 (23%)]\tLoss: 0.239243\tGrad Norm: 1.740233\tLR: 0.030000\n",
      "Train Epoch: 1470 [65536/194182 (33%)]\tLoss: 0.237431\tGrad Norm: 1.718163\tLR: 0.030000\n",
      "Train Epoch: 1470 [86016/194182 (44%)]\tLoss: 0.245998\tGrad Norm: 1.693981\tLR: 0.030000\n",
      "Train Epoch: 1470 [106496/194182 (54%)]\tLoss: 0.242482\tGrad Norm: 1.372789\tLR: 0.030000\n",
      "Train Epoch: 1470 [126976/194182 (65%)]\tLoss: 0.240936\tGrad Norm: 1.380902\tLR: 0.030000\n",
      "Train Epoch: 1470 [147456/194182 (75%)]\tLoss: 0.237847\tGrad Norm: 1.403779\tLR: 0.030000\n",
      "Train Epoch: 1470 [167936/194182 (85%)]\tLoss: 0.250277\tGrad Norm: 1.899559\tLR: 0.030000\n",
      "Train Epoch: 1470 [188416/194182 (96%)]\tLoss: 0.244674\tGrad Norm: 1.696829\tLR: 0.030000\n",
      "Train set: Average loss: 0.2434\n",
      "Test set: Average loss: 0.2563, Average MAE: 0.3569\n",
      "Epoch 1470: Mean reward = 0.050 +/- 0.045\n",
      "Train Epoch: 1471 [4096/194182 (2%)]\tLoss: 0.239153\tGrad Norm: 1.722338\tLR: 0.030000\n",
      "Train Epoch: 1471 [24576/194182 (12%)]\tLoss: 0.243968\tGrad Norm: 1.522308\tLR: 0.030000\n",
      "Train Epoch: 1471 [45056/194182 (23%)]\tLoss: 0.244269\tGrad Norm: 1.617069\tLR: 0.030000\n",
      "Train Epoch: 1471 [65536/194182 (33%)]\tLoss: 0.247525\tGrad Norm: 1.985962\tLR: 0.030000\n",
      "Train Epoch: 1471 [86016/194182 (44%)]\tLoss: 0.248692\tGrad Norm: 1.712052\tLR: 0.030000\n",
      "Train Epoch: 1471 [106496/194182 (54%)]\tLoss: 0.243500\tGrad Norm: 1.643863\tLR: 0.030000\n",
      "Train Epoch: 1471 [126976/194182 (65%)]\tLoss: 0.245079\tGrad Norm: 1.792938\tLR: 0.030000\n",
      "Train Epoch: 1471 [147456/194182 (75%)]\tLoss: 0.242721\tGrad Norm: 1.527631\tLR: 0.030000\n",
      "Train Epoch: 1471 [167936/194182 (85%)]\tLoss: 0.239036\tGrad Norm: 1.607812\tLR: 0.030000\n",
      "Train Epoch: 1471 [188416/194182 (96%)]\tLoss: 0.241805\tGrad Norm: 1.546058\tLR: 0.030000\n",
      "Train set: Average loss: 0.2437\n",
      "Test set: Average loss: 0.2601, Average MAE: 0.3601\n",
      "Train Epoch: 1472 [4096/194182 (2%)]\tLoss: 0.248337\tGrad Norm: 1.952491\tLR: 0.030000\n",
      "Train Epoch: 1472 [24576/194182 (12%)]\tLoss: 0.239552\tGrad Norm: 1.528862\tLR: 0.030000\n",
      "Train Epoch: 1472 [45056/194182 (23%)]\tLoss: 0.243024\tGrad Norm: 2.083823\tLR: 0.030000\n",
      "Train Epoch: 1472 [65536/194182 (33%)]\tLoss: 0.262818\tGrad Norm: 5.224642\tLR: 0.030000\n",
      "Train Epoch: 1472 [86016/194182 (44%)]\tLoss: 0.235029\tGrad Norm: 1.507902\tLR: 0.030000\n",
      "Train Epoch: 1472 [106496/194182 (54%)]\tLoss: 0.240119\tGrad Norm: 1.526616\tLR: 0.030000\n",
      "Train Epoch: 1472 [126976/194182 (65%)]\tLoss: 0.236914\tGrad Norm: 1.539762\tLR: 0.030000\n",
      "Train Epoch: 1472 [147456/194182 (75%)]\tLoss: 0.245979\tGrad Norm: 1.517652\tLR: 0.030000\n",
      "Train Epoch: 1472 [167936/194182 (85%)]\tLoss: 0.249121\tGrad Norm: 1.738935\tLR: 0.030000\n",
      "Train Epoch: 1472 [188416/194182 (96%)]\tLoss: 0.237029\tGrad Norm: 1.308901\tLR: 0.030000\n",
      "Train set: Average loss: 0.2442\n",
      "Test set: Average loss: 0.2541, Average MAE: 0.3434\n",
      "Train Epoch: 1473 [4096/194182 (2%)]\tLoss: 0.245298\tGrad Norm: 2.004241\tLR: 0.030000\n",
      "Train Epoch: 1473 [24576/194182 (12%)]\tLoss: 0.239734\tGrad Norm: 1.668356\tLR: 0.030000\n",
      "Train Epoch: 1473 [45056/194182 (23%)]\tLoss: 0.240327\tGrad Norm: 1.746268\tLR: 0.030000\n",
      "Train Epoch: 1473 [65536/194182 (33%)]\tLoss: 0.235812\tGrad Norm: 1.405931\tLR: 0.030000\n",
      "Train Epoch: 1473 [86016/194182 (44%)]\tLoss: 0.245275\tGrad Norm: 1.621295\tLR: 0.030000\n",
      "Train Epoch: 1473 [106496/194182 (54%)]\tLoss: 0.242356\tGrad Norm: 1.500147\tLR: 0.030000\n",
      "Train Epoch: 1473 [126976/194182 (65%)]\tLoss: 0.245472\tGrad Norm: 1.787514\tLR: 0.030000\n",
      "Train Epoch: 1473 [147456/194182 (75%)]\tLoss: 0.247715\tGrad Norm: 1.869730\tLR: 0.030000\n",
      "Train Epoch: 1473 [167936/194182 (85%)]\tLoss: 0.243632\tGrad Norm: 1.622372\tLR: 0.030000\n",
      "Train Epoch: 1473 [188416/194182 (96%)]\tLoss: 0.238867\tGrad Norm: 1.407830\tLR: 0.030000\n",
      "Train set: Average loss: 0.2427\n",
      "Test set: Average loss: 0.2498, Average MAE: 0.3481\n",
      "Train Epoch: 1474 [4096/194182 (2%)]\tLoss: 0.240217\tGrad Norm: 1.602631\tLR: 0.030000\n",
      "Train Epoch: 1474 [24576/194182 (12%)]\tLoss: 0.237908\tGrad Norm: 1.465044\tLR: 0.030000\n",
      "Train Epoch: 1474 [45056/194182 (23%)]\tLoss: 0.250290\tGrad Norm: 1.786801\tLR: 0.030000\n",
      "Train Epoch: 1474 [65536/194182 (33%)]\tLoss: 0.235830\tGrad Norm: 1.635733\tLR: 0.030000\n",
      "Train Epoch: 1474 [86016/194182 (44%)]\tLoss: 0.249212\tGrad Norm: 1.822796\tLR: 0.030000\n",
      "Train Epoch: 1474 [106496/194182 (54%)]\tLoss: 0.238806\tGrad Norm: 1.484522\tLR: 0.030000\n",
      "Train Epoch: 1474 [126976/194182 (65%)]\tLoss: 0.242734\tGrad Norm: 1.326314\tLR: 0.030000\n",
      "Train Epoch: 1474 [147456/194182 (75%)]\tLoss: 0.249888\tGrad Norm: 1.828514\tLR: 0.030000\n",
      "Train Epoch: 1474 [167936/194182 (85%)]\tLoss: 0.235652\tGrad Norm: 1.315821\tLR: 0.030000\n",
      "Train Epoch: 1474 [188416/194182 (96%)]\tLoss: 0.236952\tGrad Norm: 1.202534\tLR: 0.030000\n",
      "Train set: Average loss: 0.2412\n",
      "Test set: Average loss: 0.2519, Average MAE: 0.3512\n",
      "Train Epoch: 1475 [4096/194182 (2%)]\tLoss: 0.241820\tGrad Norm: 1.494974\tLR: 0.030000\n",
      "Train Epoch: 1475 [24576/194182 (12%)]\tLoss: 0.256693\tGrad Norm: 2.081908\tLR: 0.030000\n",
      "Train Epoch: 1475 [45056/194182 (23%)]\tLoss: 0.241492\tGrad Norm: 1.497601\tLR: 0.030000\n",
      "Train Epoch: 1475 [65536/194182 (33%)]\tLoss: 0.235545\tGrad Norm: 1.303720\tLR: 0.030000\n",
      "Train Epoch: 1475 [86016/194182 (44%)]\tLoss: 0.244864\tGrad Norm: 1.653046\tLR: 0.030000\n",
      "Train Epoch: 1475 [106496/194182 (54%)]\tLoss: 0.235716\tGrad Norm: 1.576793\tLR: 0.030000\n",
      "Train Epoch: 1475 [126976/194182 (65%)]\tLoss: 0.243468\tGrad Norm: 2.054297\tLR: 0.030000\n",
      "Train Epoch: 1475 [147456/194182 (75%)]\tLoss: 0.239435\tGrad Norm: 1.626617\tLR: 0.030000\n",
      "Train Epoch: 1475 [167936/194182 (85%)]\tLoss: 0.241971\tGrad Norm: 1.213057\tLR: 0.030000\n",
      "Train Epoch: 1475 [188416/194182 (96%)]\tLoss: 0.242799\tGrad Norm: 1.615387\tLR: 0.030000\n",
      "Train set: Average loss: 0.2423\n",
      "Test set: Average loss: 0.2562, Average MAE: 0.3510\n",
      "Epoch 1475: Mean reward = 0.060 +/- 0.043\n",
      "Train Epoch: 1476 [4096/194182 (2%)]\tLoss: 0.248277\tGrad Norm: 1.788735\tLR: 0.030000\n",
      "Train Epoch: 1476 [24576/194182 (12%)]\tLoss: 0.239881\tGrad Norm: 1.558413\tLR: 0.030000\n",
      "Train Epoch: 1476 [45056/194182 (23%)]\tLoss: 0.241229\tGrad Norm: 1.747867\tLR: 0.030000\n",
      "Train Epoch: 1476 [65536/194182 (33%)]\tLoss: 0.244142\tGrad Norm: 1.809305\tLR: 0.030000\n",
      "Train Epoch: 1476 [86016/194182 (44%)]\tLoss: 0.236245\tGrad Norm: 1.716741\tLR: 0.030000\n",
      "Train Epoch: 1476 [106496/194182 (54%)]\tLoss: 0.242293\tGrad Norm: 2.015765\tLR: 0.030000\n",
      "Train Epoch: 1476 [126976/194182 (65%)]\tLoss: 0.242533\tGrad Norm: 1.660288\tLR: 0.030000\n",
      "Train Epoch: 1476 [147456/194182 (75%)]\tLoss: 0.243429\tGrad Norm: 1.678080\tLR: 0.030000\n",
      "Train Epoch: 1476 [167936/194182 (85%)]\tLoss: 0.239516\tGrad Norm: 1.508522\tLR: 0.030000\n",
      "Train Epoch: 1476 [188416/194182 (96%)]\tLoss: 0.241406\tGrad Norm: 1.643776\tLR: 0.030000\n",
      "Train set: Average loss: 0.2432\n",
      "Test set: Average loss: 0.2537, Average MAE: 0.3496\n",
      "Train Epoch: 1477 [4096/194182 (2%)]\tLoss: 0.239085\tGrad Norm: 1.490653\tLR: 0.030000\n",
      "Train Epoch: 1477 [24576/194182 (12%)]\tLoss: 0.243906\tGrad Norm: 1.602171\tLR: 0.030000\n",
      "Train Epoch: 1477 [45056/194182 (23%)]\tLoss: 0.245314\tGrad Norm: 1.426290\tLR: 0.030000\n",
      "Train Epoch: 1477 [65536/194182 (33%)]\tLoss: 0.237731\tGrad Norm: 1.292575\tLR: 0.030000\n",
      "Train Epoch: 1477 [86016/194182 (44%)]\tLoss: 0.242784\tGrad Norm: 1.503005\tLR: 0.030000\n",
      "Train Epoch: 1477 [106496/194182 (54%)]\tLoss: 0.245442\tGrad Norm: 1.738099\tLR: 0.030000\n",
      "Train Epoch: 1477 [126976/194182 (65%)]\tLoss: 0.251375\tGrad Norm: 1.945363\tLR: 0.030000\n",
      "Train Epoch: 1477 [147456/194182 (75%)]\tLoss: 0.239151\tGrad Norm: 1.709092\tLR: 0.030000\n",
      "Train Epoch: 1477 [167936/194182 (85%)]\tLoss: 0.248503\tGrad Norm: 1.891210\tLR: 0.030000\n",
      "Train Epoch: 1477 [188416/194182 (96%)]\tLoss: 0.226965\tGrad Norm: 1.229963\tLR: 0.030000\n",
      "Train set: Average loss: 0.2412\n",
      "Test set: Average loss: 0.2529, Average MAE: 0.3504\n",
      "Train Epoch: 1478 [4096/194182 (2%)]\tLoss: 0.235949\tGrad Norm: 1.549206\tLR: 0.030000\n",
      "Train Epoch: 1478 [24576/194182 (12%)]\tLoss: 0.237770\tGrad Norm: 1.592500\tLR: 0.030000\n",
      "Train Epoch: 1478 [45056/194182 (23%)]\tLoss: 0.234294\tGrad Norm: 1.237015\tLR: 0.030000\n",
      "Train Epoch: 1478 [65536/194182 (33%)]\tLoss: 0.250005\tGrad Norm: 1.584322\tLR: 0.030000\n",
      "Train Epoch: 1478 [86016/194182 (44%)]\tLoss: 0.243439\tGrad Norm: 1.801410\tLR: 0.030000\n",
      "Train Epoch: 1478 [106496/194182 (54%)]\tLoss: 0.248260\tGrad Norm: 1.924624\tLR: 0.030000\n",
      "Train Epoch: 1478 [126976/194182 (65%)]\tLoss: 0.246093\tGrad Norm: 1.838966\tLR: 0.030000\n",
      "Train Epoch: 1478 [147456/194182 (75%)]\tLoss: 0.248332\tGrad Norm: 2.013545\tLR: 0.030000\n",
      "Train Epoch: 1478 [167936/194182 (85%)]\tLoss: 0.245525\tGrad Norm: 2.074801\tLR: 0.030000\n",
      "Train Epoch: 1478 [188416/194182 (96%)]\tLoss: 0.241068\tGrad Norm: 2.219857\tLR: 0.030000\n",
      "Train set: Average loss: 0.2443\n",
      "Test set: Average loss: 0.2628, Average MAE: 0.3663\n",
      "Train Epoch: 1479 [4096/194182 (2%)]\tLoss: 0.254624\tGrad Norm: 1.996125\tLR: 0.030000\n",
      "Train Epoch: 1479 [24576/194182 (12%)]\tLoss: 0.234337\tGrad Norm: 1.619404\tLR: 0.030000\n",
      "Train Epoch: 1479 [45056/194182 (23%)]\tLoss: 0.236422\tGrad Norm: 1.339543\tLR: 0.030000\n",
      "Train Epoch: 1479 [65536/194182 (33%)]\tLoss: 0.240501\tGrad Norm: 1.513928\tLR: 0.030000\n",
      "Train Epoch: 1479 [86016/194182 (44%)]\tLoss: 0.253970\tGrad Norm: 1.853384\tLR: 0.030000\n",
      "Train Epoch: 1479 [106496/194182 (54%)]\tLoss: 0.244914\tGrad Norm: 1.563950\tLR: 0.030000\n",
      "Train Epoch: 1479 [126976/194182 (65%)]\tLoss: 0.243350\tGrad Norm: 1.431974\tLR: 0.030000\n",
      "Train Epoch: 1479 [147456/194182 (75%)]\tLoss: 0.241455\tGrad Norm: 1.684891\tLR: 0.030000\n",
      "Train Epoch: 1479 [167936/194182 (85%)]\tLoss: 0.242801\tGrad Norm: 1.586694\tLR: 0.030000\n",
      "Train Epoch: 1479 [188416/194182 (96%)]\tLoss: 0.236086\tGrad Norm: 1.539794\tLR: 0.030000\n",
      "Train set: Average loss: 0.2410\n",
      "Test set: Average loss: 0.2493, Average MAE: 0.3445\n",
      "Train Epoch: 1480 [4096/194182 (2%)]\tLoss: 0.242509\tGrad Norm: 1.637521\tLR: 0.030000\n",
      "Train Epoch: 1480 [24576/194182 (12%)]\tLoss: 0.240066\tGrad Norm: 1.605883\tLR: 0.030000\n",
      "Train Epoch: 1480 [45056/194182 (23%)]\tLoss: 0.236965\tGrad Norm: 1.386892\tLR: 0.030000\n",
      "Train Epoch: 1480 [65536/194182 (33%)]\tLoss: 0.238753\tGrad Norm: 1.620207\tLR: 0.030000\n",
      "Train Epoch: 1480 [86016/194182 (44%)]\tLoss: 0.242345\tGrad Norm: 1.544560\tLR: 0.030000\n",
      "Train Epoch: 1480 [106496/194182 (54%)]\tLoss: 0.247176\tGrad Norm: 1.689183\tLR: 0.030000\n",
      "Train Epoch: 1480 [126976/194182 (65%)]\tLoss: 0.238334\tGrad Norm: 1.360452\tLR: 0.030000\n",
      "Train Epoch: 1480 [147456/194182 (75%)]\tLoss: 0.244190\tGrad Norm: 1.878134\tLR: 0.030000\n",
      "Train Epoch: 1480 [167936/194182 (85%)]\tLoss: 0.242342\tGrad Norm: 1.572021\tLR: 0.030000\n",
      "Train Epoch: 1480 [188416/194182 (96%)]\tLoss: 0.237210\tGrad Norm: 1.456934\tLR: 0.030000\n",
      "Train set: Average loss: 0.2419\n",
      "Test set: Average loss: 0.2529, Average MAE: 0.3456\n",
      "Epoch 1480: Mean reward = 0.037 +/- 0.018\n",
      "Train Epoch: 1481 [4096/194182 (2%)]\tLoss: 0.242504\tGrad Norm: 1.667532\tLR: 0.030000\n",
      "Train Epoch: 1481 [24576/194182 (12%)]\tLoss: 0.238512\tGrad Norm: 1.551111\tLR: 0.030000\n",
      "Train Epoch: 1481 [45056/194182 (23%)]\tLoss: 0.235932\tGrad Norm: 1.474757\tLR: 0.030000\n",
      "Train Epoch: 1481 [65536/194182 (33%)]\tLoss: 0.245763\tGrad Norm: 2.067118\tLR: 0.030000\n",
      "Train Epoch: 1481 [86016/194182 (44%)]\tLoss: 0.255033\tGrad Norm: 1.888686\tLR: 0.030000\n",
      "Train Epoch: 1481 [106496/194182 (54%)]\tLoss: 0.243402\tGrad Norm: 2.235164\tLR: 0.030000\n",
      "Train Epoch: 1481 [126976/194182 (65%)]\tLoss: 0.239178\tGrad Norm: 1.677957\tLR: 0.030000\n",
      "Train Epoch: 1481 [147456/194182 (75%)]\tLoss: 0.241729\tGrad Norm: 1.755025\tLR: 0.030000\n",
      "Train Epoch: 1481 [167936/194182 (85%)]\tLoss: 0.252612\tGrad Norm: 1.939644\tLR: 0.030000\n",
      "Train Epoch: 1481 [188416/194182 (96%)]\tLoss: 0.237469\tGrad Norm: 1.415842\tLR: 0.030000\n",
      "Train set: Average loss: 0.2423\n",
      "Test set: Average loss: 0.2506, Average MAE: 0.3499\n",
      "Train Epoch: 1482 [4096/194182 (2%)]\tLoss: 0.235117\tGrad Norm: 1.493035\tLR: 0.030000\n",
      "Train Epoch: 1482 [24576/194182 (12%)]\tLoss: 0.247630\tGrad Norm: 1.568576\tLR: 0.030000\n",
      "Train Epoch: 1482 [45056/194182 (23%)]\tLoss: 0.233633\tGrad Norm: 1.302590\tLR: 0.030000\n",
      "Train Epoch: 1482 [65536/194182 (33%)]\tLoss: 0.243929\tGrad Norm: 1.757201\tLR: 0.030000\n",
      "Train Epoch: 1482 [86016/194182 (44%)]\tLoss: 0.245912\tGrad Norm: 1.704536\tLR: 0.030000\n",
      "Train Epoch: 1482 [106496/194182 (54%)]\tLoss: 0.236333\tGrad Norm: 1.606145\tLR: 0.030000\n",
      "Train Epoch: 1482 [126976/194182 (65%)]\tLoss: 0.239942\tGrad Norm: 1.564792\tLR: 0.030000\n",
      "Train Epoch: 1482 [147456/194182 (75%)]\tLoss: 0.240548\tGrad Norm: 1.505828\tLR: 0.030000\n",
      "Train Epoch: 1482 [167936/194182 (85%)]\tLoss: 0.242557\tGrad Norm: 1.725024\tLR: 0.030000\n",
      "Train Epoch: 1482 [188416/194182 (96%)]\tLoss: 0.236955\tGrad Norm: 1.650345\tLR: 0.030000\n",
      "Train set: Average loss: 0.2406\n",
      "Test set: Average loss: 0.2486, Average MAE: 0.3401\n",
      "Train Epoch: 1483 [4096/194182 (2%)]\tLoss: 0.239084\tGrad Norm: 1.478488\tLR: 0.030000\n",
      "Train Epoch: 1483 [24576/194182 (12%)]\tLoss: 0.237968\tGrad Norm: 1.570434\tLR: 0.030000\n",
      "Train Epoch: 1483 [45056/194182 (23%)]\tLoss: 0.242391\tGrad Norm: 1.620154\tLR: 0.030000\n",
      "Train Epoch: 1483 [65536/194182 (33%)]\tLoss: 0.239830\tGrad Norm: 1.712484\tLR: 0.030000\n",
      "Train Epoch: 1483 [86016/194182 (44%)]\tLoss: 0.239962\tGrad Norm: 1.635834\tLR: 0.030000\n",
      "Train Epoch: 1483 [106496/194182 (54%)]\tLoss: 0.240050\tGrad Norm: 1.975647\tLR: 0.030000\n",
      "Train Epoch: 1483 [126976/194182 (65%)]\tLoss: 0.246057\tGrad Norm: 1.957208\tLR: 0.030000\n",
      "Train Epoch: 1483 [147456/194182 (75%)]\tLoss: 0.235821\tGrad Norm: 1.434765\tLR: 0.030000\n",
      "Train Epoch: 1483 [167936/194182 (85%)]\tLoss: 0.238391\tGrad Norm: 1.522850\tLR: 0.030000\n",
      "Train Epoch: 1483 [188416/194182 (96%)]\tLoss: 0.238470\tGrad Norm: 1.360766\tLR: 0.030000\n",
      "Train set: Average loss: 0.2407\n",
      "Test set: Average loss: 0.2553, Average MAE: 0.3429\n",
      "Train Epoch: 1484 [4096/194182 (2%)]\tLoss: 0.241154\tGrad Norm: 1.824518\tLR: 0.030000\n",
      "Train Epoch: 1484 [24576/194182 (12%)]\tLoss: 0.237833\tGrad Norm: 1.519093\tLR: 0.030000\n",
      "Train Epoch: 1484 [45056/194182 (23%)]\tLoss: 0.238091\tGrad Norm: 1.555940\tLR: 0.030000\n",
      "Train Epoch: 1484 [65536/194182 (33%)]\tLoss: 0.239549\tGrad Norm: 1.473875\tLR: 0.030000\n",
      "Train Epoch: 1484 [86016/194182 (44%)]\tLoss: 0.238652\tGrad Norm: 1.708864\tLR: 0.030000\n",
      "Train Epoch: 1484 [106496/194182 (54%)]\tLoss: 0.247419\tGrad Norm: 1.854283\tLR: 0.030000\n",
      "Train Epoch: 1484 [126976/194182 (65%)]\tLoss: 0.239318\tGrad Norm: 1.532311\tLR: 0.030000\n",
      "Train Epoch: 1484 [147456/194182 (75%)]\tLoss: 0.242434\tGrad Norm: 1.732405\tLR: 0.030000\n",
      "Train Epoch: 1484 [167936/194182 (85%)]\tLoss: 0.242988\tGrad Norm: 1.769173\tLR: 0.030000\n",
      "Train Epoch: 1484 [188416/194182 (96%)]\tLoss: 0.239828\tGrad Norm: 1.495818\tLR: 0.030000\n",
      "Train set: Average loss: 0.2414\n",
      "Test set: Average loss: 0.2523, Average MAE: 0.3467\n",
      "Train Epoch: 1485 [4096/194182 (2%)]\tLoss: 0.237554\tGrad Norm: 1.827128\tLR: 0.030000\n",
      "Train Epoch: 1485 [24576/194182 (12%)]\tLoss: 0.233808\tGrad Norm: 1.749846\tLR: 0.030000\n",
      "Train Epoch: 1485 [45056/194182 (23%)]\tLoss: 0.238616\tGrad Norm: 1.536249\tLR: 0.030000\n",
      "Train Epoch: 1485 [65536/194182 (33%)]\tLoss: 0.235833\tGrad Norm: 1.687845\tLR: 0.030000\n",
      "Train Epoch: 1485 [86016/194182 (44%)]\tLoss: 0.238317\tGrad Norm: 1.579061\tLR: 0.030000\n",
      "Train Epoch: 1485 [106496/194182 (54%)]\tLoss: 0.232256\tGrad Norm: 1.078186\tLR: 0.030000\n",
      "Train Epoch: 1485 [126976/194182 (65%)]\tLoss: 0.236492\tGrad Norm: 1.299907\tLR: 0.030000\n",
      "Train Epoch: 1485 [147456/194182 (75%)]\tLoss: 0.237506\tGrad Norm: 1.547592\tLR: 0.030000\n",
      "Train Epoch: 1485 [167936/194182 (85%)]\tLoss: 0.246543\tGrad Norm: 1.865928\tLR: 0.030000\n",
      "Train Epoch: 1485 [188416/194182 (96%)]\tLoss: 0.237077\tGrad Norm: 1.589020\tLR: 0.030000\n",
      "Train set: Average loss: 0.2397\n",
      "Test set: Average loss: 0.2541, Average MAE: 0.3552\n",
      "Epoch 1485: Mean reward = 0.055 +/- 0.044\n",
      "Train Epoch: 1486 [4096/194182 (2%)]\tLoss: 0.243443\tGrad Norm: 1.649456\tLR: 0.030000\n",
      "Train Epoch: 1486 [24576/194182 (12%)]\tLoss: 0.246736\tGrad Norm: 1.880135\tLR: 0.030000\n",
      "Train Epoch: 1486 [45056/194182 (23%)]\tLoss: 0.242114\tGrad Norm: 1.579928\tLR: 0.030000\n",
      "Train Epoch: 1486 [65536/194182 (33%)]\tLoss: 0.238308\tGrad Norm: 1.624002\tLR: 0.030000\n",
      "Train Epoch: 1486 [86016/194182 (44%)]\tLoss: 0.242054\tGrad Norm: 1.493853\tLR: 0.030000\n",
      "Train Epoch: 1486 [106496/194182 (54%)]\tLoss: 0.238075\tGrad Norm: 1.505268\tLR: 0.030000\n",
      "Train Epoch: 1486 [126976/194182 (65%)]\tLoss: 0.249496\tGrad Norm: 1.702910\tLR: 0.030000\n",
      "Train Epoch: 1486 [147456/194182 (75%)]\tLoss: 0.236320\tGrad Norm: 1.619855\tLR: 0.030000\n",
      "Train Epoch: 1486 [167936/194182 (85%)]\tLoss: 0.237688\tGrad Norm: 1.375740\tLR: 0.030000\n",
      "Train Epoch: 1486 [188416/194182 (96%)]\tLoss: 0.237368\tGrad Norm: 1.612170\tLR: 0.030000\n",
      "Train set: Average loss: 0.2408\n",
      "Test set: Average loss: 0.2568, Average MAE: 0.3390\n",
      "Train Epoch: 1487 [4096/194182 (2%)]\tLoss: 0.247571\tGrad Norm: 2.098298\tLR: 0.030000\n",
      "Train Epoch: 1487 [24576/194182 (12%)]\tLoss: 0.238366\tGrad Norm: 1.641093\tLR: 0.030000\n",
      "Train Epoch: 1487 [45056/194182 (23%)]\tLoss: 0.242119\tGrad Norm: 1.706229\tLR: 0.030000\n",
      "Train Epoch: 1487 [65536/194182 (33%)]\tLoss: 0.253016\tGrad Norm: 2.029898\tLR: 0.030000\n",
      "Train Epoch: 1487 [86016/194182 (44%)]\tLoss: 0.240626\tGrad Norm: 1.806746\tLR: 0.030000\n",
      "Train Epoch: 1487 [106496/194182 (54%)]\tLoss: 0.238766\tGrad Norm: 1.782380\tLR: 0.030000\n",
      "Train Epoch: 1487 [126976/194182 (65%)]\tLoss: 0.241485\tGrad Norm: 1.772514\tLR: 0.030000\n",
      "Train Epoch: 1487 [147456/194182 (75%)]\tLoss: 0.239692\tGrad Norm: 1.669928\tLR: 0.030000\n",
      "Train Epoch: 1487 [167936/194182 (85%)]\tLoss: 0.239429\tGrad Norm: 1.413370\tLR: 0.030000\n",
      "Train Epoch: 1487 [188416/194182 (96%)]\tLoss: 0.248539\tGrad Norm: 1.865415\tLR: 0.030000\n",
      "Train set: Average loss: 0.2428\n",
      "Test set: Average loss: 0.2544, Average MAE: 0.3461\n",
      "Train Epoch: 1488 [4096/194182 (2%)]\tLoss: 0.245377\tGrad Norm: 1.709493\tLR: 0.030000\n",
      "Train Epoch: 1488 [24576/194182 (12%)]\tLoss: 0.236087\tGrad Norm: 1.805017\tLR: 0.030000\n",
      "Train Epoch: 1488 [45056/194182 (23%)]\tLoss: 0.245924\tGrad Norm: 1.746316\tLR: 0.030000\n",
      "Train Epoch: 1488 [65536/194182 (33%)]\tLoss: 0.247009\tGrad Norm: 1.742395\tLR: 0.030000\n",
      "Train Epoch: 1488 [86016/194182 (44%)]\tLoss: 0.239099\tGrad Norm: 1.503178\tLR: 0.030000\n",
      "Train Epoch: 1488 [106496/194182 (54%)]\tLoss: 0.240153\tGrad Norm: 1.531744\tLR: 0.030000\n",
      "Train Epoch: 1488 [126976/194182 (65%)]\tLoss: 0.250342\tGrad Norm: 1.908731\tLR: 0.030000\n",
      "Train Epoch: 1488 [147456/194182 (75%)]\tLoss: 0.240047\tGrad Norm: 1.580952\tLR: 0.030000\n",
      "Train Epoch: 1488 [167936/194182 (85%)]\tLoss: 0.237717\tGrad Norm: 1.521713\tLR: 0.030000\n",
      "Train Epoch: 1488 [188416/194182 (96%)]\tLoss: 0.240391\tGrad Norm: 1.690461\tLR: 0.030000\n",
      "Train set: Average loss: 0.2412\n",
      "Test set: Average loss: 0.2520, Average MAE: 0.3479\n",
      "Train Epoch: 1489 [4096/194182 (2%)]\tLoss: 0.239949\tGrad Norm: 1.702179\tLR: 0.030000\n",
      "Train Epoch: 1489 [24576/194182 (12%)]\tLoss: 0.237183\tGrad Norm: 1.344213\tLR: 0.030000\n",
      "Train Epoch: 1489 [45056/194182 (23%)]\tLoss: 0.247787\tGrad Norm: 1.565273\tLR: 0.030000\n",
      "Train Epoch: 1489 [65536/194182 (33%)]\tLoss: 0.243149\tGrad Norm: 1.515813\tLR: 0.030000\n",
      "Train Epoch: 1489 [86016/194182 (44%)]\tLoss: 0.240946\tGrad Norm: 1.342435\tLR: 0.030000\n",
      "Train Epoch: 1489 [106496/194182 (54%)]\tLoss: 0.233952\tGrad Norm: 1.459773\tLR: 0.030000\n",
      "Train Epoch: 1489 [126976/194182 (65%)]\tLoss: 0.239877\tGrad Norm: 1.655110\tLR: 0.030000\n",
      "Train Epoch: 1489 [147456/194182 (75%)]\tLoss: 0.244047\tGrad Norm: 1.804154\tLR: 0.030000\n",
      "Train Epoch: 1489 [167936/194182 (85%)]\tLoss: 0.242500\tGrad Norm: 1.733466\tLR: 0.030000\n",
      "Train Epoch: 1489 [188416/194182 (96%)]\tLoss: 0.246713\tGrad Norm: 1.703348\tLR: 0.030000\n",
      "Train set: Average loss: 0.2397\n",
      "Test set: Average loss: 0.2597, Average MAE: 0.3592\n",
      "Train Epoch: 1490 [4096/194182 (2%)]\tLoss: 0.245925\tGrad Norm: 1.758750\tLR: 0.030000\n",
      "Train Epoch: 1490 [24576/194182 (12%)]\tLoss: 0.246119\tGrad Norm: 1.890762\tLR: 0.030000\n",
      "Train Epoch: 1490 [45056/194182 (23%)]\tLoss: 0.236100\tGrad Norm: 1.747582\tLR: 0.030000\n",
      "Train Epoch: 1490 [65536/194182 (33%)]\tLoss: 0.246244\tGrad Norm: 2.026458\tLR: 0.030000\n",
      "Train Epoch: 1490 [86016/194182 (44%)]\tLoss: 0.242875\tGrad Norm: 1.926904\tLR: 0.030000\n",
      "Train Epoch: 1490 [106496/194182 (54%)]\tLoss: 0.240409\tGrad Norm: 1.678576\tLR: 0.030000\n",
      "Train Epoch: 1490 [126976/194182 (65%)]\tLoss: 0.242687\tGrad Norm: 1.731226\tLR: 0.030000\n",
      "Train Epoch: 1490 [147456/194182 (75%)]\tLoss: 0.246905\tGrad Norm: 1.759374\tLR: 0.030000\n",
      "Train Epoch: 1490 [167936/194182 (85%)]\tLoss: 0.236808\tGrad Norm: 1.733272\tLR: 0.030000\n",
      "Train Epoch: 1490 [188416/194182 (96%)]\tLoss: 0.232828\tGrad Norm: 1.313187\tLR: 0.030000\n",
      "Train set: Average loss: 0.2414\n",
      "Test set: Average loss: 0.2564, Average MAE: 0.3557\n",
      "Epoch 1490: Mean reward = 0.063 +/- 0.067\n",
      "Train Epoch: 1491 [4096/194182 (2%)]\tLoss: 0.245792\tGrad Norm: 1.659527\tLR: 0.030000\n",
      "Train Epoch: 1491 [24576/194182 (12%)]\tLoss: 0.245693\tGrad Norm: 1.755798\tLR: 0.030000\n",
      "Train Epoch: 1491 [45056/194182 (23%)]\tLoss: 0.240338\tGrad Norm: 1.378054\tLR: 0.030000\n",
      "Train Epoch: 1491 [65536/194182 (33%)]\tLoss: 0.241057\tGrad Norm: 1.822118\tLR: 0.030000\n",
      "Train Epoch: 1491 [86016/194182 (44%)]\tLoss: 0.244905\tGrad Norm: 1.885948\tLR: 0.030000\n",
      "Train Epoch: 1491 [106496/194182 (54%)]\tLoss: 0.242963\tGrad Norm: 1.672130\tLR: 0.030000\n",
      "Train Epoch: 1491 [126976/194182 (65%)]\tLoss: 0.240435\tGrad Norm: 1.438288\tLR: 0.030000\n",
      "Train Epoch: 1491 [147456/194182 (75%)]\tLoss: 0.237857\tGrad Norm: 1.621111\tLR: 0.030000\n",
      "Train Epoch: 1491 [167936/194182 (85%)]\tLoss: 0.244478\tGrad Norm: 1.609627\tLR: 0.030000\n",
      "Train Epoch: 1491 [188416/194182 (96%)]\tLoss: 0.234230\tGrad Norm: 1.388160\tLR: 0.030000\n",
      "Train set: Average loss: 0.2400\n",
      "Test set: Average loss: 0.2513, Average MAE: 0.3472\n",
      "Train Epoch: 1492 [4096/194182 (2%)]\tLoss: 0.239014\tGrad Norm: 1.407054\tLR: 0.030000\n",
      "Train Epoch: 1492 [24576/194182 (12%)]\tLoss: 0.246261\tGrad Norm: 1.703107\tLR: 0.030000\n",
      "Train Epoch: 1492 [45056/194182 (23%)]\tLoss: 0.238371\tGrad Norm: 1.750769\tLR: 0.030000\n",
      "Train Epoch: 1492 [65536/194182 (33%)]\tLoss: 0.241588\tGrad Norm: 1.752897\tLR: 0.030000\n",
      "Train Epoch: 1492 [86016/194182 (44%)]\tLoss: 0.245419\tGrad Norm: 2.068902\tLR: 0.030000\n",
      "Train Epoch: 1492 [106496/194182 (54%)]\tLoss: 0.237801\tGrad Norm: 1.543256\tLR: 0.030000\n",
      "Train Epoch: 1492 [126976/194182 (65%)]\tLoss: 0.241333\tGrad Norm: 1.522062\tLR: 0.030000\n",
      "Train Epoch: 1492 [147456/194182 (75%)]\tLoss: 0.239105\tGrad Norm: 1.550088\tLR: 0.030000\n",
      "Train Epoch: 1492 [167936/194182 (85%)]\tLoss: 0.238960\tGrad Norm: 1.945003\tLR: 0.030000\n",
      "Train Epoch: 1492 [188416/194182 (96%)]\tLoss: 0.243436\tGrad Norm: 1.674821\tLR: 0.030000\n",
      "Train set: Average loss: 0.2407\n",
      "Test set: Average loss: 0.2500, Average MAE: 0.3452\n",
      "Train Epoch: 1493 [4096/194182 (2%)]\tLoss: 0.240866\tGrad Norm: 1.476505\tLR: 0.030000\n",
      "Train Epoch: 1493 [24576/194182 (12%)]\tLoss: 0.235998\tGrad Norm: 1.316179\tLR: 0.030000\n",
      "Train Epoch: 1493 [45056/194182 (23%)]\tLoss: 0.231150\tGrad Norm: 1.522147\tLR: 0.030000\n",
      "Train Epoch: 1493 [65536/194182 (33%)]\tLoss: 0.237980\tGrad Norm: 1.707789\tLR: 0.030000\n",
      "Train Epoch: 1493 [86016/194182 (44%)]\tLoss: 0.235062\tGrad Norm: 1.495956\tLR: 0.030000\n",
      "Train Epoch: 1493 [106496/194182 (54%)]\tLoss: 0.235560\tGrad Norm: 1.554356\tLR: 0.030000\n",
      "Train Epoch: 1493 [126976/194182 (65%)]\tLoss: 0.243531\tGrad Norm: 1.574807\tLR: 0.030000\n",
      "Train Epoch: 1493 [147456/194182 (75%)]\tLoss: 0.239463\tGrad Norm: 1.752568\tLR: 0.030000\n",
      "Train Epoch: 1493 [167936/194182 (85%)]\tLoss: 0.242590\tGrad Norm: 1.443591\tLR: 0.030000\n",
      "Train Epoch: 1493 [188416/194182 (96%)]\tLoss: 0.245461\tGrad Norm: 1.718601\tLR: 0.030000\n",
      "Train set: Average loss: 0.2381\n",
      "Test set: Average loss: 0.2500, Average MAE: 0.3539\n",
      "Train Epoch: 1494 [4096/194182 (2%)]\tLoss: 0.231387\tGrad Norm: 1.533974\tLR: 0.030000\n",
      "Train Epoch: 1494 [24576/194182 (12%)]\tLoss: 0.243974\tGrad Norm: 2.202802\tLR: 0.030000\n",
      "Train Epoch: 1494 [45056/194182 (23%)]\tLoss: 0.240833\tGrad Norm: 1.810664\tLR: 0.030000\n",
      "Train Epoch: 1494 [65536/194182 (33%)]\tLoss: 0.246186\tGrad Norm: 1.821568\tLR: 0.030000\n",
      "Train Epoch: 1494 [86016/194182 (44%)]\tLoss: 0.246114\tGrad Norm: 1.929830\tLR: 0.030000\n",
      "Train Epoch: 1494 [106496/194182 (54%)]\tLoss: 0.243310\tGrad Norm: 1.810456\tLR: 0.030000\n",
      "Train Epoch: 1494 [126976/194182 (65%)]\tLoss: 0.239377\tGrad Norm: 1.651937\tLR: 0.030000\n",
      "Train Epoch: 1494 [147456/194182 (75%)]\tLoss: 0.242616\tGrad Norm: 1.559908\tLR: 0.030000\n",
      "Train Epoch: 1494 [167936/194182 (85%)]\tLoss: 0.241056\tGrad Norm: 1.421680\tLR: 0.030000\n",
      "Train Epoch: 1494 [188416/194182 (96%)]\tLoss: 0.245629\tGrad Norm: 1.752390\tLR: 0.030000\n",
      "Train set: Average loss: 0.2412\n",
      "Test set: Average loss: 0.2569, Average MAE: 0.3555\n",
      "Train Epoch: 1495 [4096/194182 (2%)]\tLoss: 0.241704\tGrad Norm: 1.842282\tLR: 0.030000\n",
      "Train Epoch: 1495 [24576/194182 (12%)]\tLoss: 0.235004\tGrad Norm: 1.403907\tLR: 0.030000\n",
      "Train Epoch: 1495 [45056/194182 (23%)]\tLoss: 0.236510\tGrad Norm: 1.608402\tLR: 0.030000\n",
      "Train Epoch: 1495 [65536/194182 (33%)]\tLoss: 0.236262\tGrad Norm: 1.457373\tLR: 0.030000\n",
      "Train Epoch: 1495 [86016/194182 (44%)]\tLoss: 0.243982\tGrad Norm: 1.829936\tLR: 0.030000\n",
      "Train Epoch: 1495 [106496/194182 (54%)]\tLoss: 0.231350\tGrad Norm: 1.210787\tLR: 0.030000\n",
      "Train Epoch: 1495 [126976/194182 (65%)]\tLoss: 0.234314\tGrad Norm: 1.454266\tLR: 0.030000\n",
      "Train Epoch: 1495 [147456/194182 (75%)]\tLoss: 0.235087\tGrad Norm: 1.232375\tLR: 0.030000\n",
      "Train Epoch: 1495 [167936/194182 (85%)]\tLoss: 0.244520\tGrad Norm: 1.617795\tLR: 0.030000\n",
      "Train Epoch: 1495 [188416/194182 (96%)]\tLoss: 0.241593\tGrad Norm: 1.698478\tLR: 0.030000\n",
      "Train set: Average loss: 0.2382\n",
      "Test set: Average loss: 0.2481, Average MAE: 0.3431\n",
      "Epoch 1495: Mean reward = 0.049 +/- 0.029\n",
      "Train Epoch: 1496 [4096/194182 (2%)]\tLoss: 0.236865\tGrad Norm: 1.204784\tLR: 0.030000\n",
      "Train Epoch: 1496 [24576/194182 (12%)]\tLoss: 0.231710\tGrad Norm: 1.630574\tLR: 0.030000\n",
      "Train Epoch: 1496 [45056/194182 (23%)]\tLoss: 0.241980\tGrad Norm: 1.691980\tLR: 0.030000\n",
      "Train Epoch: 1496 [65536/194182 (33%)]\tLoss: 0.247984\tGrad Norm: 2.116643\tLR: 0.030000\n",
      "Train Epoch: 1496 [86016/194182 (44%)]\tLoss: 0.239868\tGrad Norm: 2.102029\tLR: 0.030000\n",
      "Train Epoch: 1496 [106496/194182 (54%)]\tLoss: 0.242829\tGrad Norm: 1.790207\tLR: 0.030000\n",
      "Train Epoch: 1496 [126976/194182 (65%)]\tLoss: 0.239913\tGrad Norm: 1.698400\tLR: 0.030000\n",
      "Train Epoch: 1496 [147456/194182 (75%)]\tLoss: 0.235382\tGrad Norm: 1.596963\tLR: 0.030000\n",
      "Train Epoch: 1496 [167936/194182 (85%)]\tLoss: 0.239749\tGrad Norm: 1.738907\tLR: 0.030000\n",
      "Train Epoch: 1496 [188416/194182 (96%)]\tLoss: 0.235945\tGrad Norm: 1.946739\tLR: 0.030000\n",
      "Train set: Average loss: 0.2408\n",
      "Test set: Average loss: 0.2525, Average MAE: 0.3387\n",
      "Train Epoch: 1497 [4096/194182 (2%)]\tLoss: 0.240654\tGrad Norm: 1.635179\tLR: 0.030000\n",
      "Train Epoch: 1497 [24576/194182 (12%)]\tLoss: 0.237264\tGrad Norm: 1.931539\tLR: 0.030000\n",
      "Train Epoch: 1497 [45056/194182 (23%)]\tLoss: 0.239890\tGrad Norm: 1.922227\tLR: 0.030000\n",
      "Train Epoch: 1497 [65536/194182 (33%)]\tLoss: 0.242764\tGrad Norm: 1.589777\tLR: 0.030000\n",
      "Train Epoch: 1497 [86016/194182 (44%)]\tLoss: 0.238652\tGrad Norm: 1.488857\tLR: 0.030000\n",
      "Train Epoch: 1497 [106496/194182 (54%)]\tLoss: 0.239525\tGrad Norm: 1.556726\tLR: 0.030000\n",
      "Train Epoch: 1497 [126976/194182 (65%)]\tLoss: 0.245228\tGrad Norm: 1.646530\tLR: 0.030000\n",
      "Train Epoch: 1497 [147456/194182 (75%)]\tLoss: 0.241358\tGrad Norm: 1.741244\tLR: 0.030000\n",
      "Train Epoch: 1497 [167936/194182 (85%)]\tLoss: 0.251587\tGrad Norm: 1.739493\tLR: 0.030000\n",
      "Train Epoch: 1497 [188416/194182 (96%)]\tLoss: 0.246513\tGrad Norm: 1.725856\tLR: 0.030000\n",
      "Train set: Average loss: 0.2406\n",
      "Test set: Average loss: 0.2536, Average MAE: 0.3441\n",
      "Train Epoch: 1498 [4096/194182 (2%)]\tLoss: 0.240883\tGrad Norm: 1.761008\tLR: 0.030000\n",
      "Train Epoch: 1498 [24576/194182 (12%)]\tLoss: 0.235799\tGrad Norm: 1.530946\tLR: 0.030000\n",
      "Train Epoch: 1498 [45056/194182 (23%)]\tLoss: 0.233000\tGrad Norm: 1.615842\tLR: 0.030000\n",
      "Train Epoch: 1498 [65536/194182 (33%)]\tLoss: 0.242489\tGrad Norm: 1.661206\tLR: 0.030000\n",
      "Train Epoch: 1498 [86016/194182 (44%)]\tLoss: 0.243653\tGrad Norm: 1.827797\tLR: 0.030000\n",
      "Train Epoch: 1498 [106496/194182 (54%)]\tLoss: 0.240480\tGrad Norm: 1.568354\tLR: 0.030000\n",
      "Train Epoch: 1498 [126976/194182 (65%)]\tLoss: 0.240037\tGrad Norm: 1.834774\tLR: 0.030000\n",
      "Train Epoch: 1498 [147456/194182 (75%)]\tLoss: 0.234797\tGrad Norm: 1.455008\tLR: 0.030000\n",
      "Train Epoch: 1498 [167936/194182 (85%)]\tLoss: 0.232553\tGrad Norm: 1.357290\tLR: 0.030000\n",
      "Train Epoch: 1498 [188416/194182 (96%)]\tLoss: 0.231008\tGrad Norm: 1.374935\tLR: 0.030000\n",
      "Train set: Average loss: 0.2388\n",
      "Test set: Average loss: 0.2520, Average MAE: 0.3503\n",
      "Train Epoch: 1499 [4096/194182 (2%)]\tLoss: 0.233749\tGrad Norm: 1.592338\tLR: 0.030000\n",
      "Train Epoch: 1499 [24576/194182 (12%)]\tLoss: 0.236252\tGrad Norm: 1.561857\tLR: 0.030000\n",
      "Train Epoch: 1499 [45056/194182 (23%)]\tLoss: 0.249861\tGrad Norm: 2.051880\tLR: 0.030000\n",
      "Train Epoch: 1499 [65536/194182 (33%)]\tLoss: 0.229933\tGrad Norm: 1.348920\tLR: 0.030000\n",
      "Train Epoch: 1499 [86016/194182 (44%)]\tLoss: 0.240072\tGrad Norm: 1.469391\tLR: 0.030000\n",
      "Train Epoch: 1499 [106496/194182 (54%)]\tLoss: 0.240475\tGrad Norm: 1.752028\tLR: 0.030000\n",
      "Train Epoch: 1499 [126976/194182 (65%)]\tLoss: 0.233624\tGrad Norm: 1.502676\tLR: 0.030000\n",
      "Train Epoch: 1499 [147456/194182 (75%)]\tLoss: 0.234225\tGrad Norm: 1.507561\tLR: 0.030000\n",
      "Train Epoch: 1499 [167936/194182 (85%)]\tLoss: 0.237860\tGrad Norm: 1.762041\tLR: 0.030000\n",
      "Train Epoch: 1499 [188416/194182 (96%)]\tLoss: 0.240720\tGrad Norm: 1.855425\tLR: 0.030000\n",
      "Train set: Average loss: 0.2398\n",
      "Test set: Average loss: 0.2567, Average MAE: 0.3433\n",
      "Train Epoch: 1500 [4096/194182 (2%)]\tLoss: 0.238654\tGrad Norm: 1.780978\tLR: 0.030000\n",
      "Train Epoch: 1500 [24576/194182 (12%)]\tLoss: 0.233126\tGrad Norm: 1.489689\tLR: 0.030000\n",
      "Train Epoch: 1500 [45056/194182 (23%)]\tLoss: 0.237518\tGrad Norm: 1.765474\tLR: 0.030000\n",
      "Train Epoch: 1500 [65536/194182 (33%)]\tLoss: 0.237989\tGrad Norm: 1.849055\tLR: 0.030000\n",
      "Train Epoch: 1500 [86016/194182 (44%)]\tLoss: 0.242578\tGrad Norm: 1.825669\tLR: 0.030000\n",
      "Train Epoch: 1500 [106496/194182 (54%)]\tLoss: 0.245386\tGrad Norm: 1.761941\tLR: 0.030000\n",
      "Train Epoch: 1500 [126976/194182 (65%)]\tLoss: 0.239379\tGrad Norm: 1.518866\tLR: 0.030000\n",
      "Train Epoch: 1500 [147456/194182 (75%)]\tLoss: 0.239526\tGrad Norm: 1.526999\tLR: 0.030000\n",
      "Train Epoch: 1500 [167936/194182 (85%)]\tLoss: 0.239016\tGrad Norm: 1.561822\tLR: 0.030000\n",
      "Train Epoch: 1500 [188416/194182 (96%)]\tLoss: 0.242098\tGrad Norm: 1.258748\tLR: 0.030000\n",
      "Train set: Average loss: 0.2390\n",
      "Test set: Average loss: 0.2500, Average MAE: 0.3434\n",
      "Epoch 1500: Mean reward = 0.042 +/- 0.014\n",
      "Train Epoch: 1501 [4096/194182 (2%)]\tLoss: 0.233235\tGrad Norm: 1.473896\tLR: 0.030000\n",
      "Train Epoch: 1501 [24576/194182 (12%)]\tLoss: 0.237631\tGrad Norm: 1.364727\tLR: 0.030000\n",
      "Train Epoch: 1501 [45056/194182 (23%)]\tLoss: 0.244593\tGrad Norm: 1.945747\tLR: 0.030000\n",
      "Train Epoch: 1501 [65536/194182 (33%)]\tLoss: 0.238905\tGrad Norm: 1.880479\tLR: 0.030000\n",
      "Train Epoch: 1501 [86016/194182 (44%)]\tLoss: 0.246217\tGrad Norm: 1.762955\tLR: 0.030000\n",
      "Train Epoch: 1501 [106496/194182 (54%)]\tLoss: 0.238112\tGrad Norm: 1.650936\tLR: 0.030000\n",
      "Train Epoch: 1501 [126976/194182 (65%)]\tLoss: 0.239556\tGrad Norm: 1.655799\tLR: 0.030000\n",
      "Train Epoch: 1501 [147456/194182 (75%)]\tLoss: 0.234923\tGrad Norm: 1.297056\tLR: 0.030000\n",
      "Train Epoch: 1501 [167936/194182 (85%)]\tLoss: 0.238606\tGrad Norm: 1.747472\tLR: 0.030000\n",
      "Train Epoch: 1501 [188416/194182 (96%)]\tLoss: 0.238557\tGrad Norm: 1.560110\tLR: 0.030000\n",
      "Train set: Average loss: 0.2390\n",
      "Test set: Average loss: 0.2544, Average MAE: 0.3538\n",
      "Train Epoch: 1502 [4096/194182 (2%)]\tLoss: 0.237662\tGrad Norm: 1.708190\tLR: 0.030000\n",
      "Train Epoch: 1502 [24576/194182 (12%)]\tLoss: 0.237390\tGrad Norm: 1.511486\tLR: 0.030000\n",
      "Train Epoch: 1502 [45056/194182 (23%)]\tLoss: 0.240419\tGrad Norm: 1.853134\tLR: 0.030000\n",
      "Train Epoch: 1502 [65536/194182 (33%)]\tLoss: 0.241155\tGrad Norm: 1.696262\tLR: 0.030000\n",
      "Train Epoch: 1502 [86016/194182 (44%)]\tLoss: 0.243075\tGrad Norm: 1.842954\tLR: 0.030000\n",
      "Train Epoch: 1502 [106496/194182 (54%)]\tLoss: 0.239520\tGrad Norm: 1.629099\tLR: 0.030000\n",
      "Train Epoch: 1502 [126976/194182 (65%)]\tLoss: 0.239997\tGrad Norm: 1.632659\tLR: 0.030000\n",
      "Train Epoch: 1502 [147456/194182 (75%)]\tLoss: 0.239145\tGrad Norm: 1.586340\tLR: 0.030000\n",
      "Train Epoch: 1502 [167936/194182 (85%)]\tLoss: 0.243750\tGrad Norm: 1.629874\tLR: 0.030000\n",
      "Train Epoch: 1502 [188416/194182 (96%)]\tLoss: 0.242122\tGrad Norm: 1.628391\tLR: 0.030000\n",
      "Train set: Average loss: 0.2386\n",
      "Test set: Average loss: 0.2549, Average MAE: 0.3546\n",
      "Train Epoch: 1503 [4096/194182 (2%)]\tLoss: 0.240190\tGrad Norm: 1.703726\tLR: 0.030000\n",
      "Train Epoch: 1503 [24576/194182 (12%)]\tLoss: 0.234496\tGrad Norm: 1.411084\tLR: 0.030000\n",
      "Train Epoch: 1503 [45056/194182 (23%)]\tLoss: 0.239450\tGrad Norm: 1.706301\tLR: 0.030000\n",
      "Train Epoch: 1503 [65536/194182 (33%)]\tLoss: 0.243721\tGrad Norm: 2.002258\tLR: 0.030000\n",
      "Train Epoch: 1503 [86016/194182 (44%)]\tLoss: 0.230320\tGrad Norm: 1.427453\tLR: 0.030000\n",
      "Train Epoch: 1503 [106496/194182 (54%)]\tLoss: 0.230346\tGrad Norm: 1.575902\tLR: 0.030000\n",
      "Train Epoch: 1503 [126976/194182 (65%)]\tLoss: 0.236012\tGrad Norm: 1.875853\tLR: 0.030000\n",
      "Train Epoch: 1503 [147456/194182 (75%)]\tLoss: 0.241856\tGrad Norm: 1.887234\tLR: 0.030000\n",
      "Train Epoch: 1503 [167936/194182 (85%)]\tLoss: 0.237986\tGrad Norm: 1.849676\tLR: 0.030000\n",
      "Train Epoch: 1503 [188416/194182 (96%)]\tLoss: 0.241713\tGrad Norm: 1.923732\tLR: 0.030000\n",
      "Train set: Average loss: 0.2391\n",
      "Test set: Average loss: 0.2537, Average MAE: 0.3573\n",
      "Train Epoch: 1504 [4096/194182 (2%)]\tLoss: 0.237800\tGrad Norm: 1.647714\tLR: 0.030000\n",
      "Train Epoch: 1504 [24576/194182 (12%)]\tLoss: 0.234864\tGrad Norm: 1.624907\tLR: 0.030000\n",
      "Train Epoch: 1504 [45056/194182 (23%)]\tLoss: 0.243882\tGrad Norm: 1.911590\tLR: 0.030000\n",
      "Train Epoch: 1504 [65536/194182 (33%)]\tLoss: 0.239968\tGrad Norm: 1.793876\tLR: 0.030000\n",
      "Train Epoch: 1504 [86016/194182 (44%)]\tLoss: 0.239057\tGrad Norm: 1.548630\tLR: 0.030000\n",
      "Train Epoch: 1504 [106496/194182 (54%)]\tLoss: 0.235606\tGrad Norm: 1.419752\tLR: 0.030000\n",
      "Train Epoch: 1504 [126976/194182 (65%)]\tLoss: 0.240569\tGrad Norm: 1.542717\tLR: 0.030000\n",
      "Train Epoch: 1504 [147456/194182 (75%)]\tLoss: 0.237741\tGrad Norm: 1.538645\tLR: 0.030000\n",
      "Train Epoch: 1504 [167936/194182 (85%)]\tLoss: 0.232614\tGrad Norm: 1.453493\tLR: 0.030000\n",
      "Train Epoch: 1504 [188416/194182 (96%)]\tLoss: 0.230568\tGrad Norm: 1.395938\tLR: 0.030000\n",
      "Train set: Average loss: 0.2385\n",
      "Test set: Average loss: 0.2516, Average MAE: 0.3486\n",
      "Train Epoch: 1505 [4096/194182 (2%)]\tLoss: 0.233092\tGrad Norm: 1.378963\tLR: 0.030000\n",
      "Train Epoch: 1505 [24576/194182 (12%)]\tLoss: 0.238548\tGrad Norm: 1.588828\tLR: 0.030000\n",
      "Train Epoch: 1505 [45056/194182 (23%)]\tLoss: 0.243078\tGrad Norm: 1.811746\tLR: 0.030000\n",
      "Train Epoch: 1505 [65536/194182 (33%)]\tLoss: 0.237364\tGrad Norm: 1.497066\tLR: 0.030000\n",
      "Train Epoch: 1505 [86016/194182 (44%)]\tLoss: 0.232765\tGrad Norm: 1.428917\tLR: 0.030000\n",
      "Train Epoch: 1505 [106496/194182 (54%)]\tLoss: 0.235633\tGrad Norm: 1.451495\tLR: 0.030000\n",
      "Train Epoch: 1505 [126976/194182 (65%)]\tLoss: 0.243903\tGrad Norm: 1.873460\tLR: 0.030000\n",
      "Train Epoch: 1505 [147456/194182 (75%)]\tLoss: 0.237737\tGrad Norm: 1.898913\tLR: 0.030000\n",
      "Train Epoch: 1505 [167936/194182 (85%)]\tLoss: 0.239492\tGrad Norm: 1.894602\tLR: 0.030000\n",
      "Train Epoch: 1505 [188416/194182 (96%)]\tLoss: 0.237913\tGrad Norm: 1.747812\tLR: 0.030000\n",
      "Train set: Average loss: 0.2386\n",
      "Test set: Average loss: 0.2517, Average MAE: 0.3359\n",
      "Epoch 1505: Mean reward = 0.053 +/- 0.039\n",
      "Train Epoch: 1506 [4096/194182 (2%)]\tLoss: 0.241924\tGrad Norm: 1.851030\tLR: 0.030000\n",
      "Train Epoch: 1506 [24576/194182 (12%)]\tLoss: 0.249526\tGrad Norm: 2.133684\tLR: 0.030000\n",
      "Train Epoch: 1506 [45056/194182 (23%)]\tLoss: 0.238422\tGrad Norm: 1.694213\tLR: 0.030000\n",
      "Train Epoch: 1506 [65536/194182 (33%)]\tLoss: 0.239656\tGrad Norm: 1.709433\tLR: 0.030000\n",
      "Train Epoch: 1506 [86016/194182 (44%)]\tLoss: 0.236607\tGrad Norm: 1.465568\tLR: 0.030000\n",
      "Train Epoch: 1506 [106496/194182 (54%)]\tLoss: 0.248345\tGrad Norm: 1.955471\tLR: 0.030000\n",
      "Train Epoch: 1506 [126976/194182 (65%)]\tLoss: 0.240974\tGrad Norm: 1.562893\tLR: 0.030000\n",
      "Train Epoch: 1506 [147456/194182 (75%)]\tLoss: 0.242496\tGrad Norm: 1.487495\tLR: 0.030000\n",
      "Train Epoch: 1506 [167936/194182 (85%)]\tLoss: 0.248641\tGrad Norm: 1.795283\tLR: 0.030000\n",
      "Train Epoch: 1506 [188416/194182 (96%)]\tLoss: 0.238039\tGrad Norm: 1.832063\tLR: 0.030000\n",
      "Train set: Average loss: 0.2395\n",
      "Test set: Average loss: 0.2544, Average MAE: 0.3537\n",
      "Train Epoch: 1507 [4096/194182 (2%)]\tLoss: 0.238024\tGrad Norm: 1.573404\tLR: 0.030000\n",
      "Train Epoch: 1507 [24576/194182 (12%)]\tLoss: 0.236971\tGrad Norm: 1.249314\tLR: 0.030000\n",
      "Train Epoch: 1507 [45056/194182 (23%)]\tLoss: 0.225092\tGrad Norm: 1.328834\tLR: 0.030000\n",
      "Train Epoch: 1507 [65536/194182 (33%)]\tLoss: 0.236494\tGrad Norm: 1.472243\tLR: 0.030000\n",
      "Train Epoch: 1507 [86016/194182 (44%)]\tLoss: 0.240202\tGrad Norm: 1.682127\tLR: 0.030000\n",
      "Train Epoch: 1507 [106496/194182 (54%)]\tLoss: 0.237774\tGrad Norm: 1.764673\tLR: 0.030000\n",
      "Train Epoch: 1507 [126976/194182 (65%)]\tLoss: 0.240056\tGrad Norm: 1.726260\tLR: 0.030000\n",
      "Train Epoch: 1507 [147456/194182 (75%)]\tLoss: 0.237169\tGrad Norm: 1.626561\tLR: 0.030000\n",
      "Train Epoch: 1507 [167936/194182 (85%)]\tLoss: 0.245366\tGrad Norm: 1.937047\tLR: 0.030000\n",
      "Train Epoch: 1507 [188416/194182 (96%)]\tLoss: 0.240360\tGrad Norm: 1.663561\tLR: 0.030000\n",
      "Train set: Average loss: 0.2368\n",
      "Test set: Average loss: 0.2550, Average MAE: 0.3511\n",
      "Train Epoch: 1508 [4096/194182 (2%)]\tLoss: 0.239378\tGrad Norm: 1.709978\tLR: 0.030000\n",
      "Train Epoch: 1508 [24576/194182 (12%)]\tLoss: 0.232471\tGrad Norm: 1.402196\tLR: 0.030000\n",
      "Train Epoch: 1508 [45056/194182 (23%)]\tLoss: 0.234823\tGrad Norm: 1.855124\tLR: 0.030000\n",
      "Train Epoch: 1508 [65536/194182 (33%)]\tLoss: 0.236842\tGrad Norm: 1.745116\tLR: 0.030000\n",
      "Train Epoch: 1508 [86016/194182 (44%)]\tLoss: 0.240352\tGrad Norm: 1.596488\tLR: 0.030000\n",
      "Train Epoch: 1508 [106496/194182 (54%)]\tLoss: 0.234410\tGrad Norm: 1.655711\tLR: 0.030000\n",
      "Train Epoch: 1508 [126976/194182 (65%)]\tLoss: 0.235551\tGrad Norm: 1.648769\tLR: 0.030000\n",
      "Train Epoch: 1508 [147456/194182 (75%)]\tLoss: 0.239225\tGrad Norm: 1.797113\tLR: 0.030000\n",
      "Train Epoch: 1508 [167936/194182 (85%)]\tLoss: 0.233745\tGrad Norm: 1.720819\tLR: 0.030000\n",
      "Train Epoch: 1508 [188416/194182 (96%)]\tLoss: 0.241343\tGrad Norm: 1.813290\tLR: 0.030000\n",
      "Train set: Average loss: 0.2385\n",
      "Test set: Average loss: 0.2562, Average MAE: 0.3386\n",
      "Train Epoch: 1509 [4096/194182 (2%)]\tLoss: 0.240139\tGrad Norm: 1.994385\tLR: 0.030000\n",
      "Train Epoch: 1509 [24576/194182 (12%)]\tLoss: 0.244228\tGrad Norm: 2.234468\tLR: 0.030000\n",
      "Train Epoch: 1509 [45056/194182 (23%)]\tLoss: 0.237469\tGrad Norm: 1.842235\tLR: 0.030000\n",
      "Train Epoch: 1509 [65536/194182 (33%)]\tLoss: 0.240494\tGrad Norm: 1.604295\tLR: 0.030000\n",
      "Train Epoch: 1509 [86016/194182 (44%)]\tLoss: 0.235385\tGrad Norm: 1.739262\tLR: 0.030000\n",
      "Train Epoch: 1509 [106496/194182 (54%)]\tLoss: 0.241308\tGrad Norm: 1.538329\tLR: 0.030000\n",
      "Train Epoch: 1509 [126976/194182 (65%)]\tLoss: 0.246495\tGrad Norm: 1.758731\tLR: 0.030000\n",
      "Train Epoch: 1509 [147456/194182 (75%)]\tLoss: 0.235682\tGrad Norm: 1.650868\tLR: 0.030000\n",
      "Train Epoch: 1509 [167936/194182 (85%)]\tLoss: 0.239185\tGrad Norm: 1.749987\tLR: 0.030000\n",
      "Train Epoch: 1509 [188416/194182 (96%)]\tLoss: 0.227143\tGrad Norm: 1.524409\tLR: 0.030000\n",
      "Train set: Average loss: 0.2397\n",
      "Test set: Average loss: 0.2498, Average MAE: 0.3432\n",
      "Train Epoch: 1510 [4096/194182 (2%)]\tLoss: 0.237522\tGrad Norm: 1.411893\tLR: 0.030000\n",
      "Train Epoch: 1510 [24576/194182 (12%)]\tLoss: 0.235507\tGrad Norm: 1.691992\tLR: 0.030000\n",
      "Train Epoch: 1510 [45056/194182 (23%)]\tLoss: 0.235272\tGrad Norm: 1.286295\tLR: 0.030000\n",
      "Train Epoch: 1510 [65536/194182 (33%)]\tLoss: 0.238890\tGrad Norm: 1.521375\tLR: 0.030000\n",
      "Train Epoch: 1510 [86016/194182 (44%)]\tLoss: 0.238894\tGrad Norm: 1.651833\tLR: 0.030000\n",
      "Train Epoch: 1510 [106496/194182 (54%)]\tLoss: 0.239236\tGrad Norm: 1.627533\tLR: 0.030000\n",
      "Train Epoch: 1510 [126976/194182 (65%)]\tLoss: 0.234401\tGrad Norm: 1.439642\tLR: 0.030000\n",
      "Train Epoch: 1510 [147456/194182 (75%)]\tLoss: 0.233445\tGrad Norm: 1.409279\tLR: 0.030000\n",
      "Train Epoch: 1510 [167936/194182 (85%)]\tLoss: 0.233441\tGrad Norm: 1.625836\tLR: 0.030000\n",
      "Train Epoch: 1510 [188416/194182 (96%)]\tLoss: 0.247792\tGrad Norm: 2.015705\tLR: 0.030000\n",
      "Train set: Average loss: 0.2369\n",
      "Test set: Average loss: 0.2586, Average MAE: 0.3594\n",
      "Epoch 1510: Mean reward = 0.046 +/- 0.001\n",
      "Train Epoch: 1511 [4096/194182 (2%)]\tLoss: 0.238790\tGrad Norm: 1.737024\tLR: 0.030000\n",
      "Train Epoch: 1511 [24576/194182 (12%)]\tLoss: 0.241759\tGrad Norm: 1.770649\tLR: 0.030000\n",
      "Train Epoch: 1511 [45056/194182 (23%)]\tLoss: 0.228267\tGrad Norm: 1.424343\tLR: 0.030000\n",
      "Train Epoch: 1511 [65536/194182 (33%)]\tLoss: 0.234719\tGrad Norm: 2.060688\tLR: 0.030000\n",
      "Train Epoch: 1511 [86016/194182 (44%)]\tLoss: 0.229135\tGrad Norm: 1.570078\tLR: 0.030000\n",
      "Train Epoch: 1511 [106496/194182 (54%)]\tLoss: 0.250224\tGrad Norm: 2.142209\tLR: 0.030000\n",
      "Train Epoch: 1511 [126976/194182 (65%)]\tLoss: 0.247246\tGrad Norm: 1.799618\tLR: 0.030000\n",
      "Train Epoch: 1511 [147456/194182 (75%)]\tLoss: 0.227068\tGrad Norm: 1.209742\tLR: 0.030000\n",
      "Train Epoch: 1511 [167936/194182 (85%)]\tLoss: 0.229391\tGrad Norm: 1.055461\tLR: 0.030000\n",
      "Train Epoch: 1511 [188416/194182 (96%)]\tLoss: 0.239204\tGrad Norm: 1.561427\tLR: 0.030000\n",
      "Train set: Average loss: 0.2366\n",
      "Test set: Average loss: 0.2533, Average MAE: 0.3514\n",
      "Train Epoch: 1512 [4096/194182 (2%)]\tLoss: 0.233847\tGrad Norm: 1.499099\tLR: 0.030000\n",
      "Train Epoch: 1512 [24576/194182 (12%)]\tLoss: 0.228139\tGrad Norm: 1.383527\tLR: 0.030000\n",
      "Train Epoch: 1512 [45056/194182 (23%)]\tLoss: 0.244079\tGrad Norm: 1.960727\tLR: 0.030000\n",
      "Train Epoch: 1512 [65536/194182 (33%)]\tLoss: 0.240800\tGrad Norm: 1.811375\tLR: 0.030000\n",
      "Train Epoch: 1512 [86016/194182 (44%)]\tLoss: 0.239725\tGrad Norm: 1.679082\tLR: 0.030000\n",
      "Train Epoch: 1512 [106496/194182 (54%)]\tLoss: 0.243304\tGrad Norm: 1.681677\tLR: 0.030000\n",
      "Train Epoch: 1512 [126976/194182 (65%)]\tLoss: 0.229552\tGrad Norm: 1.491309\tLR: 0.030000\n",
      "Train Epoch: 1512 [147456/194182 (75%)]\tLoss: 0.237397\tGrad Norm: 1.685792\tLR: 0.030000\n",
      "Train Epoch: 1512 [167936/194182 (85%)]\tLoss: 0.239033\tGrad Norm: 1.852232\tLR: 0.030000\n",
      "Train Epoch: 1512 [188416/194182 (96%)]\tLoss: 0.234644\tGrad Norm: 1.585461\tLR: 0.030000\n",
      "Train set: Average loss: 0.2374\n",
      "Test set: Average loss: 0.2607, Average MAE: 0.3578\n",
      "Train Epoch: 1513 [4096/194182 (2%)]\tLoss: 0.244486\tGrad Norm: 1.782760\tLR: 0.030000\n",
      "Train Epoch: 1513 [24576/194182 (12%)]\tLoss: 0.240953\tGrad Norm: 1.944750\tLR: 0.030000\n",
      "Train Epoch: 1513 [45056/194182 (23%)]\tLoss: 0.242893\tGrad Norm: 1.970702\tLR: 0.030000\n",
      "Train Epoch: 1513 [65536/194182 (33%)]\tLoss: 0.239159\tGrad Norm: 1.724510\tLR: 0.030000\n",
      "Train Epoch: 1513 [86016/194182 (44%)]\tLoss: 0.233181\tGrad Norm: 1.447514\tLR: 0.030000\n",
      "Train Epoch: 1513 [106496/194182 (54%)]\tLoss: 0.233348\tGrad Norm: 2.532754\tLR: 0.030000\n",
      "Train Epoch: 1513 [126976/194182 (65%)]\tLoss: 0.235042\tGrad Norm: 1.654803\tLR: 0.030000\n",
      "Train Epoch: 1513 [147456/194182 (75%)]\tLoss: 0.240737\tGrad Norm: 2.029050\tLR: 0.030000\n",
      "Train Epoch: 1513 [167936/194182 (85%)]\tLoss: 0.250132\tGrad Norm: 2.131227\tLR: 0.030000\n",
      "Train Epoch: 1513 [188416/194182 (96%)]\tLoss: 0.236761\tGrad Norm: 1.519815\tLR: 0.030000\n",
      "Train set: Average loss: 0.2379\n",
      "Test set: Average loss: 0.2492, Average MAE: 0.3474\n",
      "Train Epoch: 1514 [4096/194182 (2%)]\tLoss: 0.232008\tGrad Norm: 1.419589\tLR: 0.030000\n",
      "Train Epoch: 1514 [24576/194182 (12%)]\tLoss: 0.237175\tGrad Norm: 1.548846\tLR: 0.030000\n",
      "Train Epoch: 1514 [45056/194182 (23%)]\tLoss: 0.236676\tGrad Norm: 1.516430\tLR: 0.030000\n",
      "Train Epoch: 1514 [65536/194182 (33%)]\tLoss: 0.232031\tGrad Norm: 1.528872\tLR: 0.030000\n",
      "Train Epoch: 1514 [86016/194182 (44%)]\tLoss: 0.237490\tGrad Norm: 1.693624\tLR: 0.030000\n",
      "Train Epoch: 1514 [106496/194182 (54%)]\tLoss: 0.231874\tGrad Norm: 1.445652\tLR: 0.030000\n",
      "Train Epoch: 1514 [126976/194182 (65%)]\tLoss: 0.241531\tGrad Norm: 1.470654\tLR: 0.030000\n",
      "Train Epoch: 1514 [147456/194182 (75%)]\tLoss: 0.238659\tGrad Norm: 1.585994\tLR: 0.030000\n",
      "Train Epoch: 1514 [167936/194182 (85%)]\tLoss: 0.237650\tGrad Norm: 1.698966\tLR: 0.030000\n",
      "Train Epoch: 1514 [188416/194182 (96%)]\tLoss: 0.239442\tGrad Norm: 1.501720\tLR: 0.030000\n",
      "Train set: Average loss: 0.2366\n",
      "Test set: Average loss: 0.2519, Average MAE: 0.3464\n",
      "Train Epoch: 1515 [4096/194182 (2%)]\tLoss: 0.237692\tGrad Norm: 1.634059\tLR: 0.030000\n",
      "Train Epoch: 1515 [24576/194182 (12%)]\tLoss: 0.229387\tGrad Norm: 1.507699\tLR: 0.030000\n",
      "Train Epoch: 1515 [45056/194182 (23%)]\tLoss: 0.241858\tGrad Norm: 1.613641\tLR: 0.030000\n",
      "Train Epoch: 1515 [65536/194182 (33%)]\tLoss: 0.228043\tGrad Norm: 1.345333\tLR: 0.030000\n",
      "Train Epoch: 1515 [86016/194182 (44%)]\tLoss: 0.236094\tGrad Norm: 1.814751\tLR: 0.030000\n",
      "Train Epoch: 1515 [106496/194182 (54%)]\tLoss: 0.242423\tGrad Norm: 2.113643\tLR: 0.030000\n",
      "Train Epoch: 1515 [126976/194182 (65%)]\tLoss: 0.239576\tGrad Norm: 1.894078\tLR: 0.030000\n",
      "Train Epoch: 1515 [147456/194182 (75%)]\tLoss: 0.245094\tGrad Norm: 1.917806\tLR: 0.030000\n",
      "Train Epoch: 1515 [167936/194182 (85%)]\tLoss: 0.242339\tGrad Norm: 1.714434\tLR: 0.030000\n",
      "Train Epoch: 1515 [188416/194182 (96%)]\tLoss: 0.241776\tGrad Norm: 1.928609\tLR: 0.030000\n",
      "Train set: Average loss: 0.2389\n",
      "Test set: Average loss: 0.2506, Average MAE: 0.3355\n",
      "Epoch 1515: Mean reward = 0.046 +/- 0.037\n",
      "Train Epoch: 1516 [4096/194182 (2%)]\tLoss: 0.233541\tGrad Norm: 1.681606\tLR: 0.030000\n",
      "Train Epoch: 1516 [24576/194182 (12%)]\tLoss: 0.240416\tGrad Norm: 1.813401\tLR: 0.030000\n",
      "Train Epoch: 1516 [45056/194182 (23%)]\tLoss: 0.237090\tGrad Norm: 1.729413\tLR: 0.030000\n",
      "Train Epoch: 1516 [65536/194182 (33%)]\tLoss: 0.235341\tGrad Norm: 1.619433\tLR: 0.030000\n",
      "Train Epoch: 1516 [86016/194182 (44%)]\tLoss: 0.239529\tGrad Norm: 1.722016\tLR: 0.030000\n",
      "Train Epoch: 1516 [106496/194182 (54%)]\tLoss: 0.243770\tGrad Norm: 1.840686\tLR: 0.030000\n",
      "Train Epoch: 1516 [126976/194182 (65%)]\tLoss: 0.239438\tGrad Norm: 1.620372\tLR: 0.030000\n",
      "Train Epoch: 1516 [147456/194182 (75%)]\tLoss: 0.232003\tGrad Norm: 1.395937\tLR: 0.030000\n",
      "Train Epoch: 1516 [167936/194182 (85%)]\tLoss: 0.236311\tGrad Norm: 1.681689\tLR: 0.030000\n",
      "Train Epoch: 1516 [188416/194182 (96%)]\tLoss: 0.232920\tGrad Norm: 1.412858\tLR: 0.030000\n",
      "Train set: Average loss: 0.2374\n",
      "Test set: Average loss: 0.2528, Average MAE: 0.3530\n",
      "Train Epoch: 1517 [4096/194182 (2%)]\tLoss: 0.233500\tGrad Norm: 1.476305\tLR: 0.030000\n",
      "Train Epoch: 1517 [24576/194182 (12%)]\tLoss: 0.230117\tGrad Norm: 1.684329\tLR: 0.030000\n",
      "Train Epoch: 1517 [45056/194182 (23%)]\tLoss: 0.230732\tGrad Norm: 1.714713\tLR: 0.030000\n",
      "Train Epoch: 1517 [65536/194182 (33%)]\tLoss: 0.245174\tGrad Norm: 2.020987\tLR: 0.030000\n",
      "Train Epoch: 1517 [86016/194182 (44%)]\tLoss: 0.238385\tGrad Norm: 1.812247\tLR: 0.030000\n",
      "Train Epoch: 1517 [106496/194182 (54%)]\tLoss: 0.240780\tGrad Norm: 1.805696\tLR: 0.030000\n",
      "Train Epoch: 1517 [126976/194182 (65%)]\tLoss: 0.237502\tGrad Norm: 1.594641\tLR: 0.030000\n",
      "Train Epoch: 1517 [147456/194182 (75%)]\tLoss: 0.233477\tGrad Norm: 1.482922\tLR: 0.030000\n",
      "Train Epoch: 1517 [167936/194182 (85%)]\tLoss: 0.236813\tGrad Norm: 1.715540\tLR: 0.030000\n",
      "Train Epoch: 1517 [188416/194182 (96%)]\tLoss: 0.237777\tGrad Norm: 1.565419\tLR: 0.030000\n",
      "Train set: Average loss: 0.2372\n",
      "Test set: Average loss: 0.2597, Average MAE: 0.3552\n",
      "Train Epoch: 1518 [4096/194182 (2%)]\tLoss: 0.241710\tGrad Norm: 1.740732\tLR: 0.030000\n",
      "Train Epoch: 1518 [24576/194182 (12%)]\tLoss: 0.240980\tGrad Norm: 1.700218\tLR: 0.030000\n",
      "Train Epoch: 1518 [45056/194182 (23%)]\tLoss: 0.237317\tGrad Norm: 1.546159\tLR: 0.030000\n",
      "Train Epoch: 1518 [65536/194182 (33%)]\tLoss: 0.229119\tGrad Norm: 1.222308\tLR: 0.030000\n",
      "Train Epoch: 1518 [86016/194182 (44%)]\tLoss: 0.236689\tGrad Norm: 1.654287\tLR: 0.030000\n",
      "Train Epoch: 1518 [106496/194182 (54%)]\tLoss: 0.247534\tGrad Norm: 2.106476\tLR: 0.030000\n",
      "Train Epoch: 1518 [126976/194182 (65%)]\tLoss: 0.245094\tGrad Norm: 2.049188\tLR: 0.030000\n",
      "Train Epoch: 1518 [147456/194182 (75%)]\tLoss: 0.240484\tGrad Norm: 1.740984\tLR: 0.030000\n",
      "Train Epoch: 1518 [167936/194182 (85%)]\tLoss: 0.241047\tGrad Norm: 1.886670\tLR: 0.030000\n",
      "Train Epoch: 1518 [188416/194182 (96%)]\tLoss: 0.234593\tGrad Norm: 1.625722\tLR: 0.030000\n",
      "Train set: Average loss: 0.2379\n",
      "Test set: Average loss: 0.2488, Average MAE: 0.3372\n",
      "Train Epoch: 1519 [4096/194182 (2%)]\tLoss: 0.226902\tGrad Norm: 1.363821\tLR: 0.030000\n",
      "Train Epoch: 1519 [24576/194182 (12%)]\tLoss: 0.232011\tGrad Norm: 1.406274\tLR: 0.030000\n",
      "Train Epoch: 1519 [45056/194182 (23%)]\tLoss: 0.238401\tGrad Norm: 1.808218\tLR: 0.030000\n",
      "Train Epoch: 1519 [65536/194182 (33%)]\tLoss: 0.238614\tGrad Norm: 1.637220\tLR: 0.030000\n",
      "Train Epoch: 1519 [86016/194182 (44%)]\tLoss: 0.233271\tGrad Norm: 1.568254\tLR: 0.030000\n",
      "Train Epoch: 1519 [106496/194182 (54%)]\tLoss: 0.234551\tGrad Norm: 1.762786\tLR: 0.030000\n",
      "Train Epoch: 1519 [126976/194182 (65%)]\tLoss: 0.246941\tGrad Norm: 1.901394\tLR: 0.030000\n",
      "Train Epoch: 1519 [147456/194182 (75%)]\tLoss: 0.241391\tGrad Norm: 1.784490\tLR: 0.030000\n",
      "Train Epoch: 1519 [167936/194182 (85%)]\tLoss: 0.234548\tGrad Norm: 1.448002\tLR: 0.030000\n",
      "Train Epoch: 1519 [188416/194182 (96%)]\tLoss: 0.239243\tGrad Norm: 1.732484\tLR: 0.030000\n",
      "Train set: Average loss: 0.2363\n",
      "Test set: Average loss: 0.2558, Average MAE: 0.3545\n",
      "Train Epoch: 1520 [4096/194182 (2%)]\tLoss: 0.230119\tGrad Norm: 1.667064\tLR: 0.030000\n",
      "Train Epoch: 1520 [24576/194182 (12%)]\tLoss: 0.228714\tGrad Norm: 1.879072\tLR: 0.030000\n",
      "Train Epoch: 1520 [45056/194182 (23%)]\tLoss: 0.238836\tGrad Norm: 1.734205\tLR: 0.030000\n",
      "Train Epoch: 1520 [65536/194182 (33%)]\tLoss: 0.237994\tGrad Norm: 2.001256\tLR: 0.030000\n",
      "Train Epoch: 1520 [86016/194182 (44%)]\tLoss: 0.261412\tGrad Norm: 2.090493\tLR: 0.030000\n",
      "Train Epoch: 1520 [106496/194182 (54%)]\tLoss: 0.232332\tGrad Norm: 1.556953\tLR: 0.030000\n",
      "Train Epoch: 1520 [126976/194182 (65%)]\tLoss: 0.231932\tGrad Norm: 1.607914\tLR: 0.030000\n",
      "Train Epoch: 1520 [147456/194182 (75%)]\tLoss: 0.246239\tGrad Norm: 1.993418\tLR: 0.030000\n",
      "Train Epoch: 1520 [167936/194182 (85%)]\tLoss: 0.232307\tGrad Norm: 1.725860\tLR: 0.030000\n",
      "Train Epoch: 1520 [188416/194182 (96%)]\tLoss: 0.247978\tGrad Norm: 1.962212\tLR: 0.030000\n",
      "Train set: Average loss: 0.2387\n",
      "Test set: Average loss: 0.2595, Average MAE: 0.3564\n",
      "Epoch 1520: Mean reward = 0.049 +/- 0.043\n",
      "Train Epoch: 1521 [4096/194182 (2%)]\tLoss: 0.241964\tGrad Norm: 1.972734\tLR: 0.030000\n",
      "Train Epoch: 1521 [24576/194182 (12%)]\tLoss: 0.229988\tGrad Norm: 1.255356\tLR: 0.030000\n",
      "Train Epoch: 1521 [45056/194182 (23%)]\tLoss: 0.230844\tGrad Norm: 1.327402\tLR: 0.030000\n",
      "Train Epoch: 1521 [65536/194182 (33%)]\tLoss: 0.232476\tGrad Norm: 1.592158\tLR: 0.030000\n",
      "Train Epoch: 1521 [86016/194182 (44%)]\tLoss: 0.241903\tGrad Norm: 1.749607\tLR: 0.030000\n",
      "Train Epoch: 1521 [106496/194182 (54%)]\tLoss: 0.236071\tGrad Norm: 1.701398\tLR: 0.030000\n",
      "Train Epoch: 1521 [126976/194182 (65%)]\tLoss: 0.230882\tGrad Norm: 1.366136\tLR: 0.030000\n",
      "Train Epoch: 1521 [147456/194182 (75%)]\tLoss: 0.223868\tGrad Norm: 1.433969\tLR: 0.030000\n",
      "Train Epoch: 1521 [167936/194182 (85%)]\tLoss: 0.232684\tGrad Norm: 1.596145\tLR: 0.030000\n",
      "Train Epoch: 1521 [188416/194182 (96%)]\tLoss: 0.238611\tGrad Norm: 1.581505\tLR: 0.030000\n",
      "Train set: Average loss: 0.2339\n",
      "Test set: Average loss: 0.2506, Average MAE: 0.3427\n",
      "Train Epoch: 1522 [4096/194182 (2%)]\tLoss: 0.237308\tGrad Norm: 1.560736\tLR: 0.030000\n",
      "Train Epoch: 1522 [24576/194182 (12%)]\tLoss: 0.235529\tGrad Norm: 1.525698\tLR: 0.030000\n",
      "Train Epoch: 1522 [45056/194182 (23%)]\tLoss: 0.246658\tGrad Norm: 1.923850\tLR: 0.030000\n",
      "Train Epoch: 1522 [65536/194182 (33%)]\tLoss: 0.235251\tGrad Norm: 1.537534\tLR: 0.030000\n",
      "Train Epoch: 1522 [86016/194182 (44%)]\tLoss: 0.233748\tGrad Norm: 1.725711\tLR: 0.030000\n",
      "Train Epoch: 1522 [106496/194182 (54%)]\tLoss: 0.235052\tGrad Norm: 1.647093\tLR: 0.030000\n",
      "Train Epoch: 1522 [126976/194182 (65%)]\tLoss: 0.238176\tGrad Norm: 1.769055\tLR: 0.030000\n",
      "Train Epoch: 1522 [147456/194182 (75%)]\tLoss: 0.235581\tGrad Norm: 1.506385\tLR: 0.030000\n",
      "Train Epoch: 1522 [167936/194182 (85%)]\tLoss: 0.232456\tGrad Norm: 1.642126\tLR: 0.030000\n",
      "Train Epoch: 1522 [188416/194182 (96%)]\tLoss: 0.242851\tGrad Norm: 1.706376\tLR: 0.030000\n",
      "Train set: Average loss: 0.2363\n",
      "Test set: Average loss: 0.2537, Average MAE: 0.3552\n",
      "Train Epoch: 1523 [4096/194182 (2%)]\tLoss: 0.231419\tGrad Norm: 1.650024\tLR: 0.030000\n",
      "Train Epoch: 1523 [24576/194182 (12%)]\tLoss: 0.229044\tGrad Norm: 1.485195\tLR: 0.030000\n",
      "Train Epoch: 1523 [45056/194182 (23%)]\tLoss: 0.234763\tGrad Norm: 1.655270\tLR: 0.030000\n",
      "Train Epoch: 1523 [65536/194182 (33%)]\tLoss: 0.233652\tGrad Norm: 1.601281\tLR: 0.030000\n",
      "Train Epoch: 1523 [86016/194182 (44%)]\tLoss: 0.234282\tGrad Norm: 1.638612\tLR: 0.030000\n",
      "Train Epoch: 1523 [106496/194182 (54%)]\tLoss: 0.234589\tGrad Norm: 1.636622\tLR: 0.030000\n",
      "Train Epoch: 1523 [126976/194182 (65%)]\tLoss: 0.229242\tGrad Norm: 1.436582\tLR: 0.030000\n",
      "Train Epoch: 1523 [147456/194182 (75%)]\tLoss: 0.230823\tGrad Norm: 1.522299\tLR: 0.030000\n",
      "Train Epoch: 1523 [167936/194182 (85%)]\tLoss: 0.244607\tGrad Norm: 1.625353\tLR: 0.030000\n",
      "Train Epoch: 1523 [188416/194182 (96%)]\tLoss: 0.241706\tGrad Norm: 1.508149\tLR: 0.030000\n",
      "Train set: Average loss: 0.2347\n",
      "Test set: Average loss: 0.2491, Average MAE: 0.3410\n",
      "Train Epoch: 1524 [4096/194182 (2%)]\tLoss: 0.228362\tGrad Norm: 1.441686\tLR: 0.030000\n",
      "Train Epoch: 1524 [24576/194182 (12%)]\tLoss: 0.243359\tGrad Norm: 1.650789\tLR: 0.030000\n",
      "Train Epoch: 1524 [45056/194182 (23%)]\tLoss: 0.231961\tGrad Norm: 1.264803\tLR: 0.030000\n",
      "Train Epoch: 1524 [65536/194182 (33%)]\tLoss: 0.236804\tGrad Norm: 1.623739\tLR: 0.030000\n",
      "Train Epoch: 1524 [86016/194182 (44%)]\tLoss: 0.242136\tGrad Norm: 2.197745\tLR: 0.030000\n",
      "Train Epoch: 1524 [106496/194182 (54%)]\tLoss: 0.240539\tGrad Norm: 1.926388\tLR: 0.030000\n",
      "Train Epoch: 1524 [126976/194182 (65%)]\tLoss: 0.249543\tGrad Norm: 2.313234\tLR: 0.030000\n",
      "Train Epoch: 1524 [147456/194182 (75%)]\tLoss: 0.236429\tGrad Norm: 1.602409\tLR: 0.030000\n",
      "Train Epoch: 1524 [167936/194182 (85%)]\tLoss: 0.238066\tGrad Norm: 2.035038\tLR: 0.030000\n",
      "Train Epoch: 1524 [188416/194182 (96%)]\tLoss: 0.223880\tGrad Norm: 1.194820\tLR: 0.030000\n",
      "Train set: Average loss: 0.2374\n",
      "Test set: Average loss: 0.2523, Average MAE: 0.3512\n",
      "Train Epoch: 1525 [4096/194182 (2%)]\tLoss: 0.226687\tGrad Norm: 1.331458\tLR: 0.030000\n",
      "Train Epoch: 1525 [24576/194182 (12%)]\tLoss: 0.227715\tGrad Norm: 1.313422\tLR: 0.030000\n",
      "Train Epoch: 1525 [45056/194182 (23%)]\tLoss: 0.231467\tGrad Norm: 1.539005\tLR: 0.030000\n",
      "Train Epoch: 1525 [65536/194182 (33%)]\tLoss: 0.243005\tGrad Norm: 2.128466\tLR: 0.030000\n",
      "Train Epoch: 1525 [86016/194182 (44%)]\tLoss: 0.236202\tGrad Norm: 1.588073\tLR: 0.030000\n",
      "Train Epoch: 1525 [106496/194182 (54%)]\tLoss: 0.224220\tGrad Norm: 1.347395\tLR: 0.030000\n",
      "Train Epoch: 1525 [126976/194182 (65%)]\tLoss: 0.239926\tGrad Norm: 1.691855\tLR: 0.030000\n",
      "Train Epoch: 1525 [147456/194182 (75%)]\tLoss: 0.237032\tGrad Norm: 1.705518\tLR: 0.030000\n",
      "Train Epoch: 1525 [167936/194182 (85%)]\tLoss: 0.236032\tGrad Norm: 1.796436\tLR: 0.030000\n",
      "Train Epoch: 1525 [188416/194182 (96%)]\tLoss: 0.234844\tGrad Norm: 1.774039\tLR: 0.030000\n",
      "Train set: Average loss: 0.2354\n",
      "Test set: Average loss: 0.2507, Average MAE: 0.3441\n",
      "Epoch 1525: Mean reward = 0.046 +/- 0.036\n",
      "Train Epoch: 1526 [4096/194182 (2%)]\tLoss: 0.237283\tGrad Norm: 1.631908\tLR: 0.030000\n",
      "Train Epoch: 1526 [24576/194182 (12%)]\tLoss: 0.235391\tGrad Norm: 1.575314\tLR: 0.030000\n",
      "Train Epoch: 1526 [45056/194182 (23%)]\tLoss: 0.230002\tGrad Norm: 1.378855\tLR: 0.030000\n",
      "Train Epoch: 1526 [65536/194182 (33%)]\tLoss: 0.238311\tGrad Norm: 1.773555\tLR: 0.030000\n",
      "Train Epoch: 1526 [86016/194182 (44%)]\tLoss: 0.231110\tGrad Norm: 1.813854\tLR: 0.030000\n",
      "Train Epoch: 1526 [106496/194182 (54%)]\tLoss: 0.239511\tGrad Norm: 1.860341\tLR: 0.030000\n",
      "Train Epoch: 1526 [126976/194182 (65%)]\tLoss: 0.231486\tGrad Norm: 1.662251\tLR: 0.030000\n",
      "Train Epoch: 1526 [147456/194182 (75%)]\tLoss: 0.242335\tGrad Norm: 1.598344\tLR: 0.030000\n",
      "Train Epoch: 1526 [167936/194182 (85%)]\tLoss: 0.225904\tGrad Norm: 1.307083\tLR: 0.030000\n",
      "Train Epoch: 1526 [188416/194182 (96%)]\tLoss: 0.236343\tGrad Norm: 1.879194\tLR: 0.030000\n",
      "Train set: Average loss: 0.2359\n",
      "Test set: Average loss: 0.2571, Average MAE: 0.3506\n",
      "Train Epoch: 1527 [4096/194182 (2%)]\tLoss: 0.238779\tGrad Norm: 1.605383\tLR: 0.030000\n",
      "Train Epoch: 1527 [24576/194182 (12%)]\tLoss: 0.240884\tGrad Norm: 1.681369\tLR: 0.030000\n",
      "Train Epoch: 1527 [45056/194182 (23%)]\tLoss: 0.231062\tGrad Norm: 1.563537\tLR: 0.030000\n",
      "Train Epoch: 1527 [65536/194182 (33%)]\tLoss: 0.240082\tGrad Norm: 1.957997\tLR: 0.030000\n",
      "Train Epoch: 1527 [86016/194182 (44%)]\tLoss: 0.238458\tGrad Norm: 1.499715\tLR: 0.030000\n",
      "Train Epoch: 1527 [106496/194182 (54%)]\tLoss: 0.238039\tGrad Norm: 1.587890\tLR: 0.030000\n",
      "Train Epoch: 1527 [126976/194182 (65%)]\tLoss: 0.241724\tGrad Norm: 1.983725\tLR: 0.030000\n",
      "Train Epoch: 1527 [147456/194182 (75%)]\tLoss: 0.229379\tGrad Norm: 1.510965\tLR: 0.030000\n",
      "Train Epoch: 1527 [167936/194182 (85%)]\tLoss: 0.230150\tGrad Norm: 1.632750\tLR: 0.030000\n",
      "Train Epoch: 1527 [188416/194182 (96%)]\tLoss: 0.234299\tGrad Norm: 1.655439\tLR: 0.030000\n",
      "Train set: Average loss: 0.2356\n",
      "Test set: Average loss: 0.2543, Average MAE: 0.3375\n",
      "Train Epoch: 1528 [4096/194182 (2%)]\tLoss: 0.231500\tGrad Norm: 1.821752\tLR: 0.030000\n",
      "Train Epoch: 1528 [24576/194182 (12%)]\tLoss: 0.233499\tGrad Norm: 1.692039\tLR: 0.030000\n",
      "Train Epoch: 1528 [45056/194182 (23%)]\tLoss: 0.238966\tGrad Norm: 2.032963\tLR: 0.030000\n",
      "Train Epoch: 1528 [65536/194182 (33%)]\tLoss: 0.240169\tGrad Norm: 1.926126\tLR: 0.030000\n",
      "Train Epoch: 1528 [86016/194182 (44%)]\tLoss: 0.234425\tGrad Norm: 1.591459\tLR: 0.030000\n",
      "Train Epoch: 1528 [106496/194182 (54%)]\tLoss: 0.233155\tGrad Norm: 1.614975\tLR: 0.030000\n",
      "Train Epoch: 1528 [126976/194182 (65%)]\tLoss: 0.232453\tGrad Norm: 1.501520\tLR: 0.030000\n",
      "Train Epoch: 1528 [147456/194182 (75%)]\tLoss: 0.241447\tGrad Norm: 1.664919\tLR: 0.030000\n",
      "Train Epoch: 1528 [167936/194182 (85%)]\tLoss: 0.233956\tGrad Norm: 1.318091\tLR: 0.030000\n",
      "Train Epoch: 1528 [188416/194182 (96%)]\tLoss: 0.227852\tGrad Norm: 1.455912\tLR: 0.030000\n",
      "Train set: Average loss: 0.2358\n",
      "Test set: Average loss: 0.2528, Average MAE: 0.3494\n",
      "Train Epoch: 1529 [4096/194182 (2%)]\tLoss: 0.226269\tGrad Norm: 1.526922\tLR: 0.030000\n",
      "Train Epoch: 1529 [24576/194182 (12%)]\tLoss: 0.231360\tGrad Norm: 1.623628\tLR: 0.030000\n",
      "Train Epoch: 1529 [45056/194182 (23%)]\tLoss: 0.230627\tGrad Norm: 1.556430\tLR: 0.030000\n",
      "Train Epoch: 1529 [65536/194182 (33%)]\tLoss: 0.232684\tGrad Norm: 1.469199\tLR: 0.030000\n",
      "Train Epoch: 1529 [86016/194182 (44%)]\tLoss: 0.235333\tGrad Norm: 1.511670\tLR: 0.030000\n",
      "Train Epoch: 1529 [106496/194182 (54%)]\tLoss: 0.240184\tGrad Norm: 2.008281\tLR: 0.030000\n",
      "Train Epoch: 1529 [126976/194182 (65%)]\tLoss: 0.226684\tGrad Norm: 1.633335\tLR: 0.030000\n",
      "Train Epoch: 1529 [147456/194182 (75%)]\tLoss: 0.237649\tGrad Norm: 1.697101\tLR: 0.030000\n",
      "Train Epoch: 1529 [167936/194182 (85%)]\tLoss: 0.237103\tGrad Norm: 1.418099\tLR: 0.030000\n",
      "Train Epoch: 1529 [188416/194182 (96%)]\tLoss: 0.245833\tGrad Norm: 1.772886\tLR: 0.030000\n",
      "Train set: Average loss: 0.2342\n",
      "Test set: Average loss: 0.2525, Average MAE: 0.3437\n",
      "Train Epoch: 1530 [4096/194182 (2%)]\tLoss: 0.239407\tGrad Norm: 1.594635\tLR: 0.030000\n",
      "Train Epoch: 1530 [24576/194182 (12%)]\tLoss: 0.232795\tGrad Norm: 1.480967\tLR: 0.030000\n",
      "Train Epoch: 1530 [45056/194182 (23%)]\tLoss: 0.232906\tGrad Norm: 1.534813\tLR: 0.030000\n",
      "Train Epoch: 1530 [65536/194182 (33%)]\tLoss: 0.234415\tGrad Norm: 1.522938\tLR: 0.030000\n",
      "Train Epoch: 1530 [86016/194182 (44%)]\tLoss: 0.225572\tGrad Norm: 1.452245\tLR: 0.030000\n",
      "Train Epoch: 1530 [106496/194182 (54%)]\tLoss: 0.234007\tGrad Norm: 1.741904\tLR: 0.030000\n",
      "Train Epoch: 1530 [126976/194182 (65%)]\tLoss: 0.230197\tGrad Norm: 1.850135\tLR: 0.030000\n",
      "Train Epoch: 1530 [147456/194182 (75%)]\tLoss: 0.234074\tGrad Norm: 1.780817\tLR: 0.030000\n",
      "Train Epoch: 1530 [167936/194182 (85%)]\tLoss: 0.230287\tGrad Norm: 1.462674\tLR: 0.030000\n",
      "Train Epoch: 1530 [188416/194182 (96%)]\tLoss: 0.230729\tGrad Norm: 1.650827\tLR: 0.030000\n",
      "Train set: Average loss: 0.2342\n",
      "Test set: Average loss: 0.2536, Average MAE: 0.3551\n",
      "Epoch 1530: Mean reward = 0.049 +/- 0.029\n",
      "Train Epoch: 1531 [4096/194182 (2%)]\tLoss: 0.229326\tGrad Norm: 1.564088\tLR: 0.030000\n",
      "Train Epoch: 1531 [24576/194182 (12%)]\tLoss: 0.237183\tGrad Norm: 1.550915\tLR: 0.030000\n",
      "Train Epoch: 1531 [45056/194182 (23%)]\tLoss: 0.228675\tGrad Norm: 1.586961\tLR: 0.030000\n",
      "Train Epoch: 1531 [65536/194182 (33%)]\tLoss: 0.235237\tGrad Norm: 1.778880\tLR: 0.030000\n",
      "Train Epoch: 1531 [86016/194182 (44%)]\tLoss: 0.241451\tGrad Norm: 1.961521\tLR: 0.030000\n",
      "Train Epoch: 1531 [106496/194182 (54%)]\tLoss: 0.231248\tGrad Norm: 1.763747\tLR: 0.030000\n",
      "Train Epoch: 1531 [126976/194182 (65%)]\tLoss: 0.238826\tGrad Norm: 1.895769\tLR: 0.030000\n",
      "Train Epoch: 1531 [147456/194182 (75%)]\tLoss: 0.235670\tGrad Norm: 1.769263\tLR: 0.030000\n",
      "Train Epoch: 1531 [167936/194182 (85%)]\tLoss: 0.228406\tGrad Norm: 1.358782\tLR: 0.030000\n",
      "Train Epoch: 1531 [188416/194182 (96%)]\tLoss: 0.229652\tGrad Norm: 1.330812\tLR: 0.030000\n",
      "Train set: Average loss: 0.2360\n",
      "Test set: Average loss: 0.2518, Average MAE: 0.3575\n",
      "Train Epoch: 1532 [4096/194182 (2%)]\tLoss: 0.234991\tGrad Norm: 1.483335\tLR: 0.030000\n",
      "Train Epoch: 1532 [24576/194182 (12%)]\tLoss: 0.234096\tGrad Norm: 1.580235\tLR: 0.030000\n",
      "Train Epoch: 1532 [45056/194182 (23%)]\tLoss: 0.232015\tGrad Norm: 1.569605\tLR: 0.030000\n",
      "Train Epoch: 1532 [65536/194182 (33%)]\tLoss: 0.237599\tGrad Norm: 1.872421\tLR: 0.030000\n",
      "Train Epoch: 1532 [86016/194182 (44%)]\tLoss: 0.227582\tGrad Norm: 1.530395\tLR: 0.030000\n",
      "Train Epoch: 1532 [106496/194182 (54%)]\tLoss: 0.236621\tGrad Norm: 1.623120\tLR: 0.030000\n",
      "Train Epoch: 1532 [126976/194182 (65%)]\tLoss: 0.239048\tGrad Norm: 1.872491\tLR: 0.030000\n",
      "Train Epoch: 1532 [147456/194182 (75%)]\tLoss: 0.240853\tGrad Norm: 1.968286\tLR: 0.030000\n",
      "Train Epoch: 1532 [167936/194182 (85%)]\tLoss: 0.231628\tGrad Norm: 1.475239\tLR: 0.030000\n",
      "Train Epoch: 1532 [188416/194182 (96%)]\tLoss: 0.230211\tGrad Norm: 1.372106\tLR: 0.030000\n",
      "Train set: Average loss: 0.2338\n",
      "Test set: Average loss: 0.2532, Average MAE: 0.3489\n",
      "Train Epoch: 1533 [4096/194182 (2%)]\tLoss: 0.229668\tGrad Norm: 1.667180\tLR: 0.030000\n",
      "Train Epoch: 1533 [24576/194182 (12%)]\tLoss: 0.236355\tGrad Norm: 1.578607\tLR: 0.030000\n",
      "Train Epoch: 1533 [45056/194182 (23%)]\tLoss: 0.239007\tGrad Norm: 1.783821\tLR: 0.030000\n",
      "Train Epoch: 1533 [65536/194182 (33%)]\tLoss: 0.240827\tGrad Norm: 1.757249\tLR: 0.030000\n",
      "Train Epoch: 1533 [86016/194182 (44%)]\tLoss: 0.233549\tGrad Norm: 1.362101\tLR: 0.030000\n",
      "Train Epoch: 1533 [106496/194182 (54%)]\tLoss: 0.244572\tGrad Norm: 1.682115\tLR: 0.030000\n",
      "Train Epoch: 1533 [126976/194182 (65%)]\tLoss: 0.239879\tGrad Norm: 1.844062\tLR: 0.030000\n",
      "Train Epoch: 1533 [147456/194182 (75%)]\tLoss: 0.239703\tGrad Norm: 1.628127\tLR: 0.030000\n",
      "Train Epoch: 1533 [167936/194182 (85%)]\tLoss: 0.225241\tGrad Norm: 1.232262\tLR: 0.030000\n",
      "Train Epoch: 1533 [188416/194182 (96%)]\tLoss: 0.237523\tGrad Norm: 1.845176\tLR: 0.030000\n",
      "Train set: Average loss: 0.2357\n",
      "Test set: Average loss: 0.2607, Average MAE: 0.3539\n",
      "Train Epoch: 1534 [4096/194182 (2%)]\tLoss: 0.239527\tGrad Norm: 1.965790\tLR: 0.030000\n",
      "Train Epoch: 1534 [24576/194182 (12%)]\tLoss: 0.232385\tGrad Norm: 1.561286\tLR: 0.030000\n",
      "Train Epoch: 1534 [45056/194182 (23%)]\tLoss: 0.229050\tGrad Norm: 1.224526\tLR: 0.030000\n",
      "Train Epoch: 1534 [65536/194182 (33%)]\tLoss: 0.224950\tGrad Norm: 1.220692\tLR: 0.030000\n",
      "Train Epoch: 1534 [86016/194182 (44%)]\tLoss: 0.231253\tGrad Norm: 1.460050\tLR: 0.030000\n",
      "Train Epoch: 1534 [106496/194182 (54%)]\tLoss: 0.233209\tGrad Norm: 1.438547\tLR: 0.030000\n",
      "Train Epoch: 1534 [126976/194182 (65%)]\tLoss: 0.227844\tGrad Norm: 1.634061\tLR: 0.030000\n",
      "Train Epoch: 1534 [147456/194182 (75%)]\tLoss: 0.236268\tGrad Norm: 2.032253\tLR: 0.030000\n",
      "Train Epoch: 1534 [167936/194182 (85%)]\tLoss: 0.236266\tGrad Norm: 1.727084\tLR: 0.030000\n",
      "Train Epoch: 1534 [188416/194182 (96%)]\tLoss: 0.230194\tGrad Norm: 1.768523\tLR: 0.030000\n",
      "Train set: Average loss: 0.2342\n",
      "Test set: Average loss: 0.2560, Average MAE: 0.3405\n",
      "Train Epoch: 1535 [4096/194182 (2%)]\tLoss: 0.240750\tGrad Norm: 2.100174\tLR: 0.030000\n",
      "Train Epoch: 1535 [24576/194182 (12%)]\tLoss: 0.236173\tGrad Norm: 2.047093\tLR: 0.030000\n",
      "Train Epoch: 1535 [45056/194182 (23%)]\tLoss: 0.238297\tGrad Norm: 1.947428\tLR: 0.030000\n",
      "Train Epoch: 1535 [65536/194182 (33%)]\tLoss: 0.234897\tGrad Norm: 1.576660\tLR: 0.030000\n",
      "Train Epoch: 1535 [86016/194182 (44%)]\tLoss: 0.247313\tGrad Norm: 1.779289\tLR: 0.030000\n",
      "Train Epoch: 1535 [106496/194182 (54%)]\tLoss: 0.239293\tGrad Norm: 1.669273\tLR: 0.030000\n",
      "Train Epoch: 1535 [126976/194182 (65%)]\tLoss: 0.228081\tGrad Norm: 1.317009\tLR: 0.030000\n",
      "Train Epoch: 1535 [147456/194182 (75%)]\tLoss: 0.239854\tGrad Norm: 1.484083\tLR: 0.030000\n",
      "Train Epoch: 1535 [167936/194182 (85%)]\tLoss: 0.234703\tGrad Norm: 1.494102\tLR: 0.030000\n",
      "Train Epoch: 1535 [188416/194182 (96%)]\tLoss: 0.234782\tGrad Norm: 1.528579\tLR: 0.030000\n",
      "Train set: Average loss: 0.2353\n",
      "Test set: Average loss: 0.2530, Average MAE: 0.3531\n",
      "Epoch 1535: Mean reward = 0.055 +/- 0.028\n",
      "Train Epoch: 1536 [4096/194182 (2%)]\tLoss: 0.233632\tGrad Norm: 1.542473\tLR: 0.030000\n",
      "Train Epoch: 1536 [24576/194182 (12%)]\tLoss: 0.243505\tGrad Norm: 1.769947\tLR: 0.030000\n",
      "Train Epoch: 1536 [45056/194182 (23%)]\tLoss: 0.231322\tGrad Norm: 1.550933\tLR: 0.030000\n",
      "Train Epoch: 1536 [65536/194182 (33%)]\tLoss: 0.238045\tGrad Norm: 1.610474\tLR: 0.030000\n",
      "Train Epoch: 1536 [86016/194182 (44%)]\tLoss: 0.234618\tGrad Norm: 1.820875\tLR: 0.030000\n",
      "Train Epoch: 1536 [106496/194182 (54%)]\tLoss: 0.239215\tGrad Norm: 2.079213\tLR: 0.030000\n",
      "Train Epoch: 1536 [126976/194182 (65%)]\tLoss: 0.236815\tGrad Norm: 1.791438\tLR: 0.030000\n",
      "Train Epoch: 1536 [147456/194182 (75%)]\tLoss: 0.234577\tGrad Norm: 1.744600\tLR: 0.030000\n",
      "Train Epoch: 1536 [167936/194182 (85%)]\tLoss: 0.230438\tGrad Norm: 1.538860\tLR: 0.030000\n",
      "Train Epoch: 1536 [188416/194182 (96%)]\tLoss: 0.245988\tGrad Norm: 1.998668\tLR: 0.030000\n",
      "Train set: Average loss: 0.2355\n",
      "Test set: Average loss: 0.2570, Average MAE: 0.3445\n",
      "Train Epoch: 1537 [4096/194182 (2%)]\tLoss: 0.238484\tGrad Norm: 1.873285\tLR: 0.030000\n",
      "Train Epoch: 1537 [24576/194182 (12%)]\tLoss: 0.232711\tGrad Norm: 1.660570\tLR: 0.030000\n",
      "Train Epoch: 1537 [45056/194182 (23%)]\tLoss: 0.228804\tGrad Norm: 1.503427\tLR: 0.030000\n",
      "Train Epoch: 1537 [65536/194182 (33%)]\tLoss: 0.234383\tGrad Norm: 1.681923\tLR: 0.030000\n",
      "Train Epoch: 1537 [86016/194182 (44%)]\tLoss: 0.230721\tGrad Norm: 1.841554\tLR: 0.030000\n",
      "Train Epoch: 1537 [106496/194182 (54%)]\tLoss: 0.228451\tGrad Norm: 1.128445\tLR: 0.030000\n",
      "Train Epoch: 1537 [126976/194182 (65%)]\tLoss: 0.232569\tGrad Norm: 1.395303\tLR: 0.030000\n",
      "Train Epoch: 1537 [147456/194182 (75%)]\tLoss: 0.229778\tGrad Norm: 1.512637\tLR: 0.030000\n",
      "Train Epoch: 1537 [167936/194182 (85%)]\tLoss: 0.229771\tGrad Norm: 1.510660\tLR: 0.030000\n",
      "Train Epoch: 1537 [188416/194182 (96%)]\tLoss: 0.235824\tGrad Norm: 1.365773\tLR: 0.030000\n",
      "Train set: Average loss: 0.2330\n",
      "Test set: Average loss: 0.2523, Average MAE: 0.3508\n",
      "Train Epoch: 1538 [4096/194182 (2%)]\tLoss: 0.232220\tGrad Norm: 1.487721\tLR: 0.030000\n",
      "Train Epoch: 1538 [24576/194182 (12%)]\tLoss: 0.229576\tGrad Norm: 1.378487\tLR: 0.030000\n",
      "Train Epoch: 1538 [45056/194182 (23%)]\tLoss: 0.235405\tGrad Norm: 1.825473\tLR: 0.030000\n",
      "Train Epoch: 1538 [65536/194182 (33%)]\tLoss: 0.235326\tGrad Norm: 1.857084\tLR: 0.030000\n",
      "Train Epoch: 1538 [86016/194182 (44%)]\tLoss: 0.237866\tGrad Norm: 1.667963\tLR: 0.030000\n",
      "Train Epoch: 1538 [106496/194182 (54%)]\tLoss: 0.235000\tGrad Norm: 1.577931\tLR: 0.030000\n",
      "Train Epoch: 1538 [126976/194182 (65%)]\tLoss: 0.229676\tGrad Norm: 1.401966\tLR: 0.030000\n",
      "Train Epoch: 1538 [147456/194182 (75%)]\tLoss: 0.240136\tGrad Norm: 1.803247\tLR: 0.030000\n",
      "Train Epoch: 1538 [167936/194182 (85%)]\tLoss: 0.238290\tGrad Norm: 1.677113\tLR: 0.030000\n",
      "Train Epoch: 1538 [188416/194182 (96%)]\tLoss: 0.234909\tGrad Norm: 1.461488\tLR: 0.030000\n",
      "Train set: Average loss: 0.2344\n",
      "Test set: Average loss: 0.2555, Average MAE: 0.3514\n",
      "Train Epoch: 1539 [4096/194182 (2%)]\tLoss: 0.233651\tGrad Norm: 1.702918\tLR: 0.030000\n",
      "Train Epoch: 1539 [24576/194182 (12%)]\tLoss: 0.227052\tGrad Norm: 1.529954\tLR: 0.030000\n",
      "Train Epoch: 1539 [45056/194182 (23%)]\tLoss: 0.226172\tGrad Norm: 1.632894\tLR: 0.030000\n",
      "Train Epoch: 1539 [65536/194182 (33%)]\tLoss: 0.231531\tGrad Norm: 1.403051\tLR: 0.030000\n",
      "Train Epoch: 1539 [86016/194182 (44%)]\tLoss: 0.237270\tGrad Norm: 1.879773\tLR: 0.030000\n",
      "Train Epoch: 1539 [106496/194182 (54%)]\tLoss: 0.238092\tGrad Norm: 1.799941\tLR: 0.030000\n",
      "Train Epoch: 1539 [126976/194182 (65%)]\tLoss: 0.231905\tGrad Norm: 1.902422\tLR: 0.030000\n",
      "Train Epoch: 1539 [147456/194182 (75%)]\tLoss: 0.229961\tGrad Norm: 1.684098\tLR: 0.030000\n",
      "Train Epoch: 1539 [167936/194182 (85%)]\tLoss: 0.236343\tGrad Norm: 1.724313\tLR: 0.030000\n",
      "Train Epoch: 1539 [188416/194182 (96%)]\tLoss: 0.236681\tGrad Norm: 1.681693\tLR: 0.030000\n",
      "Train set: Average loss: 0.2349\n",
      "Test set: Average loss: 0.2565, Average MAE: 0.3519\n",
      "Train Epoch: 1540 [4096/194182 (2%)]\tLoss: 0.236875\tGrad Norm: 1.630208\tLR: 0.030000\n",
      "Train Epoch: 1540 [24576/194182 (12%)]\tLoss: 0.227694\tGrad Norm: 1.317047\tLR: 0.030000\n",
      "Train Epoch: 1540 [45056/194182 (23%)]\tLoss: 0.234522\tGrad Norm: 1.786149\tLR: 0.030000\n",
      "Train Epoch: 1540 [65536/194182 (33%)]\tLoss: 0.238202\tGrad Norm: 1.788378\tLR: 0.030000\n",
      "Train Epoch: 1540 [86016/194182 (44%)]\tLoss: 0.225102\tGrad Norm: 1.455429\tLR: 0.030000\n",
      "Train Epoch: 1540 [106496/194182 (54%)]\tLoss: 0.241019\tGrad Norm: 1.826473\tLR: 0.030000\n",
      "Train Epoch: 1540 [126976/194182 (65%)]\tLoss: 0.244967\tGrad Norm: 1.964507\tLR: 0.030000\n",
      "Train Epoch: 1540 [147456/194182 (75%)]\tLoss: 0.236742\tGrad Norm: 1.418670\tLR: 0.030000\n",
      "Train Epoch: 1540 [167936/194182 (85%)]\tLoss: 0.236965\tGrad Norm: 1.737490\tLR: 0.030000\n",
      "Train Epoch: 1540 [188416/194182 (96%)]\tLoss: 0.229813\tGrad Norm: 1.400134\tLR: 0.030000\n",
      "Train set: Average loss: 0.2341\n",
      "Test set: Average loss: 0.2522, Average MAE: 0.3466\n",
      "Epoch 1540: Mean reward = 0.041 +/- 0.014\n",
      "Train Epoch: 1541 [4096/194182 (2%)]\tLoss: 0.232381\tGrad Norm: 1.454211\tLR: 0.030000\n",
      "Train Epoch: 1541 [24576/194182 (12%)]\tLoss: 0.235018\tGrad Norm: 1.586343\tLR: 0.030000\n",
      "Train Epoch: 1541 [45056/194182 (23%)]\tLoss: 0.231708\tGrad Norm: 1.550349\tLR: 0.030000\n",
      "Train Epoch: 1541 [65536/194182 (33%)]\tLoss: 0.236744\tGrad Norm: 1.480510\tLR: 0.030000\n",
      "Train Epoch: 1541 [86016/194182 (44%)]\tLoss: 0.221336\tGrad Norm: 1.413342\tLR: 0.030000\n",
      "Train Epoch: 1541 [106496/194182 (54%)]\tLoss: 0.235359\tGrad Norm: 1.677570\tLR: 0.030000\n",
      "Train Epoch: 1541 [126976/194182 (65%)]\tLoss: 0.235715\tGrad Norm: 1.869372\tLR: 0.030000\n",
      "Train Epoch: 1541 [147456/194182 (75%)]\tLoss: 0.234747\tGrad Norm: 1.431676\tLR: 0.030000\n",
      "Train Epoch: 1541 [167936/194182 (85%)]\tLoss: 0.235186\tGrad Norm: 1.679343\tLR: 0.030000\n",
      "Train Epoch: 1541 [188416/194182 (96%)]\tLoss: 0.237114\tGrad Norm: 1.940290\tLR: 0.030000\n",
      "Train set: Average loss: 0.2339\n",
      "Test set: Average loss: 0.2645, Average MAE: 0.3652\n",
      "Train Epoch: 1542 [4096/194182 (2%)]\tLoss: 0.234176\tGrad Norm: 1.959335\tLR: 0.030000\n",
      "Train Epoch: 1542 [24576/194182 (12%)]\tLoss: 0.231769\tGrad Norm: 1.658621\tLR: 0.030000\n",
      "Train Epoch: 1542 [45056/194182 (23%)]\tLoss: 0.238191\tGrad Norm: 2.211660\tLR: 0.030000\n",
      "Train Epoch: 1542 [65536/194182 (33%)]\tLoss: 0.243799\tGrad Norm: 2.180795\tLR: 0.030000\n",
      "Train Epoch: 1542 [86016/194182 (44%)]\tLoss: 0.241929\tGrad Norm: 2.090788\tLR: 0.030000\n",
      "Train Epoch: 1542 [106496/194182 (54%)]\tLoss: 0.230456\tGrad Norm: 1.403833\tLR: 0.030000\n",
      "Train Epoch: 1542 [126976/194182 (65%)]\tLoss: 0.229390\tGrad Norm: 1.316973\tLR: 0.030000\n",
      "Train Epoch: 1542 [147456/194182 (75%)]\tLoss: 0.236632\tGrad Norm: 1.742095\tLR: 0.030000\n",
      "Train Epoch: 1542 [167936/194182 (85%)]\tLoss: 0.229082\tGrad Norm: 1.595479\tLR: 0.030000\n",
      "Train Epoch: 1542 [188416/194182 (96%)]\tLoss: 0.229617\tGrad Norm: 1.691437\tLR: 0.030000\n",
      "Train set: Average loss: 0.2358\n",
      "Test set: Average loss: 0.2510, Average MAE: 0.3462\n",
      "Train Epoch: 1543 [4096/194182 (2%)]\tLoss: 0.233774\tGrad Norm: 1.492536\tLR: 0.030000\n",
      "Train Epoch: 1543 [24576/194182 (12%)]\tLoss: 0.227960\tGrad Norm: 1.325446\tLR: 0.030000\n",
      "Train Epoch: 1543 [45056/194182 (23%)]\tLoss: 0.229405\tGrad Norm: 1.597524\tLR: 0.030000\n",
      "Train Epoch: 1543 [65536/194182 (33%)]\tLoss: 0.235162\tGrad Norm: 1.544908\tLR: 0.030000\n",
      "Train Epoch: 1543 [86016/194182 (44%)]\tLoss: 0.241015\tGrad Norm: 1.503296\tLR: 0.030000\n",
      "Train Epoch: 1543 [106496/194182 (54%)]\tLoss: 0.228913\tGrad Norm: 1.208332\tLR: 0.030000\n",
      "Train Epoch: 1543 [126976/194182 (65%)]\tLoss: 0.227124\tGrad Norm: 1.387859\tLR: 0.030000\n",
      "Train Epoch: 1543 [147456/194182 (75%)]\tLoss: 0.235685\tGrad Norm: 1.871501\tLR: 0.030000\n",
      "Train Epoch: 1543 [167936/194182 (85%)]\tLoss: 0.243013\tGrad Norm: 2.063840\tLR: 0.030000\n",
      "Train Epoch: 1543 [188416/194182 (96%)]\tLoss: 0.238987\tGrad Norm: 2.115019\tLR: 0.030000\n",
      "Train set: Average loss: 0.2336\n",
      "Test set: Average loss: 0.2500, Average MAE: 0.3393\n",
      "Train Epoch: 1544 [4096/194182 (2%)]\tLoss: 0.229708\tGrad Norm: 1.462505\tLR: 0.030000\n",
      "Train Epoch: 1544 [24576/194182 (12%)]\tLoss: 0.228863\tGrad Norm: 1.603157\tLR: 0.030000\n",
      "Train Epoch: 1544 [45056/194182 (23%)]\tLoss: 0.236899\tGrad Norm: 1.907763\tLR: 0.030000\n",
      "Train Epoch: 1544 [65536/194182 (33%)]\tLoss: 0.234584\tGrad Norm: 1.676654\tLR: 0.030000\n",
      "Train Epoch: 1544 [86016/194182 (44%)]\tLoss: 0.230278\tGrad Norm: 1.801214\tLR: 0.030000\n",
      "Train Epoch: 1544 [106496/194182 (54%)]\tLoss: 0.230077\tGrad Norm: 1.406505\tLR: 0.030000\n",
      "Train Epoch: 1544 [126976/194182 (65%)]\tLoss: 0.243937\tGrad Norm: 2.171114\tLR: 0.030000\n",
      "Train Epoch: 1544 [147456/194182 (75%)]\tLoss: 0.228554\tGrad Norm: 1.365038\tLR: 0.030000\n",
      "Train Epoch: 1544 [167936/194182 (85%)]\tLoss: 0.226585\tGrad Norm: 1.310926\tLR: 0.030000\n",
      "Train Epoch: 1544 [188416/194182 (96%)]\tLoss: 0.236410\tGrad Norm: 1.532588\tLR: 0.030000\n",
      "Train set: Average loss: 0.2332\n",
      "Test set: Average loss: 0.2522, Average MAE: 0.3410\n",
      "Train Epoch: 1545 [4096/194182 (2%)]\tLoss: 0.235314\tGrad Norm: 1.671193\tLR: 0.030000\n",
      "Train Epoch: 1545 [24576/194182 (12%)]\tLoss: 0.231786\tGrad Norm: 1.573887\tLR: 0.030000\n",
      "Train Epoch: 1545 [45056/194182 (23%)]\tLoss: 0.232461\tGrad Norm: 1.747078\tLR: 0.030000\n",
      "Train Epoch: 1545 [65536/194182 (33%)]\tLoss: 0.230287\tGrad Norm: 1.542248\tLR: 0.030000\n",
      "Train Epoch: 1545 [86016/194182 (44%)]\tLoss: 0.229285\tGrad Norm: 1.819642\tLR: 0.030000\n",
      "Train Epoch: 1545 [106496/194182 (54%)]\tLoss: 0.238888\tGrad Norm: 2.058457\tLR: 0.030000\n",
      "Train Epoch: 1545 [126976/194182 (65%)]\tLoss: 0.232444\tGrad Norm: 1.772332\tLR: 0.030000\n",
      "Train Epoch: 1545 [147456/194182 (75%)]\tLoss: 0.223960\tGrad Norm: 1.367818\tLR: 0.030000\n",
      "Train Epoch: 1545 [167936/194182 (85%)]\tLoss: 0.224925\tGrad Norm: 1.489832\tLR: 0.030000\n",
      "Train Epoch: 1545 [188416/194182 (96%)]\tLoss: 0.230286\tGrad Norm: 1.540915\tLR: 0.030000\n",
      "Train set: Average loss: 0.2332\n",
      "Test set: Average loss: 0.2556, Average MAE: 0.3432\n",
      "Epoch 1545: Mean reward = 0.052 +/- 0.036\n",
      "Train Epoch: 1546 [4096/194182 (2%)]\tLoss: 0.231697\tGrad Norm: 1.826510\tLR: 0.030000\n",
      "Train Epoch: 1546 [24576/194182 (12%)]\tLoss: 0.230029\tGrad Norm: 1.687029\tLR: 0.030000\n",
      "Train Epoch: 1546 [45056/194182 (23%)]\tLoss: 0.230308\tGrad Norm: 1.858719\tLR: 0.030000\n",
      "Train Epoch: 1546 [65536/194182 (33%)]\tLoss: 0.243196\tGrad Norm: 2.055770\tLR: 0.030000\n",
      "Train Epoch: 1546 [86016/194182 (44%)]\tLoss: 0.235460\tGrad Norm: 1.892422\tLR: 0.030000\n",
      "Train Epoch: 1546 [106496/194182 (54%)]\tLoss: 0.233748\tGrad Norm: 1.537746\tLR: 0.030000\n",
      "Train Epoch: 1546 [126976/194182 (65%)]\tLoss: 0.228656\tGrad Norm: 1.410713\tLR: 0.030000\n",
      "Train Epoch: 1546 [147456/194182 (75%)]\tLoss: 0.229670\tGrad Norm: 1.208600\tLR: 0.030000\n",
      "Train Epoch: 1546 [167936/194182 (85%)]\tLoss: 0.233195\tGrad Norm: 1.413443\tLR: 0.030000\n",
      "Train Epoch: 1546 [188416/194182 (96%)]\tLoss: 0.231705\tGrad Norm: 1.627804\tLR: 0.030000\n",
      "Train set: Average loss: 0.2329\n",
      "Test set: Average loss: 0.2560, Average MAE: 0.3482\n",
      "Train Epoch: 1547 [4096/194182 (2%)]\tLoss: 0.234545\tGrad Norm: 1.597253\tLR: 0.030000\n",
      "Train Epoch: 1547 [24576/194182 (12%)]\tLoss: 0.239705\tGrad Norm: 1.749270\tLR: 0.030000\n",
      "Train Epoch: 1547 [45056/194182 (23%)]\tLoss: 0.229709\tGrad Norm: 1.485236\tLR: 0.030000\n",
      "Train Epoch: 1547 [65536/194182 (33%)]\tLoss: 0.237311\tGrad Norm: 1.625893\tLR: 0.030000\n",
      "Train Epoch: 1547 [86016/194182 (44%)]\tLoss: 0.227270\tGrad Norm: 1.352973\tLR: 0.030000\n",
      "Train Epoch: 1547 [106496/194182 (54%)]\tLoss: 0.244265\tGrad Norm: 1.993141\tLR: 0.030000\n",
      "Train Epoch: 1547 [126976/194182 (65%)]\tLoss: 0.230804\tGrad Norm: 1.756090\tLR: 0.030000\n",
      "Train Epoch: 1547 [147456/194182 (75%)]\tLoss: 0.233934\tGrad Norm: 1.692584\tLR: 0.030000\n",
      "Train Epoch: 1547 [167936/194182 (85%)]\tLoss: 0.222556\tGrad Norm: 1.592293\tLR: 0.030000\n",
      "Train Epoch: 1547 [188416/194182 (96%)]\tLoss: 0.230047\tGrad Norm: 1.632378\tLR: 0.030000\n",
      "Train set: Average loss: 0.2327\n",
      "Test set: Average loss: 0.2529, Average MAE: 0.3510\n",
      "Train Epoch: 1548 [4096/194182 (2%)]\tLoss: 0.229547\tGrad Norm: 1.511664\tLR: 0.030000\n",
      "Train Epoch: 1548 [24576/194182 (12%)]\tLoss: 0.228216\tGrad Norm: 1.344499\tLR: 0.030000\n",
      "Train Epoch: 1548 [45056/194182 (23%)]\tLoss: 0.231620\tGrad Norm: 1.810580\tLR: 0.030000\n",
      "Train Epoch: 1548 [65536/194182 (33%)]\tLoss: 0.239164\tGrad Norm: 1.836577\tLR: 0.030000\n",
      "Train Epoch: 1548 [86016/194182 (44%)]\tLoss: 0.230750\tGrad Norm: 1.491187\tLR: 0.030000\n",
      "Train Epoch: 1548 [106496/194182 (54%)]\tLoss: 0.232343\tGrad Norm: 1.747300\tLR: 0.030000\n",
      "Train Epoch: 1548 [126976/194182 (65%)]\tLoss: 0.225709\tGrad Norm: 1.283420\tLR: 0.030000\n",
      "Train Epoch: 1548 [147456/194182 (75%)]\tLoss: 0.219775\tGrad Norm: 1.347699\tLR: 0.030000\n",
      "Train Epoch: 1548 [167936/194182 (85%)]\tLoss: 0.235517\tGrad Norm: 1.651697\tLR: 0.030000\n",
      "Train Epoch: 1548 [188416/194182 (96%)]\tLoss: 0.243875\tGrad Norm: 1.812722\tLR: 0.030000\n",
      "Train set: Average loss: 0.2315\n",
      "Test set: Average loss: 0.2571, Average MAE: 0.3467\n",
      "Train Epoch: 1549 [4096/194182 (2%)]\tLoss: 0.239660\tGrad Norm: 1.986337\tLR: 0.030000\n",
      "Train Epoch: 1549 [24576/194182 (12%)]\tLoss: 0.230922\tGrad Norm: 1.445099\tLR: 0.030000\n",
      "Train Epoch: 1549 [45056/194182 (23%)]\tLoss: 0.230790\tGrad Norm: 1.561823\tLR: 0.030000\n",
      "Train Epoch: 1549 [65536/194182 (33%)]\tLoss: 0.235959\tGrad Norm: 1.607216\tLR: 0.030000\n",
      "Train Epoch: 1549 [86016/194182 (44%)]\tLoss: 0.232017\tGrad Norm: 1.566744\tLR: 0.030000\n",
      "Train Epoch: 1549 [106496/194182 (54%)]\tLoss: 0.233226\tGrad Norm: 1.711886\tLR: 0.030000\n",
      "Train Epoch: 1549 [126976/194182 (65%)]\tLoss: 0.235471\tGrad Norm: 1.928015\tLR: 0.030000\n",
      "Train Epoch: 1549 [147456/194182 (75%)]\tLoss: 0.231508\tGrad Norm: 1.779317\tLR: 0.030000\n",
      "Train Epoch: 1549 [167936/194182 (85%)]\tLoss: 0.231421\tGrad Norm: 1.576801\tLR: 0.030000\n",
      "Train Epoch: 1549 [188416/194182 (96%)]\tLoss: 0.242662\tGrad Norm: 2.062445\tLR: 0.030000\n",
      "Train set: Average loss: 0.2344\n",
      "Test set: Average loss: 0.2546, Average MAE: 0.3394\n",
      "Train Epoch: 1550 [4096/194182 (2%)]\tLoss: 0.233391\tGrad Norm: 1.999641\tLR: 0.030000\n",
      "Train Epoch: 1550 [24576/194182 (12%)]\tLoss: 0.232160\tGrad Norm: 1.767601\tLR: 0.030000\n",
      "Train Epoch: 1550 [45056/194182 (23%)]\tLoss: 0.235892\tGrad Norm: 1.859048\tLR: 0.030000\n",
      "Train Epoch: 1550 [65536/194182 (33%)]\tLoss: 0.229907\tGrad Norm: 1.576985\tLR: 0.030000\n",
      "Train Epoch: 1550 [86016/194182 (44%)]\tLoss: 0.223467\tGrad Norm: 1.326215\tLR: 0.030000\n",
      "Train Epoch: 1550 [106496/194182 (54%)]\tLoss: 0.225280\tGrad Norm: 1.251045\tLR: 0.030000\n",
      "Train Epoch: 1550 [126976/194182 (65%)]\tLoss: 0.222699\tGrad Norm: 1.401697\tLR: 0.030000\n",
      "Train Epoch: 1550 [147456/194182 (75%)]\tLoss: 0.231113\tGrad Norm: 1.646864\tLR: 0.030000\n",
      "Train Epoch: 1550 [167936/194182 (85%)]\tLoss: 0.234433\tGrad Norm: 1.510272\tLR: 0.030000\n",
      "Train Epoch: 1550 [188416/194182 (96%)]\tLoss: 0.230058\tGrad Norm: 1.481209\tLR: 0.030000\n",
      "Train set: Average loss: 0.2312\n",
      "Test set: Average loss: 0.2518, Average MAE: 0.3377\n",
      "Epoch 1550: Mean reward = 0.062 +/- 0.059\n",
      "Train Epoch: 1551 [4096/194182 (2%)]\tLoss: 0.226800\tGrad Norm: 1.634716\tLR: 0.030000\n",
      "Train Epoch: 1551 [24576/194182 (12%)]\tLoss: 0.230126\tGrad Norm: 1.529362\tLR: 0.030000\n",
      "Train Epoch: 1551 [45056/194182 (23%)]\tLoss: 0.236422\tGrad Norm: 1.960946\tLR: 0.030000\n",
      "Train Epoch: 1551 [65536/194182 (33%)]\tLoss: 0.230378\tGrad Norm: 1.701348\tLR: 0.030000\n",
      "Train Epoch: 1551 [86016/194182 (44%)]\tLoss: 0.238354\tGrad Norm: 2.021929\tLR: 0.030000\n",
      "Train Epoch: 1551 [106496/194182 (54%)]\tLoss: 0.241189\tGrad Norm: 1.949460\tLR: 0.030000\n",
      "Train Epoch: 1551 [126976/194182 (65%)]\tLoss: 0.235680\tGrad Norm: 1.942791\tLR: 0.030000\n",
      "Train Epoch: 1551 [147456/194182 (75%)]\tLoss: 0.231988\tGrad Norm: 1.703714\tLR: 0.030000\n",
      "Train Epoch: 1551 [167936/194182 (85%)]\tLoss: 0.238512\tGrad Norm: 1.825420\tLR: 0.030000\n",
      "Train Epoch: 1551 [188416/194182 (96%)]\tLoss: 0.231457\tGrad Norm: 1.703621\tLR: 0.030000\n",
      "Train set: Average loss: 0.2343\n",
      "Test set: Average loss: 0.2546, Average MAE: 0.3479\n",
      "Train Epoch: 1552 [4096/194182 (2%)]\tLoss: 0.231044\tGrad Norm: 1.717695\tLR: 0.030000\n",
      "Train Epoch: 1552 [24576/194182 (12%)]\tLoss: 0.224073\tGrad Norm: 1.324891\tLR: 0.030000\n",
      "Train Epoch: 1552 [45056/194182 (23%)]\tLoss: 0.233649\tGrad Norm: 1.881066\tLR: 0.030000\n",
      "Train Epoch: 1552 [65536/194182 (33%)]\tLoss: 0.219369\tGrad Norm: 1.391000\tLR: 0.030000\n",
      "Train Epoch: 1552 [86016/194182 (44%)]\tLoss: 0.226048\tGrad Norm: 1.276144\tLR: 0.030000\n",
      "Train Epoch: 1552 [106496/194182 (54%)]\tLoss: 0.231729\tGrad Norm: 1.624828\tLR: 0.030000\n",
      "Train Epoch: 1552 [126976/194182 (65%)]\tLoss: 0.234000\tGrad Norm: 1.742554\tLR: 0.030000\n",
      "Train Epoch: 1552 [147456/194182 (75%)]\tLoss: 0.233833\tGrad Norm: 1.573062\tLR: 0.030000\n",
      "Train Epoch: 1552 [167936/194182 (85%)]\tLoss: 0.229628\tGrad Norm: 1.467249\tLR: 0.030000\n",
      "Train Epoch: 1552 [188416/194182 (96%)]\tLoss: 0.229886\tGrad Norm: 1.710360\tLR: 0.030000\n",
      "Train set: Average loss: 0.2313\n",
      "Test set: Average loss: 0.2569, Average MAE: 0.3422\n",
      "Train Epoch: 1553 [4096/194182 (2%)]\tLoss: 0.233782\tGrad Norm: 1.951065\tLR: 0.030000\n",
      "Train Epoch: 1553 [24576/194182 (12%)]\tLoss: 0.226708\tGrad Norm: 1.569448\tLR: 0.030000\n",
      "Train Epoch: 1553 [45056/194182 (23%)]\tLoss: 0.232434\tGrad Norm: 1.640621\tLR: 0.030000\n",
      "Train Epoch: 1553 [65536/194182 (33%)]\tLoss: 0.223592\tGrad Norm: 1.326601\tLR: 0.030000\n",
      "Train Epoch: 1553 [86016/194182 (44%)]\tLoss: 0.235621\tGrad Norm: 2.408132\tLR: 0.030000\n",
      "Train Epoch: 1553 [106496/194182 (54%)]\tLoss: 0.230146\tGrad Norm: 1.506557\tLR: 0.030000\n",
      "Train Epoch: 1553 [126976/194182 (65%)]\tLoss: 0.225745\tGrad Norm: 1.548738\tLR: 0.030000\n",
      "Train Epoch: 1553 [147456/194182 (75%)]\tLoss: 0.230905\tGrad Norm: 1.531246\tLR: 0.030000\n",
      "Train Epoch: 1553 [167936/194182 (85%)]\tLoss: 0.236376\tGrad Norm: 1.760811\tLR: 0.030000\n",
      "Train Epoch: 1553 [188416/194182 (96%)]\tLoss: 0.240029\tGrad Norm: 1.704017\tLR: 0.030000\n",
      "Train set: Average loss: 0.2324\n",
      "Test set: Average loss: 0.2519, Average MAE: 0.3487\n",
      "Train Epoch: 1554 [4096/194182 (2%)]\tLoss: 0.235170\tGrad Norm: 1.501433\tLR: 0.030000\n",
      "Train Epoch: 1554 [24576/194182 (12%)]\tLoss: 0.234626\tGrad Norm: 1.666978\tLR: 0.030000\n",
      "Train Epoch: 1554 [45056/194182 (23%)]\tLoss: 0.228876\tGrad Norm: 1.400290\tLR: 0.030000\n",
      "Train Epoch: 1554 [65536/194182 (33%)]\tLoss: 0.221827\tGrad Norm: 1.151823\tLR: 0.030000\n",
      "Train Epoch: 1554 [86016/194182 (44%)]\tLoss: 0.231788\tGrad Norm: 1.554511\tLR: 0.030000\n",
      "Train Epoch: 1554 [106496/194182 (54%)]\tLoss: 0.233216\tGrad Norm: 1.598480\tLR: 0.030000\n",
      "Train Epoch: 1554 [126976/194182 (65%)]\tLoss: 0.246287\tGrad Norm: 2.395077\tLR: 0.030000\n",
      "Train Epoch: 1554 [147456/194182 (75%)]\tLoss: 0.238129\tGrad Norm: 2.041135\tLR: 0.030000\n",
      "Train Epoch: 1554 [167936/194182 (85%)]\tLoss: 0.228243\tGrad Norm: 1.500114\tLR: 0.030000\n",
      "Train Epoch: 1554 [188416/194182 (96%)]\tLoss: 0.231417\tGrad Norm: 1.791051\tLR: 0.030000\n",
      "Train set: Average loss: 0.2313\n",
      "Test set: Average loss: 0.2504, Average MAE: 0.3398\n",
      "Train Epoch: 1555 [4096/194182 (2%)]\tLoss: 0.224116\tGrad Norm: 1.419837\tLR: 0.030000\n",
      "Train Epoch: 1555 [24576/194182 (12%)]\tLoss: 0.239083\tGrad Norm: 1.699882\tLR: 0.030000\n",
      "Train Epoch: 1555 [45056/194182 (23%)]\tLoss: 0.227418\tGrad Norm: 1.648202\tLR: 0.030000\n",
      "Train Epoch: 1555 [65536/194182 (33%)]\tLoss: 0.236576\tGrad Norm: 2.074258\tLR: 0.030000\n",
      "Train Epoch: 1555 [86016/194182 (44%)]\tLoss: 0.226509\tGrad Norm: 1.342115\tLR: 0.030000\n",
      "Train Epoch: 1555 [106496/194182 (54%)]\tLoss: 0.228177\tGrad Norm: 1.526894\tLR: 0.030000\n",
      "Train Epoch: 1555 [126976/194182 (65%)]\tLoss: 0.234506\tGrad Norm: 1.725199\tLR: 0.030000\n",
      "Train Epoch: 1555 [147456/194182 (75%)]\tLoss: 0.228666\tGrad Norm: 1.486004\tLR: 0.030000\n",
      "Train Epoch: 1555 [167936/194182 (85%)]\tLoss: 0.228170\tGrad Norm: 1.568790\tLR: 0.030000\n",
      "Train Epoch: 1555 [188416/194182 (96%)]\tLoss: 0.235610\tGrad Norm: 2.150350\tLR: 0.030000\n",
      "Train set: Average loss: 0.2321\n",
      "Test set: Average loss: 0.2638, Average MAE: 0.3655\n",
      "Epoch 1555: Mean reward = 0.080 +/- 0.065\n",
      "Train Epoch: 1556 [4096/194182 (2%)]\tLoss: 0.244097\tGrad Norm: 1.965145\tLR: 0.030000\n",
      "Train Epoch: 1556 [24576/194182 (12%)]\tLoss: 0.229928\tGrad Norm: 1.804944\tLR: 0.030000\n",
      "Train Epoch: 1556 [45056/194182 (23%)]\tLoss: 0.236365\tGrad Norm: 1.682585\tLR: 0.030000\n",
      "Train Epoch: 1556 [65536/194182 (33%)]\tLoss: 0.229450\tGrad Norm: 1.469464\tLR: 0.030000\n",
      "Train Epoch: 1556 [86016/194182 (44%)]\tLoss: 0.231241\tGrad Norm: 1.648176\tLR: 0.030000\n",
      "Train Epoch: 1556 [106496/194182 (54%)]\tLoss: 0.233573\tGrad Norm: 1.792899\tLR: 0.030000\n",
      "Train Epoch: 1556 [126976/194182 (65%)]\tLoss: 0.234562\tGrad Norm: 1.767926\tLR: 0.030000\n",
      "Train Epoch: 1556 [147456/194182 (75%)]\tLoss: 0.231228\tGrad Norm: 1.477624\tLR: 0.030000\n",
      "Train Epoch: 1556 [167936/194182 (85%)]\tLoss: 0.231663\tGrad Norm: 1.476322\tLR: 0.030000\n",
      "Train Epoch: 1556 [188416/194182 (96%)]\tLoss: 0.231649\tGrad Norm: 1.837173\tLR: 0.030000\n",
      "Train set: Average loss: 0.2316\n",
      "Test set: Average loss: 0.2513, Average MAE: 0.3427\n",
      "Train Epoch: 1557 [4096/194182 (2%)]\tLoss: 0.228221\tGrad Norm: 1.462360\tLR: 0.030000\n",
      "Train Epoch: 1557 [24576/194182 (12%)]\tLoss: 0.242063\tGrad Norm: 2.126109\tLR: 0.030000\n",
      "Train Epoch: 1557 [45056/194182 (23%)]\tLoss: 0.241849\tGrad Norm: 1.931990\tLR: 0.030000\n",
      "Train Epoch: 1557 [65536/194182 (33%)]\tLoss: 0.236504\tGrad Norm: 2.026292\tLR: 0.030000\n",
      "Train Epoch: 1557 [86016/194182 (44%)]\tLoss: 0.229452\tGrad Norm: 1.652013\tLR: 0.030000\n",
      "Train Epoch: 1557 [106496/194182 (54%)]\tLoss: 0.233956\tGrad Norm: 1.598671\tLR: 0.030000\n",
      "Train Epoch: 1557 [126976/194182 (65%)]\tLoss: 0.234524\tGrad Norm: 1.575732\tLR: 0.030000\n",
      "Train Epoch: 1557 [147456/194182 (75%)]\tLoss: 0.235895\tGrad Norm: 1.835551\tLR: 0.030000\n",
      "Train Epoch: 1557 [167936/194182 (85%)]\tLoss: 0.231075\tGrad Norm: 1.503622\tLR: 0.030000\n",
      "Train Epoch: 1557 [188416/194182 (96%)]\tLoss: 0.232066\tGrad Norm: 1.219022\tLR: 0.030000\n",
      "Train set: Average loss: 0.2327\n",
      "Test set: Average loss: 0.2508, Average MAE: 0.3498\n",
      "Train Epoch: 1558 [4096/194182 (2%)]\tLoss: 0.226252\tGrad Norm: 1.407312\tLR: 0.030000\n",
      "Train Epoch: 1558 [24576/194182 (12%)]\tLoss: 0.235863\tGrad Norm: 1.877362\tLR: 0.030000\n",
      "Train Epoch: 1558 [45056/194182 (23%)]\tLoss: 0.227514\tGrad Norm: 1.459576\tLR: 0.030000\n",
      "Train Epoch: 1558 [65536/194182 (33%)]\tLoss: 0.220101\tGrad Norm: 1.471684\tLR: 0.030000\n",
      "Train Epoch: 1558 [86016/194182 (44%)]\tLoss: 0.239011\tGrad Norm: 1.970590\tLR: 0.030000\n",
      "Train Epoch: 1558 [106496/194182 (54%)]\tLoss: 0.235950\tGrad Norm: 1.986763\tLR: 0.030000\n",
      "Train Epoch: 1558 [126976/194182 (65%)]\tLoss: 0.226941\tGrad Norm: 1.414760\tLR: 0.030000\n",
      "Train Epoch: 1558 [147456/194182 (75%)]\tLoss: 0.236732\tGrad Norm: 2.197187\tLR: 0.030000\n",
      "Train Epoch: 1558 [167936/194182 (85%)]\tLoss: 0.231007\tGrad Norm: 1.780105\tLR: 0.030000\n",
      "Train Epoch: 1558 [188416/194182 (96%)]\tLoss: 0.222794\tGrad Norm: 1.235450\tLR: 0.030000\n",
      "Train set: Average loss: 0.2314\n",
      "Test set: Average loss: 0.2528, Average MAE: 0.3460\n",
      "Train Epoch: 1559 [4096/194182 (2%)]\tLoss: 0.232341\tGrad Norm: 1.399238\tLR: 0.030000\n",
      "Train Epoch: 1559 [24576/194182 (12%)]\tLoss: 0.231117\tGrad Norm: 1.498695\tLR: 0.030000\n",
      "Train Epoch: 1559 [45056/194182 (23%)]\tLoss: 0.233235\tGrad Norm: 1.683394\tLR: 0.030000\n",
      "Train Epoch: 1559 [65536/194182 (33%)]\tLoss: 0.235962\tGrad Norm: 1.643790\tLR: 0.030000\n",
      "Train Epoch: 1559 [86016/194182 (44%)]\tLoss: 0.230116\tGrad Norm: 1.356580\tLR: 0.030000\n",
      "Train Epoch: 1559 [106496/194182 (54%)]\tLoss: 0.230383\tGrad Norm: 1.646194\tLR: 0.030000\n",
      "Train Epoch: 1559 [126976/194182 (65%)]\tLoss: 0.229314\tGrad Norm: 1.474413\tLR: 0.030000\n",
      "Train Epoch: 1559 [147456/194182 (75%)]\tLoss: 0.234107\tGrad Norm: 1.459083\tLR: 0.030000\n",
      "Train Epoch: 1559 [167936/194182 (85%)]\tLoss: 0.229365\tGrad Norm: 1.566639\tLR: 0.030000\n",
      "Train Epoch: 1559 [188416/194182 (96%)]\tLoss: 0.245359\tGrad Norm: 2.172551\tLR: 0.030000\n",
      "Train set: Average loss: 0.2308\n",
      "Test set: Average loss: 0.2611, Average MAE: 0.3434\n",
      "Train Epoch: 1560 [4096/194182 (2%)]\tLoss: 0.230635\tGrad Norm: 1.914507\tLR: 0.030000\n",
      "Train Epoch: 1560 [24576/194182 (12%)]\tLoss: 0.224791\tGrad Norm: 1.589417\tLR: 0.030000\n",
      "Train Epoch: 1560 [45056/194182 (23%)]\tLoss: 0.232545\tGrad Norm: 1.743721\tLR: 0.030000\n",
      "Train Epoch: 1560 [65536/194182 (33%)]\tLoss: 0.238367\tGrad Norm: 1.887421\tLR: 0.030000\n",
      "Train Epoch: 1560 [86016/194182 (44%)]\tLoss: 0.233332\tGrad Norm: 1.773628\tLR: 0.030000\n",
      "Train Epoch: 1560 [106496/194182 (54%)]\tLoss: 0.231270\tGrad Norm: 1.671923\tLR: 0.030000\n",
      "Train Epoch: 1560 [126976/194182 (65%)]\tLoss: 0.230120\tGrad Norm: 1.860757\tLR: 0.030000\n",
      "Train Epoch: 1560 [147456/194182 (75%)]\tLoss: 0.232663\tGrad Norm: 1.579404\tLR: 0.030000\n",
      "Train Epoch: 1560 [167936/194182 (85%)]\tLoss: 0.233210\tGrad Norm: 1.846543\tLR: 0.030000\n",
      "Train Epoch: 1560 [188416/194182 (96%)]\tLoss: 0.234204\tGrad Norm: 1.935833\tLR: 0.030000\n",
      "Train set: Average loss: 0.2331\n",
      "Test set: Average loss: 0.2527, Average MAE: 0.3390\n",
      "Epoch 1560: Mean reward = 0.064 +/- 0.067\n",
      "Train Epoch: 1561 [4096/194182 (2%)]\tLoss: 0.232304\tGrad Norm: 1.844706\tLR: 0.030000\n",
      "Train Epoch: 1561 [24576/194182 (12%)]\tLoss: 0.237248\tGrad Norm: 1.829499\tLR: 0.030000\n",
      "Train Epoch: 1561 [45056/194182 (23%)]\tLoss: 0.228714\tGrad Norm: 1.621314\tLR: 0.030000\n",
      "Train Epoch: 1561 [65536/194182 (33%)]\tLoss: 0.226929\tGrad Norm: 1.796632\tLR: 0.030000\n",
      "Train Epoch: 1561 [86016/194182 (44%)]\tLoss: 0.233857\tGrad Norm: 2.051642\tLR: 0.030000\n",
      "Train Epoch: 1561 [106496/194182 (54%)]\tLoss: 0.233027\tGrad Norm: 1.769220\tLR: 0.030000\n",
      "Train Epoch: 1561 [126976/194182 (65%)]\tLoss: 0.227446\tGrad Norm: 1.468359\tLR: 0.030000\n",
      "Train Epoch: 1561 [147456/194182 (75%)]\tLoss: 0.226310\tGrad Norm: 1.533572\tLR: 0.030000\n",
      "Train Epoch: 1561 [167936/194182 (85%)]\tLoss: 0.225837\tGrad Norm: 1.709720\tLR: 0.030000\n",
      "Train Epoch: 1561 [188416/194182 (96%)]\tLoss: 0.230931\tGrad Norm: 1.482441\tLR: 0.030000\n",
      "Train set: Average loss: 0.2316\n",
      "Test set: Average loss: 0.2538, Average MAE: 0.3492\n",
      "Train Epoch: 1562 [4096/194182 (2%)]\tLoss: 0.226924\tGrad Norm: 1.590155\tLR: 0.030000\n",
      "Train Epoch: 1562 [24576/194182 (12%)]\tLoss: 0.228152\tGrad Norm: 1.634068\tLR: 0.030000\n",
      "Train Epoch: 1562 [45056/194182 (23%)]\tLoss: 0.220672\tGrad Norm: 1.365109\tLR: 0.030000\n",
      "Train Epoch: 1562 [65536/194182 (33%)]\tLoss: 0.229364\tGrad Norm: 1.888055\tLR: 0.030000\n",
      "Train Epoch: 1562 [86016/194182 (44%)]\tLoss: 0.236449\tGrad Norm: 1.522392\tLR: 0.030000\n",
      "Train Epoch: 1562 [106496/194182 (54%)]\tLoss: 0.237292\tGrad Norm: 1.878629\tLR: 0.030000\n",
      "Train Epoch: 1562 [126976/194182 (65%)]\tLoss: 0.226008\tGrad Norm: 1.253676\tLR: 0.030000\n",
      "Train Epoch: 1562 [147456/194182 (75%)]\tLoss: 0.232100\tGrad Norm: 1.529258\tLR: 0.030000\n",
      "Train Epoch: 1562 [167936/194182 (85%)]\tLoss: 0.225618\tGrad Norm: 1.614346\tLR: 0.030000\n",
      "Train Epoch: 1562 [188416/194182 (96%)]\tLoss: 0.242271\tGrad Norm: 2.438671\tLR: 0.030000\n",
      "Train set: Average loss: 0.2314\n",
      "Test set: Average loss: 0.2609, Average MAE: 0.3617\n",
      "Train Epoch: 1563 [4096/194182 (2%)]\tLoss: 0.235328\tGrad Norm: 1.986837\tLR: 0.030000\n",
      "Train Epoch: 1563 [24576/194182 (12%)]\tLoss: 0.237444\tGrad Norm: 1.522065\tLR: 0.030000\n",
      "Train Epoch: 1563 [45056/194182 (23%)]\tLoss: 0.236613\tGrad Norm: 1.607876\tLR: 0.030000\n",
      "Train Epoch: 1563 [65536/194182 (33%)]\tLoss: 0.232565\tGrad Norm: 1.696575\tLR: 0.030000\n",
      "Train Epoch: 1563 [86016/194182 (44%)]\tLoss: 0.238448\tGrad Norm: 1.921062\tLR: 0.030000\n",
      "Train Epoch: 1563 [106496/194182 (54%)]\tLoss: 0.234538\tGrad Norm: 1.653571\tLR: 0.030000\n",
      "Train Epoch: 1563 [126976/194182 (65%)]\tLoss: 0.227272\tGrad Norm: 1.534788\tLR: 0.030000\n",
      "Train Epoch: 1563 [147456/194182 (75%)]\tLoss: 0.233879\tGrad Norm: 1.642317\tLR: 0.030000\n",
      "Train Epoch: 1563 [167936/194182 (85%)]\tLoss: 0.228836\tGrad Norm: 1.457807\tLR: 0.030000\n",
      "Train Epoch: 1563 [188416/194182 (96%)]\tLoss: 0.230526\tGrad Norm: 1.700710\tLR: 0.030000\n",
      "Train set: Average loss: 0.2300\n",
      "Test set: Average loss: 0.2574, Average MAE: 0.3521\n",
      "Train Epoch: 1564 [4096/194182 (2%)]\tLoss: 0.231148\tGrad Norm: 1.770199\tLR: 0.030000\n",
      "Train Epoch: 1564 [24576/194182 (12%)]\tLoss: 0.231222\tGrad Norm: 1.883736\tLR: 0.030000\n",
      "Train Epoch: 1564 [45056/194182 (23%)]\tLoss: 0.231228\tGrad Norm: 1.708880\tLR: 0.030000\n",
      "Train Epoch: 1564 [65536/194182 (33%)]\tLoss: 0.229890\tGrad Norm: 1.588988\tLR: 0.030000\n",
      "Train Epoch: 1564 [86016/194182 (44%)]\tLoss: 0.232737\tGrad Norm: 1.625743\tLR: 0.030000\n",
      "Train Epoch: 1564 [106496/194182 (54%)]\tLoss: 0.230313\tGrad Norm: 1.514938\tLR: 0.030000\n",
      "Train Epoch: 1564 [126976/194182 (65%)]\tLoss: 0.234299\tGrad Norm: 1.873572\tLR: 0.030000\n",
      "Train Epoch: 1564 [147456/194182 (75%)]\tLoss: 0.235399\tGrad Norm: 1.810250\tLR: 0.030000\n",
      "Train Epoch: 1564 [167936/194182 (85%)]\tLoss: 0.235603\tGrad Norm: 1.737289\tLR: 0.030000\n",
      "Train Epoch: 1564 [188416/194182 (96%)]\tLoss: 0.230303\tGrad Norm: 1.716463\tLR: 0.030000\n",
      "Train set: Average loss: 0.2310\n",
      "Test set: Average loss: 0.2541, Average MAE: 0.3467\n",
      "Train Epoch: 1565 [4096/194182 (2%)]\tLoss: 0.233260\tGrad Norm: 1.733301\tLR: 0.030000\n",
      "Train Epoch: 1565 [24576/194182 (12%)]\tLoss: 0.228019\tGrad Norm: 1.299276\tLR: 0.030000\n",
      "Train Epoch: 1565 [45056/194182 (23%)]\tLoss: 0.226301\tGrad Norm: 1.473733\tLR: 0.030000\n",
      "Train Epoch: 1565 [65536/194182 (33%)]\tLoss: 0.228420\tGrad Norm: 1.852833\tLR: 0.030000\n",
      "Train Epoch: 1565 [86016/194182 (44%)]\tLoss: 0.239718\tGrad Norm: 2.261687\tLR: 0.030000\n",
      "Train Epoch: 1565 [106496/194182 (54%)]\tLoss: 0.235947\tGrad Norm: 2.018281\tLR: 0.030000\n",
      "Train Epoch: 1565 [126976/194182 (65%)]\tLoss: 0.228108\tGrad Norm: 1.485176\tLR: 0.030000\n",
      "Train Epoch: 1565 [147456/194182 (75%)]\tLoss: 0.228294\tGrad Norm: 1.487957\tLR: 0.030000\n",
      "Train Epoch: 1565 [167936/194182 (85%)]\tLoss: 0.227819\tGrad Norm: 1.528732\tLR: 0.030000\n",
      "Train Epoch: 1565 [188416/194182 (96%)]\tLoss: 0.225231\tGrad Norm: 1.441774\tLR: 0.030000\n",
      "Train set: Average loss: 0.2306\n",
      "Test set: Average loss: 0.2540, Average MAE: 0.3416\n",
      "Epoch 1565: Mean reward = 0.049 +/- 0.044\n",
      "Train Epoch: 1566 [4096/194182 (2%)]\tLoss: 0.225615\tGrad Norm: 1.687389\tLR: 0.030000\n",
      "Train Epoch: 1566 [24576/194182 (12%)]\tLoss: 0.227362\tGrad Norm: 1.470787\tLR: 0.030000\n",
      "Train Epoch: 1566 [45056/194182 (23%)]\tLoss: 0.221145\tGrad Norm: 1.466484\tLR: 0.030000\n",
      "Train Epoch: 1566 [65536/194182 (33%)]\tLoss: 0.232035\tGrad Norm: 1.734259\tLR: 0.030000\n",
      "Train Epoch: 1566 [86016/194182 (44%)]\tLoss: 0.232340\tGrad Norm: 1.504522\tLR: 0.030000\n",
      "Train Epoch: 1566 [106496/194182 (54%)]\tLoss: 0.227626\tGrad Norm: 1.528476\tLR: 0.030000\n",
      "Train Epoch: 1566 [126976/194182 (65%)]\tLoss: 0.236224\tGrad Norm: 1.943673\tLR: 0.030000\n",
      "Train Epoch: 1566 [147456/194182 (75%)]\tLoss: 0.235609\tGrad Norm: 1.671270\tLR: 0.030000\n",
      "Train Epoch: 1566 [167936/194182 (85%)]\tLoss: 0.233224\tGrad Norm: 1.549724\tLR: 0.030000\n",
      "Train Epoch: 1566 [188416/194182 (96%)]\tLoss: 0.244964\tGrad Norm: 2.176475\tLR: 0.030000\n",
      "Train set: Average loss: 0.2303\n",
      "Test set: Average loss: 0.2608, Average MAE: 0.3463\n",
      "Train Epoch: 1567 [4096/194182 (2%)]\tLoss: 0.232768\tGrad Norm: 2.028680\tLR: 0.030000\n",
      "Train Epoch: 1567 [24576/194182 (12%)]\tLoss: 0.229920\tGrad Norm: 1.526349\tLR: 0.030000\n",
      "Train Epoch: 1567 [45056/194182 (23%)]\tLoss: 0.222319\tGrad Norm: 1.340939\tLR: 0.030000\n",
      "Train Epoch: 1567 [65536/194182 (33%)]\tLoss: 0.239723\tGrad Norm: 1.945561\tLR: 0.030000\n",
      "Train Epoch: 1567 [86016/194182 (44%)]\tLoss: 0.228799\tGrad Norm: 1.824488\tLR: 0.030000\n",
      "Train Epoch: 1567 [106496/194182 (54%)]\tLoss: 0.231598\tGrad Norm: 1.715073\tLR: 0.030000\n",
      "Train Epoch: 1567 [126976/194182 (65%)]\tLoss: 0.240491\tGrad Norm: 1.707495\tLR: 0.030000\n",
      "Train Epoch: 1567 [147456/194182 (75%)]\tLoss: 0.221514\tGrad Norm: 1.396100\tLR: 0.030000\n",
      "Train Epoch: 1567 [167936/194182 (85%)]\tLoss: 0.227521\tGrad Norm: 1.595991\tLR: 0.030000\n",
      "Train Epoch: 1567 [188416/194182 (96%)]\tLoss: 0.235965\tGrad Norm: 1.657325\tLR: 0.030000\n",
      "Train set: Average loss: 0.2304\n",
      "Test set: Average loss: 0.2560, Average MAE: 0.3570\n",
      "Train Epoch: 1568 [4096/194182 (2%)]\tLoss: 0.225678\tGrad Norm: 1.471243\tLR: 0.030000\n",
      "Train Epoch: 1568 [24576/194182 (12%)]\tLoss: 0.227109\tGrad Norm: 1.698749\tLR: 0.030000\n",
      "Train Epoch: 1568 [45056/194182 (23%)]\tLoss: 0.228549\tGrad Norm: 1.667225\tLR: 0.030000\n",
      "Train Epoch: 1568 [65536/194182 (33%)]\tLoss: 0.233446\tGrad Norm: 1.920409\tLR: 0.030000\n",
      "Train Epoch: 1568 [86016/194182 (44%)]\tLoss: 0.235669\tGrad Norm: 1.867393\tLR: 0.030000\n",
      "Train Epoch: 1568 [106496/194182 (54%)]\tLoss: 0.220049\tGrad Norm: 1.355118\tLR: 0.030000\n",
      "Train Epoch: 1568 [126976/194182 (65%)]\tLoss: 0.235989\tGrad Norm: 1.642002\tLR: 0.030000\n",
      "Train Epoch: 1568 [147456/194182 (75%)]\tLoss: 0.233152\tGrad Norm: 1.383682\tLR: 0.030000\n",
      "Train Epoch: 1568 [167936/194182 (85%)]\tLoss: 0.232389\tGrad Norm: 1.622666\tLR: 0.030000\n",
      "Train Epoch: 1568 [188416/194182 (96%)]\tLoss: 0.233156\tGrad Norm: 1.793498\tLR: 0.030000\n",
      "Train set: Average loss: 0.2307\n",
      "Test set: Average loss: 0.2621, Average MAE: 0.3465\n",
      "Train Epoch: 1569 [4096/194182 (2%)]\tLoss: 0.239508\tGrad Norm: 2.202147\tLR: 0.030000\n",
      "Train Epoch: 1569 [24576/194182 (12%)]\tLoss: 0.224973\tGrad Norm: 1.456911\tLR: 0.030000\n",
      "Train Epoch: 1569 [45056/194182 (23%)]\tLoss: 0.236838\tGrad Norm: 2.020892\tLR: 0.030000\n",
      "Train Epoch: 1569 [65536/194182 (33%)]\tLoss: 0.236048\tGrad Norm: 2.184983\tLR: 0.030000\n",
      "Train Epoch: 1569 [86016/194182 (44%)]\tLoss: 0.228914\tGrad Norm: 1.762465\tLR: 0.030000\n",
      "Train Epoch: 1569 [106496/194182 (54%)]\tLoss: 0.228077\tGrad Norm: 1.545365\tLR: 0.030000\n",
      "Train Epoch: 1569 [126976/194182 (65%)]\tLoss: 0.231587\tGrad Norm: 1.525110\tLR: 0.030000\n",
      "Train Epoch: 1569 [147456/194182 (75%)]\tLoss: 0.228558\tGrad Norm: 1.708625\tLR: 0.030000\n",
      "Train Epoch: 1569 [167936/194182 (85%)]\tLoss: 0.231307\tGrad Norm: 1.619217\tLR: 0.030000\n",
      "Train Epoch: 1569 [188416/194182 (96%)]\tLoss: 0.232740\tGrad Norm: 1.677315\tLR: 0.030000\n",
      "Train set: Average loss: 0.2314\n",
      "Test set: Average loss: 0.2541, Average MAE: 0.3530\n",
      "Train Epoch: 1570 [4096/194182 (2%)]\tLoss: 0.230706\tGrad Norm: 1.522041\tLR: 0.030000\n",
      "Train Epoch: 1570 [24576/194182 (12%)]\tLoss: 0.229138\tGrad Norm: 1.581094\tLR: 0.030000\n",
      "Train Epoch: 1570 [45056/194182 (23%)]\tLoss: 0.225474\tGrad Norm: 1.577852\tLR: 0.030000\n",
      "Train Epoch: 1570 [65536/194182 (33%)]\tLoss: 0.227311\tGrad Norm: 1.718889\tLR: 0.030000\n",
      "Train Epoch: 1570 [86016/194182 (44%)]\tLoss: 0.231172\tGrad Norm: 1.809141\tLR: 0.030000\n",
      "Train Epoch: 1570 [106496/194182 (54%)]\tLoss: 0.229097\tGrad Norm: 1.511003\tLR: 0.030000\n",
      "Train Epoch: 1570 [126976/194182 (65%)]\tLoss: 0.227293\tGrad Norm: 1.286572\tLR: 0.030000\n",
      "Train Epoch: 1570 [147456/194182 (75%)]\tLoss: 0.231960\tGrad Norm: 1.544446\tLR: 0.030000\n",
      "Train Epoch: 1570 [167936/194182 (85%)]\tLoss: 0.231758\tGrad Norm: 2.018599\tLR: 0.030000\n",
      "Train Epoch: 1570 [188416/194182 (96%)]\tLoss: 0.226785\tGrad Norm: 1.566693\tLR: 0.030000\n",
      "Train set: Average loss: 0.2300\n",
      "Test set: Average loss: 0.2560, Average MAE: 0.3493\n",
      "Epoch 1570: Mean reward = 0.073 +/- 0.070\n",
      "Train Epoch: 1571 [4096/194182 (2%)]\tLoss: 0.230124\tGrad Norm: 1.728448\tLR: 0.030000\n",
      "Train Epoch: 1571 [24576/194182 (12%)]\tLoss: 0.229715\tGrad Norm: 1.639362\tLR: 0.030000\n",
      "Train Epoch: 1571 [45056/194182 (23%)]\tLoss: 0.228432\tGrad Norm: 1.823376\tLR: 0.030000\n",
      "Train Epoch: 1571 [65536/194182 (33%)]\tLoss: 0.226467\tGrad Norm: 1.583733\tLR: 0.030000\n",
      "Train Epoch: 1571 [86016/194182 (44%)]\tLoss: 0.225131\tGrad Norm: 1.486284\tLR: 0.030000\n",
      "Train Epoch: 1571 [106496/194182 (54%)]\tLoss: 0.231705\tGrad Norm: 2.042058\tLR: 0.030000\n",
      "Train Epoch: 1571 [126976/194182 (65%)]\tLoss: 0.230003\tGrad Norm: 1.915888\tLR: 0.030000\n",
      "Train Epoch: 1571 [147456/194182 (75%)]\tLoss: 0.233891\tGrad Norm: 1.866793\tLR: 0.030000\n",
      "Train Epoch: 1571 [167936/194182 (85%)]\tLoss: 0.230637\tGrad Norm: 1.931648\tLR: 0.030000\n",
      "Train Epoch: 1571 [188416/194182 (96%)]\tLoss: 0.229500\tGrad Norm: 1.759036\tLR: 0.030000\n",
      "Train set: Average loss: 0.2320\n",
      "Test set: Average loss: 0.2578, Average MAE: 0.3637\n",
      "Train Epoch: 1572 [4096/194182 (2%)]\tLoss: 0.228870\tGrad Norm: 1.746192\tLR: 0.030000\n",
      "Train Epoch: 1572 [24576/194182 (12%)]\tLoss: 0.227620\tGrad Norm: 1.332673\tLR: 0.030000\n",
      "Train Epoch: 1572 [45056/194182 (23%)]\tLoss: 0.220787\tGrad Norm: 1.581122\tLR: 0.030000\n",
      "Train Epoch: 1572 [65536/194182 (33%)]\tLoss: 0.234597\tGrad Norm: 1.804882\tLR: 0.030000\n",
      "Train Epoch: 1572 [86016/194182 (44%)]\tLoss: 0.224787\tGrad Norm: 1.803596\tLR: 0.030000\n",
      "Train Epoch: 1572 [106496/194182 (54%)]\tLoss: 0.240444\tGrad Norm: 2.093539\tLR: 0.030000\n",
      "Train Epoch: 1572 [126976/194182 (65%)]\tLoss: 0.231275\tGrad Norm: 1.703327\tLR: 0.030000\n",
      "Train Epoch: 1572 [147456/194182 (75%)]\tLoss: 0.237564\tGrad Norm: 1.878399\tLR: 0.030000\n",
      "Train Epoch: 1572 [167936/194182 (85%)]\tLoss: 0.226845\tGrad Norm: 1.571552\tLR: 0.030000\n",
      "Train Epoch: 1572 [188416/194182 (96%)]\tLoss: 0.229187\tGrad Norm: 1.491413\tLR: 0.030000\n",
      "Train set: Average loss: 0.2308\n",
      "Test set: Average loss: 0.2518, Average MAE: 0.3455\n",
      "Train Epoch: 1573 [4096/194182 (2%)]\tLoss: 0.223239\tGrad Norm: 1.362202\tLR: 0.030000\n",
      "Train Epoch: 1573 [24576/194182 (12%)]\tLoss: 0.220809\tGrad Norm: 1.245931\tLR: 0.030000\n",
      "Train Epoch: 1573 [45056/194182 (23%)]\tLoss: 0.224076\tGrad Norm: 1.638326\tLR: 0.030000\n",
      "Train Epoch: 1573 [65536/194182 (33%)]\tLoss: 0.226551\tGrad Norm: 1.585463\tLR: 0.030000\n",
      "Train Epoch: 1573 [86016/194182 (44%)]\tLoss: 0.228104\tGrad Norm: 1.485763\tLR: 0.030000\n",
      "Train Epoch: 1573 [106496/194182 (54%)]\tLoss: 0.232861\tGrad Norm: 1.771322\tLR: 0.030000\n",
      "Train Epoch: 1573 [126976/194182 (65%)]\tLoss: 0.237430\tGrad Norm: 1.951278\tLR: 0.030000\n",
      "Train Epoch: 1573 [147456/194182 (75%)]\tLoss: 0.219861\tGrad Norm: 1.507387\tLR: 0.030000\n",
      "Train Epoch: 1573 [167936/194182 (85%)]\tLoss: 0.228538\tGrad Norm: 1.464108\tLR: 0.030000\n",
      "Train Epoch: 1573 [188416/194182 (96%)]\tLoss: 0.237503\tGrad Norm: 1.689886\tLR: 0.030000\n",
      "Train set: Average loss: 0.2281\n",
      "Test set: Average loss: 0.2534, Average MAE: 0.3494\n",
      "Train Epoch: 1574 [4096/194182 (2%)]\tLoss: 0.227762\tGrad Norm: 1.465632\tLR: 0.030000\n",
      "Train Epoch: 1574 [24576/194182 (12%)]\tLoss: 0.227869\tGrad Norm: 1.533085\tLR: 0.030000\n",
      "Train Epoch: 1574 [45056/194182 (23%)]\tLoss: 0.229921\tGrad Norm: 1.664235\tLR: 0.030000\n",
      "Train Epoch: 1574 [65536/194182 (33%)]\tLoss: 0.226468\tGrad Norm: 1.855913\tLR: 0.030000\n",
      "Train Epoch: 1574 [86016/194182 (44%)]\tLoss: 0.233324\tGrad Norm: 2.029247\tLR: 0.030000\n",
      "Train Epoch: 1574 [106496/194182 (54%)]\tLoss: 0.229782\tGrad Norm: 1.833326\tLR: 0.030000\n",
      "Train Epoch: 1574 [126976/194182 (65%)]\tLoss: 0.242974\tGrad Norm: 1.931579\tLR: 0.030000\n",
      "Train Epoch: 1574 [147456/194182 (75%)]\tLoss: 0.225319\tGrad Norm: 1.260116\tLR: 0.030000\n",
      "Train Epoch: 1574 [167936/194182 (85%)]\tLoss: 0.232531\tGrad Norm: 1.903197\tLR: 0.030000\n",
      "Train Epoch: 1574 [188416/194182 (96%)]\tLoss: 0.230641\tGrad Norm: 1.456490\tLR: 0.030000\n",
      "Train set: Average loss: 0.2297\n",
      "Test set: Average loss: 0.2546, Average MAE: 0.3522\n",
      "Train Epoch: 1575 [4096/194182 (2%)]\tLoss: 0.229018\tGrad Norm: 1.579626\tLR: 0.030000\n",
      "Train Epoch: 1575 [24576/194182 (12%)]\tLoss: 0.226337\tGrad Norm: 1.551004\tLR: 0.030000\n",
      "Train Epoch: 1575 [45056/194182 (23%)]\tLoss: 0.231536\tGrad Norm: 1.387446\tLR: 0.030000\n",
      "Train Epoch: 1575 [65536/194182 (33%)]\tLoss: 0.229588\tGrad Norm: 1.704089\tLR: 0.030000\n",
      "Train Epoch: 1575 [86016/194182 (44%)]\tLoss: 0.224388\tGrad Norm: 1.402528\tLR: 0.030000\n",
      "Train Epoch: 1575 [106496/194182 (54%)]\tLoss: 0.223459\tGrad Norm: 1.185834\tLR: 0.030000\n",
      "Train Epoch: 1575 [126976/194182 (65%)]\tLoss: 0.222726\tGrad Norm: 1.550437\tLR: 0.030000\n",
      "Train Epoch: 1575 [147456/194182 (75%)]\tLoss: 0.220290\tGrad Norm: 1.501642\tLR: 0.030000\n",
      "Train Epoch: 1575 [167936/194182 (85%)]\tLoss: 0.226537\tGrad Norm: 1.863644\tLR: 0.030000\n",
      "Train Epoch: 1575 [188416/194182 (96%)]\tLoss: 0.229340\tGrad Norm: 1.886041\tLR: 0.030000\n",
      "Train set: Average loss: 0.2279\n",
      "Test set: Average loss: 0.2542, Average MAE: 0.3395\n",
      "Epoch 1575: Mean reward = 0.046 +/- 0.034\n",
      "Train Epoch: 1576 [4096/194182 (2%)]\tLoss: 0.219027\tGrad Norm: 1.623961\tLR: 0.030000\n",
      "Train Epoch: 1576 [24576/194182 (12%)]\tLoss: 0.229571\tGrad Norm: 1.886201\tLR: 0.030000\n",
      "Train Epoch: 1576 [45056/194182 (23%)]\tLoss: 0.229358\tGrad Norm: 1.836724\tLR: 0.030000\n",
      "Train Epoch: 1576 [65536/194182 (33%)]\tLoss: 0.228176\tGrad Norm: 1.576279\tLR: 0.030000\n",
      "Train Epoch: 1576 [86016/194182 (44%)]\tLoss: 0.230408\tGrad Norm: 1.589106\tLR: 0.030000\n",
      "Train Epoch: 1576 [106496/194182 (54%)]\tLoss: 0.228743\tGrad Norm: 1.619147\tLR: 0.030000\n",
      "Train Epoch: 1576 [126976/194182 (65%)]\tLoss: 0.232243\tGrad Norm: 1.916446\tLR: 0.030000\n",
      "Train Epoch: 1576 [147456/194182 (75%)]\tLoss: 0.237653\tGrad Norm: 1.819599\tLR: 0.030000\n",
      "Train Epoch: 1576 [167936/194182 (85%)]\tLoss: 0.233757\tGrad Norm: 1.422039\tLR: 0.030000\n",
      "Train Epoch: 1576 [188416/194182 (96%)]\tLoss: 0.231154\tGrad Norm: 1.808758\tLR: 0.030000\n",
      "Train set: Average loss: 0.2308\n",
      "Test set: Average loss: 0.2523, Average MAE: 0.3472\n",
      "Train Epoch: 1577 [4096/194182 (2%)]\tLoss: 0.228231\tGrad Norm: 1.509979\tLR: 0.030000\n",
      "Train Epoch: 1577 [24576/194182 (12%)]\tLoss: 0.229257\tGrad Norm: 2.137727\tLR: 0.030000\n",
      "Train Epoch: 1577 [45056/194182 (23%)]\tLoss: 0.224161\tGrad Norm: 1.447793\tLR: 0.030000\n",
      "Train Epoch: 1577 [65536/194182 (33%)]\tLoss: 0.223476\tGrad Norm: 1.663809\tLR: 0.030000\n",
      "Train Epoch: 1577 [86016/194182 (44%)]\tLoss: 0.222184\tGrad Norm: 1.498278\tLR: 0.030000\n",
      "Train Epoch: 1577 [106496/194182 (54%)]\tLoss: 0.234481\tGrad Norm: 1.871048\tLR: 0.030000\n",
      "Train Epoch: 1577 [126976/194182 (65%)]\tLoss: 0.233326\tGrad Norm: 1.764658\tLR: 0.030000\n",
      "Train Epoch: 1577 [147456/194182 (75%)]\tLoss: 0.243424\tGrad Norm: 2.192141\tLR: 0.030000\n",
      "Train Epoch: 1577 [167936/194182 (85%)]\tLoss: 0.229158\tGrad Norm: 1.561216\tLR: 0.030000\n",
      "Train Epoch: 1577 [188416/194182 (96%)]\tLoss: 0.225727\tGrad Norm: 1.552442\tLR: 0.030000\n",
      "Train set: Average loss: 0.2299\n",
      "Test set: Average loss: 0.2592, Average MAE: 0.3399\n",
      "Train Epoch: 1578 [4096/194182 (2%)]\tLoss: 0.229984\tGrad Norm: 2.055380\tLR: 0.030000\n",
      "Train Epoch: 1578 [24576/194182 (12%)]\tLoss: 0.230777\tGrad Norm: 1.971923\tLR: 0.030000\n",
      "Train Epoch: 1578 [45056/194182 (23%)]\tLoss: 0.234636\tGrad Norm: 1.906829\tLR: 0.030000\n",
      "Train Epoch: 1578 [65536/194182 (33%)]\tLoss: 0.236728\tGrad Norm: 1.847542\tLR: 0.030000\n",
      "Train Epoch: 1578 [86016/194182 (44%)]\tLoss: 0.231069\tGrad Norm: 1.658904\tLR: 0.030000\n",
      "Train Epoch: 1578 [106496/194182 (54%)]\tLoss: 0.226117\tGrad Norm: 1.489915\tLR: 0.030000\n",
      "Train Epoch: 1578 [126976/194182 (65%)]\tLoss: 0.231424\tGrad Norm: 1.813148\tLR: 0.030000\n",
      "Train Epoch: 1578 [147456/194182 (75%)]\tLoss: 0.232078\tGrad Norm: 1.826716\tLR: 0.030000\n",
      "Train Epoch: 1578 [167936/194182 (85%)]\tLoss: 0.226784\tGrad Norm: 1.370009\tLR: 0.030000\n",
      "Train Epoch: 1578 [188416/194182 (96%)]\tLoss: 0.219933\tGrad Norm: 1.362248\tLR: 0.030000\n",
      "Train set: Average loss: 0.2302\n",
      "Test set: Average loss: 0.2523, Average MAE: 0.3459\n",
      "Train Epoch: 1579 [4096/194182 (2%)]\tLoss: 0.235866\tGrad Norm: 1.641676\tLR: 0.030000\n",
      "Train Epoch: 1579 [24576/194182 (12%)]\tLoss: 0.227782\tGrad Norm: 1.710456\tLR: 0.030000\n",
      "Train Epoch: 1579 [45056/194182 (23%)]\tLoss: 0.218871\tGrad Norm: 1.436670\tLR: 0.030000\n",
      "Train Epoch: 1579 [65536/194182 (33%)]\tLoss: 0.226218\tGrad Norm: 1.759086\tLR: 0.030000\n",
      "Train Epoch: 1579 [86016/194182 (44%)]\tLoss: 0.217115\tGrad Norm: 1.540786\tLR: 0.030000\n",
      "Train Epoch: 1579 [106496/194182 (54%)]\tLoss: 0.235293\tGrad Norm: 1.978201\tLR: 0.030000\n",
      "Train Epoch: 1579 [126976/194182 (65%)]\tLoss: 0.231423\tGrad Norm: 2.080587\tLR: 0.030000\n",
      "Train Epoch: 1579 [147456/194182 (75%)]\tLoss: 0.225429\tGrad Norm: 1.506301\tLR: 0.030000\n",
      "Train Epoch: 1579 [167936/194182 (85%)]\tLoss: 0.231201\tGrad Norm: 1.806803\tLR: 0.030000\n",
      "Train Epoch: 1579 [188416/194182 (96%)]\tLoss: 0.230494\tGrad Norm: 1.786371\tLR: 0.030000\n",
      "Train set: Average loss: 0.2294\n",
      "Test set: Average loss: 0.2553, Average MAE: 0.3482\n",
      "Train Epoch: 1580 [4096/194182 (2%)]\tLoss: 0.224998\tGrad Norm: 1.614471\tLR: 0.030000\n",
      "Train Epoch: 1580 [24576/194182 (12%)]\tLoss: 0.232944\tGrad Norm: 1.852391\tLR: 0.030000\n",
      "Train Epoch: 1580 [45056/194182 (23%)]\tLoss: 0.226033\tGrad Norm: 1.549029\tLR: 0.030000\n",
      "Train Epoch: 1580 [65536/194182 (33%)]\tLoss: 0.228122\tGrad Norm: 1.592456\tLR: 0.030000\n",
      "Train Epoch: 1580 [86016/194182 (44%)]\tLoss: 0.218100\tGrad Norm: 1.270744\tLR: 0.030000\n",
      "Train Epoch: 1580 [106496/194182 (54%)]\tLoss: 0.222970\tGrad Norm: 1.346773\tLR: 0.030000\n",
      "Train Epoch: 1580 [126976/194182 (65%)]\tLoss: 0.223395\tGrad Norm: 1.396305\tLR: 0.030000\n",
      "Train Epoch: 1580 [147456/194182 (75%)]\tLoss: 0.228891\tGrad Norm: 1.532117\tLR: 0.030000\n",
      "Train Epoch: 1580 [167936/194182 (85%)]\tLoss: 0.221167\tGrad Norm: 1.556122\tLR: 0.030000\n",
      "Train Epoch: 1580 [188416/194182 (96%)]\tLoss: 0.227596\tGrad Norm: 1.569414\tLR: 0.030000\n",
      "Train set: Average loss: 0.2273\n",
      "Test set: Average loss: 0.2516, Average MAE: 0.3529\n",
      "Epoch 1580: Mean reward = 0.047 +/- 0.022\n",
      "Train Epoch: 1581 [4096/194182 (2%)]\tLoss: 0.225869\tGrad Norm: 1.473782\tLR: 0.030000\n",
      "Train Epoch: 1581 [24576/194182 (12%)]\tLoss: 0.220644\tGrad Norm: 1.285984\tLR: 0.030000\n",
      "Train Epoch: 1581 [45056/194182 (23%)]\tLoss: 0.228697\tGrad Norm: 1.586192\tLR: 0.030000\n",
      "Train Epoch: 1581 [65536/194182 (33%)]\tLoss: 0.232313\tGrad Norm: 1.613945\tLR: 0.030000\n",
      "Train Epoch: 1581 [86016/194182 (44%)]\tLoss: 0.227512\tGrad Norm: 1.730938\tLR: 0.030000\n",
      "Train Epoch: 1581 [106496/194182 (54%)]\tLoss: 0.223518\tGrad Norm: 1.464695\tLR: 0.030000\n",
      "Train Epoch: 1581 [126976/194182 (65%)]\tLoss: 0.221541\tGrad Norm: 1.734052\tLR: 0.030000\n",
      "Train Epoch: 1581 [147456/194182 (75%)]\tLoss: 0.227950\tGrad Norm: 1.772734\tLR: 0.030000\n",
      "Train Epoch: 1581 [167936/194182 (85%)]\tLoss: 0.235139\tGrad Norm: 1.962613\tLR: 0.030000\n",
      "Train Epoch: 1581 [188416/194182 (96%)]\tLoss: 0.242241\tGrad Norm: 2.419427\tLR: 0.030000\n",
      "Train set: Average loss: 0.2300\n",
      "Test set: Average loss: 0.2586, Average MAE: 0.3614\n",
      "Train Epoch: 1582 [4096/194182 (2%)]\tLoss: 0.229395\tGrad Norm: 1.831437\tLR: 0.030000\n",
      "Train Epoch: 1582 [24576/194182 (12%)]\tLoss: 0.228464\tGrad Norm: 1.655433\tLR: 0.030000\n",
      "Train Epoch: 1582 [45056/194182 (23%)]\tLoss: 0.243436\tGrad Norm: 2.125390\tLR: 0.030000\n",
      "Train Epoch: 1582 [65536/194182 (33%)]\tLoss: 0.237737\tGrad Norm: 2.011528\tLR: 0.030000\n",
      "Train Epoch: 1582 [86016/194182 (44%)]\tLoss: 0.228287\tGrad Norm: 1.684293\tLR: 0.030000\n",
      "Train Epoch: 1582 [106496/194182 (54%)]\tLoss: 0.233983\tGrad Norm: 1.692269\tLR: 0.030000\n",
      "Train Epoch: 1582 [126976/194182 (65%)]\tLoss: 0.233075\tGrad Norm: 1.734227\tLR: 0.030000\n",
      "Train Epoch: 1582 [147456/194182 (75%)]\tLoss: 0.237108\tGrad Norm: 1.868929\tLR: 0.030000\n",
      "Train Epoch: 1582 [167936/194182 (85%)]\tLoss: 0.234102\tGrad Norm: 1.784940\tLR: 0.030000\n",
      "Train Epoch: 1582 [188416/194182 (96%)]\tLoss: 0.226155\tGrad Norm: 1.699865\tLR: 0.030000\n",
      "Train set: Average loss: 0.2311\n",
      "Test set: Average loss: 0.2563, Average MAE: 0.3532\n",
      "Train Epoch: 1583 [4096/194182 (2%)]\tLoss: 0.227260\tGrad Norm: 1.678674\tLR: 0.030000\n",
      "Train Epoch: 1583 [24576/194182 (12%)]\tLoss: 0.220906\tGrad Norm: 1.402020\tLR: 0.030000\n",
      "Train Epoch: 1583 [45056/194182 (23%)]\tLoss: 0.224814\tGrad Norm: 1.321100\tLR: 0.030000\n",
      "Train Epoch: 1583 [65536/194182 (33%)]\tLoss: 0.226377\tGrad Norm: 1.454346\tLR: 0.030000\n",
      "Train Epoch: 1583 [86016/194182 (44%)]\tLoss: 0.230763\tGrad Norm: 1.717998\tLR: 0.030000\n",
      "Train Epoch: 1583 [106496/194182 (54%)]\tLoss: 0.223687\tGrad Norm: 1.734077\tLR: 0.030000\n",
      "Train Epoch: 1583 [126976/194182 (65%)]\tLoss: 0.232698\tGrad Norm: 1.637625\tLR: 0.030000\n",
      "Train Epoch: 1583 [147456/194182 (75%)]\tLoss: 0.230702\tGrad Norm: 1.356861\tLR: 0.030000\n",
      "Train Epoch: 1583 [167936/194182 (85%)]\tLoss: 0.228577\tGrad Norm: 1.730968\tLR: 0.030000\n",
      "Train Epoch: 1583 [188416/194182 (96%)]\tLoss: 0.220821\tGrad Norm: 1.393150\tLR: 0.030000\n",
      "Train set: Average loss: 0.2262\n",
      "Test set: Average loss: 0.2545, Average MAE: 0.3518\n",
      "Train Epoch: 1584 [4096/194182 (2%)]\tLoss: 0.225843\tGrad Norm: 1.478956\tLR: 0.030000\n",
      "Train Epoch: 1584 [24576/194182 (12%)]\tLoss: 0.226395\tGrad Norm: 1.755452\tLR: 0.030000\n",
      "Train Epoch: 1584 [45056/194182 (23%)]\tLoss: 0.232272\tGrad Norm: 1.731435\tLR: 0.030000\n",
      "Train Epoch: 1584 [65536/194182 (33%)]\tLoss: 0.229153\tGrad Norm: 1.761920\tLR: 0.030000\n",
      "Train Epoch: 1584 [86016/194182 (44%)]\tLoss: 0.228855\tGrad Norm: 1.868793\tLR: 0.030000\n",
      "Train Epoch: 1584 [106496/194182 (54%)]\tLoss: 0.232809\tGrad Norm: 1.694069\tLR: 0.030000\n",
      "Train Epoch: 1584 [126976/194182 (65%)]\tLoss: 0.230545\tGrad Norm: 1.681539\tLR: 0.030000\n",
      "Train Epoch: 1584 [147456/194182 (75%)]\tLoss: 0.227812\tGrad Norm: 1.818888\tLR: 0.030000\n",
      "Train Epoch: 1584 [167936/194182 (85%)]\tLoss: 0.233621\tGrad Norm: 1.816641\tLR: 0.030000\n",
      "Train Epoch: 1584 [188416/194182 (96%)]\tLoss: 0.239201\tGrad Norm: 2.175064\tLR: 0.030000\n",
      "Train set: Average loss: 0.2306\n",
      "Test set: Average loss: 0.2606, Average MAE: 0.3489\n",
      "Train Epoch: 1585 [4096/194182 (2%)]\tLoss: 0.230092\tGrad Norm: 2.877087\tLR: 0.030000\n",
      "Train Epoch: 1585 [24576/194182 (12%)]\tLoss: 0.231766\tGrad Norm: 1.860821\tLR: 0.030000\n",
      "Train Epoch: 1585 [45056/194182 (23%)]\tLoss: 0.236044\tGrad Norm: 2.348459\tLR: 0.030000\n",
      "Train Epoch: 1585 [65536/194182 (33%)]\tLoss: 0.241368\tGrad Norm: 2.219292\tLR: 0.030000\n",
      "Train Epoch: 1585 [86016/194182 (44%)]\tLoss: 0.225050\tGrad Norm: 1.429637\tLR: 0.030000\n",
      "Train Epoch: 1585 [106496/194182 (54%)]\tLoss: 0.234502\tGrad Norm: 1.650396\tLR: 0.030000\n",
      "Train Epoch: 1585 [126976/194182 (65%)]\tLoss: 0.225933\tGrad Norm: 1.641715\tLR: 0.030000\n",
      "Train Epoch: 1585 [147456/194182 (75%)]\tLoss: 0.234431\tGrad Norm: 1.575087\tLR: 0.030000\n",
      "Train Epoch: 1585 [167936/194182 (85%)]\tLoss: 0.225017\tGrad Norm: 1.593228\tLR: 0.030000\n",
      "Train Epoch: 1585 [188416/194182 (96%)]\tLoss: 0.222013\tGrad Norm: 1.149402\tLR: 0.030000\n",
      "Train set: Average loss: 0.2298\n",
      "Test set: Average loss: 0.2512, Average MAE: 0.3456\n",
      "Epoch 1585: Mean reward = 0.060 +/- 0.043\n",
      "Train Epoch: 1586 [4096/194182 (2%)]\tLoss: 0.222765\tGrad Norm: 1.309760\tLR: 0.030000\n",
      "Train Epoch: 1586 [24576/194182 (12%)]\tLoss: 0.223074\tGrad Norm: 1.495629\tLR: 0.030000\n",
      "Train Epoch: 1586 [45056/194182 (23%)]\tLoss: 0.229754\tGrad Norm: 1.647734\tLR: 0.030000\n",
      "Train Epoch: 1586 [65536/194182 (33%)]\tLoss: 0.226972\tGrad Norm: 2.211862\tLR: 0.030000\n",
      "Train Epoch: 1586 [86016/194182 (44%)]\tLoss: 0.232539\tGrad Norm: 1.844402\tLR: 0.030000\n",
      "Train Epoch: 1586 [106496/194182 (54%)]\tLoss: 0.227041\tGrad Norm: 1.502002\tLR: 0.030000\n",
      "Train Epoch: 1586 [126976/194182 (65%)]\tLoss: 0.224937\tGrad Norm: 1.587601\tLR: 0.030000\n",
      "Train Epoch: 1586 [147456/194182 (75%)]\tLoss: 0.228740\tGrad Norm: 1.856458\tLR: 0.030000\n",
      "Train Epoch: 1586 [167936/194182 (85%)]\tLoss: 0.223823\tGrad Norm: 1.814746\tLR: 0.030000\n",
      "Train Epoch: 1586 [188416/194182 (96%)]\tLoss: 0.229276\tGrad Norm: 1.581144\tLR: 0.030000\n",
      "Train set: Average loss: 0.2278\n",
      "Test set: Average loss: 0.2563, Average MAE: 0.3524\n",
      "Train Epoch: 1587 [4096/194182 (2%)]\tLoss: 0.231266\tGrad Norm: 1.656542\tLR: 0.030000\n",
      "Train Epoch: 1587 [24576/194182 (12%)]\tLoss: 0.224356\tGrad Norm: 1.470321\tLR: 0.030000\n",
      "Train Epoch: 1587 [45056/194182 (23%)]\tLoss: 0.227277\tGrad Norm: 1.734220\tLR: 0.030000\n",
      "Train Epoch: 1587 [65536/194182 (33%)]\tLoss: 0.232470\tGrad Norm: 1.814816\tLR: 0.030000\n",
      "Train Epoch: 1587 [86016/194182 (44%)]\tLoss: 0.225124\tGrad Norm: 1.577156\tLR: 0.030000\n",
      "Train Epoch: 1587 [106496/194182 (54%)]\tLoss: 0.232976\tGrad Norm: 1.921061\tLR: 0.030000\n",
      "Train Epoch: 1587 [126976/194182 (65%)]\tLoss: 0.224448\tGrad Norm: 1.558054\tLR: 0.030000\n",
      "Train Epoch: 1587 [147456/194182 (75%)]\tLoss: 0.230883\tGrad Norm: 1.645866\tLR: 0.030000\n",
      "Train Epoch: 1587 [167936/194182 (85%)]\tLoss: 0.228452\tGrad Norm: 1.670503\tLR: 0.030000\n",
      "Train Epoch: 1587 [188416/194182 (96%)]\tLoss: 0.227746\tGrad Norm: 1.477955\tLR: 0.030000\n",
      "Train set: Average loss: 0.2281\n",
      "Test set: Average loss: 0.2543, Average MAE: 0.3490\n",
      "Train Epoch: 1588 [4096/194182 (2%)]\tLoss: 0.223950\tGrad Norm: 1.535718\tLR: 0.030000\n",
      "Train Epoch: 1588 [24576/194182 (12%)]\tLoss: 0.219604\tGrad Norm: 1.512235\tLR: 0.030000\n",
      "Train Epoch: 1588 [45056/194182 (23%)]\tLoss: 0.228080\tGrad Norm: 1.658887\tLR: 0.030000\n",
      "Train Epoch: 1588 [65536/194182 (33%)]\tLoss: 0.233672\tGrad Norm: 1.809604\tLR: 0.030000\n",
      "Train Epoch: 1588 [86016/194182 (44%)]\tLoss: 0.223393\tGrad Norm: 1.393703\tLR: 0.030000\n",
      "Train Epoch: 1588 [106496/194182 (54%)]\tLoss: 0.223852\tGrad Norm: 1.537247\tLR: 0.030000\n",
      "Train Epoch: 1588 [126976/194182 (65%)]\tLoss: 0.228672\tGrad Norm: 1.408598\tLR: 0.030000\n",
      "Train Epoch: 1588 [147456/194182 (75%)]\tLoss: 0.230985\tGrad Norm: 2.020728\tLR: 0.030000\n",
      "Train Epoch: 1588 [167936/194182 (85%)]\tLoss: 0.227512\tGrad Norm: 1.615181\tLR: 0.030000\n",
      "Train Epoch: 1588 [188416/194182 (96%)]\tLoss: 0.228774\tGrad Norm: 1.485141\tLR: 0.030000\n",
      "Train set: Average loss: 0.2281\n",
      "Test set: Average loss: 0.2488, Average MAE: 0.3476\n",
      "Train Epoch: 1589 [4096/194182 (2%)]\tLoss: 0.226918\tGrad Norm: 1.343253\tLR: 0.030000\n",
      "Train Epoch: 1589 [24576/194182 (12%)]\tLoss: 0.228625\tGrad Norm: 1.639911\tLR: 0.030000\n",
      "Train Epoch: 1589 [45056/194182 (23%)]\tLoss: 0.233084\tGrad Norm: 1.852553\tLR: 0.030000\n",
      "Train Epoch: 1589 [65536/194182 (33%)]\tLoss: 0.239313\tGrad Norm: 3.463714\tLR: 0.030000\n",
      "Train Epoch: 1589 [86016/194182 (44%)]\tLoss: 0.232049\tGrad Norm: 1.804384\tLR: 0.030000\n",
      "Train Epoch: 1589 [106496/194182 (54%)]\tLoss: 0.220003\tGrad Norm: 1.538485\tLR: 0.030000\n",
      "Train Epoch: 1589 [126976/194182 (65%)]\tLoss: 0.222946\tGrad Norm: 1.454428\tLR: 0.030000\n",
      "Train Epoch: 1589 [147456/194182 (75%)]\tLoss: 0.222391\tGrad Norm: 1.644503\tLR: 0.030000\n",
      "Train Epoch: 1589 [167936/194182 (85%)]\tLoss: 0.235517\tGrad Norm: 1.726760\tLR: 0.030000\n",
      "Train Epoch: 1589 [188416/194182 (96%)]\tLoss: 0.243753\tGrad Norm: 2.073483\tLR: 0.030000\n",
      "Train set: Average loss: 0.2291\n",
      "Test set: Average loss: 0.2603, Average MAE: 0.3631\n",
      "Train Epoch: 1590 [4096/194182 (2%)]\tLoss: 0.223700\tGrad Norm: 1.870087\tLR: 0.030000\n",
      "Train Epoch: 1590 [24576/194182 (12%)]\tLoss: 0.229875\tGrad Norm: 1.762602\tLR: 0.030000\n",
      "Train Epoch: 1590 [45056/194182 (23%)]\tLoss: 0.219184\tGrad Norm: 1.474132\tLR: 0.030000\n",
      "Train Epoch: 1590 [65536/194182 (33%)]\tLoss: 0.225054\tGrad Norm: 1.553325\tLR: 0.030000\n",
      "Train Epoch: 1590 [86016/194182 (44%)]\tLoss: 0.231831\tGrad Norm: 2.111661\tLR: 0.030000\n",
      "Train Epoch: 1590 [106496/194182 (54%)]\tLoss: 0.238772\tGrad Norm: 2.168810\tLR: 0.030000\n",
      "Train Epoch: 1590 [126976/194182 (65%)]\tLoss: 0.228370\tGrad Norm: 1.462140\tLR: 0.030000\n",
      "Train Epoch: 1590 [147456/194182 (75%)]\tLoss: 0.231378\tGrad Norm: 1.646764\tLR: 0.030000\n",
      "Train Epoch: 1590 [167936/194182 (85%)]\tLoss: 0.227105\tGrad Norm: 1.726865\tLR: 0.030000\n",
      "Train Epoch: 1590 [188416/194182 (96%)]\tLoss: 0.233208\tGrad Norm: 1.502314\tLR: 0.030000\n",
      "Train set: Average loss: 0.2287\n",
      "Test set: Average loss: 0.2593, Average MAE: 0.3525\n",
      "Epoch 1590: Mean reward = 0.074 +/- 0.070\n",
      "Train Epoch: 1591 [4096/194182 (2%)]\tLoss: 0.232569\tGrad Norm: 1.939052\tLR: 0.030000\n",
      "Train Epoch: 1591 [24576/194182 (12%)]\tLoss: 0.227463\tGrad Norm: 1.676853\tLR: 0.030000\n",
      "Train Epoch: 1591 [45056/194182 (23%)]\tLoss: 0.225664\tGrad Norm: 1.665142\tLR: 0.030000\n",
      "Train Epoch: 1591 [65536/194182 (33%)]\tLoss: 0.233397\tGrad Norm: 1.700779\tLR: 0.030000\n",
      "Train Epoch: 1591 [86016/194182 (44%)]\tLoss: 0.223916\tGrad Norm: 1.384853\tLR: 0.030000\n",
      "Train Epoch: 1591 [106496/194182 (54%)]\tLoss: 0.221803\tGrad Norm: 1.502825\tLR: 0.030000\n",
      "Train Epoch: 1591 [126976/194182 (65%)]\tLoss: 0.230818\tGrad Norm: 1.446639\tLR: 0.030000\n",
      "Train Epoch: 1591 [147456/194182 (75%)]\tLoss: 0.229417\tGrad Norm: 1.704237\tLR: 0.030000\n",
      "Train Epoch: 1591 [167936/194182 (85%)]\tLoss: 0.233791\tGrad Norm: 1.863744\tLR: 0.030000\n",
      "Train Epoch: 1591 [188416/194182 (96%)]\tLoss: 0.227885\tGrad Norm: 1.658849\tLR: 0.030000\n",
      "Train set: Average loss: 0.2273\n",
      "Test set: Average loss: 0.2583, Average MAE: 0.3617\n",
      "Train Epoch: 1592 [4096/194182 (2%)]\tLoss: 0.230026\tGrad Norm: 1.739452\tLR: 0.030000\n",
      "Train Epoch: 1592 [24576/194182 (12%)]\tLoss: 0.227355\tGrad Norm: 1.883700\tLR: 0.030000\n",
      "Train Epoch: 1592 [45056/194182 (23%)]\tLoss: 0.226374\tGrad Norm: 1.658534\tLR: 0.030000\n",
      "Train Epoch: 1592 [65536/194182 (33%)]\tLoss: 0.224106\tGrad Norm: 1.769762\tLR: 0.030000\n",
      "Train Epoch: 1592 [86016/194182 (44%)]\tLoss: 0.229119\tGrad Norm: 1.790513\tLR: 0.030000\n",
      "Train Epoch: 1592 [106496/194182 (54%)]\tLoss: 0.223600\tGrad Norm: 1.635154\tLR: 0.030000\n",
      "Train Epoch: 1592 [126976/194182 (65%)]\tLoss: 0.230823\tGrad Norm: 1.912508\tLR: 0.030000\n",
      "Train Epoch: 1592 [147456/194182 (75%)]\tLoss: 0.226674\tGrad Norm: 1.451899\tLR: 0.030000\n",
      "Train Epoch: 1592 [167936/194182 (85%)]\tLoss: 0.224387\tGrad Norm: 1.559973\tLR: 0.030000\n",
      "Train Epoch: 1592 [188416/194182 (96%)]\tLoss: 0.232873\tGrad Norm: 1.986557\tLR: 0.030000\n",
      "Train set: Average loss: 0.2281\n",
      "Test set: Average loss: 0.2650, Average MAE: 0.3562\n",
      "Train Epoch: 1593 [4096/194182 (2%)]\tLoss: 0.236972\tGrad Norm: 2.123622\tLR: 0.030000\n",
      "Train Epoch: 1593 [24576/194182 (12%)]\tLoss: 0.227391\tGrad Norm: 1.667228\tLR: 0.030000\n",
      "Train Epoch: 1593 [45056/194182 (23%)]\tLoss: 0.228948\tGrad Norm: 1.434362\tLR: 0.030000\n",
      "Train Epoch: 1593 [65536/194182 (33%)]\tLoss: 0.227105\tGrad Norm: 1.662594\tLR: 0.030000\n",
      "Train Epoch: 1593 [86016/194182 (44%)]\tLoss: 0.229417\tGrad Norm: 1.603218\tLR: 0.030000\n",
      "Train Epoch: 1593 [106496/194182 (54%)]\tLoss: 0.218161\tGrad Norm: 1.605811\tLR: 0.030000\n",
      "Train Epoch: 1593 [126976/194182 (65%)]\tLoss: 0.236049\tGrad Norm: 1.815424\tLR: 0.030000\n",
      "Train Epoch: 1593 [147456/194182 (75%)]\tLoss: 0.228320\tGrad Norm: 1.490114\tLR: 0.030000\n",
      "Train Epoch: 1593 [167936/194182 (85%)]\tLoss: 0.228549\tGrad Norm: 1.535451\tLR: 0.030000\n",
      "Train Epoch: 1593 [188416/194182 (96%)]\tLoss: 0.226312\tGrad Norm: 1.596313\tLR: 0.030000\n",
      "Train set: Average loss: 0.2282\n",
      "Test set: Average loss: 0.2547, Average MAE: 0.3533\n",
      "Train Epoch: 1594 [4096/194182 (2%)]\tLoss: 0.222154\tGrad Norm: 1.682046\tLR: 0.030000\n",
      "Train Epoch: 1594 [24576/194182 (12%)]\tLoss: 0.220913\tGrad Norm: 1.520107\tLR: 0.030000\n",
      "Train Epoch: 1594 [45056/194182 (23%)]\tLoss: 0.226850\tGrad Norm: 1.441179\tLR: 0.030000\n",
      "Train Epoch: 1594 [65536/194182 (33%)]\tLoss: 0.231280\tGrad Norm: 1.699299\tLR: 0.030000\n",
      "Train Epoch: 1594 [86016/194182 (44%)]\tLoss: 0.222435\tGrad Norm: 1.334700\tLR: 0.030000\n",
      "Train Epoch: 1594 [106496/194182 (54%)]\tLoss: 0.224394\tGrad Norm: 1.756410\tLR: 0.030000\n",
      "Train Epoch: 1594 [126976/194182 (65%)]\tLoss: 0.229266\tGrad Norm: 1.999618\tLR: 0.030000\n",
      "Train Epoch: 1594 [147456/194182 (75%)]\tLoss: 0.225546\tGrad Norm: 1.916425\tLR: 0.030000\n",
      "Train Epoch: 1594 [167936/194182 (85%)]\tLoss: 0.229407\tGrad Norm: 1.968434\tLR: 0.030000\n",
      "Train Epoch: 1594 [188416/194182 (96%)]\tLoss: 0.224768\tGrad Norm: 1.567502\tLR: 0.030000\n",
      "Train set: Average loss: 0.2278\n",
      "Test set: Average loss: 0.2559, Average MAE: 0.3538\n",
      "Train Epoch: 1595 [4096/194182 (2%)]\tLoss: 0.226988\tGrad Norm: 1.585716\tLR: 0.030000\n",
      "Train Epoch: 1595 [24576/194182 (12%)]\tLoss: 0.229878\tGrad Norm: 1.865922\tLR: 0.030000\n",
      "Train Epoch: 1595 [45056/194182 (23%)]\tLoss: 0.226380\tGrad Norm: 1.790650\tLR: 0.030000\n",
      "Train Epoch: 1595 [65536/194182 (33%)]\tLoss: 0.226578\tGrad Norm: 1.738567\tLR: 0.030000\n",
      "Train Epoch: 1595 [86016/194182 (44%)]\tLoss: 0.233438\tGrad Norm: 1.962770\tLR: 0.030000\n",
      "Train Epoch: 1595 [106496/194182 (54%)]\tLoss: 0.236748\tGrad Norm: 1.934409\tLR: 0.030000\n",
      "Train Epoch: 1595 [126976/194182 (65%)]\tLoss: 0.221961\tGrad Norm: 1.568551\tLR: 0.030000\n",
      "Train Epoch: 1595 [147456/194182 (75%)]\tLoss: 0.224457\tGrad Norm: 1.426664\tLR: 0.030000\n",
      "Train Epoch: 1595 [167936/194182 (85%)]\tLoss: 0.222442\tGrad Norm: 1.589578\tLR: 0.030000\n",
      "Train Epoch: 1595 [188416/194182 (96%)]\tLoss: 0.230687\tGrad Norm: 1.908910\tLR: 0.030000\n",
      "Train set: Average loss: 0.2290\n",
      "Test set: Average loss: 0.2560, Average MAE: 0.3511\n",
      "Epoch 1595: Mean reward = 0.050 +/- 0.062\n",
      "Train Epoch: 1596 [4096/194182 (2%)]\tLoss: 0.225961\tGrad Norm: 1.617130\tLR: 0.030000\n",
      "Train Epoch: 1596 [24576/194182 (12%)]\tLoss: 0.237174\tGrad Norm: 1.803669\tLR: 0.030000\n",
      "Train Epoch: 1596 [45056/194182 (23%)]\tLoss: 0.223387\tGrad Norm: 1.516194\tLR: 0.030000\n",
      "Train Epoch: 1596 [65536/194182 (33%)]\tLoss: 0.216698\tGrad Norm: 1.253681\tLR: 0.030000\n",
      "Train Epoch: 1596 [86016/194182 (44%)]\tLoss: 0.228289\tGrad Norm: 1.742588\tLR: 0.030000\n",
      "Train Epoch: 1596 [106496/194182 (54%)]\tLoss: 0.231216\tGrad Norm: 1.875442\tLR: 0.030000\n",
      "Train Epoch: 1596 [126976/194182 (65%)]\tLoss: 0.230618\tGrad Norm: 1.715753\tLR: 0.030000\n",
      "Train Epoch: 1596 [147456/194182 (75%)]\tLoss: 0.221086\tGrad Norm: 1.483013\tLR: 0.030000\n",
      "Train Epoch: 1596 [167936/194182 (85%)]\tLoss: 0.226103\tGrad Norm: 1.842516\tLR: 0.030000\n",
      "Train Epoch: 1596 [188416/194182 (96%)]\tLoss: 0.237748\tGrad Norm: 2.029931\tLR: 0.030000\n",
      "Train set: Average loss: 0.2269\n",
      "Test set: Average loss: 0.2512, Average MAE: 0.3410\n",
      "Train Epoch: 1597 [4096/194182 (2%)]\tLoss: 0.218499\tGrad Norm: 1.257285\tLR: 0.030000\n",
      "Train Epoch: 1597 [24576/194182 (12%)]\tLoss: 0.224768\tGrad Norm: 1.688915\tLR: 0.030000\n",
      "Train Epoch: 1597 [45056/194182 (23%)]\tLoss: 0.226153\tGrad Norm: 1.635921\tLR: 0.030000\n",
      "Train Epoch: 1597 [65536/194182 (33%)]\tLoss: 0.220446\tGrad Norm: 1.330780\tLR: 0.030000\n",
      "Train Epoch: 1597 [86016/194182 (44%)]\tLoss: 0.229571\tGrad Norm: 1.522257\tLR: 0.030000\n",
      "Train Epoch: 1597 [106496/194182 (54%)]\tLoss: 0.229876\tGrad Norm: 1.744894\tLR: 0.030000\n",
      "Train Epoch: 1597 [126976/194182 (65%)]\tLoss: 0.225239\tGrad Norm: 1.478072\tLR: 0.030000\n",
      "Train Epoch: 1597 [147456/194182 (75%)]\tLoss: 0.228379\tGrad Norm: 1.417819\tLR: 0.030000\n",
      "Train Epoch: 1597 [167936/194182 (85%)]\tLoss: 0.229186\tGrad Norm: 1.688763\tLR: 0.030000\n",
      "Train Epoch: 1597 [188416/194182 (96%)]\tLoss: 0.223590\tGrad Norm: 1.598607\tLR: 0.030000\n",
      "Train set: Average loss: 0.2260\n",
      "Test set: Average loss: 0.2525, Average MAE: 0.3501\n",
      "Train Epoch: 1598 [4096/194182 (2%)]\tLoss: 0.221007\tGrad Norm: 1.284788\tLR: 0.030000\n",
      "Train Epoch: 1598 [24576/194182 (12%)]\tLoss: 0.234687\tGrad Norm: 1.908096\tLR: 0.030000\n",
      "Train Epoch: 1598 [45056/194182 (23%)]\tLoss: 0.222457\tGrad Norm: 1.676304\tLR: 0.030000\n",
      "Train Epoch: 1598 [65536/194182 (33%)]\tLoss: 0.237810\tGrad Norm: 1.974487\tLR: 0.030000\n",
      "Train Epoch: 1598 [86016/194182 (44%)]\tLoss: 0.220509\tGrad Norm: 1.341005\tLR: 0.030000\n",
      "Train Epoch: 1598 [106496/194182 (54%)]\tLoss: 0.224935\tGrad Norm: 1.498091\tLR: 0.030000\n",
      "Train Epoch: 1598 [126976/194182 (65%)]\tLoss: 0.226439\tGrad Norm: 1.593987\tLR: 0.030000\n",
      "Train Epoch: 1598 [147456/194182 (75%)]\tLoss: 0.227971\tGrad Norm: 1.564362\tLR: 0.030000\n",
      "Train Epoch: 1598 [167936/194182 (85%)]\tLoss: 0.233488\tGrad Norm: 1.597808\tLR: 0.030000\n",
      "Train Epoch: 1598 [188416/194182 (96%)]\tLoss: 0.235246\tGrad Norm: 2.105916\tLR: 0.030000\n",
      "Train set: Average loss: 0.2270\n",
      "Test set: Average loss: 0.2558, Average MAE: 0.3559\n",
      "Train Epoch: 1599 [4096/194182 (2%)]\tLoss: 0.222892\tGrad Norm: 1.514227\tLR: 0.030000\n",
      "Train Epoch: 1599 [24576/194182 (12%)]\tLoss: 0.229310\tGrad Norm: 1.811248\tLR: 0.030000\n",
      "Train Epoch: 1599 [45056/194182 (23%)]\tLoss: 0.228473\tGrad Norm: 1.488715\tLR: 0.030000\n",
      "Train Epoch: 1599 [65536/194182 (33%)]\tLoss: 0.225280\tGrad Norm: 1.567167\tLR: 0.030000\n",
      "Train Epoch: 1599 [86016/194182 (44%)]\tLoss: 0.231986\tGrad Norm: 1.639561\tLR: 0.030000\n",
      "Train Epoch: 1599 [106496/194182 (54%)]\tLoss: 0.231410\tGrad Norm: 1.994027\tLR: 0.030000\n",
      "Train Epoch: 1599 [126976/194182 (65%)]\tLoss: 0.229028\tGrad Norm: 1.743143\tLR: 0.030000\n",
      "Train Epoch: 1599 [147456/194182 (75%)]\tLoss: 0.224427\tGrad Norm: 1.585776\tLR: 0.030000\n",
      "Train Epoch: 1599 [167936/194182 (85%)]\tLoss: 0.226614\tGrad Norm: 1.761007\tLR: 0.030000\n",
      "Train Epoch: 1599 [188416/194182 (96%)]\tLoss: 0.226486\tGrad Norm: 2.141795\tLR: 0.030000\n",
      "Train set: Average loss: 0.2276\n",
      "Test set: Average loss: 0.2594, Average MAE: 0.3637\n",
      "Train Epoch: 1600 [4096/194182 (2%)]\tLoss: 0.228471\tGrad Norm: 1.834295\tLR: 0.030000\n",
      "Train Epoch: 1600 [24576/194182 (12%)]\tLoss: 0.240136\tGrad Norm: 2.322791\tLR: 0.030000\n",
      "Train Epoch: 1600 [45056/194182 (23%)]\tLoss: 0.239613\tGrad Norm: 2.426408\tLR: 0.030000\n",
      "Train Epoch: 1600 [65536/194182 (33%)]\tLoss: 0.229445\tGrad Norm: 1.503302\tLR: 0.030000\n",
      "Train Epoch: 1600 [86016/194182 (44%)]\tLoss: 0.223626\tGrad Norm: 1.656113\tLR: 0.030000\n",
      "Train Epoch: 1600 [106496/194182 (54%)]\tLoss: 0.230143\tGrad Norm: 1.462502\tLR: 0.030000\n",
      "Train Epoch: 1600 [126976/194182 (65%)]\tLoss: 0.233872\tGrad Norm: 1.747365\tLR: 0.030000\n",
      "Train Epoch: 1600 [147456/194182 (75%)]\tLoss: 0.223612\tGrad Norm: 1.426617\tLR: 0.030000\n",
      "Train Epoch: 1600 [167936/194182 (85%)]\tLoss: 0.227236\tGrad Norm: 1.487918\tLR: 0.030000\n",
      "Train Epoch: 1600 [188416/194182 (96%)]\tLoss: 0.232758\tGrad Norm: 1.620979\tLR: 0.030000\n",
      "Train set: Average loss: 0.2288\n",
      "Test set: Average loss: 0.2540, Average MAE: 0.3444\n",
      "Epoch 1600: Mean reward = 0.054 +/- 0.042\n",
      "Train Epoch: 1601 [4096/194182 (2%)]\tLoss: 0.226082\tGrad Norm: 1.609995\tLR: 0.030000\n",
      "Train Epoch: 1601 [24576/194182 (12%)]\tLoss: 0.223908\tGrad Norm: 1.582096\tLR: 0.030000\n",
      "Train Epoch: 1601 [45056/194182 (23%)]\tLoss: 0.233419\tGrad Norm: 1.599862\tLR: 0.030000\n",
      "Train Epoch: 1601 [65536/194182 (33%)]\tLoss: 0.221602\tGrad Norm: 1.606678\tLR: 0.030000\n",
      "Train Epoch: 1601 [86016/194182 (44%)]\tLoss: 0.217891\tGrad Norm: 1.340418\tLR: 0.030000\n",
      "Train Epoch: 1601 [106496/194182 (54%)]\tLoss: 0.232355\tGrad Norm: 1.824331\tLR: 0.030000\n",
      "Train Epoch: 1601 [126976/194182 (65%)]\tLoss: 0.229727\tGrad Norm: 1.772039\tLR: 0.030000\n",
      "Train Epoch: 1601 [147456/194182 (75%)]\tLoss: 0.233283\tGrad Norm: 1.861388\tLR: 0.030000\n",
      "Train Epoch: 1601 [167936/194182 (85%)]\tLoss: 0.220465\tGrad Norm: 1.404794\tLR: 0.030000\n",
      "Train Epoch: 1601 [188416/194182 (96%)]\tLoss: 0.229815\tGrad Norm: 1.794001\tLR: 0.030000\n",
      "Train set: Average loss: 0.2262\n",
      "Test set: Average loss: 0.2529, Average MAE: 0.3402\n",
      "Train Epoch: 1602 [4096/194182 (2%)]\tLoss: 0.226195\tGrad Norm: 1.562584\tLR: 0.030000\n",
      "Train Epoch: 1602 [24576/194182 (12%)]\tLoss: 0.227971\tGrad Norm: 1.707647\tLR: 0.030000\n",
      "Train Epoch: 1602 [45056/194182 (23%)]\tLoss: 0.229218\tGrad Norm: 1.765186\tLR: 0.030000\n",
      "Train Epoch: 1602 [65536/194182 (33%)]\tLoss: 0.227918\tGrad Norm: 1.683454\tLR: 0.030000\n",
      "Train Epoch: 1602 [86016/194182 (44%)]\tLoss: 0.225513\tGrad Norm: 1.832563\tLR: 0.030000\n",
      "Train Epoch: 1602 [106496/194182 (54%)]\tLoss: 0.226541\tGrad Norm: 1.628532\tLR: 0.030000\n",
      "Train Epoch: 1602 [126976/194182 (65%)]\tLoss: 0.234637\tGrad Norm: 1.851275\tLR: 0.030000\n",
      "Train Epoch: 1602 [147456/194182 (75%)]\tLoss: 0.223807\tGrad Norm: 1.610410\tLR: 0.030000\n",
      "Train Epoch: 1602 [167936/194182 (85%)]\tLoss: 0.222366\tGrad Norm: 1.755224\tLR: 0.030000\n",
      "Train Epoch: 1602 [188416/194182 (96%)]\tLoss: 0.219992\tGrad Norm: 1.384991\tLR: 0.030000\n",
      "Train set: Average loss: 0.2272\n",
      "Test set: Average loss: 0.2584, Average MAE: 0.3585\n",
      "Train Epoch: 1603 [4096/194182 (2%)]\tLoss: 0.222665\tGrad Norm: 1.712882\tLR: 0.030000\n",
      "Train Epoch: 1603 [24576/194182 (12%)]\tLoss: 0.225662\tGrad Norm: 1.787750\tLR: 0.030000\n",
      "Train Epoch: 1603 [45056/194182 (23%)]\tLoss: 0.234171\tGrad Norm: 2.052106\tLR: 0.030000\n",
      "Train Epoch: 1603 [65536/194182 (33%)]\tLoss: 0.235078\tGrad Norm: 1.757767\tLR: 0.030000\n",
      "Train Epoch: 1603 [86016/194182 (44%)]\tLoss: 0.230614\tGrad Norm: 2.142559\tLR: 0.030000\n",
      "Train Epoch: 1603 [106496/194182 (54%)]\tLoss: 0.220002\tGrad Norm: 1.455418\tLR: 0.030000\n",
      "Train Epoch: 1603 [126976/194182 (65%)]\tLoss: 0.227210\tGrad Norm: 1.621821\tLR: 0.030000\n",
      "Train Epoch: 1603 [147456/194182 (75%)]\tLoss: 0.231242\tGrad Norm: 1.758207\tLR: 0.030000\n",
      "Train Epoch: 1603 [167936/194182 (85%)]\tLoss: 0.218736\tGrad Norm: 1.228095\tLR: 0.030000\n",
      "Train Epoch: 1603 [188416/194182 (96%)]\tLoss: 0.235467\tGrad Norm: 1.921864\tLR: 0.030000\n",
      "Train set: Average loss: 0.2273\n",
      "Test set: Average loss: 0.2592, Average MAE: 0.3511\n",
      "Train Epoch: 1604 [4096/194182 (2%)]\tLoss: 0.227372\tGrad Norm: 1.804094\tLR: 0.030000\n",
      "Train Epoch: 1604 [24576/194182 (12%)]\tLoss: 0.225909\tGrad Norm: 1.420434\tLR: 0.030000\n",
      "Train Epoch: 1604 [45056/194182 (23%)]\tLoss: 0.225719\tGrad Norm: 1.625731\tLR: 0.030000\n",
      "Train Epoch: 1604 [65536/194182 (33%)]\tLoss: 0.229562\tGrad Norm: 1.908134\tLR: 0.030000\n",
      "Train Epoch: 1604 [86016/194182 (44%)]\tLoss: 0.232229\tGrad Norm: 1.639309\tLR: 0.030000\n",
      "Train Epoch: 1604 [106496/194182 (54%)]\tLoss: 0.225468\tGrad Norm: 1.753284\tLR: 0.030000\n",
      "Train Epoch: 1604 [126976/194182 (65%)]\tLoss: 0.227421\tGrad Norm: 1.809170\tLR: 0.030000\n",
      "Train Epoch: 1604 [147456/194182 (75%)]\tLoss: 0.230909\tGrad Norm: 1.945256\tLR: 0.030000\n",
      "Train Epoch: 1604 [167936/194182 (85%)]\tLoss: 0.228565\tGrad Norm: 1.787354\tLR: 0.030000\n",
      "Train Epoch: 1604 [188416/194182 (96%)]\tLoss: 0.217012\tGrad Norm: 1.367275\tLR: 0.030000\n",
      "Train set: Average loss: 0.2262\n",
      "Test set: Average loss: 0.2501, Average MAE: 0.3442\n",
      "Train Epoch: 1605 [4096/194182 (2%)]\tLoss: 0.218164\tGrad Norm: 1.425211\tLR: 0.030000\n",
      "Train Epoch: 1605 [24576/194182 (12%)]\tLoss: 0.221330\tGrad Norm: 1.438569\tLR: 0.030000\n",
      "Train Epoch: 1605 [45056/194182 (23%)]\tLoss: 0.221927\tGrad Norm: 1.478450\tLR: 0.030000\n",
      "Train Epoch: 1605 [65536/194182 (33%)]\tLoss: 0.224789\tGrad Norm: 1.743332\tLR: 0.030000\n",
      "Train Epoch: 1605 [86016/194182 (44%)]\tLoss: 0.229834\tGrad Norm: 1.776941\tLR: 0.030000\n",
      "Train Epoch: 1605 [106496/194182 (54%)]\tLoss: 0.223111\tGrad Norm: 1.498885\tLR: 0.030000\n",
      "Train Epoch: 1605 [126976/194182 (65%)]\tLoss: 0.213892\tGrad Norm: 1.320445\tLR: 0.030000\n",
      "Train Epoch: 1605 [147456/194182 (75%)]\tLoss: 0.228584\tGrad Norm: 1.377685\tLR: 0.030000\n",
      "Train Epoch: 1605 [167936/194182 (85%)]\tLoss: 0.225880\tGrad Norm: 1.534859\tLR: 0.030000\n",
      "Train Epoch: 1605 [188416/194182 (96%)]\tLoss: 0.226347\tGrad Norm: 1.595077\tLR: 0.030000\n",
      "Train set: Average loss: 0.2255\n",
      "Test set: Average loss: 0.2645, Average MAE: 0.3561\n",
      "Epoch 1605: Mean reward = 0.037 +/- 0.019\n",
      "Train Epoch: 1606 [4096/194182 (2%)]\tLoss: 0.230511\tGrad Norm: 2.024343\tLR: 0.030000\n",
      "Train Epoch: 1606 [24576/194182 (12%)]\tLoss: 0.219483\tGrad Norm: 1.394981\tLR: 0.030000\n",
      "Train Epoch: 1606 [45056/194182 (23%)]\tLoss: 0.227012\tGrad Norm: 1.476154\tLR: 0.030000\n",
      "Train Epoch: 1606 [65536/194182 (33%)]\tLoss: 0.232048\tGrad Norm: 1.772801\tLR: 0.030000\n",
      "Train Epoch: 1606 [86016/194182 (44%)]\tLoss: 0.222105\tGrad Norm: 1.495332\tLR: 0.030000\n",
      "Train Epoch: 1606 [106496/194182 (54%)]\tLoss: 0.236850\tGrad Norm: 2.094911\tLR: 0.030000\n",
      "Train Epoch: 1606 [126976/194182 (65%)]\tLoss: 0.236260\tGrad Norm: 2.109164\tLR: 0.030000\n",
      "Train Epoch: 1606 [147456/194182 (75%)]\tLoss: 0.217890\tGrad Norm: 1.509955\tLR: 0.030000\n",
      "Train Epoch: 1606 [167936/194182 (85%)]\tLoss: 0.228319\tGrad Norm: 1.646872\tLR: 0.030000\n",
      "Train Epoch: 1606 [188416/194182 (96%)]\tLoss: 0.228467\tGrad Norm: 1.547976\tLR: 0.030000\n",
      "Train set: Average loss: 0.2276\n",
      "Test set: Average loss: 0.2581, Average MAE: 0.3525\n",
      "Train Epoch: 1607 [4096/194182 (2%)]\tLoss: 0.220711\tGrad Norm: 1.663819\tLR: 0.030000\n",
      "Train Epoch: 1607 [24576/194182 (12%)]\tLoss: 0.230422\tGrad Norm: 1.776880\tLR: 0.030000\n",
      "Train Epoch: 1607 [45056/194182 (23%)]\tLoss: 0.230483\tGrad Norm: 1.737718\tLR: 0.030000\n",
      "Train Epoch: 1607 [65536/194182 (33%)]\tLoss: 0.221775\tGrad Norm: 1.672445\tLR: 0.030000\n",
      "Train Epoch: 1607 [86016/194182 (44%)]\tLoss: 0.227620\tGrad Norm: 1.988497\tLR: 0.030000\n",
      "Train Epoch: 1607 [106496/194182 (54%)]\tLoss: 0.230584\tGrad Norm: 1.876979\tLR: 0.030000\n",
      "Train Epoch: 1607 [126976/194182 (65%)]\tLoss: 0.228788\tGrad Norm: 1.823859\tLR: 0.030000\n",
      "Train Epoch: 1607 [147456/194182 (75%)]\tLoss: 0.229485\tGrad Norm: 1.770826\tLR: 0.030000\n",
      "Train Epoch: 1607 [167936/194182 (85%)]\tLoss: 0.223417\tGrad Norm: 1.296757\tLR: 0.030000\n",
      "Train Epoch: 1607 [188416/194182 (96%)]\tLoss: 0.226104\tGrad Norm: 1.539724\tLR: 0.030000\n",
      "Train set: Average loss: 0.2264\n",
      "Test set: Average loss: 0.2572, Average MAE: 0.3530\n",
      "Train Epoch: 1608 [4096/194182 (2%)]\tLoss: 0.224793\tGrad Norm: 1.524619\tLR: 0.030000\n",
      "Train Epoch: 1608 [24576/194182 (12%)]\tLoss: 0.222801\tGrad Norm: 1.395430\tLR: 0.030000\n",
      "Train Epoch: 1608 [45056/194182 (23%)]\tLoss: 0.228876\tGrad Norm: 1.993175\tLR: 0.030000\n",
      "Train Epoch: 1608 [65536/194182 (33%)]\tLoss: 0.230350\tGrad Norm: 1.888998\tLR: 0.030000\n",
      "Train Epoch: 1608 [86016/194182 (44%)]\tLoss: 0.226988\tGrad Norm: 1.847560\tLR: 0.030000\n",
      "Train Epoch: 1608 [106496/194182 (54%)]\tLoss: 0.228315\tGrad Norm: 1.659605\tLR: 0.030000\n",
      "Train Epoch: 1608 [126976/194182 (65%)]\tLoss: 0.223794\tGrad Norm: 1.450328\tLR: 0.030000\n",
      "Train Epoch: 1608 [147456/194182 (75%)]\tLoss: 0.221504\tGrad Norm: 1.600548\tLR: 0.030000\n",
      "Train Epoch: 1608 [167936/194182 (85%)]\tLoss: 0.226721\tGrad Norm: 1.697134\tLR: 0.030000\n",
      "Train Epoch: 1608 [188416/194182 (96%)]\tLoss: 0.222089\tGrad Norm: 1.641343\tLR: 0.030000\n",
      "Train set: Average loss: 0.2263\n",
      "Test set: Average loss: 0.2638, Average MAE: 0.3558\n",
      "Train Epoch: 1609 [4096/194182 (2%)]\tLoss: 0.230721\tGrad Norm: 1.912847\tLR: 0.030000\n",
      "Train Epoch: 1609 [24576/194182 (12%)]\tLoss: 0.227410\tGrad Norm: 1.821239\tLR: 0.030000\n",
      "Train Epoch: 1609 [45056/194182 (23%)]\tLoss: 0.218334\tGrad Norm: 1.559464\tLR: 0.030000\n",
      "Train Epoch: 1609 [65536/194182 (33%)]\tLoss: 0.227491\tGrad Norm: 1.515104\tLR: 0.030000\n",
      "Train Epoch: 1609 [86016/194182 (44%)]\tLoss: 0.227004\tGrad Norm: 1.925493\tLR: 0.030000\n",
      "Train Epoch: 1609 [106496/194182 (54%)]\tLoss: 0.226554\tGrad Norm: 1.925552\tLR: 0.030000\n",
      "Train Epoch: 1609 [126976/194182 (65%)]\tLoss: 0.225926\tGrad Norm: 1.741274\tLR: 0.030000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[1;32m      2\u001b[0m todays_date \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mpretrain_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpert_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_env\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_gamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3e-2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_cuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot_curves\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensorboard_log_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtb_logs/imitation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoints/imitation_PPO_\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtodays_date\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomet_ml_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNo20MKxPKu7vWLOUQCFBRO8mo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomet_ml_project_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpretraining_rl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomet_ml_experiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPPO_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10.0\u001b[39;49m\n\u001b[1;32m     28\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/thomas/GitHub/sogym_v2/sogym/pretraining.py:323\u001b[0m, in \u001b[0;36mpretrain_agent\u001b[0;34m(student, expert_dataset, env, test_env, batch_size, epochs, scheduler_gamma, learning_rate, log_interval, no_cuda, seed, test_batch_size, early_stopping_patience, plot_curves, tensorboard_log_dir, verbose, checkpoint_dir, load_checkpoint, comet_ml_api_key, comet_ml_project_name, comet_ml_experiment_name, n_eval_episodes, eval_freq, l2_reg_strength, max_grad_norm)\u001b[0m\n\u001b[1;32m    320\u001b[0m     experiment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 323\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m     test_loss, test_mae \u001b[38;5;241m=\u001b[39m test(model, device, test_loader)\n\u001b[1;32m    326\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "File \u001b[0;32m/scratch/thomas/GitHub/sogym_v2/sogym/pretraining.py:155\u001b[0m, in \u001b[0;36mpretrain_agent.<locals>.train\u001b[0;34m(model, device, train_loader, optimizer, epoch, max_grad_norm)\u001b[0m\n\u001b[1;32m    153\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l2_reg_strength \u001b[38;5;241m*\u001b[39m l2_reg_loss\n\u001b[1;32m    154\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 155\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Clip gradients\u001b[39;00m\n\u001b[1;32m    158\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_grad_norm)\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "todays_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "pretrain_agent(\n",
    "    model,\n",
    "    expert_dataset,\n",
    "    env,\n",
    "    test_env = eval_env,\n",
    "    batch_size=4096,\n",
    "    epochs=2000,\n",
    "    scheduler_gamma=1.0,\n",
    "    learning_rate= 3e-2,\n",
    "    log_interval=5,\n",
    "    no_cuda=False,\n",
    "    seed=1,\n",
    "    verbose=True,\n",
    "    test_batch_size=1024,\n",
    "    early_stopping_patience=100,\n",
    "    plot_curves=True,\n",
    "    tensorboard_log_dir=\"tb_logs/imitation/PPO_{}\".format(todays_date)\",\n",
    "    checkpoint_dir=\"checkpoints/imitation/PPO_{}\".format(todays_date),\n",
    "    load_checkpoint=None,\n",
    "    comet_ml_api_key=\"No20MKxPKu7vWLOUQCFBRO8mo\",\n",
    "    comet_ml_project_name=\"pretraining_rl\",\n",
    "    comet_ml_experiment_name=\"PPO_{}\".format(todays_date),\n",
    "    eval_freq = 5,\n",
    "    l2_reg_strength=0.001,\n",
    "    max_grad_norm = 10.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a29b8af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./checkpoints/PPO_pretrained')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820005a-be57-4236-b66b-0b34ed558aff",
   "metadata": {},
   "source": [
    "---\n",
    "### Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d3e210c1-817a-42a8-ab28-2ea437ed2c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 64 cpus!\n"
     ]
    }
   ],
   "source": [
    "#from transformers import AutoTokenizer, AutoModel\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecCheckNan\n",
    "import multiprocessing\n",
    "\n",
    "# Set number of cpus to use automatically:\n",
    "num_cpu = multiprocessing.cpu_count()\n",
    "print(\"Using {} cpus!\".format(num_cpu))\n",
    "\n",
    "algorithm_name = \"PPO\"  # or \"TD3\"\n",
    "# Load the YAML file\n",
    "with open(\"algorithms.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Extract the parameters for the desired algorithm\n",
    "algorithm_params = config[algorithm_name]\n",
    "\n",
    "observation_type = \"topopt_game\"\n",
    "vol_constraint_type = \"hard\"\n",
    "use_std_strain = False\n",
    "check_connectivity = True\n",
    "resolution = 50\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# Create the tb_log_name string\n",
    "\n",
    "train_critic_only = False  # if True, we freeze everything except the critic\n",
    "pretrained_run = None\n",
    "restart_run = \"PPO_20240516_093938\"\n",
    "restart_run = None\n",
    "\n",
    "if restart_run:\n",
    "    log_name=restart_run\n",
    "else:\n",
    "    log_name = f\"{algorithm_name}_{current_datetime}\"\n",
    "    #check if ./runs/log_name exists, if not create it:\n",
    "    if not os.path.exists(f'./runs/{log_name}'):\n",
    "        os.makedirs(f'./runs/{log_name}')\n",
    "        #create a yaml file with the algorithm parameters and the additional parameters defined in this cell:\n",
    "        # I first need to append to algorithm_params dict the parameters defined above:\n",
    "        algorithm_params['algorithm_name'] = algorithm_name\n",
    "        algorithm_params['observation_type'] = observation_type\n",
    "        algorithm_params['vol_constraint_type'] = vol_constraint_type\n",
    "        algorithm_params['use_std_strain'] = use_std_strain\n",
    "        algorithm_params['check_connectivity'] = check_connectivity\n",
    "        algorithm_params['resolution'] = resolution\n",
    "\n",
    "        with open(f'./runs/{log_name}/config.yaml', 'w') as file:\n",
    "            yaml.dump(algorithm_params, file)\n",
    "\n",
    "\n",
    "\n",
    "train_env = sogym(mode='train',observation_type=observation_type,vol_constraint_type = 'hard',resolution=50,check_connectivity=True)#,model=model,tokenizer=tokenizer)\n",
    "env= make_vec_env(lambda:train_env, n_envs=num_cpu,vec_env_cls=SubprocVecEnv)\n",
    "env = VecCheckNan(env, raise_exception=True)\n",
    "\n",
    "eval_env = sogym(mode='test',observation_type=observation_type,vol_constraint_type='hard',resolution=50,check_connectivity=True)#,model=model,tokenizer=tokenizer)\n",
    "eval_env = make_vec_env(lambda:eval_env, n_envs=1,vec_env_cls=SubprocVecEnv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b6f2e-5ceb-447f-ac10-a52f0379cb31",
   "metadata": {},
   "source": [
    "--- \n",
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a1132db-34f4-4a72-bb12-1339a9e00a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "# Get the current date and time\n",
    "\n",
    "# The noise objects for TD3\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.5 * np.ones(n_actions))\n",
    "\n",
    "chosen_policy = \"MlpPolicy\" if observation_type == 'box_dense' else \"MultiInputPolicy\"\n",
    "feature_extractor = ImageDictExtractor if observation_type == 'image' or observation_type==\"topopt_game\" else CustomBoxDense\n",
    "\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=feature_extractor,\n",
    "    net_arch = config['common']['net_arch'],\n",
    "    share_features_extractor = False,\n",
    ")\n",
    "\n",
    "# Create the model based on the algorithm name and parameters\n",
    "if algorithm_name == \"SAC\":\n",
    "    model = SAC(env=env,\n",
    "                policy = chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                #action_noise = action_noise,\n",
    "                ent_coef = 0.0,\n",
    "                device=device, \n",
    "                **algorithm_params)\n",
    "\n",
    "elif algorithm_name == \"PPO\":\n",
    "    model = PPO(env=env, \n",
    "                policy = chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                n_steps= 64*386 // num_cpu//100,\n",
    "                batch_size= 16384//4,\n",
    "                tensorboard_log  ='./runs/{}'.format(log_name),\n",
    "                device = device, \n",
    "                **algorithm_params)\n",
    "\n",
    "elif algorithm_name == \"TD3\":\n",
    "    # Create the action noise object\n",
    "    n_actions = env.action_space.shape[-1]\n",
    "    action_noise_params = algorithm_params.pop(\"action_noise\")\n",
    "    action_noise = NormalActionNoise(mean=action_noise_params[\"mean\"] * np.ones(n_actions),\n",
    "                                     sigma=action_noise_params[\"sigma\"] * np.ones(n_actions))\n",
    "    model = TD3(env=env,\n",
    "                policy =chosen_policy, \n",
    "                policy_kwargs=policy_kwargs,\n",
    "                action_noise=action_noise,\n",
    "                device=device, \n",
    "                **algorithm_params)\n",
    "if load_from_pretrained and pretrained_checkpoint is not None:\n",
    "    model.set_parameters(pretrained_checkpoint)\n",
    "\n",
    "if restart_run:\n",
    "    model = model.load(\"./runs/{}/checkpoints/best_model.zip\".format(log_name),env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3ec8bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a checkpoint every 1000 steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=250_000//num_cpu,\n",
    "  save_path=\"./runs/{}/checkpoints/\".format(log_name),\n",
    "  name_prefix=log_name,\n",
    "  save_replay_buffer=True,\n",
    "  save_vecnormalize=True,\n",
    ")\n",
    "\n",
    "eval_callback = EvalCallback(eval_env,\n",
    "                             log_path='./runs/{}/'.format(log_name), \n",
    "                             eval_freq=10_000//num_cpu,\n",
    "                             deterministic=True,\n",
    "                             n_eval_episodes=10,\n",
    "                             render=False,\n",
    "                             best_model_save_path='./runs/{}/checkpoints/'.format(log_name),\n",
    "                             verbose=0)\n",
    "\n",
    "callback_list = CallbackList([eval_callback,\n",
    "                         checkpoint_callback,\n",
    "                         MaxRewardCallback(verbose=1),\n",
    "                         GradientClippingCallback(clip_value=1.0, verbose=1),\n",
    "                         GradientNormCallback(verbose=1),\n",
    "                         FigureRecorderCallback(eval_env=eval_env, check_freq=10_000//num_cpu, figure_size=(8, 6))\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f70cf1-e15e-4fed-ab75-12bdca996f2d",
   "metadata": {},
   "source": [
    "--- \n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "978d7768-b7bb-4c14-b620-28a278392b6b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "if train_critic_only:\n",
    "    #Freeze everything:\n",
    "    for name, param in model.policy.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            param.requires_grad=False\n",
    "\n",
    "    if algorithm_name =='SAC':\n",
    "        # Unfreeze critic:\n",
    "        for param in model.policy.critic.parameters():\n",
    "            if param.requires_grad==False:\n",
    "                param.requires_grad=True\n",
    "\n",
    "        for param in model.policy.critic_target.parameters():\n",
    "            if param.requires_grad==False:\n",
    "                param.requires_grad=True\n",
    "\n",
    "\n",
    "    if algorithm_name == 'PPO':\n",
    "        for param in model.policy.mlp_extractor.value_net.parameters():\n",
    "            if param.requires_grad==False:\n",
    "                param.requires_grad=True\n",
    "            \n",
    "        for param in model.policy.value_net.parameters():\n",
    "            if param.requires_grad==False:\n",
    "                param.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c876f635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:414: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_check_nan.VecCheckNan object at 0x7fca50e26350> != <stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fca396f96c0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward` for environment variables or `env.get_wrapper_attr('reward')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "Process ForkServerProcess-435:\n",
      "Process ForkServerProcess-415:\n",
      "Process ForkServerProcess-428:\n",
      "Process ForkServerProcess-414:\n",
      "Process ForkServerProcess-402:\n",
      "Process ForkServerProcess-427:\n",
      "Process ForkServerProcess-393:\n",
      "Process ForkServerProcess-439:\n",
      "Process ForkServerProcess-429:\n",
      "Process ForkServerProcess-434:\n",
      "Process ForkServerProcess-390:\n",
      "Process ForkServerProcess-421:\n",
      "Process ForkServerProcess-424:\n",
      "Process ForkServerProcess-405:\n",
      "Process ForkServerProcess-438:\n",
      "Process ForkServerProcess-441:\n",
      "Process ForkServerProcess-413:\n",
      "Process ForkServerProcess-422:\n",
      "Process ForkServerProcess-412:\n",
      "Process ForkServerProcess-396:\n",
      "Process ForkServerProcess-403:\n",
      "Process ForkServerProcess-416:\n",
      "Process ForkServerProcess-395:\n",
      "Process ForkServerProcess-394:\n",
      "Process ForkServerProcess-440:\n",
      "Process ForkServerProcess-401:\n",
      "Process ForkServerProcess-404:\n",
      "Process ForkServerProcess-411:\n",
      "Process ForkServerProcess-417:\n",
      "Process ForkServerProcess-437:\n",
      "Process ForkServerProcess-423:\n",
      "Process ForkServerProcess-387:\n",
      "Process ForkServerProcess-406:\n",
      "Traceback (most recent call last):\n",
      "Process ForkServerProcess-410:\n",
      "Process ForkServerProcess-391:\n",
      "Process ForkServerProcess-430:\n",
      "Process ForkServerProcess-398:\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-389:\n",
      "Process ForkServerProcess-420:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkServerProcess-388:\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-400:\n",
      "Process ForkServerProcess-392:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-425:\n",
      "Process ForkServerProcess-365:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-374:\n",
      "Process ForkServerProcess-383:\n",
      "Process ForkServerProcess-369:\n",
      "Traceback (most recent call last):\n",
      "Process ForkServerProcess-384:\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-356:\n",
      "Process ForkServerProcess-357:\n",
      "Traceback (most recent call last):\n",
      "Process ForkServerProcess-352:\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-433:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "Process ForkServerProcess-322:\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-373:\n",
      "Process ForkServerProcess-340:\n",
      "Process ForkServerProcess-377:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "Process ForkServerProcess-354:\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "Process ForkServerProcess-348:\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-436:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "Process ForkServerProcess-349:\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-379:\n",
      "Process ForkServerProcess-370:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "Traceback (most recent call last):\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-376:\n",
      "Process ForkServerProcess-380:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "Process ForkServerProcess-358:\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkServerProcess-339:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-338:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-367:\n",
      "Process ForkServerProcess-342:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-346:\n",
      "Process ForkServerProcess-328:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-351:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-323:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-332:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-385:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-378:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-381:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-334:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-353:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-343:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-347:\n",
      "Process ForkServerProcess-330:\n",
      "Process ForkServerProcess-399:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-329:\n",
      "Process ForkServerProcess-324:\n",
      "Process ForkServerProcess-368:\n",
      "Process ForkServerProcess-337:\n",
      "Process ForkServerProcess-364:\n",
      "Process ForkServerProcess-326:\n",
      "Process ForkServerProcess-372:\n",
      "Process ForkServerProcess-408:\n",
      "Process ForkServerProcess-335:\n",
      "Process ForkServerProcess-431:\n",
      "Process ForkServerProcess-419:\n",
      "Process ForkServerProcess-363:\n",
      "Process ForkServerProcess-382:\n",
      "Process ForkServerProcess-341:\n",
      "Process ForkServerProcess-361:\n",
      "Process ForkServerProcess-350:\n",
      "Process ForkServerProcess-360:\n",
      "Process ForkServerProcess-331:\n",
      "Process ForkServerProcess-366:\n",
      "Process ForkServerProcess-426:\n",
      "Process ForkServerProcess-355:\n",
      "Process ForkServerProcess-432:\n",
      "Process ForkServerProcess-397:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "Traceback (most recent call last):\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "Traceback (most recent call last):\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-345:\n",
      "Process ForkServerProcess-362:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-375:\n",
      "Process ForkServerProcess-418:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-325:\n",
      "Process ForkServerProcess-407:\n",
      "Process ForkServerProcess-409:\n",
      "Process ForkServerProcess-327:\n",
      "Process ForkServerProcess-344:\n",
      "Process ForkServerProcess-333:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkServerProcess-359:\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Process ForkServerProcess-336:\n",
      "Process ForkServerProcess-371:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 255, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 419, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/multiprocessing/connection.py\", line 384, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.plot to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.plot` for environment variables or `env.get_wrapper_attr('plot')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m25_000_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m           \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m           \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrestart\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m           \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# save the model:\u001b[39;00m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./runs/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/checkpoints/final_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:299\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mrecord(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime/total_timesteps\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps, exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdump(step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps)\n\u001b[0;32m--> 299\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:217\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_sde:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mreset_noise(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m--> 217\u001b[0m values, log_prob, entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrollout_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# Normalize advantage\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/policies.py:736\u001b[0m, in \u001b[0;36mActorCriticPolicy.evaluate_actions\u001b[0;34m(self, obs, actions)\u001b[0m\n\u001b[1;32m    734\u001b[0m     latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor\u001b[38;5;241m.\u001b[39mforward_critic(vf_features)\n\u001b[1;32m    735\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_action_dist_from_latent(latent_pi)\n\u001b[0;32m--> 736\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    737\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(latent_vf)\n\u001b[1;32m    738\u001b[0m entropy \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mentropy()\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/distributions.py:176\u001b[0m, in \u001b[0;36mDiagGaussianDistribution.log_prob\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mGet the log probabilities of actions according to the distribution.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03mNote that you must first call the ``proba_distribution()`` method.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m:return:\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    175\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribution\u001b[38;5;241m.\u001b[39mlog_prob(actions)\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msum_independent_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/distributions.py:119\u001b[0m, in \u001b[0;36msum_independent_dims\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03mContinuous actions are usually considered to be independent,\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03mso we can sum components of the ``log_prob`` or the entropy.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m:return: shape: (n_batch,)\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensor\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/site-packages/torch/fx/traceback.py:57\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m current_stack\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/traceback.py:213\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    212\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m format_list(\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/traceback.py:227\u001b[0m, in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[0;32m--> 227\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/traceback.py:364\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    362\u001b[0m result \u001b[38;5;241m=\u001b[39m klass()\n\u001b[1;32m    363\u001b[0m fnames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f, lineno \u001b[38;5;129;01min\u001b[39;00m frame_gen:\n\u001b[1;32m    365\u001b[0m     co \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mf_code\n\u001b[1;32m    366\u001b[0m     filename \u001b[38;5;241m=\u001b[39m co\u001b[38;5;241m.\u001b[39mco_filename\n",
      "File \u001b[0;32m~/anaconda3/envs/SB3_update/lib/python3.10/traceback.py:317\u001b[0m, in \u001b[0;36mwalk_stack\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\u001b[38;5;241m.\u001b[39mf_back\n\u001b[0;32m--> 317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m f, f\u001b[38;5;241m.\u001b[39mf_lineno\n\u001b[1;32m    319\u001b[0m     f \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mf_back\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model.learn(25_000_000,\n",
    "           callback=callback_list, \n",
    "           tb_log_name=log_name,\n",
    "           reset_num_timesteps=not restart\n",
    "           )\n",
    "\n",
    "# save the model:\n",
    "model.save('./runs/{}/checkpoints/final_model')\n",
    "if algorithm_name != 'PPO':\n",
    "    model.save_replay_buffer(\"./runs/{}/checkpoints/final_buffer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c27da122",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('checkpoints/imitation_PPO_critic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2827c8cc-028d-4448-b109-d6542816df2a",
   "metadata": {},
   "source": [
    "---\n",
    "### Let's visualize the agent's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6728408e-71c2-4f7b-a8f1-f0cc6b43dcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sogym.env.sogym at 0x7f98b10dbc40>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env=sogym(mode='test',observation_type='topopt_game',vol_constraint_type='hard' ,resolution = 50)\n",
    "#env= make_vec_env(lambda:env, n_envs=1,vec_env_cls=SubprocVecEnv)\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cbb153d-80e3-4d6c-ada3-f4d323360b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.75501883 -1.          0.32291675  0.64145327  0.9568924   0.9920023 ]\n",
      "[-0.903395    0.6623455  -0.42170578 -0.5872051   0.7409822   0.8139564 ]\n",
      "[-0.94604313 -0.02086864  0.11097331  1.          0.8015071   0.8562474 ]\n",
      "[-1.          1.          0.08024888 -0.32997712  1.          1.        ]\n",
      "[ 0.07595539 -0.7214547   0.4731991   0.3694167   0.37574175  0.34123728]\n",
      "[-0.37390336 -0.01163775  0.40241554 -0.27844983  0.45322505  0.43657506]\n",
      "[-0.16364224 -0.55042976  0.31846595  1.          0.4676532   0.45424548]\n",
      "[-0.12401124  0.69860417  0.35660243 -0.06428144  0.7142122   0.7212964 ]\n",
      "Desired volume: 0.41 Obtained volume: 0.30805051633717917\n",
      "Env reward: 0.1713260659137914\n"
     ]
    }
   ],
   "source": [
    "obs,info=env.reset()\n",
    "dones=False\n",
    "saved_conditions = env.conditions\n",
    "saved_nelx, saved_nely = env.nelx, env.nely\n",
    "saved_dx, saved_dy = env.dx, env.dy\n",
    "#use deepcopy to save \n",
    "while dones== False:\n",
    "    action, _states = model.predict(obs,deterministic=True)\n",
    "    print(action)\n",
    "    obs, rewards, dones,truncated, info = env.step(action)\n",
    "print(\"Desired volume:\",saved_conditions['volfrac'],\"Obtained volume:\",env.volume)\n",
    "print(\"Env reward:\",rewards)\n",
    "fig = env.plot()\n",
    "fig.savefig('trained_agent.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10b8b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('trained_agent.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b4d5238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/anaconda3/envs/SB3_update/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07606540508568287\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "mean_reward, std_reward = evaluate_policy(model.policy, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(mean_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d2de1f-dd90-40b2-acc9-b3227fd2888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xval, f0val,it, H, Phimax, allPhi, den, N, cfg = run_mmc(saved_conditions,saved_nelx,saved_nely,saved_dx,saved_dy,plotting='contour')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('SB3_update')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "e21ef5adabae340b8408649b4e28a9d7d4d8eaab8fdd4faf01af585df564eed2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
