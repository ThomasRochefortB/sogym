{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from dataset.dataset_utils import generate_all\n",
    "#generate_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.data.types import Trajectory\n",
    "import numpy as np\n",
    "\n",
    "# Let's load an expert sample:\n",
    "import json\n",
    "  \n",
    "\n",
    "# This function will load a trajectory (one episode):\n",
    "def make_trajectory(key):\n",
    "    observations = []\n",
    "    actions = []\n",
    "    infos = []\n",
    "    for i in range(9):\n",
    "        # Opening JSON file\n",
    "        f = open('dataset/trajectories/data_{}_{}.json'.format(key,i))\n",
    "        data = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        obs = np.concatenate(( np.array(data['conditions']),\n",
    "                            np.array([data['volume']]),\n",
    "                            np.array([data['n_steps_left']]),\n",
    "                            np.array(data['design_variables']).squeeze(),\n",
    "        ))\n",
    "                            \n",
    "\n",
    "        acts = np.array(data['action'])\n",
    "        \n",
    "        observations.append(obs)\n",
    "        infos.append({})\n",
    "        \n",
    "        if acts != []:\n",
    "            actions.append(acts.squeeze())\n",
    "\n",
    "        terminal = True\n",
    "    \n",
    "    return np.array(observations),np.array(actions),terminal,infos\n",
    "\n",
    "trajectories = []\n",
    "for z in range(7500):\n",
    "    try:\n",
    "        obs,acts,terminal,infos = make_trajectory(z)\n",
    "        trajectories.append(Trajectory(obs=obs,acts=acts,terminal=terminal,infos=None))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories[0].obs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sogym.env import sogym\n",
    "train_env = sogym(nelx=100,nely=50,mode='train',observation_type='box_dense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from imitation.algorithms.adversarial.airl import AIRL\n",
    "from imitation.rewards.reward_nets import BasicShapedRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize, SubprocVecEnv\n",
    "\n",
    "from imitation.util import logger as imit_logger\n",
    "\n",
    "# Create the vectorized environment\n",
    "num_cpu = 4 # Number of processes to use\n",
    "venv= make_vec_env(lambda:train_env, n_envs=num_cpu,vec_env_cls=SubprocVecEnv)\n",
    "#venv=VecNormalize(venv,gamma=1.0,norm_obs=False)\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    net_arch=dict(pi=[512,512,512], vf=[512,512,512]),\n",
    ")\n",
    "\n",
    "learner = PPO(\"MlpPolicy\",\n",
    "    env=venv,\n",
    "    n_steps=2048*4//num_cpu,  #def: 224\n",
    "    #policy_kwargs=policy_kwargs,\n",
    "    batch_size=2048*4, #def: 50*224\n",
    "    ent_coef=3e-4,\n",
    "    #gamma=1.0,\n",
    "    learning_rate=3e-4,\n",
    "    tensorboard_log=\"tb_logs\",\n",
    "\n",
    ")\n",
    "\n",
    "custom_logger = imit_logger.configure(\n",
    "        folder=\"test_logs\",\n",
    "        format_strs=[\"tensorboard\", \"stdout\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_net = BasicShapedRewardNet(\n",
    "    venv.observation_space, venv.action_space,\n",
    "    #reward_hid_sizes=(32,32),\n",
    "    #potential_hid_sizes=(32, 32),\n",
    "    #discount_factor=1.0,\n",
    "    #dropout_prob=0.5\n",
    ")\n",
    "\n",
    "#learner=PPO.load('imit',env=venv,device='cuda')\n",
    "\n",
    "airl_trainer = AIRL(\n",
    "    demonstrations=trajectories,\n",
    "    demo_batch_size=2048,\n",
    "    debug_use_ground_truth=True,\n",
    "    init_tensorboard=True,\n",
    "    log_dir=\"test_logs\",\n",
    "    n_disc_updates_per_round=1,\n",
    "    venv=venv,\n",
    "    gen_algo=learner,\n",
    "    reward_net=reward_net,\n",
    "    custom_logger= custom_logger\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airl_trainer.train(4000000)  # Note: set to 300000 for better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('imit',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=sogym(mode='train',observation_type='box_dense')\n",
    "env = make_vec_env(lambda:env, n_envs=1,vec_env_cls=SubprocVecEnv)\n",
    "model=learner.load('imit')\n",
    "#model.set_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs=env.reset()\n",
    "dones=False\n",
    "\n",
    "while dones== False:\n",
    "    action, _states = model.predict(obs,deterministic=False)\n",
    "    print(action)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "  \n",
    "print(\"Reward:\",rewards[0])\n",
    "fig=env.env_method('plot')[0]\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb3",
   "language": "python",
   "name": "sb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3b6ca1e3bc4bd245b8203d35c9c119653662e79def748c08031a6cb1f74695a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
